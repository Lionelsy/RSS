<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Arxiv论文推荐</title>
    <link>https://github.com/lionelsy/RSS</link>
    <description>Arxiv论文推荐</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>zh-CN</language>
    <lastBuildDate>Fri, 20 Sep 2024 09:07:28 +0800</lastBuildDate>
    <item>
      <title>PromptST: Prompt-Enhanced Spatio-Temporal Multi-Attribute Prediction</title>
      <link>http://arxiv.org/abs/2309.09500v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 在信息爆炸的时代，时空数据挖掘成为城市管理的重要组成部分。&lt;br&gt;&lt;h4&gt;2. 研究目的&lt;/h4&gt;   - 预测多个时空属性（如交通状态、人类活动和社会事件）可以减轻监管压力，促进智慧城市的建设。&lt;br&gt;&lt;h4&gt;3. 现有研究的局限性&lt;/h4&gt;   - 当前研究未能有效处理复杂的时空多属性预测，主要原因在于不同属性之间的复杂关系。&lt;br&gt;&lt;h4&gt;4. 主要挑战&lt;/h4&gt;   - 如何处理共同的时空模式，同时应对各属性之间的差异性。&lt;br&gt;&lt;h4&gt;5. 提出的解决方案&lt;/h4&gt;   - 本文提出了一个有效的解决方案，PromptST，专注于时空多属性预测。&lt;br&gt;&lt;h4&gt;6. 模型设计&lt;/h4&gt;   - 设计了一种时空变换器和参数共享训练方案，以处理不同时空属性之间的共同知识。&lt;br&gt;&lt;h4&gt;7. 轻量化策略&lt;/h4&gt;   - 详细阐述了一种时空提示调优策略，以轻量化方式适应特定属性。&lt;br&gt;&lt;h4&gt;8. 模型训练流程&lt;/h4&gt;   - 通过预训练和提示调优阶段，PromptST能够增强对特定时空特征的捕捉，同时保持已学得的共同知识。&lt;br&gt;&lt;h4&gt;9. 实验验证&lt;/h4&gt;   - 在真实世界数据集上的广泛实验验证了PromptST的卓越表现。&lt;br&gt;&lt;h4&gt;10. 迁移能力&lt;/h4&gt;    - 证明了PromptST在未见时空属性上的良好迁移性，展现出在城市计算中的应用潜力。&lt;br&gt;&lt;h4&gt;11. 可重复性&lt;/h4&gt;    - 提供了实现代码，以便于研究的可重复性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2023-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the era of information explosion, spatio-temporal data mining serves as a
critical part of urban management. Considering the various fields demanding
attention, e.g., traffic state, human activity, and social event, predicting
multiple spatio-temporal attributes simultaneously can alleviate regulatory
pressure and foster smart city construction. However, current research can not
handle the spatio-temporal multi-attribute prediction well due to the complex
relationships between diverse attributes. The key challenge lies in how to
address the common spatio-temporal patterns while tackling their distinctions.
In this paper, we propose an effective solution for spatio-temporal
multi-attribute prediction, PromptST. We devise a spatio-temporal transformer
and a parameter-sharing training scheme to address the common knowledge among
different spatio-temporal attributes. Then, we elaborate a spatio-temporal
prompt tuning strategy to fit the specific attributes in a lightweight manner.
Through the pretrain and prompt tuning phases, our PromptST is able to enhance
the specific spatio-temoral characteristic capture by prompting the backbone
model to fit the specific target attribute while maintaining the learned common
knowledge. Extensive experiments on real-world datasets verify that our
PromptST attains state-of-the-art performance. Furthermore, we also prove
PromptST owns good transferability on unseen spatio-temporal attributes, which
brings promising application potential in urban computing. The implementation
code is available to ease reproducibility.</description>
      <guid isPermaLink="false">2309.09500v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Latent mixed-effect models for high-dimensional longitudinal data</title>
      <link>http://arxiv.org/abs/2409.11008v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under review&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 建模纵向数据是一项重要但具有挑战性的任务，这些数据集通常是高维的，包含非线性效应和时间变化的协变量。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 基于高斯过程（GP）先验的变分自编码器（VAE）在建模时间序列数据方面表现良好，但训练成本高，难以充分利用纵向数据的丰富协变量。&lt;br&gt;&lt;h4&gt;3. 研究目标&lt;/h4&gt;   - 本文旨在通过结合线性混合模型（LMMs）和摊销变分推断，为VAE提供条件先验，从而提出LMM-VAE模型。&lt;br&gt;&lt;h4&gt;4. 模型特性&lt;/h4&gt;   - LMM-VAE是一种可扩展、可解释且可识别的模型。&lt;br&gt;&lt;h4&gt;5. 理论联系&lt;/h4&gt;   - 强调了LMM-VAE与基于GP的技术之间的理论联系，提供了一种统一的框架。&lt;br&gt;&lt;h4&gt;6. 性能比较&lt;/h4&gt;   - 在模拟数据集和真实世界数据集上的表现与现有方法相竞争，显示出良好的效果。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modelling longitudinal data is an important yet challenging task. These
datasets can be high-dimensional, contain non-linear effects and time-varying
covariates. Gaussian process (GP) prior-based variational autoencoders (VAEs)
have emerged as a promising approach due to their ability to model time-series
data. However, they are costly to train and struggle to fully exploit the rich
covariates characteristic of longitudinal data, making them difficult for
practitioners to use effectively. In this work, we leverage linear mixed models
(LMMs) and amortized variational inference to provide conditional priors for
VAEs, and propose LMM-VAE, a scalable, interpretable and identifiable model. We
highlight theoretical connections between it and GP-based techniques, providing
a unified framework for this class of methods. Our proposal performs
competitively compared to existing approaches across simulated and real-world
datasets.</description>
      <guid isPermaLink="false">2409.11008v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Marginalizing and Conditioning Gaussians onto Linear Approximations of Smooth Manifolds with Applications in Robotics</title>
      <link>http://arxiv.org/abs/2409.09871v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to IEEE ICRA 2025&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究内容&lt;/h4&gt;   - 本文提出了对高斯分布在线性流形上进行边际化和条件化的闭合表达式。&lt;br&gt;&lt;h4&gt;2. 应用扩展&lt;/h4&gt;   - 演示了如何通过线性化将这些表达式应用于平滑的非线性流形。&lt;br&gt;&lt;h4&gt;3. 现有方法的局限性&lt;/h4&gt;   - 虽然在轴对齐流形上的边际化和条件化是成熟的过程，但在非轴对齐流形上的应用尚不充分理解。&lt;br&gt;&lt;h4&gt;4. 应用实例&lt;/h4&gt;   - 通过三个实例展示了表达式的实用性：&lt;br&gt;     &lt;h4&gt;1. 投影正态分布近似&lt;/h4&gt;        - 线性化近似的质量随着问题非线性的降低而提高。&lt;br&gt;     &lt;h4&gt;2. Koopman SLAM中的协方差提取&lt;/h4&gt;        - 在真实世界数据集上，展示了我们的协方差一致性。&lt;br&gt;     &lt;h4&gt;3. 约束GTSAM中的协方差提取&lt;/h4&gt;        - 在仿真中，证明了我们的协方差的一致性。&lt;br&gt;&lt;h4&gt;5. 贡献总结&lt;/h4&gt;   - 本研究为非轴对齐流形上的高斯处理提供了新的理论基础和应用实例。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present closed-form expressions for marginalizing and conditioning
Gaussians onto linear manifolds, and demonstrate how to apply these expressions
to smooth nonlinear manifolds through linearization. Although marginalization
and conditioning onto axis-aligned manifolds are well-established procedures,
doing so onto non-axis-aligned manifolds is not as well understood. We
demonstrate the utility of our expressions through three applications: 1)
approximation of the projected normal distribution, where the quality of our
linearized approximation increases as problem nonlinearity decreases; 2)
covariance extraction in Koopman SLAM, where our covariances are shown to be
consistent on a real-world dataset; and 3) covariance extraction in constrained
GTSAM, where our covariances are shown to be consistent in simulation.</description>
      <guid isPermaLink="false">2409.09871v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Mismatched: Evaluating the Limits of Image Matching Approaches and Benchmarks</title>
      <link>http://arxiv.org/abs/2408.16445v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages, 5 figures&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究领域&lt;/h4&gt;   - 三维（3D）重建从二维图像中提取信息是计算机视觉中的一个活跃研究领域，应用包括导航、物体跟踪、分割和三维建模。&lt;br&gt;&lt;h4&gt;2. 传统与新方法&lt;/h4&gt;   - 传统上，该任务采用参数技术，但最近的进展表明，学习基础的方法逐渐成为主流。&lt;br&gt;&lt;h4&gt;3. 研究的重要性&lt;/h4&gt;   - 随着研究的快速发展和新图像匹配方法的频繁出现，对这些方法进行评估变得至关重要。&lt;br&gt;&lt;h4&gt;4. 研究内容&lt;/h4&gt;   - 本文提供了对多种图像匹配方法的全面评估，使用结构从运动（SfM）管道进行分析。&lt;br&gt;&lt;h4&gt;5. 性能评估&lt;/h4&gt;   - 评估在领域内和领域外的数据集上进行，识别方法和基准的关键局限性。&lt;br&gt;&lt;h4&gt;6. 边缘检测的影响&lt;/h4&gt;   - 研究了边缘检测作为预处理步骤的影响。&lt;br&gt;&lt;h4&gt;7. 研究发现&lt;/h4&gt;   - 分析显示，3D重建中的图像匹配仍然是一个开放的挑战，需针对特定场景仔细选择和调整模型。&lt;br&gt;&lt;h4&gt;8. 性能度量的问题&lt;/h4&gt;   - 强调了当前度量方法在表现模型性能时的失配问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/surgical-vision/colmap-match-converter&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Three-dimensional (3D) reconstruction from two-dimensional images is an
active research field in computer vision, with applications ranging from
navigation and object tracking to segmentation and three-dimensional modeling.
Traditionally, parametric techniques have been employed for this task. However,
recent advancements have seen a shift towards learning-based methods. Given the
rapid pace of research and the frequent introduction of new image matching
methods, it is essential to evaluate them. In this paper, we present a
comprehensive evaluation of various image matching methods using a
structure-from-motion pipeline. We assess the performance of these methods on
both in-domain and out-of-domain datasets, identifying key limitations in both
the methods and benchmarks. We also investigate the impact of edge detection as
a pre-processing step. Our analysis reveals that image matching for 3D
reconstruction remains an open challenge, necessitating careful selection and
tuning of models for specific scenarios, while also highlighting mismatches in
how metrics currently represent method performance.</description>
      <guid isPermaLink="false">2408.16445v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Dynamic Graph Representation Learning for Passenger Behavior Prediction</title>
      <link>http://arxiv.org/abs/2408.09092v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究目标&lt;/h4&gt;   - 乘客行为预测旨在通过历史上下车数据追踪乘客出行模式，以分析城市车站乘客流量和及时进行风险管理。&lt;br&gt;&lt;h4&gt;2. 重要性&lt;/h4&gt;   - 这一研究对智慧城市发展和公共交通规划至关重要。&lt;br&gt;&lt;h4&gt;3. 现有研究的局限性&lt;/h4&gt;   - 现有研究主要依赖统计方法和序列模型，从个体历史交互中学习，忽视了乘客与车站之间的关联。&lt;br&gt;&lt;h4&gt;4. 提出的解决方案&lt;/h4&gt;   - 本文提出了DyGPP模型，利用动态图捕捉乘客行为的复杂演变。&lt;br&gt;&lt;h4&gt;5. 动态图的构建&lt;/h4&gt;   - 首先，将乘客和车站形式化为动态图中的异构顶点，顶点之间的连接代表乘客与车站之间的互动。&lt;br&gt;&lt;h4&gt;6. 历史交互序列采样&lt;/h4&gt;   - 分别对乘客和车站的历史交互序列进行采样，以捕捉个体序列中的时间模式，并关联两个序列之间的时间行为。&lt;br&gt;&lt;h4&gt;7. 模式学习&lt;/h4&gt;   - 最后，使用基于多层感知器（MLP）的编码器学习交互中的时间模式，并生成乘客和车站的实时表示。&lt;br&gt;&lt;h4&gt;8. 实验结果&lt;/h4&gt;   - 在真实世界数据集上的实验表明，DyGPP在行为预测任务上优于现有模型，展示了该模型的优势。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.3390/fi16080295&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Passenger behavior prediction aims to track passenger travel patterns through
historical boarding and alighting data, enabling the analysis of urban station
passenger flow and timely risk management. This is crucial for smart city
development and public transportation planning. Existing research primarily
relies on statistical methods and sequential models to learn from individual
historical interactions, which ignores the correlations between passengers and
stations. To address these issues, this paper proposes DyGPP, which leverages
dynamic graphs to capture the intricate evolution of passenger behavior. First,
we formalize passengers and stations as heterogeneous vertices in a dynamic
graph, with connections between vertices representing interactions between
passengers and stations. Then, we sample the historical interaction sequences
for passengers and stations separately. We capture the temporal patterns from
individual sequences and correlate the temporal behavior between the two
sequences. Finally, we use an MLP-based encoder to learn the temporal patterns
in the interactions and generate real-time representations of passengers and
stations. Experiments on real-world datasets confirmed that DyGPP outperformed
current models in the behavior prediction task, demonstrating the superiority
of our model.</description>
      <guid isPermaLink="false">2408.09092v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Unveiling Visual Biases in Audio-Visual Localization Benchmarks</title>
      <link>http://arxiv.org/abs/2409.06709v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ECCV24 AVGenL Workshop&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究目标&lt;/h4&gt;   - 声音源定位（AVSL）旨在在视频中定位声音源。&lt;br&gt;&lt;h4&gt;2. 现有基准问题&lt;/h4&gt;   - 本文识别出现有基准中的一个重大问题：发声物体通常可以仅通过视觉线索轻易识别，称之为视觉偏差。&lt;br&gt;&lt;h4&gt;3. 视觉偏差的影响&lt;/h4&gt;   - 这种视觉偏差妨碍了基准有效评估AVSL模型的能力。&lt;br&gt;&lt;h4&gt;4. 验证假设&lt;/h4&gt;   - 为验证视觉偏差的假设，作者检查了两个代表性的AVSL基准：VGG-SS和EpicSounding-Object。&lt;br&gt;&lt;h4&gt;5. 实验结果&lt;/h4&gt;   - 研究发现，单视觉模型的表现超过了所有音视频基线模型。&lt;br&gt;&lt;h4&gt;6. 基准改进的建议&lt;/h4&gt;   - 结果表明，现有AVSL基准需要进一步改进，以促进音视频学习的有效性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Audio-Visual Source Localization (AVSL) aims to localize the source of sound
within a video. In this paper, we identify a significant issue in existing
benchmarks: the sounding objects are often easily recognized based solely on
visual cues, which we refer to as visual bias. Such biases hinder these
benchmarks from effectively evaluating AVSL models. To further validate our
hypothesis regarding visual biases, we examine two representative AVSL
benchmarks, VGG-SS and EpicSounding-Object, where the vision-only models
outperform all audiovisual baselines. Our findings suggest that existing AVSL
benchmarks need further refinement to facilitate audio-visual learning.</description>
      <guid isPermaLink="false">2409.06709v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Navigating Process Mining: A Case study using pm4py</title>
      <link>http://arxiv.org/abs/2409.11294v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 过程挖掘技术的背景&lt;/h4&gt;   - 过程挖掘技术作为强大的工具，用于分析事件数据以洞察业务流程。&lt;br&gt;&lt;h4&gt;2. 研究对象&lt;/h4&gt;   - 本文对道路交通罚款管理流程进行了全面分析，使用Python中的pm4py库。&lt;br&gt;&lt;h4&gt;3. 数据导入与特征探索&lt;/h4&gt;   - 开始时导入事件日志数据集，并探索其特征，包括活动分布和流程变体。&lt;br&gt;&lt;h4&gt;4. 关键模式与变异识别&lt;/h4&gt;   - 通过过滤和统计分析，揭示了流程执行中的关键模式和变异。&lt;br&gt;&lt;h4&gt;5. 应用过程挖掘算法&lt;/h4&gt;   - 应用了多种过程挖掘算法，包括Alpha Miner、Inductive Miner和Heuristic Miner，从事件日志数据中发现过程模型。&lt;br&gt;&lt;h4&gt;6. 模型可视化&lt;/h4&gt;   - 可视化所发现的模型，以理解流程中的工作流结构和依赖关系。&lt;br&gt;&lt;h4&gt;7. 算法优缺点讨论&lt;/h4&gt;   - 讨论了每种挖掘方法在捕捉基本过程动态方面的优缺点。&lt;br&gt;&lt;h4&gt;8. 研究发现的意义&lt;/h4&gt;   - 研究结果揭示了道路交通罚款管理流程的效率和有效性，为流程优化和决策提供了宝贵见解。&lt;br&gt;&lt;h4&gt;9. pm4py的工具价值&lt;/h4&gt;   - 本研究展示了pm4py在促进过程挖掘任务中的实用性及其分析现实业务流程的潜力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Process-mining techniques have emerged as powerful tools for analyzing event
data to gain insights into business processes. In this paper, we present a
comprehensive analysis of road traffic fine management processes using the
pm4py library in Python. We start by importing an event log dataset and explore
its characteristics, including the distribution of activities and process
variants. Through filtering and statistical analysis, we uncover key patterns
and variations in the process executions. Subsequently, we apply various
process-mining algorithms, including the Alpha Miner, Inductive Miner, and
Heuristic Miner, to discover process models from the event log data. We
visualize the discovered models to understand the workflow structures and
dependencies within the process. Additionally, we discuss the strengths and
limitations of each mining approach in capturing the underlying process
dynamics. Our findings shed light on the efficiency and effectiveness of road
traffic fine management processes, providing valuable insights for process
optimization and decision-making. This study demonstrates the utility of pm4py
in facilitating process mining tasks and its potential for analyzing real-world
business processes.</description>
      <guid isPermaLink="false">2409.11294v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>D2Vformer: A Flexible Time Series Prediction Model Based on Time Position Embedding</title>
      <link>http://arxiv.org/abs/2409.11024v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 时间位置嵌入的作用&lt;/h4&gt;   - 时间位置嵌入捕捉时间步的位置信息，通常作为辅助输入以增强时间序列模型的预测能力。&lt;br&gt;&lt;h4&gt;2. 现有模型的局限性&lt;/h4&gt;   - 现有模型在捕捉复杂的时间位置信息和有效利用这些嵌入方面存在局限。&lt;br&gt;&lt;h4&gt;3. 提出的D2Vformer模型&lt;/h4&gt;   - 本文提出了一种新模型D2Vformer，不同于依赖RNN或Transformers的典型预测方法。&lt;br&gt;&lt;h4&gt;4. 灵活处理预测序列&lt;/h4&gt;   - D2Vformer能够直接处理预测序列与输入序列不相邻或长度动态变化的情况。&lt;br&gt;&lt;h4&gt;5. 节省训练资源&lt;/h4&gt;   - 相较于传统方法，D2Vformer显著节省了训练资源。&lt;br&gt;&lt;h4&gt;6. Date2Vec模块&lt;/h4&gt;   - Date2Vec模块利用时间戳信息和特征序列生成时间位置嵌入。&lt;br&gt;&lt;h4&gt;7. 新融合块的引入&lt;/h4&gt;   - D2Vformer引入了一个新的融合块，利用注意力机制探索输入序列和预测序列嵌入之间的时间位置相似性，从而基于这种相似性生成预测。&lt;br&gt;&lt;h4&gt;8. 实验验证&lt;/h4&gt;   - 通过在六个数据集上的广泛实验，证明Date2Vec在时间位置嵌入方法中表现优于其他方法，D2Vformer在固定长度和可变长度预测任务中超越了最先进的方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time position embeddings capture the positional information of time steps,
often serving as auxiliary inputs to enhance the predictive capabilities of
time series models. However, existing models exhibit limitations in capturing
intricate time positional information and effectively utilizing these
embeddings. To address these limitations, this paper proposes a novel model
called D2Vformer. Unlike typical prediction methods that rely on RNNs or
Transformers, this approach can directly handle scenarios where the predicted
sequence is not adjacent to the input sequence or where its length dynamically
changes. In comparison to conventional methods, D2Vformer undoubtedly saves a
significant amount of training resources. In D2Vformer, the Date2Vec module
uses the timestamp information and feature sequences to generate time position
embeddings. Afterward, D2Vformer introduces a new fusion block that utilizes an
attention mechanism to explore the similarity in time positions between the
embeddings of the input sequence and the predicted sequence, thereby generating
predictions based on this similarity. Through extensive experiments on six
datasets, we demonstrate that Date2Vec outperforms other time position
embedding methods, and D2Vformer surpasses state-of-the-art methods in both
fixed-length and variable-length prediction tasks.</description>
      <guid isPermaLink="false">2409.11024v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Visual Inertial SLAM with Magnetic Measurements</title>
      <link>http://arxiv.org/abs/2409.09904v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 本文提出了一种视觉惯性测程（VIO）的扩展，通过紧耦合融合磁力计测量数据。&lt;br&gt;&lt;h4&gt;2. 优化滑动窗口&lt;/h4&gt;   - 通过最小化重投影误差、相对惯性误差和相对磁力计方向误差来优化关键帧的滑动窗口。&lt;br&gt;&lt;h4&gt;3. IMU方向传播&lt;/h4&gt;   - 利用IMU的方向传播结果有效地在帧之间转换磁力计测量，生成连续帧之间的相对方向约束。&lt;br&gt;&lt;h4&gt;4. 铁效应校准&lt;/h4&gt;   - 使用椭球拟合算法校准软铁和硬铁效应。&lt;br&gt;&lt;h4&gt;5. 减少方向误差&lt;/h4&gt;   - 引入磁力计数据显著降低了方向误差，并恢复了相对于磁北的真实偏航方向。&lt;br&gt;&lt;h4&gt;6. 适用环境&lt;/h4&gt;   - 提出的框架可在慢变磁场的所有环境中运行，主要应用于户外和水下环境。&lt;br&gt;&lt;h4&gt;7. 水下应用重点&lt;/h4&gt;   - 特别关注水下领域，尤其是水下洞穴，因为狭窄的通道和湍流使得闭环和位置漂移重置变得困难。&lt;br&gt;&lt;h4&gt;8. VIO的挑战&lt;/h4&gt;   - 水下洞穴对VIO提出挑战，因缺乏环境光和受限的空间，同时又是重要的淡水来源，提供了宝贵的历史记录。&lt;br&gt;&lt;h4&gt;9. 实验结果&lt;/h4&gt;   - 来自水下洞穴的实验结果展示了所提出的VIO扩展在准确性和鲁棒性方面的改善。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/ICRA57147.2024.10611341&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents an extension to visual inertial odometry (VIO) by
introducing tightly-coupled fusion of magnetometer measurements. A sliding
window of keyframes is optimized by minimizing re-projection errors, relative
inertial errors, and relative magnetometer orientation errors. The results of
IMU orientation propagation are used to efficiently transform magnetometer
measurements between frames producing relative orientation constraints between
consecutive frames. The soft and hard iron effects are calibrated using an
ellipsoid fitting algorithm. The introduction of magnetometer data results in
significant reductions in the orientation error and also in recovery of the
true yaw orientation with respect to the magnetic north. The proposed framework
operates in all environments with slow-varying magnetic fields, mainly outdoors
and underwater. We have focused our work on the underwater domain, especially
in underwater caves, as the narrow passage and turbulent flow make it difficult
to perform loop closures and reset the localization drift. The underwater caves
present challenges to VIO due to the absence of ambient light and the confined
nature of the environment, while also being a crucial source of fresh water and
providing valuable historical records. Experimental results from underwater
caves demonstrate the improvements in accuracy and robustness introduced by the
proposed VIO extension.</description>
      <guid isPermaLink="false">2409.09904v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Robust Target Detection of Intelligent Integrated Optical Camera and mmWave Radar System</title>
      <link>http://arxiv.org/abs/2312.06983v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 目标检测的重要性&lt;/h4&gt;   - 目标检测对现代城市计算应用至关重要。&lt;br&gt;&lt;h4&gt;2. 现有图像技术的局限性&lt;/h4&gt;   - 尽管图像技术被广泛采用，但在不利天气、光线不足和遮挡等复杂环境条件下表现不佳。&lt;br&gt;&lt;h4&gt;3. 提出的系统&lt;/h4&gt;   - 本文提出了一种智能集成的光学相机和毫米波（mmWave）雷达系统，以提高复杂现实场景下的目标检测性能。&lt;br&gt;&lt;h4&gt;4. 数据融合算法&lt;/h4&gt;   - 提出了一种长期鲁棒的雷达-相机融合算法，利用物理知识和数据驱动方法，解决异构数据融合问题以提升检测效果。&lt;br&gt;&lt;h4&gt;5. 遮挡场景的检测&lt;/h4&gt;   - 在遮挡场景中，所提算法通过记忆功能实现长期检测，有效识别被遮挡的目标。&lt;br&gt;&lt;h4&gt;6. 低光环境下的检测&lt;/h4&gt;   - 在低光条件下，算法能够有效标记黑暗图像中的目标，并提供粗略的简笔画成像。&lt;br&gt;&lt;h4&gt;7. 实地测试与结果&lt;/h4&gt;   - 以上功能在真实场景中进行了测试，结果显示该集成系统在目标检测性能上具有显著增强和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2023-12-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1016/j.dsp.2023.104336&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Target detection is pivotal for modern urban computing applications. While
image-based techniques are widely adopted, they falter under challenging
environmental conditions such as adverse weather, poor lighting, and occlusion.
To improve the target detection performance under complex real-world scenarios,
this paper proposes an intelligent integrated optical camera and
millimeter-wave (mmWave) radar system. Utilizing both physical knowledge and
data-driven methods, a long-term robust radar-camera fusion algorithm is
proposed to solve the heterogeneous data fusion problem for detection
improvement. For the occlusion scenarios, the proposed algorithm can
effectively detect occluded targets with the help of memory through performing
long-term detection. For dark scenarios with low-light conditions, the proposed
algorithm can effectively mark the target in the dark picture as well as
provide rough stickman imaging. The above two innovative functions of the
hybrid optical camera and mmWave radar system are tested in real-world
scenarios. The results demonstrate the robustness and significant enhancement
in the target detection performance of our integrated system.</description>
      <guid isPermaLink="false">2312.06983v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Navigating Spatio-Temporal Heterogeneity: A Graph Transformer Approach for Traffic Forecasting</title>
      <link>http://arxiv.org/abs/2408.10822v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 交通预测的重要性&lt;/h4&gt;   - 交通预测已成为智能城市发展中的一个关键研究领域。&lt;br&gt;&lt;h4&gt;2. 现有模型的挑战&lt;/h4&gt;   - 尽管已经开发了多种复杂架构的神经网络，但仍面临两个主要挑战：&lt;br&gt;     - **性能提升的边际收益递减**：最新网络设计在建模时空相关性方面的性能提升已经开始减弱。&lt;br&gt;     - **时空异质性未被充分考虑**：大多数模型未能考虑交通数据中的时空异质性，即不同区域的交通分布和各个时间段的交通流模式显著不同。&lt;br&gt;&lt;h4&gt;3. STGormer框架的提出&lt;/h4&gt;   - 本文提出了时空图变换器（STGormer），有效整合交通数据中的属性和结构信息，以学习时空相关性。&lt;br&gt;&lt;h4&gt;4. 异质性捕获模块&lt;/h4&gt;   - 引入混合专家模块，以捕获在空间和时间维度上的异质性。&lt;br&gt;&lt;h4&gt;5. 空间编码方法&lt;/h4&gt;   - 设计了两种简单而有效的空间编码方法，基于图结构进行编码，并将时间位置编码集成到基础变换器中，以捕捉时空交通模式。&lt;br&gt;&lt;h4&gt;6. 增强的前馈神经网络&lt;/h4&gt;   - 混合专家增强的前馈神经网络（FNN）模块通过时空门控网络自适应地将适当的专家层分配给不同的模式，从而进一步提高预测准确性。&lt;br&gt;&lt;h4&gt;7. 实验验证&lt;/h4&gt;   - 在真实世界的交通数据集上的实验表明，STGormer实现了最先进的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/jasonz5/STGormer&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traffic forecasting has emerged as a crucial research area in the development
of smart cities. Although various neural networks with intricate architectures
have been developed to address this problem, they still face two key
challenges: i) Recent advancements in network designs for modeling
spatio-temporal correlations are starting to see diminishing returns in
performance enhancements. ii) Additionally, most models do not account for the
spatio-temporal heterogeneity inherent in traffic data, i.e., traffic
distribution varies significantly across different regions and traffic flow
patterns fluctuate across various time slots. To tackle these challenges, we
introduce the Spatio-Temporal Graph Transformer (STGormer), which effectively
integrates attribute and structure information inherent in traffic data for
learning spatio-temporal correlations, and a mixture-of-experts module for
capturing heterogeneity along spaital and temporal axes. Specifically, we
design two straightforward yet effective spatial encoding methods based on the
graph structure and integrate time position encoding into the vanilla
transformer to capture spatio-temporal traffic patterns. Additionally, a
mixture-of-experts enhanced feedforward neural network (FNN) module adaptively
assigns suitable expert layers to distinct patterns via a spatio-temporal
gating network, further improving overall prediction accuracy. Experiments on
real-world traffic datasets demonstrate that STGormer achieves state-of-the-art
performance.</description>
      <guid isPermaLink="false">2408.10822v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Generalized Robot Learning Framework</title>
      <link>http://arxiv.org/abs/2409.12061v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 2 figures. cs.RO&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 模仿学习的关注&lt;/h4&gt;   - 基于模仿的机器人学习近年来在机器人领域获得了显著关注，因其在可转移性和普适性上的理论潜力。&lt;br&gt;&lt;h4&gt;2. 成本问题&lt;/h4&gt;   - 然而，模仿学习在硬件和数据收集方面成本高昂，且在现实环境中部署需要精确的机器人设置和实验条件。&lt;br&gt;&lt;h4&gt;3. 提出的低成本框架&lt;/h4&gt;   - 本文提出了一种低成本的机器人学习框架，易于复制并可转移至不同的机器人和环境。&lt;br&gt;&lt;h4&gt;4. 应用范围的扩展&lt;/h4&gt;   - 我们展示了可部署的模仿学习不仅适用于昂贵的协作机器人手臂，还能成功应用于工业级机器人。&lt;br&gt;&lt;h4&gt;5. 多任务学习的实现&lt;/h4&gt;   - 研究结果表明，简单的网络架构和比之前认为的更少的演示次数即可实现多任务机器人学习。&lt;br&gt;&lt;h4&gt;6. 新的评估方法&lt;/h4&gt;   - 由于当前的评估方法在现实操作任务中几乎是主观的，我们提出了投票正率（VPR）作为一种新颖的评估策略，以提供更客观的性能评估。&lt;br&gt;&lt;h4&gt;7. 广泛的比较研究&lt;/h4&gt;   - 进行了广泛的成功率比较，涉及各种自设计任务，以验证我们的方法。&lt;br&gt;&lt;h4&gt;8. 支持社区的开放资源&lt;/h4&gt;   - 为促进合作并支持机器人学习社区，我们已开源所有相关数据集和模型检查点，地址为 huggingface.co/ZhiChengAI。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Imitation based robot learning has recently gained significant attention in
the robotics field due to its theoretical potential for transferability and
generalizability. However, it remains notoriously costly, both in terms of
hardware and data collection, and deploying it in real-world environments
demands meticulous setup of robots and precise experimental conditions. In this
paper, we present a low-cost robot learning framework that is both easily
reproducible and transferable to various robots and environments. We
demonstrate that deployable imitation learning can be successfully applied even
to industrial-grade robots, not just expensive collaborative robotic arms.
Furthermore, our results show that multi-task robot learning is achievable with
simple network architectures and fewer demonstrations than previously thought
necessary. As the current evaluating method is almost subjective when it comes
to real-world manipulation tasks, we propose Voting Positive Rate (VPR) - a
novel evaluation strategy that provides a more objective assessment of
performance. We conduct an extensive comparison of success rates across various
self-designed tasks to validate our approach. To foster collaboration and
support the robot learning community, we have open-sourced all relevant
datasets and model checkpoints, available at huggingface.co/ZhiChengAI.</description>
      <guid isPermaLink="false">2409.12061v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Connected Vehicle Data for Near-Crash Detection and Analysis in Urban Environments</title>
      <link>http://arxiv.org/abs/2409.11341v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  36 pages, 8 figures&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 城市交通安全的重要性&lt;/h4&gt;   - 城市交通安全是现代交通系统中的迫切问题，特别是在快速增长的城市地区，交通拥堵、复杂的道路网络和多样的驾驶行为增加了交通事故的风险。&lt;br&gt;&lt;h4&gt;2. 传统数据分析的局限性&lt;/h4&gt;   - 传统的交通事故数据分析提供了有价值的见解，但往往忽视了更广泛的道路安全风险。&lt;br&gt;&lt;h4&gt;3. 近碰撞事件的意义&lt;/h4&gt;   - 近碰撞事件更为频繁，能够指示潜在的碰撞，提供了对交通安全更全面的视角。&lt;br&gt;&lt;h4&gt;4. 大规模数据分析的挑战&lt;/h4&gt;   - 由于在大规模真实世界数据的收集、处理和分析方面存在显著挑战，城市规模的近碰撞事件分析依然有限。&lt;br&gt;&lt;h4&gt;5. 研究数据来源&lt;/h4&gt;   - 本研究利用了一月份的连接车辆数据，包含数十亿条记录，以检测和分析德克萨斯州圣安东尼奥市道路网络中的近碰撞事件。&lt;br&gt;&lt;h4&gt;6. 提出的框架&lt;/h4&gt;   - 提出了一个高效的框架，结合空间-时间缓冲和航向算法，准确识别和映射近碰撞事件。&lt;br&gt;&lt;h4&gt;7. 使用的分析模型&lt;/h4&gt;   - 采用二元逻辑回归模型评估道路几何、交通量和车辆类型对近碰撞风险的影响。&lt;br&gt;&lt;h4&gt;8. 空间和时间模式的研究&lt;/h4&gt;   - 研究了时间和空间模式，包括不同时间段、星期几和道路类别的变化。&lt;br&gt;&lt;h4&gt;9. 主要发现&lt;/h4&gt;   - 研究发现，超过一半的道路段上，车辆至少会涉及一次近碰撞事件。&lt;br&gt;   - 超过50%的近碰撞事件涉及行驶速度超过57.98 mph的车辆，且许多事件发生在车辆之间的短距离。&lt;br&gt;&lt;h4&gt;10. 道路特征的影响&lt;/h4&gt;    - 更宽的路面和多车道减少了近碰撞风险，而单元货车略微增加了近碰撞事件的可能性。&lt;br&gt;&lt;h4&gt;11. 高风险时段&lt;/h4&gt;    - 空间-时间分析显示，近碰撞风险在工作日高峰时段最为显著，尤其是在市中心区域。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Urban traffic safety is a pressing concern in modern transportation systems,
especially in rapidly growing metropolitan areas where increased traffic
congestion, complex road networks, and diverse driving behaviors exacerbate the
risk of traffic incidents. Traditional traffic crash data analysis offers
valuable insights but often overlooks a broader range of road safety risks.
Near-crash events, which occur more frequently and signal potential collisions,
provide a more comprehensive perspective on traffic safety. However, city-scale
analysis of near-crash events remains limited due to the significant challenges
in large-scale real-world data collection, processing, and analysis. This study
utilizes one month of connected vehicle data, comprising billions of records,
to detect and analyze near-crash events across the road network in the City of
San Antonio, Texas. We propose an efficient framework integrating
spatial-temporal buffering and heading algorithms to accurately identify and
map near-crash events. A binary logistic regression model is employed to assess
the influence of road geometry, traffic volume, and vehicle types on near-crash
risks. Additionally, we examine spatial and temporal patterns, including
variations by time of day, day of the week, and road category. The findings of
this study show that the vehicles on more than half of road segments will be
involved in at least one near-crash event. In addition, more than 50%
near-crash events involved vehicles traveling at speeds over 57.98 mph, and
many occurred at short distances between vehicles. The analysis also found that
wider roadbeds and multiple lanes reduced near-crash risks, while single-unit
trucks slightly increased the likelihood of near-crash events. Finally, the
spatial-temporal analysis revealed that near-crash risks were most prominent
during weekday peak hours, especially in downtown areas.</description>
      <guid isPermaLink="false">2409.11341v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Augmented Reality without Borders: Achieving Precise Localization Without Maps</title>
      <link>http://arxiv.org/abs/2408.17373v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 视觉定位的重要性&lt;/h4&gt;   - 视觉定位对计算机视觉和增强现实（AR）应用至关重要，准确确定相机或设备的位置和方向是与物理环境互动的基础。&lt;br&gt;&lt;h4&gt;2. 传统方法的局限性&lt;/h4&gt;   - 传统方法依赖于使用运动结构（SfM）或同时定位与地图构建（SLAM）生成的详细3D地图，这些方法计算成本高，并且在动态或大规模环境中不够实用。&lt;br&gt;&lt;h4&gt;3. MARLoc框架的介绍&lt;/h4&gt;   - 本文提出MARLoc，这是一个针对AR应用的新型定位框架，利用图像序列中的已知相对变换进行序列内三角测量，从而生成3D-2D对应关系以进行姿态估计和精细调整。&lt;br&gt;&lt;h4&gt;4. 消除预构建地图的需求&lt;/h4&gt;   - MARLoc不需要预构建的SfM地图，提供适用于动态户外环境的准确高效定位。&lt;br&gt;&lt;h4&gt;5. 性能评估&lt;/h4&gt;   - 通过基准数据集和实际实验的评估，证明了MARLoc的先进性能和鲁棒性。&lt;br&gt;&lt;h4&gt;6. 实际应用展示&lt;/h4&gt;   - 将MARLoc集成到AR设备中，展示了其在现实户外场景中实现精确定位的能力，突显其实际有效性及在AR应用中增强视觉定位的潜力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual localization is crucial for Computer Vision and Augmented Reality (AR)
applications, where determining the camera or device's position and orientation
is essential to accurately interact with the physical environment. Traditional
methods rely on detailed 3D maps constructed using Structure from Motion (SfM)
or Simultaneous Localization and Mapping (SLAM), which is computationally
expensive and impractical for dynamic or large-scale environments. We introduce
MARLoc, a novel localization framework for AR applications that uses known
relative transformations within image sequences to perform intra-sequence
triangulation, generating 3D-2D correspondences for pose estimation and
refinement. MARLoc eliminates the need for pre-built SfM maps, providing
accurate and efficient localization suitable for dynamic outdoor environments.
Evaluation with benchmark datasets and real-world experiments demonstrates
MARLoc's state-of-the-art performance and robustness. By integrating MARLoc
into an AR device, we highlight its capability to achieve precise localization
in real-world outdoor scenarios, showcasing its practical effectiveness and
potential to enhance visual localization in AR applications.</description>
      <guid isPermaLink="false">2408.17373v3</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Urban Generative Intelligence (UGI): A Foundational Platform for Agents in Embodied City Environment</title>
      <link>http://arxiv.org/abs/2312.11813v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 城市环境的复杂性&lt;/h4&gt;   - 城市环境由物理、社会、经济和环境等多层次网络构成，面临快速城市化带来的重大挑战，如交通拥堵、污染和社会不平等。&lt;br&gt;&lt;h4&gt;2. 技术干预的需求&lt;/h4&gt;   - 这些挑战需要先进的技术干预，近年来大数据、人工智能、城市计算和数字双胞胎的发展为城市建模和模拟奠定了基础。&lt;br&gt;&lt;h4&gt;3. 技术与实践之间的差距&lt;/h4&gt;   - 尽管技术能力不断提升，但在以系统智能的方式解决城市问题的实际应用中仍存在差距。&lt;br&gt;&lt;h4&gt;4. 提出的解决方案&lt;/h4&gt;   - 本文提出了城市生成智能（UGI），这是一个新颖的基础平台，将大语言模型（LLMs）集成到城市系统中，推动城市智能的新范式。&lt;br&gt;&lt;h4&gt;5. CityGPT的应用&lt;/h4&gt;   - UGI利用CityGPT这一基础模型，该模型基于城市特定的多源数据进行训练，创建用于各种城市任务的具身代理（embodied agents）。&lt;br&gt;&lt;h4&gt;6. 交互环境&lt;/h4&gt;   - 这些代理在城市模拟器和城市知识图谱模拟的文本城市环境中运作，通过自然语言界面进行交互，提供一个开放平台以支持多样的智能代理开发。&lt;br&gt;&lt;h4&gt;7. 多学科方法&lt;/h4&gt;   - 该平台不仅解决特定的城市问题，还模拟复杂的城市系统，采用多学科方法来理解和管理城市复杂性。&lt;br&gt;&lt;h4&gt;8. 研究的重要性&lt;/h4&gt;   - 该研究标志着城市科学和城市智能的变革性进步，利用LLMs的力量揭示和应对城市系统的复杂动态。&lt;br&gt;&lt;h4&gt;9. 代码库的发布&lt;/h4&gt;   - 演示的代码库将很快在指定的GitHub页面发布。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2023-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/tsinghua-fib-lab/ugi&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Urban environments, characterized by their complex, multi-layered networks
encompassing physical, social, economic, and environmental dimensions, face
significant challenges in the face of rapid urbanization. These challenges,
ranging from traffic congestion and pollution to social inequality, call for
advanced technological interventions. Recent developments in big data,
artificial intelligence, urban computing, and digital twins have laid the
groundwork for sophisticated city modeling and simulation. However, a gap
persists between these technological capabilities and their practical
implementation in addressing urban challenges in an systemic-intelligent way.
This paper proposes Urban Generative Intelligence (UGI), a novel foundational
platform integrating Large Language Models (LLMs) into urban systems to foster
a new paradigm of urban intelligence. UGI leverages CityGPT, a foundation model
trained on city-specific multi-source data, to create embodied agents for
various urban tasks. These agents, operating within a textual urban environment
emulated by city simulator and urban knowledge graph, interact through a
natural language interface, offering an open platform for diverse intelligent
and embodied agent development. This platform not only addresses specific urban
issues but also simulates complex urban systems, providing a multidisciplinary
approach to understand and manage urban complexity. This work signifies a
transformative step in city science and urban intelligence, harnessing the
power of LLMs to unravel and address the intricate dynamics of urban systems.
The code repository with demonstrations will soon be released here
https://github.com/tsinghua-fib-lab/UGI.</description>
      <guid isPermaLink="false">2312.11813v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>SHIRE: Enhancing Sample Efficiency using Human Intuition in REinforcement Learning</title>
      <link>http://arxiv.org/abs/2409.09990v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 神经网络在机器人感知和控制任务（如深度和光流估计、同时定位与地图构建（SLAM）和自动控制）中的应用越来越广泛。&lt;br&gt;&lt;h4&gt;2. 深度强化学习的应用&lt;/h4&gt;   - 深度强化学习（DeepRL）在这些任务中被广泛使用，因为它不具备监督学习相关的高训练成本。&lt;br&gt;&lt;h4&gt;3. 深度强化学习的局限性&lt;/h4&gt;   - DeepRL样本效率低，即需要大量环境交互才能收敛到可接受的解决方案。&lt;br&gt;&lt;h4&gt;4. 现代强化学习算法&lt;/h4&gt;   - 现代算法如Deep Q Learning和Soft Actor-Critic尝试解决样本效率问题，但缺乏在自主机器人应用中所需的可解释性。&lt;br&gt;&lt;h4&gt;5. 人类直觉的应用&lt;/h4&gt;   - 人类对机器人中常见的长时间序列任务的直觉理解可以使强化学习策略更具可解释性，并提高样本效率。&lt;br&gt;&lt;h4&gt;6. 提出的框架&lt;/h4&gt;   - 本文提出了SHIRE，一个使用概率图模型（PGMs）编码人类直觉并将其应用于深度强化学习训练流程的新框架，以提高样本效率。&lt;br&gt;&lt;h4&gt;7. 效率提升&lt;/h4&gt;   - 在评估的各种环境中，SHIRE框架实现了25-78%的样本效率提升，并且几乎没有额外成本。&lt;br&gt;&lt;h4&gt;8. 政策可解释性&lt;/h4&gt;   - 通过教授强化学习代理编码的基本行为，SHIRE增强了策略的可解释性。&lt;br&gt;&lt;h4&gt;9. 实际应用验证&lt;/h4&gt;   - 一项现实世界的演示进一步凸显了使用该框架训练的策略的有效性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The ability of neural networks to perform robotic perception and control
tasks such as depth and optical flow estimation, simultaneous localization and
mapping (SLAM), and automatic control has led to their widespread adoption in
recent years. Deep Reinforcement Learning has been used extensively in these
settings, as it does not have the unsustainable training costs associated with
supervised learning. However, DeepRL suffers from poor sample efficiency, i.e.,
it requires a large number of environmental interactions to converge to an
acceptable solution. Modern RL algorithms such as Deep Q Learning and Soft
Actor-Critic attempt to remedy this shortcoming but can not provide the
explainability required in applications such as autonomous robotics. Humans
intuitively understand the long-time-horizon sequential tasks common in
robotics. Properly using such intuition can make RL policies more explainable
while enhancing their sample efficiency. In this work, we propose SHIRE, a
novel framework for encoding human intuition using Probabilistic Graphical
Models (PGMs) and using it in the Deep RL training pipeline to enhance sample
efficiency. Our framework achieves 25-78% sample efficiency gains across the
environments we evaluate at negligible overhead cost. Additionally, by teaching
RL agents the encoded elementary behavior, SHIRE enhances policy
explainability. A real-world demonstration further highlights the efficacy of
policies trained using our framework.</description>
      <guid isPermaLink="false">2409.09990v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>The study of strongly intensive observables for $π^{\pm,0}$ in $pp$ collisions at LHC energy in the framework of PYTHIA model</title>
      <link>http://arxiv.org/abs/2409.00525v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages, 10 Figures ( Total 18 Figures with sub-figures)&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究主题&lt;/h4&gt;   - 研究不同类型的π介子（即$\pi^{\pm,0}$）在一维$\eta$空间中的分形和相变特性，能量为$\sqrt{s}=13~$TeV。&lt;br&gt;&lt;h4&gt;2. 方法论&lt;/h4&gt;   - 使用Scaled Factorial Moment（SFM）框架进行分析，并通过Monte Carlo事件模拟器PYTHIA生成模拟数据集，模拟条件为最小偏差（MB）。&lt;br&gt;&lt;h4&gt;3. 计算参数&lt;/h4&gt;   - 计算了多个参数，包括Levy指数$(\mu)$、多重分形度$(r)$、异常分形维度$(d_q)$、多重分形比热$(c)$和临界指数$(\nu)$。&lt;br&gt;&lt;h4&gt;4. Bose-Einstein效应&lt;/h4&gt;   - 研究了由于相同粒子（这里是π介子）导致的Bose-Einstein效应，并为混合π对（如$\{\pi^{+},\pi^{-}\}$、$\{\pi^{+},\pi^{0}\}$和$\{\pi^{-},\pi^{0}\}$）推导了这些参数，发现混合效应相较于单个分布有所减弱。&lt;br&gt;&lt;h4&gt;5. 夸克-强子相变&lt;/h4&gt;   - 在Ginzburg-Landau（GL）理论框架内进行了夸克-强子相变的研究，分析显示PYTHIA生成的MB事件中，存在夸克-强子相变的明显迹象。&lt;br&gt;&lt;h4&gt;6. 多重分形比热&lt;/h4&gt;   - 各种$\pi^{+}$、$\pi^{-}$、$\pi^{0}$及其混合对的多重分形比热$(c)$值表明，在$\sqrt{s}=13~$TeV的$pp$碰撞中，从多重分形性过渡到单一分形性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The fractal and phase transitional properties of each type of pions (i.e.
$\pi^{\pm,0}$) through one-dimensional $\eta-$space, at an energy of
$\sqrt{s}=13~$TeV, have been studied with the help of the Scaled Factorial
Moment (SFM) framework. To generate simulated data sets for $pp$ collisions
under the minimum bias (MB) condition at $\sqrt{s}=13~$TeV, we have employed
the Monte Carlo-based event simulator PYTHIA. Various parameters such as the
Levy index $(\mu)$, degree of multifractality $(r)$, anomalous fractal
dimension $(d_q)$, multifractal specific heat $(c)$ and critical exponent
$(\nu)$ have been calculated. To study the Bose Einstein(BE) effect due to
identical particles (here pions) we have also derived these parameters for
mixed pion pairs (i.e. $\{\pi^{+},\pi^{-}\}$, $\{\pi^{+},\pi^{0}\}$ and
$\{\pi^{-},\pi^{0}\}$) and we find that the effects of identical particles
weakened for the mixture with respect to the individual distributions. The
quest for the quark-hadron phase transition has also been conducted within the
framework of the Ginzburg-Landau (GL) theory of second-order phase transition.
Analysis revealed that for PYTHIA-generated MB events, there is a clear
indication of the quark-hadron phase transition according to the GL theory.
Furthermore, the values of the multifractal specific heat ($c$) for each
$\pi^{+}, \pi^{-}, \pi^{0}$ and the mixture pair data sets of pions generated
by PYTHIA model at MB condition, indicate a transition from multifractality to
monofractality in $pp$ collisions at $\sqrt{s}=13~$TeV.</description>
      <guid isPermaLink="false">2409.00525v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>A proof of contribution in blockchain using game theoretical deep learning model</title>
      <link>http://arxiv.org/abs/2409.07460v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 构建弹性和可扩展的边缘资源是提供平台化智能城市服务的必要前提。&lt;br&gt;&lt;h4&gt;2. 智能城市服务&lt;/h4&gt;   - 智能城市服务通过边缘计算提供低延迟应用，但边缘计算面临资源有限的挑战。&lt;br&gt;&lt;h4&gt;3. 边缘设备的局限性&lt;/h4&gt;   - 单个边缘设备无法承担智能城市中的多种智能计算，迫使不同服务提供商的大规模边缘设备部署成为必要。&lt;br&gt;&lt;h4&gt;4. 计算能力选择&lt;/h4&gt;   - 从不同服务提供商选择计算能力是一个博弈论问题。&lt;br&gt;&lt;h4&gt;5. 激励机制&lt;/h4&gt;   - 为了激励服务提供商积极贡献其宝贵资源并提供低延迟的协同计算能力，提出了一个博弈论深度学习模型，以达成服务提供商之间的任务调度和资源供应共识。&lt;br&gt;&lt;h4&gt;6. 传统方法的不足&lt;/h4&gt;   - 传统的集中式资源管理方法效率低下且缺乏可信度，引入区块链技术可以实现去中心化的资源交易和调度。&lt;br&gt;&lt;h4&gt;7. 贡献基础的证明机制&lt;/h4&gt;   - 提出了一个基于贡献的证明机制，以提供边缘计算的低延迟服务。&lt;br&gt;&lt;h4&gt;8. 深度学习模型结构&lt;/h4&gt;   - 模型由双编码器和单解码器组成，其中GNN（图神经网络）编码器处理结构化决策行动数据，RNN（递归神经网络）编码器处理时间序列任务调度数据。&lt;br&gt;&lt;h4&gt;9. 实验结果&lt;/h4&gt;   - 大量实验表明，与最先进的方法相比，模型将延迟降低了584%。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Building elastic and scalable edge resources is an inevitable prerequisite
for providing platform-based smart city services. Smart city services are
delivered through edge computing to provide low-latency applications. However,
edge computing has always faced the challenge of limited resources. A single
edge device cannot undertake the various intelligent computations in a smart
city, and the large-scale deployment of edge devices from different service
providers to build an edge resource platform has become a necessity. Selecting
computing power from different service providers is a game-theoretic problem.
To incentivize service providers to actively contribute their valuable
resources and provide low-latency collaborative computing power, we introduce a
game-theoretic deep learning model to reach a consensus among service providers
on task scheduling and resource provisioning. Traditional centralized resource
management approaches are inefficient and lack credibility, while the
introduction of blockchain technology can enable decentralized resource trading
and scheduling. We propose a contribution-based proof mechanism to provide the
low-latency service of edge computing. The deep learning model consists of dual
encoders and a single decoder, where the GNN (Graph Neural Network) encoder
processes structured decision action data, and the RNN (Recurrent Neural
Network) encoder handles time-series task scheduling data. Extensive
experiments have demonstrated that our model reduces latency by 584% compared
to the state-of-the-art.</description>
      <guid isPermaLink="false">2409.07460v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Online Refractive Camera Model Calibration in Visual Inertial Odometry</title>
      <link>http://arxiv.org/abs/2409.12074v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the 2024 IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS 2024), 8 pages&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究主题&lt;/h4&gt;   - 本文提出了一种通用的折射相机模型以及未知介质的里程计和折射率的在线共同估计。&lt;br&gt;&lt;h4&gt;2. 应用场景&lt;/h4&gt;   - 该模型能够在多种不同的折射流体中操作，仅需在空气中进行相机标定。&lt;br&gt;&lt;h4&gt;3. 折射率估计&lt;/h4&gt;   - 折射率作为单目视觉惯性里程计框架的状态变量在线估计，采用迭代方法与所提相机模型结合。&lt;br&gt;&lt;h4&gt;4. 验证方法&lt;/h4&gt;   - 该方法在使用水下机器人在池中移动时收集的数据上进行了验证。&lt;br&gt;&lt;h4&gt;5. 评估结果&lt;/h4&gt;   - 评估结果表明，尽管初始化存在显著扰动，方法仍能收敛到水的理想折射率。&lt;br&gt;&lt;h4&gt;6. 性能特点&lt;/h4&gt;   - 该方法在折射介质中实现了与视觉惯性里程计相当的性能，无需预先知道折射率或特定介质的相机标定。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a general refractive camera model and online
co-estimation of odometry and the refractive index of unknown media. This
enables operation in diverse and varying refractive fluids, given only the
camera calibration in air. The refractive index is estimated online as a state
variable of a monocular visual-inertial odometry framework in an iterative
formulation using the proposed camera model. The method was verified on data
collected using an underwater robot traversing inside a pool. The evaluations
demonstrate convergence to the ideal refractive index for water despite
significant perturbations in the initialization. Simultaneously, the approach
enables on-par visual-inertial odometry performance in refractive media without
prior knowledge of the refractive index or requirement of medium-specific
camera calibration.</description>
      <guid isPermaLink="false">2409.12074v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Micro-orchestration of RAN functions accelerated in FPGA SoC devices</title>
      <link>http://arxiv.org/abs/2409.11362v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Article accepted in the IEEE International Conference on 6G
  Networking (6GNet 2024)&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 本文探讨如何解决FPGA SoC设备在5G和边缘计算基础设施中计算资源的低利用率问题。&lt;br&gt;&lt;h4&gt;2. 首要步骤&lt;/h4&gt;   - 实现一个资源管理层，能够根据上下文事件迁移和扩展功能。&lt;br&gt;&lt;h4&gt;3. 设计基础&lt;/h4&gt;   - 该资源管理层为设计一个分层的数据驱动微编排器奠定基础，负责FPGA SoC设备中功能的生命周期管理。&lt;br&gt;&lt;h4&gt;4. O-RAN背景&lt;/h4&gt;   - 在O-RAN（开放无线接入网）环境中，预计微编排器将以xApp/rApp组合的形式出现，利用RAN（无线接入网）流量和上下文数据进行训练。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work provides a vision on how to tackle the underutilization of compute
resources in FPGA SoC devices used across 5G and edge computing
infrastructures. A first step towards this end is the implementation of a
resource management layer able to migrate and scale functions in such devices,
based on context events. This layer sets the basis to design a hierarchical
data-driven micro-orchestrator in charge of providing the lifecycle management
of functions in FPGA SoC devices. In the O-RAN context, the micro-orchestrator
is foreseen to take the form of an xApp/rApp tandem trained with RAN traffic
and context data.</description>
      <guid isPermaLink="false">2409.11362v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Towards Time Series Reasoning with LLMs</title>
      <link>http://arxiv.org/abs/2409.11376v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 多模态大型语言模型（MLLMs）在视觉领域的理解和推理方面取得了显著进展，但在时间序列领域尚未见到类似的成功。&lt;br&gt;&lt;h4&gt;2. 现有问题&lt;/h4&gt;   - 尽管以前的时间序列MLLMs在时间序列预测中表现出色，但很少有研究探讨LLM如何在自然语言中进行时间序列推理。&lt;br&gt;&lt;h4&gt;3. 研究提议&lt;/h4&gt;   - 提出一种新颖的多模态时间序列LLM方法，旨在学习跨多个领域的可泛化信息，并具备强大的零-shot表现。&lt;br&gt;&lt;h4&gt;4. 模型构建&lt;/h4&gt;   - 首先，在LLM的基础上训练一个轻量级的时间序列编码器，以直接提取时间序列信息。&lt;br&gt;&lt;h4&gt;5. 模型微调&lt;/h4&gt;   - 通过增强链式思维的时间序列任务对模型进行微调，以鼓励模型生成推理路径。&lt;br&gt;&lt;h4&gt;6. 表现验证&lt;/h4&gt;   - 研究表明，该模型学习到的潜在表示能够反映特定的时间序列特征（如斜率、频率）。&lt;br&gt;&lt;h4&gt;7. 性能比较&lt;/h4&gt;   - 在多种领域的零-shot推理任务中，该模型的表现优于GPT-4o。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-modal large language models (MLLMs) have enabled numerous advances in
understanding and reasoning in domains like vision, but we have not yet seen
this broad success for time-series. Although prior works on time-series MLLMs
have shown promising performance in time-series forecasting, very few works
show how an LLM could be used for time-series reasoning in natural language. We
propose a novel multi-modal time-series LLM approach that learns generalizable
information across various domains with powerful zero-shot performance. First,
we train a lightweight time-series encoder on top of an LLM to directly extract
time-series information. Then, we fine-tune our model with chain-of-thought
augmented time-series tasks to encourage the model to generate reasoning paths.
We show that our model learns a latent representation that reflects specific
time-series features (e.g. slope, frequency), as well as outperforming GPT-4o
on a set of zero-shot reasoning tasks on a variety of domains.</description>
      <guid isPermaLink="false">2409.11376v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>EgoHDM: An Online Egocentric-Inertial Human Motion Capture, Localization, and Dense Mapping System</title>
      <link>http://arxiv.org/abs/2409.00343v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page: https://handiyin.github.io/EgoHDM/&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 系统介绍&lt;/h4&gt;   - 提出了EgoHDM，一个在线的自我中心惯性人类动作捕捉、定位和高密度映射系统。&lt;br&gt;&lt;h4&gt;2. 硬件组成&lt;/h4&gt;   - 系统使用6个惯性测量单元（IMUs）和一台普通的头戴RGB摄像头。&lt;br&gt;&lt;h4&gt;3. 首次实现&lt;/h4&gt;   - EgoHDM是首个在近实时下提供高密度场景映射的人类动作捕捉系统。&lt;br&gt;&lt;h4&gt;4. 初始化和闭环&lt;/h4&gt;   - 系统初始化快速且稳健，能够完全闭合物理上合理的地图感知全局人类运动估计与动作捕捉感知3D场景重建之间的循环。&lt;br&gt;&lt;h4&gt;5. 关键理念&lt;/h4&gt;   - 通过双向集成相机定位和映射信息与惯性人类动作捕捉，提出了系统的关键理念。&lt;br&gt;&lt;h4&gt;6. 模块设计&lt;/h4&gt;   - 设计了紧密耦合的动作捕捉感知密集束调整和基于物理的身体姿态校正模块，利用局部身体中心的高程图。&lt;br&gt;&lt;h4&gt;7. 接触控制器&lt;/h4&gt;   - 引入了一种新颖的地形感知接触PD控制器，使角色能够与给定的局部高程图物理接触，减少人类漂浮或穿透现象。&lt;br&gt;&lt;h4&gt;8. 性能验证&lt;/h4&gt;   - 在现有的合成和真实世界基准上展示了系统的性能，结果表明与最先进的方法相比，EgoHDM在人类定位、相机姿态和映射精度上分别减少了41%、71%和46%的误差。&lt;br&gt;&lt;h4&gt;9. 定性评估&lt;/h4&gt;   - 对新捕获数据的定性评估进一步证明EgoHDM能够覆盖非平坦地形中的挑战场景，包括跨越楼梯和户外野外场景。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present EgoHDM, an online egocentric-inertial human motion capture
(mocap), localization, and dense mapping system. Our system uses 6 inertial
measurement units (IMUs) and a commodity head-mounted RGB camera. EgoHDM is the
first human mocap system that offers dense scene mapping in near real-time.
Further, it is fast and robust to initialize and fully closes the loop between
physically plausible map-aware global human motion estimation and mocap-aware
3D scene reconstruction. Our key idea is integrating camera localization and
mapping information with inertial human motion capture bidirectionally in our
system. To achieve this, we design a tightly coupled mocap-aware dense bundle
adjustment and physics-based body pose correction module leveraging a local
body-centric elevation map. The latter introduces a novel terrain-aware contact
PD controller, which enables characters to physically contact the given local
elevation map thereby reducing human floating or penetration. We demonstrate
the performance of our system on established synthetic and real-world
benchmarks. The results show that our method reduces human localization, camera
pose, and mapping accuracy error by 41%, 71%, 46%, respectively, compared to
the state of the art. Our qualitative evaluations on newly captured data
further demonstrate that EgoHDM can cover challenging scenarios in non-flat
terrain including stepping over stairs and outdoor scenes in the wild.</description>
      <guid isPermaLink="false">2409.00343v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>P2U-SLAM: A Monocular Wide-FoV SLAM System Based on Point Uncertainty and Pose Uncertainty</title>
      <link>http://arxiv.org/abs/2409.10143v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The source code will be made publicly available at
  https://github.com/BambValley/P2U-SLAM&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 系统介绍&lt;/h4&gt;   - 本文提出了P2U-SLAM，一个基于视觉的同时定位与地图构建（SLAM）系统，使用广视场（FoV）摄像头。&lt;br&gt;&lt;h4&gt;2. 不确定性利用&lt;/h4&gt;   - 系统利用姿态不确定性和点不确定性，以提高SLAM性能。&lt;br&gt;&lt;h4&gt;3. 广视场的优势&lt;/h4&gt;   - 广视场摄像头允许对历史地图点进行大量重复观察，有助于跨视角特征匹配。&lt;br&gt;&lt;h4&gt;4. 数据属性变化问题&lt;/h4&gt;   - 在优化过程中，历史地图点和关键帧姿态的数据属性发生变化，未能考虑这些变化会导致优化缺失部分信息矩阵，增加长期定位性能下降的风险。&lt;br&gt;&lt;h4&gt;5. 研究目的&lt;/h4&gt;   - 研究旨在降低广视场视觉输入对SLAM系统的风险。&lt;br&gt;&lt;h4&gt;6. 条件概率模型&lt;/h4&gt;   - 基于条件概率模型，揭示数据属性变化对优化过程的明确影响，并具体化为点不确定性和姿态不确定性，提供了具体的数学表达形式。&lt;br&gt;&lt;h4&gt;7. 不确定性嵌入&lt;/h4&gt;   - P2U-SLAM将点不确定性和姿态不确定性分别嵌入跟踪模块和局部映射中，并在每次优化操作后更新这些不确定性，包括局部映射、地图合并和回环闭合。&lt;br&gt;&lt;h4&gt;8. 评估结果&lt;/h4&gt;   - 在27个序列中进行了全面评估，使用两个流行的公共数据集，结果表明P2U-SLAM的性能优于其他最先进的方法。&lt;br&gt;&lt;h4&gt;9. 代码开放&lt;/h4&gt;   - 源代码将在GitHub上公开，便于其他研究者使用和验证。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents P2U-SLAM, a visual Simultaneous Localization And Mapping
(SLAM) system with a wide Field of View (FoV) camera, which utilizes pose
uncertainty and point uncertainty. While the wide FoV enables considerable
repetitive observations of historical map points for matching cross-view
features, the data properties of the historical map points and the poses of
historical keyframes have changed during the optimization process. The neglect
of data property changes triggers the absence of a partial information matrix
in optimization and leads to the risk of long-term positioning performance
degradation. The purpose of our research is to reduce the risk of the wide
field of view visual input to the SLAM system. Based on the conditional
probability model, this work reveals the definite impact of the above data
properties changes on the optimization process, concretizes it as point
uncertainty and pose uncertainty, and gives a specific mathematical form.
P2U-SLAM respectively embeds point uncertainty and pose uncertainty into the
tracking module and local mapping, and updates these uncertainties after each
optimization operation including local mapping, map merging, and loop closing.
We present an exhaustive evaluation in 27 sequences from two popular public
datasets with wide-FoV visual input. P2U-SLAM shows excellent performance
compared with other state-of-the-art methods. The source code will be made
publicly available at https://github.com/BambValley/P2U-SLAM.</description>
      <guid isPermaLink="false">2409.10143v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook</title>
      <link>http://arxiv.org/abs/2402.19348v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 城市计算的重要性&lt;/h4&gt;   - 随着城市的不断发展，城市计算成为可持续发展的关键学科。&lt;br&gt;&lt;h4&gt;2. 数据融合的多样性&lt;/h4&gt;   - 利用来自不同来源（如地理、交通、社交媒体和环境数据）的跨域数据融合，结合多种数据模式（如时空、视觉和文本）。&lt;br&gt;&lt;h4&gt;3. 深度学习的应用&lt;/h4&gt;   - 近年来，越来越多的深度学习方法被应用于智能城市的跨域数据融合。&lt;br&gt;&lt;h4&gt;4. 综述的目的&lt;/h4&gt;   - 提出首次系统性综述，回顾深度学习基础的数据融合方法在城市计算中的最新进展。&lt;br&gt;&lt;h4&gt;5. 数据来源分析&lt;/h4&gt;   - 研究每种数据模式和数据源的作用。&lt;br&gt;&lt;h4&gt;6. 方法分类&lt;/h4&gt;   - 将数据融合方法分为四类：基于特征、基于对齐、基于对比和基于生成的方法。&lt;br&gt;&lt;h4&gt;7. 应用分类&lt;/h4&gt;   - 将多模态城市应用进一步分类为七种类型：城市规划、交通、经济、公共安全、社会、环境和能源。&lt;br&gt;&lt;h4&gt;8. 与以往综述的不同&lt;/h4&gt;   - 相比之前的综述，更加关注深度学习方法与城市计算应用的协同作用。&lt;br&gt;&lt;h4&gt;9. 未来研究方向&lt;/h4&gt;   - 探讨大语言模型（LLMs）与城市计算的互动，提出可能革新该领域的未来研究方向。&lt;br&gt;&lt;h4&gt;10. 研究社区的贡献&lt;/h4&gt;    - 认为综述中描述的分类、进展和前景将极大丰富研究社区的知识。&lt;br&gt;&lt;h4&gt;11. 相关资源链接&lt;/h4&gt;    - 提供了一个链接，供读者查阅全面且最新的论文列表。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-02-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1016/j.inffus.2024.102606.&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/yoshall/awesome-multimodal-urban-computing&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As cities continue to burgeon, Urban Computing emerges as a pivotal
discipline for sustainable development by harnessing the power of cross-domain
data fusion from diverse sources (e.g., geographical, traffic, social media,
and environmental data) and modalities (e.g., spatio-temporal, visual, and
textual modalities). Recently, we are witnessing a rising trend that utilizes
various deep-learning methods to facilitate cross-domain data fusion in smart
cities. To this end, we propose the first survey that systematically reviews
the latest advancements in deep learning-based data fusion methods tailored for
urban computing. Specifically, we first delve into data perspective to
comprehend the role of each modality and data source. Secondly, we classify the
methodology into four primary categories: feature-based, alignment-based,
contrast-based, and generation-based fusion methods. Thirdly, we further
categorize multi-modal urban applications into seven types: urban planning,
transportation, economy, public safety, society, environment, and energy.
Compared with previous surveys, we focus more on the synergy of deep learning
methods with urban computing applications. Furthermore, we shed light on the
interplay between Large Language Models (LLMs) and urban computing, postulating
future research directions that could revolutionize the field. We firmly
believe that the taxonomy, progress, and prospects delineated in our survey
stand poised to significantly enrich the research community. The summary of the
comprehensive and up-to-date paper list can be found at
https://github.com/yoshall/Awesome-Multimodal-Urban-Computing.</description>
      <guid isPermaLink="false">2402.19348v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Geometry-aware Feature Matching for Large-Scale Structure from Motion</title>
      <link>http://arxiv.org/abs/2409.02310v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 在多图像之间建立一致且密集的对应关系对于运动结构重建（SfM）系统至关重要。&lt;br&gt;&lt;h4&gt;2. 挑战&lt;/h4&gt;   - 面对显著的视角变化（如空对地且重叠视图非常稀疏），对应关系求解变得更加困难。&lt;br&gt;&lt;h4&gt;3. 方法介绍&lt;/h4&gt;   - 提出了一种新颖的基于优化的方法，通过引入几何线索来显著增强现有的特征匹配方法，除了颜色线索之外。&lt;br&gt;&lt;h4&gt;4. 几何验证&lt;/h4&gt;   - 将几何验证公式化为一个优化问题，指导无检测器方法中的特征匹配，并使用基于检测器的方法得到的稀疏对应关系作为锚点。&lt;br&gt;&lt;h4&gt;5. 几何约束&lt;/h4&gt;   - 通过施加Sampson距离的几何约束，确保来自无检测器方法的密集对应关系在几何上是一致且更准确的。&lt;br&gt;&lt;h4&gt;6. 混合策略的优势&lt;/h4&gt;   - 这种混合策略显著提高了对应关系的密度和准确性，减轻了多视图不一致性，进而促进了相机姿态精度和点云密度的显著提升。&lt;br&gt;&lt;h4&gt;7. 性能验证&lt;/h4&gt;   - 在基准数据集上，该方法优于最先进的特征匹配方法，并能够在极具挑战性的超大规模环境中实现特征匹配。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Establishing consistent and dense correspondences across multiple images is
crucial for Structure from Motion (SfM) systems. Significant view changes, such
as air-to-ground with very sparse view overlap, pose an even greater challenge
to the correspondence solvers. We present a novel optimization-based approach
that significantly enhances existing feature matching methods by introducing
geometry cues in addition to color cues. This helps fill gaps when there is
less overlap in large-scale scenarios. Our method formulates geometric
verification as an optimization problem, guiding feature matching within
detector-free methods and using sparse correspondences from detector-based
methods as anchor points. By enforcing geometric constraints via the Sampson
Distance, our approach ensures that the denser correspondences from
detector-free methods are geometrically consistent and more accurate. This
hybrid strategy significantly improves correspondence density and accuracy,
mitigates multi-view inconsistencies, and leads to notable advancements in
camera pose accuracy and point cloud density. It outperforms state-of-the-art
feature matching methods on benchmark datasets and enables feature matching in
challenging extreme large-scale settings.</description>
      <guid isPermaLink="false">2409.02310v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Overcoming the Barriers of Using Linked Open Data in Smart City Applications</title>
      <link>http://arxiv.org/abs/2408.14315v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究主题&lt;/h4&gt;   - 研究了在智能城市应用中使用链接开放数据（Linked Open Data）的好处与挑战。&lt;br&gt;&lt;h4&gt;2. 工具提案&lt;/h4&gt;   - 提出了一套开源、高度可扩展的工具，专注于公共租赁自行车系统。&lt;br&gt;&lt;h4&gt;3. 参考指南&lt;/h4&gt;   - 这些工具可作为其他智能城市应用的参考指南，提供实践经验和解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/MC.2022.3206144&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We study the benefits and challenges of using Linked Open Data in smart city
applications and propose a set of open source, highly scalable tools within the
case of a public-rental bicycle system, which can act as a reference guide for
other smart city applications.</description>
      <guid isPermaLink="false">2408.14315v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>The elastica sling</title>
      <link>http://arxiv.org/abs/2409.12075v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究主题&lt;/h4&gt;   - 分析了一种由滑动套筒约束的柔性弹性杆的非线性力学行为。&lt;br&gt;&lt;h4&gt;2. 平面平衡配置&lt;/h4&gt;   - 该可变长度弹性杆的平面平衡形状仅由两个约束的倾斜角度决定，而它们之间的距离仅负责缩放尺寸。&lt;br&gt;&lt;h4&gt;3. 稳定性准则扩展&lt;/h4&gt;   - 将现有的稳定性准则扩展到可变域的情况下，揭示出至多存在一个稳定平衡解。&lt;br&gt;&lt;h4&gt;4. 失去稳定性的条件&lt;/h4&gt;   - 确定了滑动套筒倾斜角度对的集合，这些条件会导致系统失去稳定性。&lt;br&gt;&lt;h4&gt;5. 柔性杆的弹射机制&lt;/h4&gt;   - 在临界条件下，柔性杆可以无限期地从滑动套筒中弹出，从而实现弹性投射功能（elastica sling）。&lt;br&gt;&lt;h4&gt;6. 实验验证&lt;/h4&gt;   - 理论结果通过物理原型的实验得到了验证。&lt;br&gt;&lt;h4&gt;7. 应用前景&lt;/h4&gt;   - 研究结果为新的驱动原理提供了基础，可能在能量采集、波减缓装置和软机器人运动中找到应用。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1016/j.euromechsol.2024.105273&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The nonlinear mechanics of a flexible elastic rod constrained at its edges by
a pair of sliding sleeves is analyzed. The planar equilibrium configurations of
this variable-length elastica are found to have shape defined only by the
inclination of the two constraints, while their distance is responsible only
for scaling the size. By extending the theoretical stability criterion
available for systems under isoperimetric constraints to the case of variable
domains, the existence of no more than one stable equilibrium solution is
revealed. The set of sliding sleeves' inclination pairs for which the stability
is lost are identified. Such critical conditions allow the indefinite ejection
of the flexible rod from the sliding sleeves, thus realizing an elastica sling.
Finally, the theoretical findings are validated by experiments on a physical
prototype. The present results lead to a novel actuation principle that may
find application as a mechanism in energy harvesting, wave mitigation devices,
and soft robotic locomotion.</description>
      <guid isPermaLink="false">2409.12075v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Multi-stage stochastic linear programming for shared autonomous vehicle system operation and design with on-demand and pre-booked requests</title>
      <link>http://arxiv.org/abs/2409.11611v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The original paper was submitted to TRB Annual Meeting 2025&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究主题&lt;/h4&gt;   - 本研究提出了优化问题，旨在共同确定共享自动驾驶车辆（SAV）系统的长期网络设计、中期车队规模策略以及短期路线和拼车匹配。&lt;br&gt;&lt;h4&gt;2. 问题背景&lt;/h4&gt;   - 研究涵盖预订和按需出行请求的场景，强调了不同时间尺度的决策。&lt;br&gt;&lt;h4&gt;3. 方法论&lt;/h4&gt;   - 基于动态交通分配框架，采用多阶段随机线性规划进行SAV系统设计和运营的联合优化。&lt;br&gt;&lt;h4&gt;4. 解决方案&lt;/h4&gt;   - 利用所提问题的线性特性，通过加权和方法和随机对偶动态规划（SDDP）来应对多个目标和动态随机性带来的计算复杂性。&lt;br&gt;&lt;h4&gt;5. 实验验证&lt;/h4&gt;   - 数值例子验证了通过SDDP获得的解决方案与最优解相近。&lt;br&gt;&lt;h4&gt;6. 预订选项的影响&lt;/h4&gt;   - 研究展示了引入预订选项对优化基础设施规划和车队规模策略的影响。&lt;br&gt;&lt;h4&gt;7. 专用车辆策略&lt;/h4&gt;   - 专门用于接送仅预订乘客的车辆可以激励乘客提前预订，而非按需请求，且对系统性能影响较小。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study presents optimization problems to jointly determine long-term
network design, mid-term fleet sizing strategy, and short-term routing and
ridesharing matching in shared autonomous vehicle (SAV) systems with pre-booked
and on-demand trip requests. Based on the dynamic traffic assignment framework,
multi-stage stochastic linear programming is formulated for joint optimization
of SAV system design and operations. Leveraging the linearity of the proposed
problem, we can tackle the computational complexity due to multiple objectives
and dynamic stochasticity through the weighted sum method and stochastic dual
dynamic programming (SDDP). Our numerical examples verify that the solution to
the proposed problem obtained through SDDP is close enough to the optimal
solution. We also demonstrate the effect of introducing pre-booking options on
optimized infrastructure planning and fleet sizing strategies. Furthermore,
dedicated vehicles to pick-up and drop-off only pre-booked travelers can lead
to incentives to reserve in advance instead of on-demand requests with little
reduction in system performance.</description>
      <guid isPermaLink="false">2409.11611v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>orbitize! v3: Orbit fitting for the High-contrast Imaging Community</title>
      <link>http://arxiv.org/abs/2409.11573v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to JOSS&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 软件介绍&lt;/h4&gt;   - **orbitize!** 是一个用于贝叶斯建模的工具，专注于从时间序列测量中解析双星体的轨道参数。&lt;br&gt;&lt;h4&gt;2. 目标用户群&lt;/h4&gt;   - 该软件最初是为高对比度成像社区开发的，但也广泛应用于双星研究领域。&lt;br&gt;&lt;h4&gt;3. 应用案例&lt;/h4&gt;   - orbitize! 的典型使用案例包括将相对天体测量时间序列（可选地结合径向速度或天体测量时间序列）转换为一组推导的轨道后验分布。&lt;br&gt;&lt;h4&gt;4. 版本更新&lt;/h4&gt;   - 本文与orbitize! 版本3.0的发布同时发布，该版本在功能和可访问性方面相较于1.0版（Blunt等，2020）有显著增强。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; orbitize! is a package for Bayesian modeling of the orbital parameters of
resolved binary objects from time series measurements. It was developed with
the needs of the high-contrast imaging community in mind, and has since also
become widely used in the binary star community. A generic orbitize! use case
involves translating relative astrometric time series, optionally combined with
radial velocity or astrometric time series, into a set of derived orbital
posteriors. This paper is published alongside the release of orbitize! version
3.0, which has seen significant enhancements in functionality and accessibility
since the release of version 1.0 (Blunt et al., 2020).</description>
      <guid isPermaLink="false">2409.11573v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Reprojection Errors as Prompts for Efficient Scene Coordinate Regression</title>
      <link>http://arxiv.org/abs/2409.04178v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ECCV2024&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 场景坐标回归（SCR）方法因其在准确视觉定位中的潜力而成为研究的一个有前景领域。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限&lt;/h4&gt;   - 许多现有的SCR方法在训练时使用来自所有图像区域的样本，包括动态物体和缺乏纹理的区域，这可能会影响模型的整体性能和效率。&lt;br&gt;&lt;h4&gt;3. 深入分析&lt;/h4&gt;   - 本研究首先进行深入分析，验证这些区域对模型性能的不利影响。&lt;br&gt;&lt;h4&gt;4. 新机制的提出&lt;/h4&gt;   - 在分析的基础上，提出了一种错误引导特征选择（EGFS）机制，结合使用Segment Anything Model（SAM）。&lt;br&gt;&lt;h4&gt;5. EGFS机制细节&lt;/h4&gt;   - 该机制将低重投影区域作为提示，并将其扩展为错误引导的掩模，利用这些掩模以迭代方式采样点并过滤掉问题区域。&lt;br&gt;&lt;h4&gt;6. 实验结果&lt;/h4&gt;   - 实验表明，所提出的方法在不依赖3D信息的情况下，在Cambridge Landmarks和Indoor6数据集上优于现有的SCR方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Scene coordinate regression (SCR) methods have emerged as a promising area of
research due to their potential for accurate visual localization. However, many
existing SCR approaches train on samples from all image regions, including
dynamic objects and texture-less areas. Utilizing these areas for optimization
during training can potentially hamper the overall performance and efficiency
of the model. In this study, we first perform an in-depth analysis to validate
the adverse impacts of these areas. Drawing inspiration from our analysis, we
then introduce an error-guided feature selection (EGFS) mechanism, in tandem
with the use of the Segment Anything Model (SAM). This mechanism seeds low
reprojection areas as prompts and expands them into error-guided masks, and
then utilizes these masks to sample points and filter out problematic areas in
an iterative manner. The experiments demonstrate that our method outperforms
existing SCR approaches that do not rely on 3D information on the Cambridge
Landmarks and Indoor6 datasets.</description>
      <guid isPermaLink="false">2409.04178v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Object Gaussian for Monocular 6D Pose Estimation from Sparse Views</title>
      <link>http://arxiv.org/abs/2409.02581v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 单目物体姿态估计是计算机视觉和机器人领域的关键任务，依赖于准确的2D-3D对应关系，通常需要昂贵的CAD模型，而这些模型可能并不容易获得。&lt;br&gt;&lt;h4&gt;2. 替代方法&lt;/h4&gt;   - 物体3D重建方法提供了替代方案，其中最近的3D高斯点云（3D Gaussian Splatting, 3DGS）进展显示出潜力，但在少量输入视图下表现不佳，且容易过拟合。&lt;br&gt;&lt;h4&gt;3. 新框架介绍&lt;/h4&gt;   - 本文提出了SGPose，一个用于稀疏视图物体姿态估计的新框架，利用基于高斯的方法。&lt;br&gt;&lt;h4&gt;4. 方法细节&lt;/h4&gt;   - SGPose仅使用十个视图，通过随机立方体初始化生成几何感知表示，避免了传统3DGS方法依赖于运动结构（SfM）管道生成的几何体。&lt;br&gt;&lt;h4&gt;5. 无CAD模型依赖&lt;/h4&gt;   - SGPose通过回归来自稀疏输入和随机初始化的图像与重建模型之间的密集2D-3D对应关系，消除了对CAD模型的依赖。&lt;br&gt;&lt;h4&gt;6. 成功关键因素&lt;/h4&gt;   - 几何一致的深度监督和在线合成视图扭曲是SGPose成功的关键。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 在典型基准测试，特别是在Occlusion LM-O数据集上的实验表明，SGPose在稀疏视图约束下的表现优于现有方法，突显了其在实际应用中的潜力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Monocular object pose estimation, as a pivotal task in computer vision and
robotics, heavily depends on accurate 2D-3D correspondences, which often demand
costly CAD models that may not be readily available. Object 3D reconstruction
methods offer an alternative, among which recent advancements in 3D Gaussian
Splatting (3DGS) afford a compelling potential. Yet its performance still
suffers and tends to overfit with fewer input views. Embracing this challenge,
we introduce SGPose, a novel framework for sparse view object pose estimation
using Gaussian-based methods. Given as few as ten views, SGPose generates a
geometric-aware representation by starting with a random cuboid initialization,
eschewing reliance on Structure-from-Motion (SfM) pipeline-derived geometry as
required by traditional 3DGS methods. SGPose removes the dependence on CAD
models by regressing dense 2D-3D correspondences between images and the
reconstructed model from sparse input and random initialization, while the
geometric-consistent depth supervision and online synthetic view warping are
key to the success. Experiments on typical benchmarks, especially on the
Occlusion LM-O dataset, demonstrate that SGPose outperforms existing methods
even under sparse view constraints, under-scoring its potential in real-world
applications.</description>
      <guid isPermaLink="false">2409.02581v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>A Survey of Route Recommendations: Methods, Applications, and Opportunities</title>
      <link>http://arxiv.org/abs/2403.00284v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  24 pages, 13 figures&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 现代城市发展正受到先进信息技术的推动，城市范围内的大数据和强大的计算资源正在智能化城市建设。&lt;br&gt;&lt;h4&gt;2. 研究重点&lt;/h4&gt;   - 路径推荐作为智能交通的重要组成部分，广泛应用于影响市民的出行习惯。基于大数据（可能是多模式）的智能高效旅行路线开发已成为路径推荐研究的核心挑战。&lt;br&gt;&lt;h4&gt;3. 调查内容概述&lt;/h4&gt;   - 本文对基于城市计算的路径推荐工作进行了全面回顾，分为以下三个部分：&lt;br&gt;   &lt;h4&gt;1. 方法论&lt;/h4&gt;      - 分类大量传统机器学习和现代深度学习方法，讨论它们的历史关系并揭示前沿进展。&lt;br&gt;   &lt;h4&gt;2. 应用&lt;/h4&gt;      - 介绍与城市计算场景下路径推荐相关的众多新应用。&lt;br&gt;   &lt;h4&gt;3. 问题与挑战&lt;/h4&gt;      - 讨论当前存在的问题和挑战，并展望几条有前景的研究方向。&lt;br&gt;&lt;h4&gt;4. 研究意义&lt;/h4&gt;   - 本调查旨在帮助相关研究人员快速熟悉路径推荐研究的现状，并引导他们了解未来的研究趋势。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-03-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Nowadays, with advanced information technologies deployed citywide, large
data volumes and powerful computational resources are intelligentizing modern
city development. As an important part of intelligent transportation, route
recommendation and its applications are widely used, directly influencing
citizens` travel habits. Developing smart and efficient travel routes based on
big data (possibly multi-modal) has become a central challenge in route
recommendation research. Our survey offers a comprehensive review of route
recommendation work based on urban computing. It is organized by the following
three parts: 1) Methodology-wise. We categorize a large volume of traditional
machine learning and modern deep learning methods. Also, we discuss their
historical relations and reveal the edge-cutting progress. 2)
Application\-wise. We present numerous novel applications related to route
commendation within urban computing scenarios. 3) We discuss current problems
and challenges and envision several promising research directions. We believe
that this survey can help relevant researchers quickly familiarize themselves
with the current state of route recommendation research and then direct them to
future research trends.</description>
      <guid isPermaLink="false">2403.00284v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>GenAI-powered Multi-Agent Paradigm for Smart Urban Mobility: Opportunities and Challenges for Integrating Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) with Intelligent Transportation Systems</title>
      <link>http://arxiv.org/abs/2409.00494v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 随着生成性人工智能的最新进展，多代理系统正在被开发以增强智能城市应用的功能和效率。&lt;br&gt;&lt;h4&gt;2. 研究目标&lt;/h4&gt;   - 本文探讨大型语言模型（LLMs）和新兴的检索增强生成（RAG）技术在智能交通系统（ITS）中的变革潜力，旨在为解决城市移动性中的关键挑战提供创新解决方案。&lt;br&gt;&lt;h4&gt;3. 现状概述&lt;/h4&gt;   - 提供关于当前移动数据、ITS和连接车辆（CV）应用的全面概述。&lt;br&gt;&lt;h4&gt;4. RAG的背景与机会&lt;/h4&gt;   - 讨论RAG的理论基础，并探讨将这些生成性人工智能（GenAI）技术整合到智能移动性领域的机会。&lt;br&gt;&lt;h4&gt;5. 概念框架&lt;/h4&gt;   - 提出一个概念框架，旨在开发能够智能和对话式提供智能移动性服务的多代理系统，服务于城市通勤者、交通运营商和决策者。&lt;br&gt;&lt;h4&gt;6. 自主智能目标&lt;/h4&gt;   - 该方法旨在促进科学依据的建议，以减少多层次的交通拥堵、事故和碳排放。&lt;br&gt;   - 促进公众教育和参与性移动管理的参与。&lt;br&gt;   - 自动化专业交通管理任务及关键ITS平台的发展，包括数据分析与解读、知识表示和交通仿真。&lt;br&gt;&lt;h4&gt;7. 技术整合的优势&lt;/h4&gt;   - 通过整合LLM和RAG，克服传统基于规则的多代理系统的局限性，这些系统依赖固定知识库和有限的推理能力。&lt;br&gt;&lt;h4&gt;8. 未来展望&lt;/h4&gt;   - 这种整合为更具可扩展性、直观性和自动化的多代理范式铺平了道路，推动ITS和城市移动性的进步。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Leveraging recent advances in generative AI, multi-agent systems are
increasingly being developed to enhance the functionality and efficiency of
smart city applications. This paper explores the transformative potential of
large language models (LLMs) and emerging Retrieval-Augmented Generation (RAG)
technologies in Intelligent Transportation Systems (ITS), paving the way for
innovative solutions to address critical challenges in urban mobility. We begin
by providing a comprehensive overview of the current state-of-the-art in
mobility data, ITS, and Connected Vehicles (CV) applications. Building on this
review, we discuss the rationale behind RAG and examine the opportunities for
integrating these Generative AI (GenAI) technologies into the smart mobility
sector. We propose a conceptual framework aimed at developing multi-agent
systems capable of intelligently and conversationally delivering smart mobility
services to urban commuters, transportation operators, and decision-makers. Our
approach seeks to foster an autonomous and intelligent approach that (a)
promotes science-based advisory to reduce traffic congestion, accidents, and
carbon emissions at multiple scales, (b) facilitates public education and
engagement in participatory mobility management, and (c) automates specialized
transportation management tasks and the development of critical ITS platforms,
such as data analytics and interpretation, knowledge representation, and
traffic simulations. By integrating LLM and RAG, our approach seeks to overcome
the limitations of traditional rule-based multi-agent systems, which rely on
fixed knowledge bases and limited reasoning capabilities. This integration
paves the way for a more scalable, intuitive, and automated multi-agent
paradigm, driving advancements in ITS and urban mobility.</description>
      <guid isPermaLink="false">2409.00494v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Weakly-supervised Camera Localization by Ground-to-satellite Image Registration</title>
      <link>http://arxiv.org/abs/2409.06471v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ECCV 2024&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 地面与卫星图像匹配/检索最初用于城市规模的地面相机定位。&lt;br&gt;&lt;h4&gt;2. 研究目标&lt;/h4&gt;   - 本文旨在通过地面与卫星图像匹配在粗略定位和方向获取后，提高相机姿态的准确性。这些粗略数据可以来自城市规模检索或消费级GPS和指南针传感器。&lt;br&gt;&lt;h4&gt;3. 现有方法的局限&lt;/h4&gt;   - 现有基于学习的方法需要准确的GPS标签来训练网络，但获取这些标签困难且成本高，通常需要昂贵的实时动态定位（RTK）设备，并且会受到信号遮挡和多路径信号干扰的影响。&lt;br&gt;&lt;h4&gt;4. 提出的新方法&lt;/h4&gt;   - 本文提出了一种弱监督学习策略，用于在仅有噪声姿态标签的情况下进行地面与卫星图像的配准。&lt;br&gt;&lt;h4&gt;5. 方法细节&lt;/h4&gt;   - 为每个地面图像推导出正负卫星图像，并利用对比学习来学习地面和卫星图像的特征表示，以便进行位移估计。&lt;br&gt;   - 还提出了一种自我监督策略，用于估计跨视图图像的相对旋转，通过创建伪查询和参考图像对来训练网络。&lt;br&gt;&lt;h4&gt;6. 实验结果&lt;/h4&gt;   - 实验结果表明，所提出的弱监督学习策略在跨区域评估中表现优于依赖准确姿态标签的最新先进方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/yujiaoshi/g2sweakly&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The ground-to-satellite image matching/retrieval was initially proposed for
city-scale ground camera localization. This work addresses the problem of
improving camera pose accuracy by ground-to-satellite image matching after a
coarse location and orientation have been obtained, either from the city-scale
retrieval or from consumer-level GPS and compass sensors. Existing
learning-based methods for solving this task require accurate GPS labels of
ground images for network training. However, obtaining such accurate GPS labels
is difficult, often requiring an expensive {\color{black}Real Time Kinematics
(RTK)} setup and suffering from signal occlusion, multi-path signal
disruptions, \etc. To alleviate this issue, this paper proposes a weakly
supervised learning strategy for ground-to-satellite image registration when
only noisy pose labels for ground images are available for network training. It
derives positive and negative satellite images for each ground image and
leverages contrastive learning to learn feature representations for ground and
satellite images useful for translation estimation. We also propose a
self-supervision strategy for cross-view image relative rotation estimation,
which trains the network by creating pseudo query and reference image pairs.
Experimental results show that our weakly supervised learning strategy achieves
the best performance on cross-area evaluation compared to recent
state-of-the-art methods that are reliant on accurate pose labels for
supervision.</description>
      <guid isPermaLink="false">2409.06471v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Active Galactic Nuclei in the Green Valley at z$\sim$0.7</title>
      <link>http://arxiv.org/abs/2409.03197v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23 pages, 6 figures, 3 tables. Accepted for publication in ApJ&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 本文采用近红外光谱（NIR spectroscopy）和MMT/MMIRS对二十九个大质量星系（$\mathrm{log\ M_* / M_{\odot} \gtrsim10}$）在红移$\mathrm{z\sim0.7}$的样本进行研究，结合了LEGA-C调查的光谱数据。&lt;br&gt;&lt;h4&gt;2. 数据优势&lt;/h4&gt;   - 拥有光学和近红外光谱，使得能够测量完整的静态光学强发射线，进而研究电离源和活跃星系核（AGN）的选择，以及测量修正尘埃影响后的$\mathrm{H\alpha}$基础星形成率（SFR）。&lt;br&gt;&lt;h4&gt;3. AGN发现&lt;/h4&gt;   - 在二十九个星系中，发现有十一处星系存在AGN。&lt;br&gt;&lt;h4&gt;4. 星形成历史推断&lt;/h4&gt;   - 使用SED拟合代码\texttt{Prospector}推断非参数星形成历史，并根据最新的特定星形成率（sSFR）将星系分类为星形成、绿色谷地或静止。&lt;br&gt;&lt;h4&gt;5. AGN与星形成的关系&lt;/h4&gt;   - 探讨AGN活动与受抑制星形成之间的联系，发现绿色谷地或以下的星系中有$89\pm15\%$的星系存在AGN，而绿色谷地以上的星系中仅有$15\%\pm8\%$存在AGN。&lt;br&gt;&lt;h4&gt;6. 星形成主序列构建&lt;/h4&gt;   - 构建星形成主序列（SFMS），发现AGN宿主星系的SFR比SFMS低0.37 dex，而没有可检测AGN的星系则与SFMS一致。&lt;br&gt;&lt;h4&gt;7. 质量匹配样本分析&lt;/h4&gt;   - 与经过重抽样的质量匹配样本比较，AGN宿主星系的SFR与完整的LEGA-C样本一致。&lt;br&gt;&lt;h4&gt;8. 结论&lt;/h4&gt;   - 基于质量匹配分析，无法排除高质量AGN样本中星形成抑制可能由其他相关过程驱动，因此不能将AGN活动与星形成的熄灭直接联系起来。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.3847/1538-4357/ad74f1&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present NIR spectroscopy using MMT/MMIRS for a sample of twenty-nine
massive galaxies ($\mathrm{log\ M_* / M_{\odot} \gtrsim10}$) at
$\mathrm{z\sim0.7}$ with optical spectroscopy from the LEGA-C survey. Having
both optical and NIR spectroscopy at this redshift allows us to measure the
full suite of rest-optical strong emission lines, enabling the study of
ionization sources and the rest-optical selection of active galactic nuclei
(AGN), as well as the measurement of dust-corrected $\mathrm{H\alpha}$-based
SFRs. We find that eleven out of twenty-nine galaxies host AGN. We infer the
nonparametric star formation histories with the SED fitting code
\texttt{Prospector} and classify galaxies as star-forming, green valley, or
quiescent based on their most recent sSFRs. We explore the connection between
AGN activity and suppressed star formation and find that $89\pm15\%$ of
galaxies in the green valley or below host AGN, while only $15\%\pm8\%$ of
galaxies above the green valley host AGN. We construct the star-forming main
sequence (SFMS) and find that the AGN host galaxies are 0.37 dex below the SFMS
while galaxies without detectable AGN are consistent with being on the SFMS.
However, when compared to a bootstrapped mass-matched sample, the SFRs of our
sample of AGN host galaxies are consistent with the full LEGA-C sample. Based
on this mass-matched analysis, we cannot rule out that this suppression of star
formation is driven by other processes associated with the higher mass of the
AGN sample. We therefore cannot link the presence of AGN activity to the
quenching of star formation.</description>
      <guid isPermaLink="false">2409.03197v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>A novel pedestrian road crossing simulator for dynamic traffic light scheduling systems</title>
      <link>http://arxiv.org/abs/2409.11623v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 智能交通系统的重大进展正在推动社会服务朝向自主化发展，路面管理需要更灵活以应对变化并保持最佳性能。&lt;br&gt;&lt;h4&gt;2. 问题陈述&lt;/h4&gt;   - 当前对行人体验的关注不足，尤其是在行人密集的城市环境中，信号交叉口的设计显得尤为重要。&lt;br&gt;&lt;h4&gt;3. 研究目标&lt;/h4&gt;   - 本文提出了一种新颖的环境，用于在信号化人行横道上进行人类运动的细粒度模拟。&lt;br&gt;&lt;h4&gt;4. 模拟功能&lt;/h4&gt;   - 该模拟不仅能够捕捉典型行为，还能处理大规模行人群体从两个方向过马路的情况。&lt;br&gt;&lt;h4&gt;5. 应用价值&lt;/h4&gt;   - 提出的模拟器对优化路面配置管理至关重要，考虑了行人的体验质量（例如等待时间）。&lt;br&gt;&lt;h4&gt;6. 验证结果&lt;/h4&gt;   - 使用实地数据验证的结果显示，估计的过街时间可达到98.37%的准确性。&lt;br&gt;&lt;h4&gt;7. 合成数据结果&lt;/h4&gt;   - 使用合成数据的结果表明，模拟器能够优化交通信号调度，减少行人的等待时间，而不影响车辆通行能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1080/15472450.2023.2186229&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The major advances in intelligent transportation systems are pushing societal
services toward autonomy where road management is to be more agile in order to
cope with changes and continue to yield optimal performance. However, the
pedestrian experience is not sufficiently considered. Particularly, signalized
intersections are expected to be popular if not dominant in urban settings
where pedestrian density is high. This paper presents the design of a novel
environment for simulating human motion on signalized crosswalks at a
fine-grained level. Such a simulation not only captures typical behavior, but
also handles cases where large pedestrian groups cross from both directions.
The proposed simulator is instrumental for optimized road configuration
management where the pedestrians' quality of experience, for example, waiting
time, is factored in. The validation results using field data show that an
accuracy of 98.37 percent can be obtained for the estimated crossing time.
Other results using synthetic data show that our simulator enables optimized
traffic light scheduling that diminishes pedestrians' waiting time without
sacrificing vehicular throughput.</description>
      <guid isPermaLink="false">2409.11623v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Label-free correlative morpho-chemical tomography of 3D kidney mesangial cells</title>
      <link>http://arxiv.org/abs/2409.10971v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 无标记生物样本特征化旨在补充现有成像技术，避免使用会干扰活体样本原生状态的对比剂。&lt;br&gt;&lt;h4&gt;2. 传统技术的挑战&lt;/h4&gt;   - 传统的无标记光学成像技术适用于活体样本，但存在以下挑战：&lt;br&gt;     - 较差的切片能力&lt;br&gt;     - 形态信息片段化&lt;br&gt;     - 缺乏化学特异性信息&lt;br&gt;&lt;h4&gt;3. 创新方法&lt;/h4&gt;   - 本研究结合了**同时无标记自发荧光多谐波显微镜（SLAM）**和**梯度光干涉显微镜（GLIM）**，用于提取3D培养的肾小球细胞的化学特异性和形态学信息。&lt;br&gt;&lt;h4&gt;4. 研究重要性&lt;/h4&gt;   - 成像3D体外肾脏模型对于理解肾脏功能和病理至关重要。&lt;br&gt;&lt;h4&gt;5. 成像与定量&lt;/h4&gt;   - 该关联方法使得细胞的成像和定量成为可能，提取形态和化学特异性信号，这对于理解肾脏功能至关重要。&lt;br&gt;&lt;h4&gt;6. SLAM的优势&lt;/h4&gt;   - SLAM提供了一个非线性成像平台，使用单一激发源同时获取自发荧光（FAD和NAD(P)H）、第二和第三谐波信号。&lt;br&gt;&lt;h4&gt;7. GLIM的补充&lt;/h4&gt;   - GLIM获取高对比度的定量相位信息，能够量化厚度达250微米的样本中的结构变化。&lt;br&gt;&lt;h4&gt;8. 研究成果&lt;/h4&gt;   - 相关成像结果展示了一个多功能且无麻烦的平台，用于形态-化学细胞断层成像，以研究肾小球细胞在受控生理条件下的代谢和基质沉积等功能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Label-free characterization of biological specimens seeks to supplement
existing imaging techniques and avoid the need for contrast agents that can
disturb the native state of living samples. Conventional label-free optical
imaging techniques are compatible with living samples but face challenges such
as poor sectioning capability, fragmentary morphology, and lack chemical
specific information. Here, we combined simultaneous label-free
autofluorescence multi-harmonic (SLAM) microscopy and gradient light
interference microscopy (GLIM) to extract both chemical specific and
morphological tomography of 3D cultured kidney mesangial cells. Imaging 3D in
vitro kidney models is essential to understand kidney function and pathology.
Our correlative approach enables imaging and quantification of these cells to
extract both morphology and chemical-specific signals that is crucial for
understanding kidney function. In our approach, SLAM offers a nonlinear imaging
platform with a single-excitation source to simultaneously acquire
autofluorescence (FAD and NAD(P)H), second, and third harmonic signal from the
3D cultured cells. Complementarily, GLIM acquires high-contrast quantitative
phase information to quantify structural changes in samples with thickness of
up to 250 micron. Our correlative imaging results demonstrate a versatile and
hassle-free platform for morpho-chemical cellular tomography to investigate
functions such as metabolism and matrix deposition of kidney mesangial cells in
3D under controlled physiological conditions.</description>
      <guid isPermaLink="false">2409.10971v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Continual Learning for Smart City: A Survey</title>
      <link>http://arxiv.org/abs/2404.00983v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint. Work in Progress&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 背景介绍&lt;/h4&gt;   - 随着现代城市的数字化，大量数据和强大的计算资源促进了智能模型在智慧城市中的快速更新。&lt;br&gt;&lt;h4&gt;2. 持续学习的定义&lt;/h4&gt;   - 持续学习（CL）是一种新颖的机器学习范式，能够不断更新模型，以适应变化的环境，学习任务、数据和分布随时间变化。&lt;br&gt;&lt;h4&gt;3. 综述内容结构&lt;/h4&gt;   - 本文的内容分为三个部分：&lt;br&gt;   &lt;h4&gt;1. 方法论&lt;/h4&gt;      - 对基本的CL方法和先进的CL框架进行分类，结合其他学习范式，如图学习、时空学习、多模态学习和联邦学习。&lt;br&gt;   &lt;h4&gt;2. 应用&lt;/h4&gt;      - 介绍多个CL应用领域，包括交通、环境、公共卫生、安全、网络及相关的城市计算数据集。&lt;br&gt;   &lt;h4&gt;3. 挑战&lt;/h4&gt;      - 讨论当前面临的问题和挑战，并展望几条有前景的研究方向。&lt;br&gt;&lt;h4&gt;4. 研究价值&lt;/h4&gt;   - 本综述旨在帮助相关研究人员快速熟悉智慧城市发展中持续学习研究的现状，并指引未来研究趋势。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-04-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the digitization of modern cities, large data volumes and powerful
computational resources facilitate the rapid update of intelligent models
deployed in smart cities. Continual learning (CL) is a novel machine learning
paradigm that constantly updates models to adapt to changing environments,
where the learning tasks, data, and distributions can vary over time. Our
survey provides a comprehensive review of continual learning methods that are
widely used in smart city development. The content consists of three parts: 1)
Methodology-wise. We categorize a large number of basic CL methods and advanced
CL frameworks in combination with other learning paradigms including graph
learning, spatial-temporal learning, multi-modal learning, and federated
learning. 2) Application-wise. We present numerous CL applications covering
transportation, environment, public health, safety, networks, and associated
datasets related to urban computing. 3) Challenges. We discuss current problems
and challenges and envision several promising research directions. We believe
this survey can help relevant researchers quickly familiarize themselves with
the current state of continual learning research used in smart city development
and direct them to future research trends.</description>
      <guid isPermaLink="false">2404.00983v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Non-Orthogonal Multiple-Access Strategies for Direct-to-Satellite IoT Networks</title>
      <link>http://arxiv.org/abs/2409.02748v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 8 figures&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 直接到卫星物联网（DtS-IoT）具有支持多个领域的潜力，包括农业、工业、智能城市和环境灾害预防。&lt;br&gt;&lt;h4&gt;2. 新方案介绍&lt;/h4&gt;   - 本文提出两种新型的DtS-IoT方案，采用功率域非正交多址接入（NOMA）进行上行传输，分别为固定传输功率（FTP）和受控传输功率（CTP）。&lt;br&gt;&lt;h4&gt;3. 技术假设&lt;/h4&gt;   - 假设物联网设备使用LoRa技术向配备有连续干扰消除（SIC）网关的轨道卫星传输数据包。&lt;br&gt;   - 假设物联网设备具备卫星轨道预测能力。&lt;br&gt;&lt;h4&gt;4. 性能评估&lt;/h4&gt;   - 使用真实的地理位置和轨迹数据，评估传输成功解码的平均数量、良好吞吐量（bytes/lap）和能量消耗（bytes/Joule）与网络设备数量的关系。&lt;br&gt;&lt;h4&gt;5. 数值结果&lt;/h4&gt;   - 结果显示所提方案在良好吞吐量和能量效率之间存在权衡。&lt;br&gt;   - 对比FTP和CTP与常规ALOHA在100个（600个）设备下的表现，发现良好吞吐量分别提高了65%（29%）和52%（101%）。&lt;br&gt;&lt;h4&gt;6. CTP的优势&lt;/h4&gt;   - CTP在网络规模增大时有效利用传输机会，表现优于其他策略。&lt;br&gt;   - 在能量效率方面，CTP的表现优于FTP和ALOHA。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Direct-to-Satellite IoT (DtS-IoT) has the potential to support multiple
verticals, including agriculture, industry, smart cities, and environmental
disaster prevention. This work introduces two novel DtS-IoT schemes using power
domain NonOrthogonal Multiple Access (NOMA) in the uplink with either fixed
(FTP) or controlled (CTP) transmit power. We consider that the IoT devices use
LoRa technology to transmit data packets to the satellite in orbit, equipped
with a Successive Interference Cancellation (SIC)-enabled gateway. We also
assume the IoT devices are empowered with a predictor of the satellite orbit.
Using real geographic location and trajectory data, we evaluate the performance
of the average number of successfully decoded transmissions, goodput
(bytes/lap), and energy consumption (bytes/Joule) as a function of the number
of network devices. Numerical results show the trade-off between goodput and
energy efficiency for both proposed schemes. Comparing FTP and CTP with regular
ALOHA for 100 (600) devices, we find goodput improvements of 65% (29%) and 52%
(101%), respectively. Notably, CTP effectively leverages transmission
opportunities as the network size increases, outperforming the other
strategies. Moreover, CTP shows the best performance in energy efficiency
compared to FTP and ALOHA.</description>
      <guid isPermaLink="false">2409.02748v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>GeoCalib: Learning Single-image Calibration with Geometric Optimization</title>
      <link>http://arxiv.org/abs/2409.06704v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Presented at ECCV 2024&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 背景介绍&lt;/h4&gt;   - 从单幅图像中，可以利用视觉线索推断相机的内在和外在参数，例如焦距和重力方向。&lt;br&gt;   - 单幅图像校准对图像编辑和3D建图等下游应用有益。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 目前的方法主要基于经典几何（如线条和消失点）或端到端训练的深度神经网络。&lt;br&gt;   - 学习型方法更为稳健，但在新环境中泛化能力较差，且准确度低于经典方法。&lt;br&gt;&lt;h4&gt;3. 假设与创新&lt;/h4&gt;   - 论文假设现有学习方法缺乏3D几何提供的约束。&lt;br&gt;   - 本文提出GeoCalib，一种通过优化过程利用3D几何的深度神经网络。&lt;br&gt;&lt;h4&gt;4. GeoCalib的特点&lt;/h4&gt;   - GeoCalib通过端到端训练来估计相机参数，并从数据中学习有用的视觉线索。&lt;br&gt;&lt;h4&gt;5. 实验结果&lt;/h4&gt;   - 在多个基准测试中的实验表明，GeoCalib在稳健性和准确性上优于现有的经典和学习方法。&lt;br&gt;   - 其内部优化过程能够估计不确定性，有助于标记失败案例，并对视觉定位等下游应用有益。&lt;br&gt;&lt;h4&gt;6. 资源共享&lt;/h4&gt;   - 代码和训练模型已公开，地址为 [GitHub - GeoCalib](https://github.com/cvg/GeoCalib)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/cvg/geocalib&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; From a single image, visual cues can help deduce intrinsic and extrinsic
camera parameters like the focal length and the gravity direction. This
single-image calibration can benefit various downstream applications like image
editing and 3D mapping. Current approaches to this problem are based on either
classical geometry with lines and vanishing points or on deep neural networks
trained end-to-end. The learned approaches are more robust but struggle to
generalize to new environments and are less accurate than their classical
counterparts. We hypothesize that they lack the constraints that 3D geometry
provides. In this work, we introduce GeoCalib, a deep neural network that
leverages universal rules of 3D geometry through an optimization process.
GeoCalib is trained end-to-end to estimate camera parameters and learns to find
useful visual cues from the data. Experiments on various benchmarks show that
GeoCalib is more robust and more accurate than existing classical and learned
approaches. Its internal optimization estimates uncertainties, which help flag
failure cases and benefit downstream applications like visual localization. The
code and trained models are publicly available at
https://github.com/cvg/GeoCalib.</description>
      <guid isPermaLink="false">2409.06704v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>The Arizona Molecular ISM Survey with the SMT: Variations in the CO(2-1)/CO(1-0) Line Ratio Across the Galaxy Population</title>
      <link>http://arxiv.org/abs/2409.03963v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to ApJ&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;1. **研究背景**:&lt;br&gt;   - 碳氧化合物的J=1→0光谱线（CO(1-0)）是分子气体的标准探测器，而CO(2-1)线常被用于替代。&lt;br&gt;2. **使用假设**:&lt;br&gt;   - 使用CO(2-1)的前提是了解CO(2-1)与CO(1-0)光度比（r21），假设该比率可以用来推断CO(1-0)的光度和分子气体质量。&lt;br&gt;3. **数据集**:&lt;br&gt;   - 本研究对122个星系进行了r21的测量，涵盖了从10^9到10^11.5 M$_\odot$的星系质量和0.08到35 M$_\odot$/yr的星形成率（SFR）。&lt;br&gt;4. **主要发现**:&lt;br&gt;   - 发现r21与星形成率、星形成表面密度、星形成效率及星形成主序列（SFMS）的偏离之间存在显著趋势。&lt;br&gt;5. **偏差影响**:&lt;br&gt;   - 假设r21为常数可能在星系群体研究中引入偏差，影响重要星系标度关系的恢复，如Kennicutt-Schmidt法则和SFMS偏离与星形成效率之间的关系。&lt;br&gt;6. **提出的方法**:&lt;br&gt;   - 提供了一种考虑r21随星形成率变化的处方，能够在仅有一条线的情况下进行CO(2-1)与CO(1-0)之间的转换。&lt;br&gt;7. **匹配验证**:&lt;br&gt;   - 该处方与AMISS和文献样本中的r21变化相匹配，可以用于从CO(2-1)观测中推导更准确的气体质量。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The J=1$\rightarrow$0 spectral line of carbon monoxide (CO(1-0)) is the
canonical tracer of molecular gas. However, CO(2-1) is frequently used in its
place, following the assumption that the higher energy line can be used to
infer the CO(1-0) luminosity and molecular gas mass. The use of CO(2-1) depends
on a knowledge of the ratio between CO(2-1) and CO(1-0) luminosities, r21. Here
we present galaxy-integrated r21 measurements for 122 galaxies spanning stellar
masses from 10$^9$ to 10$^{11.5}$ M$_\odot$ and star formation rates (SFRs)
from 0.08 to 35 M$_\odot$/yr. We find strong trends between r21 and SFR, SFR
surface density, star formation efficiency, and distance from the star
formation main sequence (SFMS). We show that the assumption of a constant r21
can introduce biases into the molecular gas trends in galaxy population studies
and demonstrate how this affects the recovery of important galaxy scaling
relations, including the Kennicutt-Schmidt law and the relation between SFMS
offset and star formation efficiency. We provide a prescription which accounts
for variations in r21 as a function of SFR and can be used to convert between
CO(2-1) and CO(1-0) when only one line is available. Our prescription matches
variations in r21 for both AMISS and literature samples and can be used to
derive more accurate gas masses from CO(2-1) observations.</description>
      <guid isPermaLink="false">2409.03963v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>A machine learning framework for acoustic reflector mapping</title>
      <link>http://arxiv.org/abs/2409.12094v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;1. **研究背景**:&lt;br&gt;   - 声纳基础的室内映射系统在机器人技术中已被广泛应用数十年，尤其是在水下和管道检查环境中。&lt;br&gt;2. **现状与挑战**:&lt;br&gt;   - 尽管声纳系统仍是主流，但由于对噪声的脆弱性，逐渐被其他技术（如摄像头和激光雷达）所取代，这些技术在不断进步中。&lt;br&gt;3. **声纳的优势**:&lt;br&gt;   - 使用声学信号和回声定位进行环境映射在恶劣场景中具有显著优势，因为声纳传感器在低光照和非反射墙面条件下表现更佳。&lt;br&gt;4. **噪声处理问题**:&lt;br&gt;   - 为了生成准确的地图，声学传感器必须有效处理背景噪声，传统信号处理技术在这些情况下并不总是有效。&lt;br&gt;5. **提出的方法**:&lt;br&gt;   - 本文提出一个框架，利用机器学习辅助传统信号处理方法，去除生成地图中的异常值和伪影，以应对背景噪声。&lt;br&gt;6. **研究目标**:&lt;br&gt;   - 旨在证明传统回声定位映射技术的性能可以在特别嘈杂的条件下显著提升，从而促进声学传感器在现代多模态机器人导航系统中的应用。&lt;br&gt;7. **实验结果**:&lt;br&gt;   - 模拟评估表明该系统在信噪比为-10 dB的情况下能够可靠运行，并且能够在不同的混响环境中操作。&lt;br&gt;8. **实际应用示例**:&lt;br&gt;   - 本文还使用提出的方法在模拟房间中进行映射，展示了其在机器人平台上的应用潜力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sonar-based indoor mapping systems have been widely employed in robotics for
several decades. While such systems are still the mainstream in underwater and
pipe inspection settings, the vulnerability to noise reduced, over time, their
general widespread usage in favour of other modalities(\textit{e.g.}, cameras,
lidars), whose technologies were encountering, instead, extraordinary
advancements. Nevertheless, mapping physical environments using acoustic
signals and echolocation can bring significant benefits to robot navigation in
adverse scenarios, thanks to their complementary characteristics compared to
other sensors. Cameras and lidars, indeed, struggle in harsh weather
conditions, when dealing with lack of illumination, or with non-reflective
walls. Yet, for acoustic sensors to be able to generate accurate maps, noise
has to be properly and effectively handled. Traditional signal processing
techniques are not always a solution in those cases. In this paper, we propose
a framework where machine learning is exploited to aid more traditional signal
processing methods to cope with background noise, by removing outliers and
artefacts from the generated maps using acoustic sensors. Our goal is to
demonstrate that the performance of traditional echolocation mapping techniques
can be greatly enhanced, even in particularly noisy conditions, facilitating
the employment of acoustic sensors in state-of-the-art multi-modal robot
navigation systems. Our simulated evaluation demonstrates that the system can
reliably operate at an SNR of $-10$dB. Moreover, we also show that the proposed
method is capable of operating in different reverberate environments. In this
paper, we also use the proposed method to map the outline of a simulated room
using a robotic platform.</description>
      <guid isPermaLink="false">2409.12094v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Hypergraph-based Motion Generation with Multi-modal Interaction Relational Reasoning</title>
      <link>http://arxiv.org/abs/2409.11676v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;1. **研究背景**:&lt;br&gt;   - 真实驾驶环境复杂多变，涉及多辆车之间的动态互动及其未来状态的多样性，这使得准确预测车辆运动状态及应对预测不确定性变得困难。&lt;br&gt;2. **挑战**:&lt;br&gt;   - 需要全面建模和推理，以捕捉车辆之间的隐含关系及其多样化行为。&lt;br&gt;3. **研究目标**:&lt;br&gt;   - 本研究提出一个集成框架，用于自主车辆（AVs）的运动预测，以应对这些复杂性。&lt;br&gt;4. **方法介绍**:&lt;br&gt;   - 引入了一种新颖的关系超图互动信息神经运动生成器（RHINO）。&lt;br&gt;5. **技术特点**:&lt;br&gt;   - RHINO利用基于超图的关系推理，整合多尺度超图神经网络，建模多辆车之间的群体互动及其多模态驾驶行为，从而提高运动预测的准确性和可靠性。&lt;br&gt;6. **实验验证**:&lt;br&gt;   - 使用真实世界数据集进行的实验验证表明，该框架在提高预测准确性和促进动态交通场景中社会意识的自动驾驶方面表现优越。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The intricate nature of real-world driving environments, characterized by
dynamic and diverse interactions among multiple vehicles and their possible
future states, presents considerable challenges in accurately predicting the
motion states of vehicles and handling the uncertainty inherent in the
predictions. Addressing these challenges requires comprehensive modeling and
reasoning to capture the implicit relations among vehicles and the
corresponding diverse behaviors. This research introduces an integrated
framework for autonomous vehicles (AVs) motion prediction to address these
complexities, utilizing a novel Relational Hypergraph Interaction-informed
Neural mOtion generator (RHINO). RHINO leverages hypergraph-based relational
reasoning by integrating a multi-scale hypergraph neural network to model
group-wise interactions among multiple vehicles and their multi-modal driving
behaviors, thereby enhancing motion prediction accuracy and reliability.
Experimental validation using real-world datasets demonstrates the superior
performance of this framework in improving predictive accuracy and fostering
socially aware automated driving in dynamic traffic scenarios.</description>
      <guid isPermaLink="false">2409.11676v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>GLC-SLAM: Gaussian Splatting SLAM with Efficient Loop Closure</title>
      <link>http://arxiv.org/abs/2409.10982v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;1. **研究背景**:&lt;br&gt;   - 3D高斯点云（3D Gaussian Splatting, 3DGS）在密集的同时定位与地图构建（SLAM）中受到广泛关注，实现了实时渲染和高保真映射。&lt;br&gt;2. **现有问题**:&lt;br&gt;   - 现有基于3DGS的SLAM方法常常面临跟踪误差累积和地图漂移的问题，尤其是在大规模环境中。&lt;br&gt;3. **提出的新方法**:&lt;br&gt;   - 本文介绍了一种新的SLAM系统，称为GLC-SLAM，整合了相机姿态和场景模型的全局优化。&lt;br&gt;4. **技术细节**:&lt;br&gt;   - 采用帧到模型的跟踪方法，并通过全球到局部的策略触发分层回环闭合，以最小化漂移累积。&lt;br&gt;   - 将场景划分为3D高斯子地图，以便在大场景中进行高效的地图更新。&lt;br&gt;5. **关键帧选择策略**:&lt;br&gt;   - 引入了一种不确定性最小化的关键帧选择策略，优先选择观察到更多有价值3D高斯的关键帧，以增强子地图优化。&lt;br&gt;6. **实验结果**:&lt;br&gt;   - 在多个数据集上的实验结果表明，GLC-SLAM在跟踪和映射性能上优于或与现有的最先进密集RGB-D SLAM系统具有竞争力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Gaussian Splatting (3DGS) has gained significant attention for its
application in dense Simultaneous Localization and Mapping (SLAM), enabling
real-time rendering and high-fidelity mapping. However, existing 3DGS-based
SLAM methods often suffer from accumulated tracking errors and map drift,
particularly in large-scale environments. To address these issues, we introduce
GLC-SLAM, a Gaussian Splatting SLAM system that integrates global optimization
of camera poses and scene models. Our approach employs frame-to-model tracking
and triggers hierarchical loop closure using a global-to-local strategy to
minimize drift accumulation. By dividing the scene into 3D Gaussian submaps, we
facilitate efficient map updates following loop corrections in large scenes.
Additionally, our uncertainty-minimized keyframe selection strategy prioritizes
keyframes observing more valuable 3D Gaussians to enhance submap optimization.
Experimental results on various datasets demonstrate that GLC-SLAM achieves
superior or competitive tracking and mapping performance compared to
state-of-the-art dense RGB-D SLAM systems.</description>
      <guid isPermaLink="false">2409.10982v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>ODMixer: Fine-grained Spatial-temporal MLP for Metro Origin-Destination Prediction</title>
      <link>http://arxiv.org/abs/2404.15734v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  In Peer Review. Our code is available at
  https://github.com/KLatitude/ODMixer&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;1. **研究主题**:&lt;br&gt;   - 地铁出发-目的地（OD）预测是城市计算中的一个重要且具有挑战性的时空预测任务，旨在准确预测跨站点的乘客流量，以优化地铁调度并提高整体交通效率。&lt;br&gt;2. **分析重要性**:&lt;br&gt;   - 有效分析站点之间的细粒度和全面关系对于进行地铁OD预测至关重要。&lt;br&gt;3. **现有问题**:&lt;br&gt;   - 现有的地铁OD模型往往要么从站点的角度混合多个OD对的信息，要么仅专注于某些OD对。这些方法可能忽视OD对之间的细粒度关系，导致在预测潜在异常情况时遇到困难。&lt;br&gt;4. **提出的方法**:&lt;br&gt;   - 为解决这些挑战，本文从所有OD对的角度分析交通变化，并提出了一种细粒度时空多层感知机（MLP）架构，称为ODMixer。&lt;br&gt;5. **ODMixer结构**:&lt;br&gt;   - ODMixer具有双分支结构，包括通道混合器、视角混合器和双向趋势学习器。&lt;br&gt;     - **通道混合器**：旨在捕捉OD对之间的短期时间关系。&lt;br&gt;     - **视角混合器**：专注于从出发地和目的地两个角度捕捉关系。&lt;br&gt;     - **双向趋势学习器**：用于建模长期时间关系。&lt;br&gt;6. **实验验证**:&lt;br&gt;   - 在两个大型地铁OD预测数据集（HZMOD和SHMO）上进行的广泛实验验证了ODMixer的优势。&lt;br&gt;7. **代码可用性**:&lt;br&gt;   - 相关代码已公开，提供在GitHub（https://github.com/KLatitude/ODMixer）上访问。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-04-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/klatitude/odmixer&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Metro Origin-Destination (OD) prediction is a crucial yet challenging
spatial-temporal prediction task in urban computing, which aims to accurately
forecast cross-station ridership for optimizing metro scheduling and enhancing
overall transport efficiency. Analyzing fine-grained and comprehensive
relations among stations effectively is imperative for metro OD prediction.
However, existing metro OD models either mix information from multiple OD pairs
from the station's perspective or exclusively focus on a subset of OD pairs.
These approaches may overlook fine-grained relations among OD pairs, leading to
difficulties in predicting potential anomalous conditions. To address these
challenges, we analyze traffic variations from the perspective of all OD pairs
and propose a fine-grained spatial-temporal MLP architecture for metro OD
prediction, namely ODMixer. Specifically, our ODMixer has double-branch
structure and involves the Channel Mixer, the Multi-view Mixer, and the
Bidirectional Trend Learner. The Channel Mixer aims to capture short-term
temporal relations among OD pairs, the Multi-view Mixer concentrates on
capturing relations from both origin and destination perspectives. To model
long-term temporal relations, we introduce the Bidirectional Trend Learner.
Extensive experiments on two large-scale metro OD prediction datasets HZMOD and
SHMO demonstrate the advantages of our ODMixer. Our code is available at
https://github.com/KLatitude/ODMixer.</description>
      <guid isPermaLink="false">2404.15734v3</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>FedModule: A Modular Federated Learning Framework</title>
      <link>http://arxiv.org/abs/2409.04849v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;1. **研究背景**:&lt;br&gt;   - 联邦学习（Federated Learning, FL）在医疗、金融和智慧城市等多个应用领域得到了广泛采用。&lt;br&gt;2. **现有问题**:&lt;br&gt;   - 随着实验场景的复杂性增加，现有的FL框架和基准测试未能跟上发展。&lt;br&gt;3. **提出的新框架**:&lt;br&gt;   - 本文介绍了FedModule，一种灵活且可扩展的FL实验框架，旨在支持多样的FL范式，并为复杂实验场景提供全面的基准测试。&lt;br&gt;4. **设计原则**:&lt;br&gt;   - FedModule遵循“一段代码，所有场景”的原则，采用模块化设计，将FL过程分解为独立组件，实现不同FL范式的无缝集成。&lt;br&gt;5. **支持的学习模式**:&lt;br&gt;   - 框架支持同步、异步和个性化的联邦学习，并实现了超过20种算法。&lt;br&gt;6. **实验验证**:&lt;br&gt;   - 在公共数据集上进行的实验展示了FedModule的灵活性和可扩展性。&lt;br&gt;7. **执行模式**:&lt;br&gt;   - FedModule提供多种执行模式，包括线性、线程、基于进程和分布式，用户可以根据实验需求定制设置。&lt;br&gt;8. **日志和测试功能**:&lt;br&gt;   - 框架提供广泛的日志记录和测试能力，便于对FL算法进行详细的性能分析。&lt;br&gt;9. **比较评估**:&lt;br&gt;   - 与现有FL工具包（如TensorFlow Federated、PySyft、Flower和FLGo）的比较评估突显了FedModule在可扩展性、灵活性和基准支持方面的优势。&lt;br&gt;10. **研究意义**:&lt;br&gt;    - FedModule通过解决当前FL框架的局限性，标志着FL实验的一次重要进展，为研究人员和实践者提供了一个强大的工具，用于在广泛场景中开发和评估FL算法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/nuaa-smartsensing/async-fl&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Federated learning (FL) has been widely adopted across various applications,
such as healthcare, finance, and smart cities. However, as experimental
scenarios become more complex, existing FL frameworks and benchmarks have
struggled to keep pace. This paper introduces FedModule, a flexible and
extensible FL experimental framework that has been open-sourced to support
diverse FL paradigms and provide comprehensive benchmarks for complex
experimental scenarios. FedModule adheres to the "one code, all scenarios"
principle and employs a modular design that breaks the FL process into
individual components, allowing for the seamless integration of different FL
paradigms. The framework supports synchronous, asynchronous, and personalized
federated learning, with over 20 implemented algorithms. Experiments conducted
on public datasets demonstrate the flexibility and extensibility of FedModule.
The framework offers multiple execution modes-including linear, threaded,
process-based, and distributed-enabling users to tailor their setups to various
experimental needs. Additionally, FedModule provides extensive logging and
testing capabilities, which facilitate detailed performance analysis of FL
algorithms. Comparative evaluations against existing FL toolkits, such as
TensorFlow Federated, PySyft, Flower, and FLGo, highlight FedModule's superior
scalability, flexibility, and comprehensive benchmark support. By addressing
the limitations of current FL frameworks, FedModule marks a significant
advancement in FL experimentation, providing researchers and practitioners with
a robust tool for developing and evaluating FL algorithms across a wide range
of scenarios.</description>
      <guid isPermaLink="false">2409.04849v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Forecasting age distribution of life-table death counts via α-transformation</title>
      <link>http://arxiv.org/abs/2409.11658v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  25 pages, 6 tables, 5 figures&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;1. **研究主题**:&lt;br&gt;   - 本文介绍了一种组合功率变换，称为{\alpha}-变换，用于建模和预测生命表死亡人数的时间序列，尤其是在老年时可能观察到零死亡人数的情况。&lt;br&gt;2. **变换概念**:&lt;br&gt;   - {\alpha}-变换是等距对数比变换的推广（即，{\alpha} = 0），依赖于可数据驱动确定的调节参数{\alpha}。&lt;br&gt;3. **数据来源**:&lt;br&gt;   - 使用了1921至2020年的澳大利亚年龄特定期间生命表死亡人数数据进行分析。&lt;br&gt;4. **预测效果**:&lt;br&gt;   - 实验表明，{\alpha}-变换在短期点预测和区间预测方面比对数比变换产生更准确的结果。&lt;br&gt;5. **应用价值**:&lt;br&gt;   - 生命表死亡人数的改进预测准确性对人口统计学家和政府规划者估算生存概率和预期寿命，以及精算师确定不同初始年龄和到期期限的年金价格和储备具有重要意义。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce a compositional power transformation, known as an
{\alpha}-transformation, to model and forecast a time series of life-table
death counts, possibly with zero counts observed at older ages. As a
generalisation of the isometric log-ratio transformation (i.e., {\alpha} = 0),
the {\alpha} transformation relies on the tuning parameter {\alpha}, which can
be determined in a data-driven manner. Using the Australian age-specific period
life-table death counts from 1921 to 2020, the {\alpha} transformation can
produce more accurate short-term point and interval forecasts than the
log-ratio transformation. The improved forecast accuracy of life-table death
counts is of great importance to demographers and government planners for
estimating survival probabilities and life expectancy and actuaries for
determining annuity prices and reserves for various initial ages and maturity
terms.</description>
      <guid isPermaLink="false">2409.11658v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>HGSLoc: 3DGS-based Heuristic Camera Pose Refinement</title>
      <link>http://arxiv.org/abs/2409.10925v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;1. **研究背景**:&lt;br&gt;   - 视觉定位是确定相机在已知场景表示中的位姿和方向的过程，常受到光照变化和视角变化等因素的影响。&lt;br&gt;2. **提出的新方法**:&lt;br&gt;   - 本文提出HGSLoc，一种新型轻量级、即插即用的位姿优化框架，结合了3D重建和启发式优化策略，以实现更高的位姿估计准确性。&lt;br&gt;3. **几何地图引入**:&lt;br&gt;   - 引入明确的几何地图用于3D表示和高保真渲染，从而生成高质量合成视图，以支持准确的视觉定位。&lt;br&gt;4. **性能优势**:&lt;br&gt;   - 与基于NeRF的神经渲染定位方法相比，HGSLoc展现了更快的渲染速度和更高的定位准确性。&lt;br&gt;5. **启发式优化策略**:&lt;br&gt;   - 采用启发式优化策略，能够快速定位目标节点，并通过设定级别优化步骤来提高小误差场景中的位姿准确性。&lt;br&gt;6. **高效的错误减少**:&lt;br&gt;   - 通过精心设计的启发式函数，提供高效的优化能力，实现对粗略定位估计的快速错误减少。&lt;br&gt;7. **减少对复杂模型的依赖**:&lt;br&gt;   - 方法减轻了对复杂神经网络模型的依赖，并在噪声和挑战性环境中表现出更好的鲁棒性和更高的定位准确性。&lt;br&gt;8. **综合优势**:&lt;br&gt;   - 提出的优化框架通过整合3D重建和启发式优化策略的优势，为视觉定位引入了新颖的方法。&lt;br&gt;9. **实验结果**:&lt;br&gt;   - 在多个基准数据集（包括7Scenes和DB数据集）上展示了强劲的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual localization refers to the process of determining camera poses and
orientation within a known scene representation. This task is often complicated
by factors such as illumination changes and variations in viewing angles. In
this paper, we propose HGSLoc, a novel lightweight, plug and-play pose
optimization framework, which integrates 3D reconstruction with a heuristic
refinement strategy to achieve higher pose estimation accuracy. Specifically,
we introduce an explicit geometric map for 3D representation and high-fidelity
rendering, allowing the generation of high-quality synthesized views to support
accurate visual localization. Our method demonstrates a faster rendering speed
and higher localization accuracy compared to NeRF-based neural rendering
localization approaches. We introduce a heuristic refinement strategy, its
efficient optimization capability can quickly locate the target node, while we
set the step-level optimization step to enhance the pose accuracy in the
scenarios with small errors. With carefully designed heuristic functions, it
offers efficient optimization capabilities, enabling rapid error reduction in
rough localization estimations. Our method mitigates the dependence on complex
neural network models while demonstrating improved robustness against noise and
higher localization accuracy in challenging environments, as compared to neural
network joint optimization strategies. The optimization framework proposed in
this paper introduces novel approaches to visual localization by integrating
the advantages of 3D reconstruction and heuristic refinement strategy, which
demonstrates strong performance across multiple benchmark datasets, including
7Scenes and DB dataset.</description>
      <guid isPermaLink="false">2409.10925v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>KRONC: Keypoint-based Robust Camera Optimization for 3D Car Reconstruction</title>
      <link>http://arxiv.org/abs/2409.05407v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ECCVW&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;1. **研究背景**:&lt;br&gt;   - 从一组图像生成物体或场景的三维表示是一个广泛讨论的话题，尤其在NeRF（神经辐射场）方法流行后，受到了更多关注。&lt;br&gt;2. **关键挑战**:&lt;br&gt;   - 一个被低估的前提条件是相机位姿的知识，尤其是外部校准参数的估计。&lt;br&gt;3. **现有方法的局限**:&lt;br&gt;   - 尽管有优秀的通用运动结构（Structure-from-Motion）方法作为预处理步骤，但其计算负担较重，需要大量帧以确保视图之间的足够重叠。&lt;br&gt;4. **提出的新方法**:&lt;br&gt;   - 本文提出了KRONC，一种通过利用关于待重建物体的先验知识和语义关键点来推断视角位姿的新方法。&lt;br&gt;5. **应用场景**:&lt;br&gt;   - KRONC专注于车辆场景，能够将视点位置估计为一个轻量优化问题的解，目标是使关键点的反投影收敛到一个单一的点。&lt;br&gt;6. **实验验证**:&lt;br&gt;   - 收集了一个特定的真实世界汽车场景数据集以验证该方法。&lt;br&gt;7. **实验结果**:&lt;br&gt;   - 实验结果确认KRONC能够从非常粗略的初始化中生成优秀的相机位姿估计。&lt;br&gt;8. **性能对比**:&lt;br&gt;   - 结果与运动结构方法相媲美，同时在计算上节省了大量资源。&lt;br&gt;9. **公开资源**:&lt;br&gt;   - 代码和数据将公开发布。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The three-dimensional representation of objects or scenes starting from a set
of images has been a widely discussed topic for years and has gained additional
attention after the diffusion of NeRF-based approaches. However, an
underestimated prerequisite is the knowledge of camera poses or, more
specifically, the estimation of the extrinsic calibration parameters. Although
excellent general-purpose Structure-from-Motion methods are available as a
pre-processing step, their computational load is high and they require a lot of
frames to guarantee sufficient overlapping among the views. This paper
introduces KRONC, a novel approach aimed at inferring view poses by leveraging
prior knowledge about the object to reconstruct and its representation through
semantic keypoints. With a focus on vehicle scenes, KRONC is able to estimate
the position of the views as a solution to a light optimization problem
targeting the convergence of keypoints' back-projections to a singular point.
To validate the method, a specific dataset of real-world car scenes has been
collected. Experiments confirm KRONC's ability to generate excellent estimates
of camera poses starting from very coarse initialization. Results are
comparable with Structure-from-Motion methods with huge savings in computation.
Code and data will be made publicly available.</description>
      <guid isPermaLink="false">2409.05407v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Cognitive Hierarchy in Day-to-day Network Flow Dynamics</title>
      <link>http://arxiv.org/abs/2409.11908v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;1. **研究背景**:&lt;br&gt;   - 旅行者在做出路线决策时，可能会考虑其他人在未来一天的行为，从而使得昨天的最短路线变得不那么吸引人。&lt;br&gt;2. **实验现象**:&lt;br&gt;   - 这种现象在一项模拟旅行者重复日常出行过程的虚拟实验中得到了体现。&lt;br&gt;3. **现有模型的局限**:&lt;br&gt;   - 当前的日常交通动态模型未能准确再现实验中收集的流量演变数据。&lt;br&gt;4. **提出的新框架**:&lt;br&gt;   - 本文提出了一种基于认知层次理论的日常交通行为建模框架，考虑旅行者的战略推理能力。&lt;br&gt;5. **推理能力差异**:&lt;br&gt;   - 不同战略推理能力的旅行者在选择路线时，会形成对低层次旅行者能力的信念。&lt;br&gt;6. **模型扩展**:&lt;br&gt;   - 两个广泛研究的日常模型（网络调节过程动态和Logit动态）被扩展到该框架中，并作为实例进行研究。&lt;br&gt;7. **实验校准**:&lt;br&gt;   - 使用扩展的网络调节过程动态对虚拟实验进行校准，结果与实验数据较为吻合。&lt;br&gt;8. **平衡状态分析**:&lt;br&gt;   - 研究表明，两个扩展动态模型具有多个平衡状态，其中之一是经典用户均衡。&lt;br&gt;9. **稳定性分析**:&lt;br&gt;   - 由于存在多个平衡状态，分析全局稳定性变得复杂，但在平衡点附近的局部稳定性通过分析和数值实验得到了验证。&lt;br&gt;10. **关键参数的影响**:&lt;br&gt;    - 揭示了关键参数如何影响用户均衡的稳定性的一般性见解。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; When making route decisions, travelers may engage in a certain degree of
reasoning about what the others will do in the upcoming day, rendering
yesterday's shortest routes less attractive. This phenomenon was manifested in
a recent virtual experiment that mimicked travelers' repeated daily trip-making
process. Unfortunately, prevailing day-to-day traffic dynamical models failed
to faithfully reproduce the collected flow evolution data therein. To this end,
we propose a day-to-day traffic behavior modeling framework based on the
Cognitive Hierarchy theory, in which travelers with different levels of
strategic-reasoning capabilities form their own beliefs about lower-step
travelers' capabilities when choosing their routes. Two widely-studied
day-to-day models, the Network Tatonnement Process dynamic and the Logit
dynamic, are extended into the framework and studied as examples. Calibration
of the virtual experiment is performed using the extended Network Tatonnement
Process dynamic, which fits the experimental data reasonably well. We show that
the two extended dynamics have multiple equilibria, one of which is the
classical user equilibrium. While analyzing global stability is intractable due
to the presence of multiple equilibria, local stabilities near equilibria are
developed analytically and verified by numerical experiments. General insights
on how key parameters affect the stability of user equilibria are unveiled.</description>
      <guid isPermaLink="false">2409.11908v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>SLAM assisted 3D tracking system for laparoscopic surgery</title>
      <link>http://arxiv.org/abs/2409.11688v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Demo: https://youtu.be/B1xZW8bj3cM&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;1. **研究背景**:&lt;br&gt;   - 最小化侵入性手术的主要限制是难以准确定位目标器官的内部解剖结构，原因在于缺乏触觉反馈和透明度。&lt;br&gt;2. **解决方案**:&lt;br&gt;   - 增强现实（AR）提供了克服这一挑战的有希望的解决方案。&lt;br&gt;3. **现有研究**:&lt;br&gt;   - 许多研究表明，结合基于学习的方法和几何方法可以实现准确的术前和术中数据注册。&lt;br&gt;4. **提出的方法**:&lt;br&gt;   - 本文提出了一种实时单目3D跟踪算法，用于后注册任务。&lt;br&gt;5. **框架选择**:&lt;br&gt;   - 采用并修改了ORB-SLAM2框架以实现基于先验的3D跟踪。&lt;br&gt;6. **初始化策略**:&lt;br&gt;   - 使用原始3D形状快速初始化单目SLAM。&lt;br&gt;7. **伪分割策略**:&lt;br&gt;   - 采用伪分割策略将目标器官与背景分离，以便进行跟踪。&lt;br&gt;8. **几何先验约束**:&lt;br&gt;   - 将3D形状的几何先验作为额外约束纳入姿态图中。&lt;br&gt;9. **实验结果**:&lt;br&gt;   - 通过体内和体外测试的实验表明，所提出的3D跟踪系统提供了稳健的3D跟踪能力，能够有效应对快速运动、视野外场景、部分可视性和“器官-背景”相对运动等典型挑战。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A major limitation of minimally invasive surgery is the difficulty in
accurately locating the internal anatomical structures of the target organ due
to the lack of tactile feedback and transparency. Augmented reality (AR) offers
a promising solution to overcome this challenge. Numerous studies have shown
that combining learning-based and geometric methods can achieve accurate
preoperative and intraoperative data registration. This work proposes a
real-time monocular 3D tracking algorithm for post-registration tasks. The
ORB-SLAM2 framework is adopted and modified for prior-based 3D tracking. The
primitive 3D shape is used for fast initialization of the monocular SLAM. A
pseudo-segmentation strategy is employed to separate the target organ from the
background for tracking purposes, and the geometric prior of the 3D shape is
incorporated as an additional constraint in the pose graph. Experiments from
in-vivo and ex-vivo tests demonstrate that the proposed 3D tracking system
provides robust 3D tracking and effectively handles typical challenges such as
fast motion, out-of-field-of-view scenarios, partial visibility, and
"organ-background" relative motion.</description>
      <guid isPermaLink="false">2409.11688v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>An Efficient Projection-Based Next-best-view Planning Framework for Reconstruction of Unknown Objects</title>
      <link>http://arxiv.org/abs/2409.12096v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;1. **研究背景**:&lt;br&gt;   - 完整高效地捕捉物体的三维数据是工业和机器人应用中的基本问题。&lt;br&gt;2. **任务描述**:&lt;br&gt;   - 下一最佳视角（NBV）规划任务旨在根据当前数据推断下一个视角的姿态，从而逐步实现完整的三维重建。&lt;br&gt;3. **现有挑战**:&lt;br&gt;   - 许多现有算法因使用光线投射（ray-casting）而面临较大的计算负担。&lt;br&gt;4. **提出的新方法**:&lt;br&gt;   - 本文提出了一种基于投影的NBV规划框架，能够以极快的速度选择下一个最佳视角，同时确保对物体的全面扫描。&lt;br&gt;5. **方法细节**:&lt;br&gt;   - 该框架将不同类型的体素簇重构为椭球体，基于体素结构进行处理。&lt;br&gt;6. **视角选择**:&lt;br&gt;   - 使用基于投影的视角质量评估函数和全局分区策略，从候选视角中选择下一个最佳视角。&lt;br&gt;7. **效率提升**:&lt;br&gt;   - 这一过程替代了体素结构中的光线投射，显著提高了计算效率。&lt;br&gt;8. **实验结果**:&lt;br&gt;   - 与其他算法在仿真环境中的对比实验表明，所提出的框架在捕获相似覆盖范围的基础上实现了10倍的效率提升。&lt;br&gt;9. **实际应用验证**:&lt;br&gt;   - 实际实验结果证明了该框架的效率和可行性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Efficiently and completely capturing the three-dimensional data of an object
is a fundamental problem in industrial and robotic applications. The task of
next-best-view (NBV) planning is to infer the pose of the next viewpoint based
on the current data, and gradually realize the complete three-dimensional
reconstruction. Many existing algorithms, however, suffer a large computational
burden due to the use of ray-casting. To address this, this paper proposes a
projection-based NBV planning framework. It can select the next best view at an
extremely fast speed while ensuring the complete scanning of the object.
Specifically, this framework refits different types of voxel clusters into
ellipsoids based on the voxel structure.Then, the next best view is selected
from the candidate views using a projection-based viewpoint quality evaluation
function in conjunction with a global partitioning strategy. This process
replaces the ray-casting in voxel structures, significantly improving the
computational efficiency. Comparative experiments with other algorithms in a
simulation environment show that the framework proposed in this paper can
achieve 10 times efficiency improvement on the basis of capturing roughly the
same coverage. The real-world experimental results also prove the efficiency
and feasibility of the framework.</description>
      <guid isPermaLink="false">2409.12096v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Prompt-Enhanced Spatio-Temporal Graph Transfer Learning</title>
      <link>http://arxiv.org/abs/2405.12452v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;1. **研究背景**:&lt;br&gt;   - 时空图神经网络在城市计算任务（如预测和克里金插值）中表现出色，能够捕捉复杂的依赖关系。&lt;br&gt;2. **性能限制**:&lt;br&gt;   - 其性能受到对特定任务大量数据训练的依赖，限制了其在不同城市领域适应新需求的能力。&lt;br&gt;3. **转移学习的挑战**:&lt;br&gt;   - 尽管提出了转移学习来跨领域利用知识，但由于缺乏统一框架，时空图转移学习方法在跨任务泛化方面仍然未被充分研究。&lt;br&gt;4. **提出的解决方案**:&lt;br&gt;   - 本文提出时空图提示（STGP）框架，一种增强提示的转移学习框架，能够适应数据稀缺领域中的多样任务。&lt;br&gt;5. **任务统一**:&lt;br&gt;   - 将不同任务统一为单一模板，并引入与该模板对齐的任务无关网络架构，从而捕捉跨任务共享的时空依赖关系。&lt;br&gt;6. **可学习提示**:&lt;br&gt;   - 采用可学习的提示，在两阶段提示管道中实现领域和任务的转移，有效捕捉领域知识和任务特性。&lt;br&gt;7. **实验结果**:&lt;br&gt;   - 大量实验表明，STGP在三个下游任务（预测、克里金插值和外推）中显著优于现有的最先进基线。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spatio-temporal graph neural networks have demonstrated efficacy in capturing
complex dependencies for urban computing tasks such as forecasting and kriging.
However, their performance is constrained by the reliance on extensive data for
training on specific tasks, which limits their adaptability to new urban
domains with varied demands. Although transfer learning has been proposed to
address this problem by leveraging knowledge across domains, cross-task
generalization remains underexplored in spatio-temporal graph transfer learning
methods due to the absence of a unified framework. To bridge this gap, we
propose Spatio-Temporal Graph Prompting (STGP), a prompt-enhanced transfer
learning framework capable of adapting to diverse tasks in data-scarce domains.
Specifically, we first unify different tasks into a single template and
introduce a task-agnostic network architecture that aligns with this template.
This approach enables the capture of spatio-temporal dependencies shared across
tasks. Furthermore, we employ learnable prompts to achieve domain and task
transfer in a two-stage prompting pipeline, enabling the prompts to effectively
capture domain knowledge and task-specific properties at each stage. Extensive
experiments demonstrate that STGP outperforms state-of-the-art baselines in
three downstream tasks forecasting, kriging, and extrapolation by a notable
margin.</description>
      <guid isPermaLink="false">2405.12452v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Resources on the Move for Smart City: A Disruptive Perspective on the Grand Convergence of Sensing, Communications, Computing, Storage, and Intelligence</title>
      <link>http://arxiv.org/abs/2409.09417v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 3 figures. Accepted by IEEE Communications Magazine&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;1. **研究背景**:&lt;br&gt;   - 街道上最常见的事物是车辆，大多数车辆用于运输人员或货物。&lt;br&gt;2. **新视角**:&lt;br&gt;   - 探讨车辆是否可以同时具备感知、通信、计算、存储和智能（SCCSI）的资源和能力。&lt;br&gt;3. **SCCSI网络构想**:&lt;br&gt;   - 提出利用SCCSI赋能的车辆设计一个服务网络，称为SCCSI网络，以帮助构建智能城市，实现成本效益高且可持续的解决方案。&lt;br&gt;4. **技术融合**:&lt;br&gt;   - 展示多维技术（感知、通信、计算、存储和智能）如何融合成统一技术，以应对新兴大规模应用的资源需求。&lt;br&gt;5. **移动资源**:&lt;br&gt;   - SCCSI网络可以使资源和能力在地面、空中和海上移动，推动SCCSI服务到边缘。&lt;br&gt;6. **激发创新思维**:&lt;br&gt;   - 期望本文能激发更多颠覆性思维，以应对重要的重大挑战。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The most commonly seen things on streets in any city are vehicles. However,
most of them are used to transport people or goods. What if they also carry
resources and capabilities for sensing, communications, computing, storage, and
intelligence (SCCSI)? We will have a web of sensors to monitor the city, a
network of powerful communicators to transport data around, a grid of computing
power to conduct data analytics and machine learning (ML), a network of
distributed storage to buffer/cache data/job for optimization, and a set of
movable AI/ML toolboxes made available for specialized smart applications. This
perspective article presents how to leverage SCCSI-empowered vehicles to design
such a service network, simply called SCCSI network, to help build a smart city
with a cost-effective and sustainable solution. It showcases how
multi-dimensional technologies, namely, sensing, communications, computing,
storage, and intelligence, converge to a unifying technology to solve grand
challenges for resource demands from emerging large-scale applications. Thus,
with SCCSI-empowered vehicles on the ground, over the air, and on the sea,
SCCSI network can make resources and capabilities on the move, practically
pushing SCCSI services to the edge! We hope this article serves as a spark to
stimulate more disruptive thinking to address grand challenges of paramount
importance.</description>
      <guid isPermaLink="false">2409.09417v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Recurrent Interpolants for Probabilistic Time Series Prediction</title>
      <link>http://arxiv.org/abs/2409.11684v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;1. **研究背景**:&lt;br&gt;   - 序列模型，如递归神经网络和基于变换器的模型，已成为多变量时间序列预测的主要工具，广泛应用于金融、生物学、医学等领域。&lt;br&gt;2. **优点**:&lt;br&gt;   - 这些模型擅长捕捉依赖关系、评估预测不确定性，并且训练效率高。&lt;br&gt;3. **挑战**:&lt;br&gt;   - 在建模高维复杂分布和特征间的交叉依赖关系时面临困难。&lt;br&gt;4. **新兴解决方案**:&lt;br&gt;   - 最近的研究通过采用生成建模（如扩散模型或流模型）来解决上述问题。&lt;br&gt;5. **成功扩展**:&lt;br&gt;   - 随机微分方程或概率流的整合成功将这些方法扩展到概率时间序列的插补和预测。&lt;br&gt;6. **可扩展性问题**:&lt;br&gt;   - 大规模生成模型预测需要一个计算友好的框架，以解决可扩展性问题。&lt;br&gt;7. **提出的新方法**:&lt;br&gt;   - 本研究提出一种新方法，将递归神经网络的计算效率与扩散模型的高质量概率建模相结合，以应对挑战并推动生成模型在时间序列预测中的应用。&lt;br&gt;8. **方法基础**:&lt;br&gt;   - 该方法基于随机插值，并扩展到更广泛的条件生成框架，配备额外的控制特性。&lt;br&gt;9. **未来发展**:&lt;br&gt;   - 本研究为该动态领域的未来发展提供了新的见解。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sequential models such as recurrent neural networks or transformer-based
models became \textit{de facto} tools for multivariate time series forecasting
in a probabilistic fashion, with applications to a wide range of datasets, such
as finance, biology, medicine, etc. Despite their adeptness in capturing
dependencies, assessing prediction uncertainty, and efficiency in training,
challenges emerge in modeling high-dimensional complex distributions and
cross-feature dependencies. To tackle these issues, recent works delve into
generative modeling by employing diffusion or flow-based models. Notably, the
integration of stochastic differential equations or probability flow
successfully extends these methods to probabilistic time series imputation and
forecasting. However, scalability issues necessitate a computational-friendly
framework for large-scale generative model-based predictions. This work
proposes a novel approach by blending the computational efficiency of recurrent
neural networks with the high-quality probabilistic modeling of the diffusion
model, which addresses challenges and advances generative models' application
in time series forecasting. Our method relies on the foundation of stochastic
interpolants and the extension to a broader conditional generation framework
with additional control features, offering insights for future developments in
this dynamic field.</description>
      <guid isPermaLink="false">2409.11684v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Obfuscation Based Privacy Preserving Representations are Recoverable Using Neighborhood Information</title>
      <link>http://arxiv.org/abs/2409.11536v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;1. **研究背景**:&lt;br&gt;   - 增长的AR/VR/MR应用及基于云的视觉定位系统引发了对用户内容隐私的关注。&lt;br&gt;2. **隐私问题**:&lt;br&gt;   - 深度神经网络能够从稀疏的3D或2D点及其描述符恢复场景详细图像，称为反演攻击，这加剧了隐私担忧。&lt;br&gt;3. **研究目标**:&lt;br&gt;   - 研究重点在于防止对查询图像关键点和场景地图3D点的反演攻击。&lt;br&gt;4. **几何模糊技术**:&lt;br&gt;   - 提出了几种几何模糊技术，通过将点提升到更高维空间（如线或平面）或交换点之间的坐标来实现。&lt;br&gt;5. **共同弱点**:&lt;br&gt;   - 本文指出这些模糊技术的共同弱点，允许在已知邻域的假设下恢复原始点位置的近似值。&lt;br&gt;6. **邻域计算**:&lt;br&gt;   - 进一步展示了如何通过学习识别在邻域中共同出现的描述符来计算这些邻域。&lt;br&gt;7. **实验结果**:&lt;br&gt;   - 大量实验表明，所提出的点恢复方法在所有现有几何模糊方案中均适用。&lt;br&gt;8. **隐私保护质疑**:&lt;br&gt;   - 研究结果表明，这些模糊方案不应被视为隐私保护措施，尽管它们声称具备隐私保护功能。&lt;br&gt;9. **代码可用性**:&lt;br&gt;   - 相关代码将可在指定的GitHub链接上获取。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Rapid growth in the popularity of AR/VR/MR applications and cloud-based
visual localization systems has given rise to an increased focus on the privacy
of user content in the localization process.
  This privacy concern has been further escalated by the ability of deep neural
networks to recover detailed images of a scene from a sparse set of 3D or 2D
points and their descriptors - the so-called inversion attacks.
  Research on privacy-preserving localization has therefore focused on
preventing these inversion attacks on both the query image keypoints and the 3D
points of the scene map.
  To this end, several geometry obfuscation techniques that lift points to
higher-dimensional spaces, i.e., lines or planes, or that swap coordinates
between points % have been proposed.
  In this paper, we point to a common weakness of these obfuscations that
allows to recover approximations of the original point positions under the
assumption of known neighborhoods.
  We further show that these neighborhoods can be computed by learning to
identify descriptors that co-occur in neighborhoods.
  Extensive experiments show that our approach for point recovery is
practically applicable to all existing geometric obfuscation schemes.
  Our results show that these schemes should not be considered
privacy-preserving, even though they are claimed to be privacy-preserving.
  Code will be available at
\url{https://github.com/kunalchelani/RecoverPointsNeighborhood}.</description>
      <guid isPermaLink="false">2409.11536v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>An Explainable Machine Learning Approach to Traffic Accident Fatality Prediction</title>
      <link>http://arxiv.org/abs/2409.11929v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 Pages, 6 figures, 2 tables, 28th International Conference on
  Knowledge-Based and Intelligent Information &amp; Engineering Systems (KES 2024)&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;1. **研究背景**:&lt;br&gt;   - 道路交通事故（RTA）对全球公共健康构成重大威胁，导致生命损失和经济负担，尤其在发展中国家（如孟加拉国）更加严重。&lt;br&gt;2. **研究重要性**:&lt;br&gt;   - 建立可靠的模型来预测事故结果对于实施有效的预防措施至关重要。&lt;br&gt;3. **研究目标**:&lt;br&gt;   - 本研究旨在开发一种基于机器学习的方法，分类致命和非致命的交通事故结果。&lt;br&gt;4. **数据来源**:&lt;br&gt;   - 使用2017至2022年间的达卡城市交通事故数据库数据。&lt;br&gt;5. **方法框架**:&lt;br&gt;   - 利用多种机器学习分类算法，包括逻辑回归、支持向量机、朴素贝叶斯、随机森林、决策树、梯度提升、LightGBM和人工神经网络。&lt;br&gt;6. **模型可解释性**:&lt;br&gt;   - 采用SHAP（SHapley Additive exPlanations）方法，优先考虑模型的可解释性，以阐明影响事故致命性的关键因素。&lt;br&gt;7. **研究结果**:&lt;br&gt;   - LightGBM模型表现最佳，ROC-AUC得分为0.72。&lt;br&gt;8. **深入分析**:&lt;br&gt;   - 进行全球、局部和特征依赖性分析，以深入了解模型行为。&lt;br&gt;9. **关键因素**:&lt;br&gt;   - SHAP分析表明，伤亡类别、事故时间、地点、车辆类型和道路类型在决定致命风险中扮演重要角色。&lt;br&gt;10. **政策建议**:&lt;br&gt;    - 研究结果为发展中国家的政策制定者和道路安全从业者提供了有价值的见解，支持实施基于证据的策略以减少交通事故死亡率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Road traffic accidents (RTA) pose a significant public health threat
worldwide, leading to considerable loss of life and economic burdens. This is
particularly acute in developing countries like Bangladesh. Building reliable
models to forecast crash outcomes is crucial for implementing effective
preventive measures. To aid in developing targeted safety interventions, this
study presents a machine learning-based approach for classifying fatal and
non-fatal road accident outcomes using data from the Dhaka metropolitan traffic
crash database from 2017 to 2022. Our framework utilizes a range of machine
learning classification algorithms, comprising Logistic Regression, Support
Vector Machines, Naive Bayes, Random Forest, Decision Tree, Gradient Boosting,
LightGBM, and Artificial Neural Network. We prioritize model interpretability
by employing the SHAP (SHapley Additive exPlanations) method, which elucidates
the key factors influencing accident fatality. Our results demonstrate that
LightGBM outperforms other models, achieving a ROC-AUC score of 0.72. The
global, local, and feature dependency analyses are conducted to acquire deeper
insights into the behavior of the model. SHAP analysis reveals that casualty
class, time of accident, location, vehicle type, and road type play pivotal
roles in determining fatality risk. These findings offer valuable insights for
policymakers and road safety practitioners in developing countries, enabling
the implementation of evidence-based strategies to reduce traffic crash
fatalities.</description>
      <guid isPermaLink="false">2409.11929v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Bi-objective trail-planning for a robot team orienteering in a hazardous environment</title>
      <link>http://arxiv.org/abs/2409.12114v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  v0.0&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;1. **研究背景**:&lt;br&gt;   - 移动机器人团队（包括空中、地面或水下机器人）在资源运输、巡逻、信息收集、农业、森林灭火、化学气体源定位与绘图，以及搜救任务中有广泛应用。&lt;br&gt;2. **挑战**:&lt;br&gt;   - 在危险环境中（如崎岖地形、强风或有敌对威胁的环境），机器人团队需要规划和协调路径，考虑到失能、毁坏或被捕的风险。&lt;br&gt;3. **规划目标**:&lt;br&gt;   - 机器人应选择最安全的路径，协调行动以共同实现团队目标，同时对机器人故障保持鲁棒性，并在访问位置的奖励与机器人损失的风险之间取得平衡。&lt;br&gt;4. **研究方法**:&lt;br&gt;   - 研究双目标路径规划，针对在危险环境中进行定位的移动机器人团队。&lt;br&gt;5. **环境建模**:&lt;br&gt;   - 危险环境被抽象为一个有向图，图中的边在机器人通过时具有已知的生存概率。&lt;br&gt;6. **节点奖励**:&lt;br&gt;   - 图的每个节点在被机器人访问时会为团队提供奖励（例如，交付物品或拍摄节点图像）。&lt;br&gt;7. **优化目标**:&lt;br&gt;   - 寻找Pareto最优的机器人团队路径规划，最大化两个相互冲突的目标：预期团队奖励和生存机器人的数量。&lt;br&gt;8. **决策支持**:&lt;br&gt;   - 人类决策者可以根据自身价值观选择在奖励与机器人生存之间取得平衡的路径规划。&lt;br&gt;9. **实现方法**:&lt;br&gt;   - 使用蚁群优化算法，结合启发式方法，搜索Pareto最优的机器人团队路径规划集。&lt;br&gt;10. **案例研究**:&lt;br&gt;    - 以艺术博物馆的信息收集任务作为案例进行说明。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Teams of mobile [aerial, ground, or aquatic] robots have applications in
resource delivery, patrolling, information-gathering, agriculture, forest fire
fighting, chemical plume source localization and mapping, and
search-and-rescue. Robot teams traversing hazardous environments -- with e.g.
rough terrain or seas, strong winds, or adversaries capable of attacking or
capturing robots -- should plan and coordinate their trails in consideration of
risks of disablement, destruction, or capture. Specifically, the robots should
take the safest trails, coordinate their trails to cooperatively achieve the
team-level objective with robustness to robot failures, and balance the reward
from visiting locations against risks of robot losses. Herein, we consider
bi-objective trail-planning for a mobile team of robots orienteering in a
hazardous environment. The hazardous environment is abstracted as a directed
graph whose arcs, when traversed by a robot, present known probabilities of
survival. Each node of the graph offers a reward to the team if visited by a
robot (which e.g. delivers a good to or images the node). We wish to search for
the Pareto-optimal robot-team trail plans that maximize two [conflicting] team
objectives: the expected (i) team reward and (ii) number of robots that survive
the mission. A human decision-maker can then select trail plans that balance,
according to their values, reward and robot survival. We implement ant colony
optimization, guided by heuristics, to search for the Pareto-optimal set of
robot team trail plans. As a case study, we illustrate with an
information-gathering mission in an art museum.</description>
      <guid isPermaLink="false">2409.12114v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>CrowdTransfer: Enabling Crowd Knowledge Transfer in AIoT Community</title>
      <link>http://arxiv.org/abs/2407.06485v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted for publication in IEEE Communications
  Surveys &amp; Tutorials. Copyright will be transferred without notice, after this
  version may no longer be accessible&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;1. **研究主题**:&lt;br&gt;   - 人工智能物联网（AIoT）是物联网（IoT）与人工智能（AI）技术深度融合的新兴领域。&lt;br&gt;2. **挑战**:&lt;br&gt;   - 尽管深度学习技术提升了复杂IoT数据的处理和智能分析效率，但在实际AIoT应用中仍面临资源限制和多样化任务需求等挑战。&lt;br&gt;3. **知识转移的作用**:&lt;br&gt;   - 知识转移是一种有效的方法，通过避免高昂的数据重收集和模型重训练成本，增强学习性能。&lt;br&gt;4. **现有文献的不足**:&lt;br&gt;   - 尽管已有一些关于迁移学习的有价值综述，但这些综述以相对孤立的方式介绍方法，缺乏对AIoT领域各种知识转移技术的最新进展。&lt;br&gt;5. **新概念**:&lt;br&gt;   - 本文提出了“群体知识转移”（Crowd Knowledge Transfer）的新概念，旨在从一群智能体学习先前知识，以降低训练成本并提升模型在复杂现实场景中的表现。&lt;br&gt;6. **转移模式**:&lt;br&gt;   - 提出了四种基于群体智能的转移模式：推导、共享、演化和融合模式。&lt;br&gt;7. **深入探讨**:&lt;br&gt;   - 在传统迁移学习方法基础上，从三个视角深入探讨了适用于各种AIoT应用的先进群体知识转移模型。&lt;br&gt;8. **应用领域**:&lt;br&gt;   - 探讨了AIoT在多个领域的应用，包括人类活动识别、城市计算、多机器人系统和智能工厂。&lt;br&gt;9. **未来方向**:&lt;br&gt;   - 讨论了知识转移在AIoT领域的开放问题，并概述了未来的研究方向。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-07-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/COMST.2024.3423319&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Artificial Intelligence of Things (AIoT) is an emerging frontier based on the
deep fusion of Internet of Things (IoT) and Artificial Intelligence (AI)
technologies. Although advanced deep learning techniques enhance the efficient
data processing and intelligent analysis of complex IoT data, they still suffer
from notable challenges when deployed to practical AIoT applications, such as
constrained resources, and diverse task requirements. Knowledge transfer is an
effective method to enhance learning performance by avoiding the exorbitant
costs associated with data recollection and model retraining. Notably, although
there are already some valuable and impressive surveys on transfer learning,
these surveys introduce approaches in a relatively isolated way and lack the
recent advances of various knowledge transfer techniques for AIoT field. This
survey endeavors to introduce a new concept of knowledge transfer, referred to
as Crowd Knowledge Transfer (CrowdTransfer), which aims to transfer prior
knowledge learned from a crowd of agents to reduce the training cost and as
well as improve the performance of the model in real-world complicated
scenarios. Particularly, we present four transfer modes from the perspective of
crowd intelligence, including derivation, sharing, evolution and fusion modes.
Building upon conventional transfer learning methods, we further delve into
advanced crowd knowledge transfer models from three perspectives for various
AIoT applications. Furthermore, we explore some applications of AIoT areas,
such as human activity recognition, urban computing, multi-robot system, and
smart factory. Finally, we discuss the open issues and outline future research
directions of knowledge transfer in AIoT community.</description>
      <guid isPermaLink="false">2407.06485v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing HAP Networks with Reconfigurable Intelligent Surfaces</title>
      <link>http://arxiv.org/abs/2409.10040v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;1. **研究主题**:&lt;br&gt;   - 本文提出并分析了一种基于可重构智能表面（RIS）的高空平台（HAP）网络。&lt;br&gt;2. **模型方法**:&lt;br&gt;   - 使用随机几何学将HAP和RIS的任意位置建模为均匀泊松点过程。&lt;br&gt;3. **信道特性**:&lt;br&gt;   - 考虑HAP、RIS和用户之间的链接采用$\kappa$--$\mu$衰落模型。&lt;br&gt;4. **性能指标**:&lt;br&gt;   - 表达了所提系统的覆盖率和时间平均容量。&lt;br&gt;5. **验证方法**:&lt;br&gt;   - 通过蒙特卡洛模拟验证分析得出的性能指标。&lt;br&gt;6. **结果展示**:&lt;br&gt;   - 结果显示系统性能有显著改善，并展示了系统参数的影响。&lt;br&gt;7. **应用前景**:&lt;br&gt;   - 提出的系统概念能够提高智能城市和密集城市环境中的连接性和数据卸载能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents and analyzes a reconfigurable intelligent surface
(RIS)-based high-altitude platform (HAP) network. Stochastic geometry is used
to model the arbitrary locations of the HAPs and RISs as a homogenous Poisson
point process. Considering that the links between the HAPs, RISs, and users are
$\kappa$--$\mu$ faded, the coverage and ergodic capacity of the proposed system
are expressed. The analytically derived performance measures are verified
through Monte Carlo simulations. Significant improvements in system performance
and the impact of system parameters are demonstrated in the results. Thus, the
proposed system concept can improve connectivity and data offloading in smart
cities and dense urban environments.</description>
      <guid isPermaLink="false">2409.10040v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Benchmarking the spectroscopic masses of 249 evolved stars using asteroseismology with TESS</title>
      <link>http://arxiv.org/abs/2409.11736v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 7 figues&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;1. **研究背景**:&lt;br&gt;   - 了解行星形成的一种方法是研究行星出现率与恒星质量之间的关系。&lt;br&gt;2. **研究挑战**:&lt;br&gt;   - 在红巨星阶段，测量恒星质量非常困难，尤其是对所谓的“退休A型星”的光谱质量存在争议。&lt;br&gt;3. **现有研究**:&lt;br&gt;   - 之前使用光谱学、干涉测量和星震学的努力未能解决质量争议。&lt;br&gt;4. **新研究的动机**:&lt;br&gt;   - 最近的一项综合研究发现了质量依赖的质量偏差，但仅基于16颗恒星的数据。&lt;br&gt;5. **研究方法**:&lt;br&gt;   - 利用NASA的TESS卫星，将调查范围扩大到92颗低亮度恒星，这些恒星与退休A型星相吻合。&lt;br&gt;6. **数据收集**:&lt;br&gt;   - 从TESS的光度时间序列中测量特征振荡频率（$\mathrm{\nu}_{\mathrm{max}}$）和大频率间隔（$\mathrm{\Delta\nu}$）。&lt;br&gt;7. **数据分析**:&lt;br&gt;   - 使用这些测量值和星震学标定关系，推导星震质量，并与来自五个调查的光谱质量进行比较。&lt;br&gt;8. **研究发现**:&lt;br&gt;   - 发现光谱法与星震法之间存在质量偏差，且这种偏差随着恒星质量增加而增大。&lt;br&gt;9. **对行星发生率的影响**:&lt;br&gt;   - 采用星震质量标定对退休A型星的行星发生率-质量-金属丰度相关性没有显著影响。&lt;br&gt;10. **额外数据报告**:&lt;br&gt;    - 报告了157颗高亮度巨星（大多数为氦核燃烧星）的星震测量和质量，这些数据来自光谱调查。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; One way to understand planet formation is through studying the correlations
between planet occurrence rates and stellar mass. However, measuring stellar
mass in the red giant regime is very difficult. In particular, the
spectroscopic masses of certain evolved stars, often referred to as "retired
A-stars", have been questioned in the literature. Efforts to resolve this mass
controversy using spectroscopy, interferometry and asteroseismology have so far
been inconclusive. A recent ensemble study found a mass-dependent mass offset,
but the result was based on only 16 stars. With NASA's Transiting Exoplanet
Survey Satellite (TESS), we expand the investigation of the mass discrepancy to
a total of 92 low-luminosity stars, synonymous with the retired A-stars. We
measure their characteristic oscillation frequency,
$\mathrm{\nu}_{\mathrm{max}}$, and the large frequency separation,
$\mathrm{\Delta\nu}$, from their TESS photometric time series. Using these
measurements and asteroseismic scaling relations, we derive asteroseismic
masses and compare them with spectroscopic masses from five surveys, to
comprehensively study the alleged mass-dependent mass offset. We find a mass
offset between spectroscopy and seismology that increases with stellar mass.
However, we note that adopting the seismic mass scale does not have a
significant effect on the planet occurrence-mass-metallicity correlation for
the so-called retired A-stars. We also report seismic measurements and masses
for 157 higher luminosity giants (mostly helium-core-burning) from the
spectroscopic surveys.</description>
      <guid isPermaLink="false">2409.11736v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>LMMCoDrive: Cooperative Driving with Large Multimodal Model</title>
      <link>http://arxiv.org/abs/2409.11981v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 5 figures&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;1. **研究背景**:&lt;br&gt;   - 解决自主需求车辆（AMoD）系统中去中心化合作调度和运动规划的复杂挑战。&lt;br&gt;2. **提出的框架**:&lt;br&gt;   - 介绍了LMMCoDrive，一个新颖的合作驾驶框架，利用大型多模态模型（LMM）提升动态城市环境中的交通效率。&lt;br&gt;3. **集成调度与规划**:&lt;br&gt;   - 框架无缝集成调度和运动规划过程，以确保合作自主车辆（CAVs）的有效运行。&lt;br&gt;4. **空间关系抽象**:&lt;br&gt;   - 将CAV与乘客请求之间的空间关系抽象为鸟瞰图（BEV），充分利用LMM的潜力。&lt;br&gt;5. **轨迹优化**:&lt;br&gt;   - 小心地优化每个CAV的轨迹，同时确保通过安全约束避免碰撞。&lt;br&gt;6. **优化策略**:&lt;br&gt;   - 提出了去中心化优化策略，利用交替方向乘子法（ADMM）推动CAV的图演化。&lt;br&gt;7. **仿真结果**:&lt;br&gt;   - 仿真结果表明，LMM在优化CAV调度和增强每辆车的去中心化合作优化过程中的关键作用和显著影响。&lt;br&gt;8. **研究意义**:&lt;br&gt;   - 这标志着在实现实用、高效和安全的AMoD系统方面的重要进展，预示着城市交通的革命。&lt;br&gt;9. **代码可用性**:&lt;br&gt;   - 相关代码可在GitHub上获取：[LMMCoDrive](https://github.com/henryhcliu/LMMCoDrive)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; To address the intricate challenges of decentralized cooperative scheduling
and motion planning in Autonomous Mobility-on-Demand (AMoD) systems, this paper
introduces LMMCoDrive, a novel cooperative driving framework that leverages a
Large Multimodal Model (LMM) to enhance traffic efficiency in dynamic urban
environments. This framework seamlessly integrates scheduling and motion
planning processes to ensure the effective operation of Cooperative Autonomous
Vehicles (CAVs). The spatial relationship between CAVs and passenger requests
is abstracted into a Bird's-Eye View (BEV) to fully exploit the potential of
the LMM. Besides, trajectories are cautiously refined for each CAV while
ensuring collision avoidance through safety constraints. A decentralized
optimization strategy, facilitated by the Alternating Direction Method of
Multipliers (ADMM) within the LMM framework, is proposed to drive the graph
evolution of CAVs. Simulation results demonstrate the pivotal role and
significant impact of LMM in optimizing CAV scheduling and enhancing
decentralized cooperative optimization process for each vehicle. This marks a
substantial stride towards achieving practical, efficient, and safe AMoD
systems that are poised to revolutionize urban transportation. The code is
available at https://github.com/henryhcliu/LMMCoDrive.</description>
      <guid isPermaLink="false">2409.11981v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Towards Global Localization using Multi-Modal Object-Instance Re-Identification</title>
      <link>http://arxiv.org/abs/2409.12002v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 5 figures, 3 tables. Submitted to ICRA 2025&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;1. **研究背景**:&lt;br&gt;   - 重识别（ReID）是计算机视觉中的一个关键挑战，主要集中在行人和车辆的识别上。&lt;br&gt;2. **研究问题**:&lt;br&gt;   - 物体实例重识别（ReID）在自主探索、长期感知和场景理解等任务中具有重要意义，但相关研究仍然不足。&lt;br&gt;3. **提出的方法**:&lt;br&gt;   - 本文提出了一种新颖的双路径物体实例重识别变换器架构，集成了多模态的RGB和深度信息。&lt;br&gt;4. **深度数据的优势**:&lt;br&gt;   - 通过利用深度数据，展示在杂乱或光照条件变化的场景中，ReID的性能得到了提升。&lt;br&gt;5. **定位框架**:&lt;br&gt;   - 开发了一种基于ReID的定位框架，实现了不同视角下的摄像头定位和姿态识别的准确性。&lt;br&gt;6. **验证方法**:&lt;br&gt;   - 使用两个自建的RGB-D数据集以及来自开源TUM RGB-D数据集的多个序列验证所提方法。&lt;br&gt;7. **性能提升**:&lt;br&gt;   - 方法在物体实例重识别（mAP达到75.18）和定位准确性（在TUM-RGBD上成功率为83%）方面显示出显著的改进。&lt;br&gt;8. **公开可用性**:&lt;br&gt;   - 研究模型、框架和数据集已公开，供其他研究人员使用。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Re-identification (ReID) is a critical challenge in computer vision,
predominantly studied in the context of pedestrians and vehicles. However,
robust object-instance ReID, which has significant implications for tasks such
as autonomous exploration, long-term perception, and scene understanding,
remains underexplored. In this work, we address this gap by proposing a novel
dual-path object-instance re-identification transformer architecture that
integrates multimodal RGB and depth information. By leveraging depth data, we
demonstrate improvements in ReID across scenes that are cluttered or have
varying illumination conditions. Additionally, we develop a ReID-based
localization framework that enables accurate camera localization and pose
identification across different viewpoints. We validate our methods using two
custom-built RGB-D datasets, as well as multiple sequences from the open-source
TUM RGB-D datasets. Our approach demonstrates significant improvements in both
object instance ReID (mAP of 75.18) and localization accuracy (success rate of
83% on TUM-RGBD), highlighting the essential role of object ReID in advancing
robotic perception. Our models, frameworks, and datasets have been made
publicly available.</description>
      <guid isPermaLink="false">2409.12002v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Residual Descent Differential Dynamic Game (RD3G) -- A Fast Newton Solver for Constrained General Sum Games</title>
      <link>http://arxiv.org/abs/2409.12152v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;1. **研究主题**:&lt;br&gt;   - 提出了残差下降微分动态博弈（RD3G），一种基于牛顿法的求解器，用于约束的多智能体游戏控制问题。&lt;br&gt;2. **主要目标**:&lt;br&gt;   - RD3G求解器旨在寻找局部纳什均衡，适用于代理通过奖励和状态约束相互耦合的问题。&lt;br&gt;3. **方法比较**:&lt;br&gt;   - 将所提出的方法与现有的先进技术进行比较，评估其性能。&lt;br&gt;4. **计算优势**:&lt;br&gt;   - 展示RD3G算法在多个示例问题上的计算优势，强调其效率和效果。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Residual Descent Differential Dynamic Game (RD3G), a Newton-based
solver for constrained multi-agent game-control problems. The proposed solver
seeks a local Nash equilibrium for problems where agents are coupled through
their rewards and state constraints. We compare the proposed method against
competing state-of-the-art techniques and showcase the computational benefits
of the RD3G algorithm on several example problems.</description>
      <guid isPermaLink="false">2409.12152v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Towards Smart Microfarming in an Urban Computing Continuum</title>
      <link>http://arxiv.org/abs/2408.02992v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper is uploaded here for research community, thus it is for
  non-commercial purposes&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;1. **研究背景**:&lt;br&gt;   - 微农场和城市计算已成为当今城市生活的两个重要可持续发展支柱。&lt;br&gt;2. **研究目的**:&lt;br&gt;   - 将微农场和城市计算这两个概念结合，并扩展至智能微农场和城市计算连续体的新概念。&lt;br&gt;3. **智能微农场**:&lt;br&gt;   - 提出利用人工智能在微农场中的应用，推动智能微农场的发展。&lt;br&gt;4. **城市计算连续体**:&lt;br&gt;   - 作为对城市计算概念的主要扩展，旨在实现高效的物联网（IoT）-边缘-云连续体。&lt;br&gt;5. **系统架构**:&lt;br&gt;   - 构建一个植物推荐系统的架构，利用边缘计算中的机器学习，从给定植物池中找到适合特定微农场的植物，基于从IoT传感器设备监测到的土壤值。&lt;br&gt;6. **数据传输方案**:&lt;br&gt;   - 集成长距离的LoRa通信解决方案，因其无许可特性和开源实现潜力，方便数据从IoT设备传输至边缘系统。&lt;br&gt;7. **数据存储协议**:&lt;br&gt;   - 提议使用开源和约束较小的应用协议解决方案，如AMQP和HTTP协议，将数据存储在云端。&lt;br&gt;8. **实验设置**:&lt;br&gt;   - 采用实验设置评估和分析数据收集过程的性能和可靠性，以及推荐解决方案的质量。&lt;br&gt;9. **信息补全方法**:&lt;br&gt;   - 使用协同过滤技术来补全关于土壤和植物的不完整信息。&lt;br&gt;10. **机器学习应用**:&lt;br&gt;    - 应用各种机器学习算法，以识别和推荐特定城市地区微农场的最佳植物方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Microfarming and urban computing have evolved as two distinct sustainability
pillars of urban living today. In this paper, we combine these two concepts,
while majorly extending them jointly towards novel concepts of smart
microfarming and urban computing continuum. Smart microfarming is proposed with
applications of artificial intelligence in microfarming, while an urban
computing continuum is proposed as a major extension of the concept towards an
efficient IoT-edge-cloud continuum. We propose and build a system architecture
for a plant recommendation system that uses machine learning at the edge to
find, from a pool of given plants, the most suitable ones for a given microfarm
using monitored soil values obtained from IoT sensor devices. Moreover, we
propose to integrate long-distance LoRa communication solution for sending the
data from IoT to the edge system, due to its unlicensed nature and potential
for open source implementations. Finally, we propose to integrate open source
and less constrained application protocol solutions, such as AMQP and HTTP
protocols, for storing the data in the cloud. An experimental setup is used to
evaluate and analyze the performance and reliability of the data collection
procedure and the quality of the recommendation solution. Furthermore,
collaborative filtering is used for the completion of an incomplete information
about soils and plants. Finally, various ML algorithms are applied to identify
and recommend the optimal plan for a specific microfarm in an urban area.</description>
      <guid isPermaLink="false">2408.02992v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>EasyST: A Simple Framework for Spatio-Temporal Prediction</title>
      <link>http://arxiv.org/abs/2409.06748v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CIKM'2024, full paper&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;1. **研究背景**:&lt;br&gt;   - 空间-时间预测是数据驱动城市计算中的重要研究领域，涉及交通、公共安全和环境监测等方面。&lt;br&gt;2. **当前挑战**:&lt;br&gt;   - 可扩展性和泛化能力是主要障碍。&lt;br&gt;   - 先进模型通常依赖图神经网络（GNN）来编码空间和时间相关性，但在处理大规模数据集时面临复杂性问题。&lt;br&gt;3. **模型局限性**:&lt;br&gt;   - 递归GNN基于消息传递的方案在实际城市感知场景中影响训练和部署。&lt;br&gt;   - 长时间跨度的大规模空间-时间数据引入分布偏移，要求更好的泛化性能。&lt;br&gt;4. **提出的方法**:&lt;br&gt;   - 提出一种简单的空间-时间预测框架——EasyST。&lt;br&gt;   - 通过有效地从复杂的空间-时间GNN中提取知识，学习轻量级且鲁棒的多层感知器（MLP）。&lt;br&gt;5. **知识蒸馏机制**:&lt;br&gt;   - 结合空间-时间信息瓶颈和教师约束回归损失，确保鲁棒的知识蒸馏，过滤掉与任务无关的噪声，避免错误引导。&lt;br&gt;6. **泛化能力增强**:&lt;br&gt;   - 通过引入空间和时间提示，增强学生模型的泛化能力，以提供下游任务上下文。&lt;br&gt;7. **实验结果**:&lt;br&gt;   - 在三个城市计算任务的空间-时间数据集上评估，结果表明EasyST在效率和准确性上超过了现有的最先进方法。&lt;br&gt;8. **代码可用性**:&lt;br&gt;   - 实现代码可在GitHub上获得：[EasyST](https://github.com/HKUDS/EasyST)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spatio-temporal prediction is a crucial research area in data-driven urban
computing, with implications for transportation, public safety, and
environmental monitoring. However, scalability and generalization challenges
remain significant obstacles. Advanced models often rely on Graph Neural
Networks to encode spatial and temporal correlations, but struggle with the
increased complexity of large-scale datasets. The recursive GNN-based message
passing schemes used in these models hinder their training and deployment in
real-life urban sensing scenarios. Moreover, long-spanning large-scale
spatio-temporal data introduce distribution shifts, necessitating improved
generalization performance. To address these challenges, we propose a simple
framework for spatio-temporal prediction - EasyST paradigm. It learns
lightweight and robust Multi-Layer Perceptrons (MLPs) by effectively distilling
knowledge from complex spatio-temporal GNNs. We ensure robust knowledge
distillation by integrating the spatio-temporal information bottleneck with
teacher-bounded regression loss, filtering out task-irrelevant noise and
avoiding erroneous guidance. We further enhance the generalization ability of
the student model by incorporating spatial and temporal prompts to provide
downstream task contexts. Evaluation on three spatio-temporal datasets for
urban computing tasks demonstrates that EasyST surpasses state-of-the-art
approaches in terms of efficiency and accuracy. The implementation code is
available at: https://github.com/HKUDS/EasyST.</description>
      <guid isPermaLink="false">2409.06748v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Robots that Learn to Safely Influence via Prediction-Informed Reach-Avoid Dynamic Games</title>
      <link>http://arxiv.org/abs/2409.12153v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;1. **研究背景**:&lt;br&gt;   - 机器人可以通过影响人们的行为来提高任务效率，例如自动驾驶汽车在交叉口向前移动或桌面操纵器优先获取物体。&lt;br&gt;2. **安全性考虑**:&lt;br&gt;   - 机器人的影响能力若未经过适当控制，可能会危及周围人的安全。&lt;br&gt;3. **研究目标**:&lt;br&gt;   - 提出并解决一种新的鲁棒性“达成-回避”动态博弈，以便在存在安全备份控制的情况下，使机器人最大限度地发挥影响力。&lt;br&gt;4. **人类行为建模**:&lt;br&gt;   - 将人类行为建模为目标驱动，并基于机器人的计划进行调整，从而捕捉到影响力。&lt;br&gt;5. **动态博弈求解**:&lt;br&gt;   - 在联合物理和信念空间中解决动态博弈，使机器人能够推理人类行为的不确定性随着时间的演变。&lt;br&gt;6. **方法实现**:&lt;br&gt;   - 该方法称为SLIDE（在动态环境中安全利用影响），应用于高维（39维）的人机协作操纵任务，使用离线博弈论强化学习解决。&lt;br&gt;7. **与基线比较**:&lt;br&gt;   - 将SLIDE与几个基线进行比较，包括：&lt;br&gt;     - 将人类视为最坏情况对手的鲁棒基线。&lt;br&gt;     - 不明确考虑影响的安全控制器。&lt;br&gt;     - 基于能量函数的安全保护措施。&lt;br&gt;8. **实验结果**:&lt;br&gt;   - SLIDE能够在安全的情况下有效利用对人类的影响，使机器人在确保高安全率的同时，减少保守性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robots can influence people to accomplish their tasks more efficiently:
autonomous cars can inch forward at an intersection to pass through, and
tabletop manipulators can go for an object on the table first. However, a
robot's ability to influence can also compromise the safety of nearby people if
naively executed. In this work, we pose and solve a novel robust reach-avoid
dynamic game which enables robots to be maximally influential, but only when a
safety backup control exists. On the human side, we model the human's behavior
as goal-driven but conditioned on the robot's plan, enabling us to capture
influence. On the robot side, we solve the dynamic game in the joint physical
and belief space, enabling the robot to reason about how its uncertainty in
human behavior will evolve over time. We instantiate our method, called SLIDE
(Safely Leveraging Influence in Dynamic Environments), in a high-dimensional
(39-D) simulated human-robot collaborative manipulation task solved via offline
game-theoretic reinforcement learning. We compare our approach to a robust
baseline that treats the human as a worst-case adversary, a safety controller
that does not explicitly reason about influence, and an energy-function-based
safety shield. We find that SLIDE consistently enables the robot to leverage
the influence it has on the human when it is safe to do so, ultimately allowing
the robot to be less conservative while still ensuring a high safety rate
during task execution.</description>
      <guid isPermaLink="false">2409.12153v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Uncertainty-Aware Visual-Inertial SLAM with Volumetric Occupancy Mapping</title>
      <link>http://arxiv.org/abs/2409.12051v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 4 figures, 5 tables, conference&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;1. **研究目标**:&lt;br&gt;   - 提出一种视觉-惯性同步定位与地图构建（SLAM）方法，结合稀疏重投影误差、惯性测量单元预积分和相对位姿因子与密集体积占用地图。&lt;br&gt;2. **深度预测融合**:&lt;br&gt;   - 使用深度神经网络的深度预测，以完全概率的方式融合深度信息。&lt;br&gt;3. **不确定性意识**:&lt;br&gt;   - 方法充分考虑不确定性：&lt;br&gt;     - 从机器人的立体设备获取深度和不确定性预测，并概率性地融合运动立体数据，以提供跨多种基线的深度信息，从而显著提高地图构建精度。&lt;br&gt;4. **深度不确定性传播**:&lt;br&gt;   - 预测和融合的深度不确定性不仅传播到占用概率中，还影响生成的密集子图之间的对齐因子，这些子图进入概率非线性最小二乘估计器。&lt;br&gt;5. **子图表示**:&lt;br&gt;   - 子图表示提供全球一致的几何形状，适应大规模环境。&lt;br&gt;6. **性能评估**:&lt;br&gt;   - 在两个基准数据集上进行全面评估，结果显示定位和地图构建精度超越当前最先进技术。&lt;br&gt;7. **实际应用**:&lt;br&gt;   - 同时提供直接可用于实时机器人规划和控制的体积占用信息。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose visual-inertial simultaneous localization and mapping that tightly
couples sparse reprojection errors, inertial measurement unit pre-integrals,
and relative pose factors with dense volumetric occupancy mapping. Hereby depth
predictions from a deep neural network are fused in a fully probabilistic
manner. Specifically, our method is rigorously uncertainty-aware: first, we use
depth and uncertainty predictions from a deep network not only from the robot's
stereo rig, but we further probabilistically fuse motion stereo that provides
depth information across a range of baselines, therefore drastically increasing
mapping accuracy. Next, predicted and fused depth uncertainty propagates not
only into occupancy probabilities but also into alignment factors between
generated dense submaps that enter the probabilistic nonlinear least squares
estimator. This submap representation offers globally consistent geometry at
scale. Our method is thoroughly evaluated in two benchmark datasets, resulting
in localization and mapping accuracy that exceeds the state of the art, while
simultaneously offering volumetric occupancy directly usable for downstream
robotic planning and control in real-time.</description>
      <guid isPermaLink="false">2409.12051v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>WeHelp: A Shared Autonomy System for Wheelchair Users</title>
      <link>http://arxiv.org/abs/2409.12159v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;1. **研究背景**:&lt;br&gt;   - 存在大量的轮椅用户，他们在日常任务中需要帮助，但由于护理人员不足，需求未得到满足。&lt;br&gt;2. **项目目标**:&lt;br&gt;   - 开发WeHelp，旨在为轮椅用户提供共享自治系统。&lt;br&gt;3. **系统模式**:&lt;br&gt;   - WeHelp系统设有三种工作模式：&lt;br&gt;     - **跟随模式**：机器人通过视觉跟踪自动跟随轮椅用户，用户可选择从后面、左侧或右侧跟随。&lt;br&gt;     - **遥控模式**：用户通过遥控器控制机器人，帮助完成复杂任务。&lt;br&gt;     - **遥操作模式**：用户使用操纵杆直接控制机器人，执行如开门、移动障碍物、获取高低处物品等任务。&lt;br&gt;4. **语音识别功能**:&lt;br&gt;   - 当轮椅用户请求帮助时，机器人通过语音识别识别命令，并切换到适当的模式（遥操作或遥控）。&lt;br&gt;5. **系统实用性**:&lt;br&gt;   - 评估结果表明，该系统对轮椅用户非常有用且实用。&lt;br&gt;6. **代码和演示**:&lt;br&gt;   - 相关源代码和演示可在此链接获取：[WeHelp GitHub](https://github.com/Walleclipse/WeHelp)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; There is a large population of wheelchair users. Most of the wheelchair users
need help with daily tasks. However, according to recent reports, their needs
are not properly satisfied due to the lack of caregivers. Therefore, in this
project, we develop WeHelp, a shared autonomy system aimed for wheelchair
users. A robot with a WeHelp system has three modes, following mode, remote
control mode and tele-operation mode. In the following mode, the robot follows
the wheelchair user automatically via visual tracking. The wheelchair user can
ask the robot to follow them from behind, by the left or by the right. When the
wheelchair user asks for help, the robot will recognize the command via speech
recognition, and then switch to the teleoperation mode or remote control mode.
In the teleoperation mode, the wheelchair user takes over the robot with a joy
stick and controls the robot to complete some complex tasks for their needs,
such as opening doors, moving obstacles on the way, reaching objects on a high
shelf or on the low ground, etc. In the remote control mode, a remote assistant
takes over the robot and helps the wheelchair user complete some complex tasks
for their needs. Our evaluation shows that the pipeline is useful and practical
for wheelchair users. Source code and demo of the paper are available at
\url{https://github.com/Walleclipse/WeHelp}.</description>
      <guid isPermaLink="false">2409.12159v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>OpenNav: Efficient Open Vocabulary 3D Object Detection for Smart Wheelchair Navigation</title>
      <link>http://arxiv.org/abs/2408.13936v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ECCVW&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;1. **研究背景**:&lt;br&gt;   - 开放词汇3D物体检测（OV3D）能够实现精准且可扩展的物体识别，这对于适应助理机器人所遇到的多样环境至关重要。&lt;br&gt;2. **提出的方法**:&lt;br&gt;   - 本文介绍了OpenNav，一个基于RGB-D图像的零样本3D物体检测管道，专为智能轮椅设计。&lt;br&gt;3. **管道组成**:&lt;br&gt;   - OpenNav管道集成了开放词汇的2D物体检测器和掩膜生成器（用于语义分割），接着进行深度隔离和点云构建，以创建3D边界框。&lt;br&gt;4. **应用场景**:&lt;br&gt;   - 智能轮椅利用这些3D边界框识别潜在目标并安全导航。&lt;br&gt;5. **实验验证**:&lt;br&gt;   - 通过在Replica数据集上的实验展示了OpenNav的性能，并报告了与真实轮椅的初步结果。&lt;br&gt;6. **性能提升**:&lt;br&gt;   - OpenNav在Replica数据集上的mAP25指标提高了9个百分点，mAP50指标提高了5个百分点，同时在mAP指标上也有边际提升。&lt;br&gt;7. **代码开放**:&lt;br&gt;   - 相关代码已公开，供公众访问：[OpenNav GitHub](https://github.com/EasyWalk-PRIN/OpenNav)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/easywalk-prin/opennav&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Open vocabulary 3D object detection (OV3D) allows precise and extensible
object recognition crucial for adapting to diverse environments encountered in
assistive robotics. This paper presents OpenNav, a zero-shot 3D object
detection pipeline based on RGB-D images for smart wheelchairs. Our pipeline
integrates an open-vocabulary 2D object detector with a mask generator for
semantic segmentation, followed by depth isolation and point cloud construction
to create 3D bounding boxes. The smart wheelchair exploits these 3D bounding
boxes to identify potential targets and navigate safely. We demonstrate
OpenNav's performance through experiments on the Replica dataset and we report
preliminary results with a real wheelchair. OpenNav improves state-of-the-art
significantly on the Replica dataset at mAP25 (+9pts) and mAP50 (+5pts) with
marginal improvement at mAP. The code is publicly available at this link:
https://github.com/EasyWalk-PRIN/OpenNav.</description>
      <guid isPermaLink="false">2408.13936v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Bundle Adjustment in the Eager Mode</title>
      <link>http://arxiv.org/abs/2409.12190v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;1. **研究背景**:&lt;br&gt;   - 捆绑调整（BA）在多种机器人应用中至关重要，如同时定位与地图构建（SLAM）、增强现实（AR）和摄影测量。&lt;br&gt;2. **BA的功能**:&lt;br&gt;   - BA优化参数（如相机姿态和3D地标）以使其与观察结果对齐。&lt;br&gt;3. **深度学习的重要性**:&lt;br&gt;   - 随着深度学习在感知系统中的日益重要，越来越需要将BA与深度学习框架整合，以提升可靠性和性能。&lt;br&gt;4. **现有框架的局限**:&lt;br&gt;   - 常用的基于C++的BA框架（如GTSAM、g²o和Ceres）与现代深度学习库（如PyTorch）缺乏原生集成，影响了其灵活性、适应性、调试便捷性和整体实现效率。&lt;br&gt;5. **提出的新框架**:&lt;br&gt;   - 本文介绍了一种与PyPose无缝集成的急切模式BA框架，提供高效的PyTorch兼容接口。&lt;br&gt;6. **设计特点**:&lt;br&gt;   - 该方法包括GPU加速、可微分和稀疏操作，专为二阶优化、李群和李代数运算以及线性求解器设计。&lt;br&gt;7. **性能提升**:&lt;br&gt;   - 我们的GPU急切模式BA显示出显著的运行时效率，相较于GTSAM、g²o和Ceres，平均加速比分别为18.5倍、22倍和23倍。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Bundle adjustment (BA) is a critical technique in various robotic
applications, such as simultaneous localization and mapping (SLAM), augmented
reality (AR), and photogrammetry. BA optimizes parameters such as camera poses
and 3D landmarks to align them with observations. With the growing importance
of deep learning in perception systems, there is an increasing need to
integrate BA with deep learning frameworks for enhanced reliability and
performance. However, widely-used C++-based BA frameworks, such as GTSAM,
g$^2$o, and Ceres, lack native integration with modern deep learning libraries
like PyTorch. This limitation affects their flexibility, adaptability, ease of
debugging, and overall implementation efficiency. To address this gap, we
introduce an eager-mode BA framework seamlessly integrated with PyPose,
providing PyTorch-compatible interfaces with high efficiency. Our approach
includes GPU-accelerated, differentiable, and sparse operations designed for
2nd-order optimization, Lie group and Lie algebra operations, and linear
solvers. Our eager-mode BA on GPU demonstrates substantial runtime efficiency,
achieving an average speedup of 18.5$\times$, 22$\times$, and 23$\times$
compared to GTSAM, g$^2$o, and Ceres, respectively.</description>
      <guid isPermaLink="false">2409.12190v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Veridical Data Science for Medical Foundation Models</title>
      <link>http://arxiv.org/abs/2409.10580v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;1. **研究背景**:&lt;br&gt;   - 基础模型（FMs），如大型语言模型（LLMs）的出现，正在推动数据科学，尤其是医学领域的文化转变。&lt;br&gt;2. **转变内容**:&lt;br&gt;   - 这种转变将重点从为特定、明确的领域问题训练的专门预测模型，转向基于大量非结构化数据预训练的通用基础模型，这些模型可以适应各种临床任务和问题。&lt;br&gt;3. **数据科学工作流程的变化**:&lt;br&gt;   - 医学领域的标准数据科学工作流程发生了根本性变化，基础模型生命周期（FMLC）现在包含明确的上游和下游过程，计算资源、模型和数据访问及决策权力在多个利益相关者之间分配。&lt;br&gt;4. **基础模型的本质**:&lt;br&gt;   - 基础模型本质上是统计模型，这种新工作流程挑战了真实数据科学（VDS）的基本原则，妨碍了在透明和科学可重复的数据科学实践中所期望的严格统计分析。&lt;br&gt;5. **对原则的审视**:&lt;br&gt;   - 本文批判性地考察了医学领域的FMLC，结合VDS的核心原则：可预测性、可计算性和稳定性（PCS），并解释其偏离标准数据科学工作流程的原因。&lt;br&gt;6. **建议与改进**:&lt;br&gt;   - 最后，提出了一套重构医学FMLC的建议，扩展和细化PCS原则，以适应VDS，并考虑基础模型固有的计算和可访问性限制。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The advent of foundation models (FMs) such as large language models (LLMs)
has led to a cultural shift in data science, both in medicine and beyond. This
shift involves moving away from specialized predictive models trained for
specific, well-defined domain questions to generalist FMs pre-trained on vast
amounts of unstructured data, which can then be adapted to various clinical
tasks and questions. As a result, the standard data science workflow in
medicine has been fundamentally altered; the foundation model lifecycle (FMLC)
now includes distinct upstream and downstream processes, in which computational
resources, model and data access, and decision-making power are distributed
among multiple stakeholders. At their core, FMs are fundamentally statistical
models, and this new workflow challenges the principles of Veridical Data
Science (VDS), hindering the rigorous statistical analysis expected in
transparent and scientifically reproducible data science practices. We
critically examine the medical FMLC in light of the core principles of VDS:
predictability, computability, and stability (PCS), and explain how it deviates
from the standard data science workflow. Finally, we propose recommendations
for a reimagined medical FMLC that expands and refines the PCS principles for
VDS including considering the computational and accessibility constraints
inherent to FMs.</description>
      <guid isPermaLink="false">2409.10580v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>PVAFN: Point-Voxel Attention Fusion Network with Multi-Pooling Enhancing for 3D Object Detection</title>
      <link>http://arxiv.org/abs/2408.14600v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  3D Object Detection&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;1. **研究背景**:&lt;br&gt;   - 点云和体素表示的结合在基于LiDAR的3D物体检测中越来越常见，但往往无法有效捕捉语义信息。&lt;br&gt;2. **问题陈述**:&lt;br&gt;   - 仅依赖兴趣区域内的点特征可能导致信息丢失和局部特征表示的局限性。&lt;br&gt;3. **提出的新方法**:&lt;br&gt;   - 本文提出了一种新颖的两阶段3D物体检测器，称为点-体素注意力融合网络（PVAFN）。&lt;br&gt;4. **特征提取阶段**:&lt;br&gt;   - PVAFN利用注意力机制改善多模态特征融合，以提高特征提取阶段的效果。&lt;br&gt;5. **精炼阶段**:&lt;br&gt;   - 在精炼阶段，PVAFN采用多池化策略，有效整合多尺度和区域特定信息。&lt;br&gt;6. **点-体素注意力机制**:&lt;br&gt;   - 该机制自适应地结合点云和基于鸟瞰视图（BEV）的体素特征，从而生成更丰富的物体表示，减少误检。&lt;br&gt;7. **多池化增强模块**:&lt;br&gt;   - 引入多池化增强模块，利用聚类池化和金字塔池化技术，有效捕获关键几何细节和细粒度形状结构，增强局部与全局特征的整合。&lt;br&gt;8. **实验验证**:&lt;br&gt;   - 在KITTI和Waymo数据集上的广泛实验表明，PVAFN在性能上具有竞争力。&lt;br&gt;9. **开放资源**:&lt;br&gt;   - 代码和模型将会公开，供研究者使用。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The integration of point and voxel representations is becoming more common in
LiDAR-based 3D object detection. However, this combination often struggles with
capturing semantic information effectively. Moreover, relying solely on point
features within regions of interest can lead to information loss and
limitations in local feature representation. To tackle these challenges, we
propose a novel two-stage 3D object detector, called Point-Voxel Attention
Fusion Network (PVAFN). PVAFN leverages an attention mechanism to improve
multi-modal feature fusion during the feature extraction phase. In the
refinement stage, it utilizes a multi-pooling strategy to integrate both
multi-scale and region-specific information effectively. The point-voxel
attention mechanism adaptively combines point cloud and voxel-based
Bird's-Eye-View (BEV) features, resulting in richer object representations that
help to reduce false detections. Additionally, a multi-pooling enhancement
module is introduced to boost the model's perception capabilities. This module
employs cluster pooling and pyramid pooling techniques to efficiently capture
key geometric details and fine-grained shape structures, thereby enhancing the
integration of local and global features. Extensive experiments on the KITTI
and Waymo datasets demonstrate that the proposed PVAFN achieves competitive
performance. The code and models will be available.</description>
      <guid isPermaLink="false">2408.14600v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Inference is All You Need: Self Example Retriever for Cross-domain Dialogue State Tracking with ChatGPT</title>
      <link>http://arxiv.org/abs/2409.06243v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;1. **研究背景**:&lt;br&gt;   - 传统对话状态跟踪方法严重依赖大量训练数据和手工特征，这限制了它们的可扩展性和适应新领域的能力。&lt;br&gt;2. **提出的新方法**:&lt;br&gt;   - 本文提出了一种新方法，利用推理和上下文学习结合ChatGPT进行对话状态跟踪中的领域转移，无需任何参数更新。&lt;br&gt;3. **方法机制**:&lt;br&gt;   - 通过引导ChatGPT的思维链，能够检索相关示例并将知识推广，以准确推断对话状态，完全依赖推理过程。&lt;br&gt;4. **实验验证**:&lt;br&gt;   - 在MultiWOZ数据集上的实验结果表明，该方法在性能上具有竞争力，并显示出跨领域的良好泛化能力。&lt;br&gt;5. **优势与贡献**:&lt;br&gt;   - 我们的无参数方法提供了一种可扩展和适应性强的解决方案，为领域转移学习打开了新的研究方向。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traditional dialogue state tracking approaches heavily rely on extensive
training data and handcrafted features, limiting their scalability and
adaptability to new domains. In this paper, we propose a novel method that
leverages inference and in-context learning with ChatGPT for domain transfer in
dialogue state tracking, without any parameter updates. By guiding ChatGPT's
chain of thought, we enable it to retrieve relevant examples and generalize
knowledge to accurately infer dialogue states, solely through inference.
Experimental results on the MultiWOZ dataset demonstrate competitive
performance and promising generalization across domains. Our parameter-free
approach offers a scalable and adaptable solution, opening new research
directions in domain transfer learning.</description>
      <guid isPermaLink="false">2409.06243v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Estimating Wage Disparities Using Foundation Models</title>
      <link>http://arxiv.org/abs/2409.09894v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;1. **研究背景**:&lt;br&gt;   - 社会科学研究中一个重要领域是将群体间结果差异分解为无法解释的部分和可观察因素解释的部分。&lt;br&gt;2. **研究主题**:&lt;br&gt;   - 本文研究性别工资差距的分解，重点在于估计工人的职业历史对性别工资差距的解释部分。&lt;br&gt;3. **传统方法的局限**:&lt;br&gt;   - 经典的工资差距分解方法使用简单的工资预测模型，这些模型仅基于少量劳动历史的简要总结，无法充分利用工人历史的复杂性。&lt;br&gt;4. **遗漏变量偏差（OVB）**:&lt;br&gt;   - 这些简单模型导致遗漏变量偏差，即未包含与性别和工资相关的协变量，从而影响结果。&lt;br&gt;5. **提出的新方法**:&lt;br&gt;   - 探索使用强大的基础模型（如大型语言模型）作为预测引擎的工资差距分解方法，能够处理复杂的高维输入。&lt;br&gt;6. **自定义模型**:&lt;br&gt;   - 使用专门构建的基础模型，该模型旨在根据完整的劳动历史预测工资，从而进行性别工资差距的分解。&lt;br&gt;7. **解决OVB问题**:&lt;br&gt;   - 证明通常训练此类模型的方式可能仍会导致OVB，并开发细化算法以实证减轻此问题。&lt;br&gt;8. **模型优势**:&lt;br&gt;   - 该模型捕捉了比简单模型更丰富的职业历史表示，能够更准确地预测工资。&lt;br&gt;9. **理论基础**:&lt;br&gt;   - 提供了一组新条件，证明基于细化基础模型的工资差距估计器是$\sqrt{n}$-一致的。&lt;br&gt;10. **细化方法**:&lt;br&gt;    - 提出了一些细化基础模型的方法，以最小化OVB。&lt;br&gt;11. **实证结果**:&lt;br&gt;    - 使用《收入动态面板研究》数据发现，职业历史解释了比标准计量经济学模型所能测量的更多的性别工资差距，并识别出重要的历史因素以减少OVB。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; One thread of empirical work in social science focuses on decomposing group
differences in outcomes into unexplained components and components explained by
observable factors. In this paper, we study gender wage decompositions, which
require estimating the portion of the gender wage gap explained by career
histories of workers. Classical methods for decomposing the wage gap employ
simple predictive models of wages which condition on a small set of simple
summaries of labor history. The problem is that these predictive models cannot
take advantage of the full complexity of a worker's history, and the resulting
decompositions thus suffer from omitted variable bias (OVB), where covariates
that are correlated with both gender and wages are not included in the model.
Here we explore an alternative methodology for wage gap decomposition that
employs powerful foundation models, such as large language models, as the
predictive engine. Foundation models excel at making accurate predictions from
complex, high-dimensional inputs. We use a custom-built foundation model,
designed to predict wages from full labor histories, to decompose the gender
wage gap. We prove that the way such models are usually trained might still
lead to OVB, but develop fine-tuning algorithms that empirically mitigate this
issue. Our model captures a richer representation of career history than simple
models and predicts wages more accurately. In detail, we first provide a novel
set of conditions under which an estimator of the wage gap based on a
fine-tuned foundation model is $\sqrt{n}$-consistent. Building on the theory,
we then propose methods for fine-tuning foundation models that minimize OVB.
Using data from the Panel Study of Income Dynamics, we find that history
explains more of the gender wage gap than standard econometric models can
measure, and we identify elements of history that are important for reducing
OVB.</description>
      <guid isPermaLink="false">2409.09894v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>BOX3D: Lightweight Camera-LiDAR Fusion for 3D Object Detection and Localization</title>
      <link>http://arxiv.org/abs/2408.14941v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Presented in MED 2024&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;1. **研究背景**:&lt;br&gt;   - 物体检测和全局定位在机器人技术中至关重要，应用范围广泛，包括自动驾驶汽车和多层次3D场景图的语义理解。&lt;br&gt;2. **提出的新方法**:&lt;br&gt;   - 本文提出了BOX3D，一种新颖的多模态轻量级方案，用于通过融合RGB相机和3D LiDAR的信息来定位感兴趣的物体。&lt;br&gt;3. **架构设计**:&lt;br&gt;   - BOX3D采用三层架构，从接收的顺序传感器数据的局部感知开始，逐步进行全局感知的精细化处理，处理离群点和每个物体观察的一致性。&lt;br&gt;4. **第一层功能**:&lt;br&gt;   - 第一层处理相机和LiDAR数据的低级融合，用于初步提取3D边界框。&lt;br&gt;5. **第二层功能**:&lt;br&gt;   - 第二层将每个LiDAR扫描获得的3D边界框转换为世界坐标系，并应用空间配对和合并机制，以保持从不同视角观察到的物体的独特性。&lt;br&gt;6. **第三层功能**:&lt;br&gt;   - 第三层迭代监督结果在全局地图上的一致性，通过点到体素的比较来识别属于某一物体的所有点。&lt;br&gt;7. **实验验证**:&lt;br&gt;   - 在多个实验中展示了所提架构的基准测试结果，使用了公共的最先进的大规模城市环境数据集。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Object detection and global localization play a crucial role in robotics,
spanning across a great spectrum of applications from autonomous cars to
multi-layered 3D Scene Graphs for semantic scene understanding. This article
proposes BOX3D, a novel multi-modal and lightweight scheme for localizing
objects of interest by fusing the information from RGB camera and 3D LiDAR.
BOX3D is structured around a three-layered architecture, building up from the
local perception of the incoming sequential sensor data to the global
perception refinement that covers for outliers and the general consistency of
each object's observation. More specifically, the first layer handles the
low-level fusion of camera and LiDAR data for initial 3D bounding box
extraction. The second layer converts each LiDAR's scan 3D bounding boxes to
the world coordinate frame and applies a spatial pairing and merging mechanism
to maintain the uniqueness of objects observed from different viewpoints.
Finally, BOX3D integrates the third layer that supervises the consistency of
the results on the global map iteratively, using a point-to-voxel comparison
for identifying all points in the global map that belong to the object.
Benchmarking results of the proposed novel architecture are showcased in
multiple experimental trials on public state-of-the-art large-scale dataset of
urban environments.</description>
      <guid isPermaLink="false">2408.14941v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Advancements in Gesture Recognition Techniques and Machine Learning for Enhanced Human-Robot Interaction: A Comprehensive Review</title>
      <link>http://arxiv.org/abs/2409.06503v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages,1 Figure&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;1. **研究背景**:&lt;br&gt;   - 近年来，机器人在日常生活中变得越来越重要，应用范围广泛。&lt;br&gt;2. **人机交互的重要性**:&lt;br&gt;   - 人机交互（HRI）在机器人领域产生了积极影响，有助于与机器人进行互动和沟通。&lt;br&gt;3. **手势识别技术的进展**:&lt;br&gt;   - 手势识别技术与机器学习算法的结合在HRI中取得了显著进展。&lt;br&gt;4. **文献综述**:&lt;br&gt;   - 本文全面回顾了手势识别方法的最新进展及其与机器学习方法的集成，以增强HRI。&lt;br&gt;5. **基于视觉的手势识别**:&lt;br&gt;   - 本文介绍了基于视觉的手势识别，利用深度传感系统实现安全可靠的人机交互。&lt;br&gt;6. **机器学习算法的角色**:&lt;br&gt;   - 分析了深度学习、强化学习和迁移学习等机器学习算法在提高手势识别系统准确性和鲁棒性方面的作用。&lt;br&gt;7. **有效沟通的目标**:&lt;br&gt;   - 探讨了如何通过改进手势识别系统来实现人类与机器人之间的有效沟通。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years robots have become an important part of our day-to-day lives
with various applications. Human-robot interaction creates a positive impact in
the field of robotics to interact and communicate with the robots. Gesture
recognition techniques combined with machine learning algorithms have shown
remarkable progress in recent years, particularly in human-robot interaction
(HRI). This paper comprehensively reviews the latest advancements in gesture
recognition methods and their integration with machine learning approaches to
enhance HRI. Furthermore, this paper represents the vision-based gesture
recognition for safe and reliable human-robot-interaction with a depth-sensing
system, analyses the role of machine learning algorithms such as deep learning,
reinforcement learning, and transfer learning in improving the accuracy and
robustness of gesture recognition systems for effective communication between
humans and robots.</description>
      <guid isPermaLink="false">2409.06503v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Causal GNNs: A GNN-Driven Instrumental Variable Approach for Causal Inference in Networks</title>
      <link>http://arxiv.org/abs/2409.08544v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;1. **合作感知的挑战**:&lt;br&gt;   - 合作感知研究中存在通信带宽与感知性能之间的权衡。&lt;br&gt;2. **当前的融合解决方案**:&lt;br&gt;   - 现有特征融合解决方案在物体检测性能上表现出色，但需要传输大量中间特征图，消耗大量带宽。&lt;br&gt;3. **模型限制**:&lt;br&gt;   - 传统融合方法通常要求使用相同检测模型的车辆。&lt;br&gt;4. **研究目标**:&lt;br&gt;   - 发展一种支持不同传感器模态的车辆之间合作感知的解决方案，以提高感知性能。&lt;br&gt;5. **改进目标**:&lt;br&gt;   - 新方法旨在提供比晚期融合技术更好的感知性能，同时在带宽需求上显著降低，与先进的中间融合精度相似。&lt;br&gt;6. **方法名称**:&lt;br&gt;   - 提出了HEAD方法，融合3D物体检测网络中分类和回归头的特征。&lt;br&gt;7. **兼容性**:&lt;br&gt;   - HEAD方法能够与不同的检测网络兼容，包括LiDAR PointPillars、SECOND、VoxelNet和相机鸟瞰图（BEV）编码器。&lt;br&gt;8. **特征融合机制**:&lt;br&gt;   - 设计了自注意力机制来融合分类头，以及一个互补特征融合层来融合回归头，以适应检测头较小的特征尺寸。&lt;br&gt;9. **实验评估**:&lt;br&gt;   - 在V2V4Real和OPV2V数据集上进行了全面评估，证明HEAD方法在通信带宽与感知性能之间有效平衡。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As network data applications continue to expand, causal inference within
networks has garnered increasing attention. However, hidden confounders
complicate the estimation of causal effects. Most methods rely on the strong
ignorability assumption, which presumes the absence of hidden confounders-an
assumption that is both difficult to validate and often unrealistic in
practice. To address this issue, we propose CgNN, a novel approach that
leverages network structure as instrumental variables (IVs), combined with
graph neural networks (GNNs) and attention mechanisms, to mitigate hidden
confounder bias and improve causal effect estimation. By utilizing network
structure as IVs, we reduce confounder bias while preserving the correlation
with treatment. Our integration of attention mechanisms enhances robustness and
improves the identification of important nodes. Validated on two real-world
datasets, our results demonstrate that CgNN effectively mitigates hidden
confounder bias and offers a robust GNN-driven IV framework for causal
inference in complex network data.</description>
      <guid isPermaLink="false">2409.08544v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>HEAD: A Bandwidth-Efficient Cooperative Perception Approach for Heterogeneous Connected and Autonomous Vehicles</title>
      <link>http://arxiv.org/abs/2408.15428v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ECCV 2024 Workshop&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;1. **研究目的**:&lt;br&gt;   - 本研究探讨低秩适应（LoRA）在细化地球观测（EO）基础模型用于洪水分割的有效性。&lt;br&gt;2. **研究假设**:&lt;br&gt;   - 假设LoRA作为一种参数高效的技术，可以显著加速大型EO模型的适应，同时保持高性能。&lt;br&gt;3. **实验设计**:&lt;br&gt;   - 将LoRA应用于对一种在多样卫星影像上预训练的先进EO基础模型进行细化，使用精选的洪水事件数据集。&lt;br&gt;4. **结果**:&lt;br&gt;   - LoRA细化（r-256）相比于冻结编码器基线，提高了F1得分6.66点和IoU 0.11，同时显著降低了计算成本。&lt;br&gt;5. **相对比较**:&lt;br&gt;   - LoRA的表现优于全面细化，后者在我们的硬件上计算上不可行。&lt;br&gt;6. **泛化能力评估**:&lt;br&gt;   - 通过对地理上不同的洪水事件进行分布外（OOD）测试，评估了模型的泛化能力。&lt;br&gt;7. **OOD性能**:&lt;br&gt;   - LoRA配置显示出相较于基线的OOD性能改善。&lt;br&gt;8. **研究贡献**:&lt;br&gt;   - 研究为基础模型在特定EO任务的高效适应提供了重要贡献，对灾害管理中的快速响应系统具有重要意义。&lt;br&gt;9. **应用前景**:&lt;br&gt;   - 研究表明LoRA有助于在资源受限以下是对论文摘要的分点解读：&lt;br&gt;1. **研究目的**:&lt;br&gt;   - 本研究探讨低秩适应（LoRA）在细化地球观测（EO）基础模型用于洪水分割的有效性。&lt;br&gt;2. **研究假设**:&lt;br&gt;   - 假设LoRA作为一种参数高效的技术，可以显著加速大型EO模型的适应，同时保持高性能。&lt;br&gt;3. **实验设计**:&lt;br&gt;   - 将LoRA应用于对一种在多样卫星影像上预训练的先进EO基础模型进行细化，使用精选的洪水事件数据集。&lt;br&gt;4. **结果**:&lt;br&gt;   - LoRA细化（r-256）相比于冻结编码器基线，提高了F1得分6.66点和IoU 0.11，同时显著降低了计算成本。&lt;br&gt;5. **相对比较**:&lt;br&gt;   - LoRA的表现优于全面细化，后者在我们的硬件上计算上不可行。&lt;br&gt;6. **泛化能力评估**:&lt;br&gt;   - 通过对地理上不同的洪水事件进行分布外（OOD）测试，评估了模型的泛化能力。&lt;br&gt;7. **OOD性能**:&lt;br&gt;   - LoRA配置显示出相较于基线的OOD性能改善。&lt;br&gt;8. **研究贡献**:&lt;br&gt;   - 研究为基础模型在特定EO任务的高效适应提供了重要贡献，对灾害管理中的快速响应系统具有重要意义。&lt;br&gt;9. **应用前景**:&lt;br&gt;   - 研究表明LoRA有助于在资源受限和时间紧迫的情况下快速部署准确的洪水分割模型。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In cooperative perception studies, there is often a trade-off between
communication bandwidth and perception performance. While current feature
fusion solutions are known for their excellent object detection performance,
transmitting the entire sets of intermediate feature maps requires substantial
bandwidth. Furthermore, these fusion approaches are typically limited to
vehicles that use identical detection models. Our goal is to develop a solution
that supports cooperative perception across vehicles equipped with different
modalities of sensors. This method aims to deliver improved perception
performance compared to late fusion techniques, while achieving precision
similar to the state-of-art intermediate fusion, but requires an order of
magnitude less bandwidth. We propose HEAD, a method that fuses features from
the classification and regression heads in 3D object detection networks. Our
method is compatible with heterogeneous detection networks such as LiDAR
PointPillars, SECOND, VoxelNet, and camera Bird's-eye View (BEV) Encoder. Given
the naturally smaller feature size in the detection heads, we design a
self-attention mechanism to fuse the classification head and a complementary
feature fusion layer to fuse the regression head. Our experiments,
comprehensively evaluated on the V2V4Real and OPV2V datasets, demonstrate that
HEAD is a fusion method that effectively balances communication bandwidth and
perception performance.</description>
      <guid isPermaLink="false">2408.15428v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Rapid Adaptation of Earth Observation Foundation Models for Segmentation</title>
      <link>http://arxiv.org/abs/2409.09907v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages 2 figures&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;1. **高效CNN网络**:&lt;br&gt;   - 多种高效卷积神经网络（CNN），如DenseNet201、InceptionV3、ResNet152v2、SEresNet152、VGG19和Xception，因其性能而受到关注。&lt;br&gt;2. **扩展的CNN范式**:&lt;br&gt;   - CNN的应用扩展到了迁移学习和集成模型，基于原始CNN架构。&lt;br&gt;3. **研究发现**:&lt;br&gt;   - 研究表明，迁移学习和集成模型能够提高深度学习模型的准确性。&lt;br&gt;4. **研究空白**:&lt;br&gt;   - 很少有研究全面实验这些技术在血液恶性肿瘤检测和定位中的应用。&lt;br&gt;5. **实验设计**:&lt;br&gt;   - 本研究进行了三项实验： &lt;br&gt;     - 第一次实验使用六个原始CNN。&lt;br&gt;     - 第二次实验应用迁移学习。&lt;br&gt;     - 第三次实验开发了一个新型集成模型DIX（DenseNet201、InceptionV3和Xception）。&lt;br&gt;6. **实验结果**:&lt;br&gt;   - DIX模型的表现优于原始CNN和迁移学习模型，提供了99.12%的准确率。&lt;br&gt;7. **迁移学习的局限性**:&lt;br&gt;   - 研究还发现，迁移学习并未提高原始CNN的准确性。&lt;br&gt;8. **血癌检测的重要性**:&lt;br&gt;   - 像许多其他癌症一样，血癌的及时识别对于有效治疗和提高生存率至关重要。&lt;br&gt;9. **CNN模型以下是对论文摘要的分点解读：&lt;br&gt;1. **高效CNN网络**:&lt;br&gt;   - 多种高效卷积神经网络（CNN），如DenseNet201、InceptionV3、ResNet152v2、SEresNet152、VGG19和Xception，因其性能而受到关注。&lt;br&gt;2. **扩展的CNN范式**:&lt;br&gt;   - CNN的应用扩展到了迁移学习和集成模型，基于原始CNN架构。&lt;br&gt;3. **研究发现**:&lt;br&gt;   - 研究表明，迁移学习和集成模型能够提高深度学习模型的准确性。&lt;br&gt;4. **研究空白**:&lt;br&gt;   - 很少有研究全面实验这些技术在血液恶性肿瘤检测和定位中的应用。&lt;br&gt;5. **实验设计**:&lt;br&gt;   - 本研究进行了三项实验： &lt;br&gt;     - 第一次实验使用六个原始CNN。&lt;br&gt;     - 第二次实验应用迁移学习。&lt;br&gt;     - 第三次实验开发了一个新型集成模型DIX（DenseNet201、InceptionV3和Xception）。&lt;br&gt;6. **实验结果**:&lt;br&gt;   - DIX模型的表现优于原始CNN和迁移学习模型，提供了99.12%的准确率。&lt;br&gt;7. **迁移学习的局限性**:&lt;br&gt;   - 研究还发现，迁移学习并未提高原始CNN的准确性。&lt;br&gt;8. **血癌检测的重要性**:&lt;br&gt;   - 像许多其他癌症一样，血癌的及时识别对于有效治疗和提高生存率至关重要。&lt;br&gt;9. **CNN模型的潜力**:&lt;br&gt;   - 高准确率表明CNN模型在血癌检测中具有前景。&lt;br&gt;10. **研究意义**:&lt;br&gt;    - 该研究在生物医学工程、计算机辅助疾病诊断和基于机器学习的疾病检测领域具有重要意义。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study investigates the efficacy of Low-Rank Adaptation (LoRA) in
fine-tuning Earth Observation (EO) foundation models for flood segmentation. We
hypothesize that LoRA, a parameter-efficient technique, can significantly
accelerate the adaptation of large-scale EO models to this critical task while
maintaining high performance. We apply LoRA to fine-tune a state-of-the-art EO
foundation model pre-trained on diverse satellite imagery, using a curated
dataset of flood events. Our results demonstrate that LoRA-based fine-tuning
(r-256) improves F1 score by 6.66 points and IoU by 0.11 compared to a frozen
encoder baseline, while significantly reducing computational costs. Notably,
LoRA outperforms full fine-tuning, which proves computationally infeasible on
our hardware. We further assess generalization through out-of-distribution
(OOD) testing on a geographically distinct flood event. While LoRA
configurations show improved OOD performance over the baseline. This work
contributes to research on efficient adaptation of foundation models for
specialized EO tasks, with implications for rapid response systems in disaster
management. Our findings demonstrate LoRA's potential for enabling faster
deployment of accurate flood segmentation models in resource-constrained,
time-critical scenarios.</description>
      <guid isPermaLink="false">2409.09907v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>A comprehensive study on Blood Cancer detection and classification using Convolutional Neural Network</title>
      <link>http://arxiv.org/abs/2409.06689v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;1. **高效CNN网络**:&lt;br&gt;   - 多种高效卷积神经网络（CNN），如DenseNet201、InceptionV3、ResNet152v2、SEresNet152、VGG19和Xception，因其性能调研不足**:&lt;br&gt;   - 当前缺乏全面的调查，汇总和总结这些感知任务及其发展。&lt;br&gt;4. **传统方法总结**:&lt;br&gt;   - 论文总结了传统的3D物体检测方法，重点关注基于相机、LiDAR和融合检测技术。&lt;br&gt;5. **优缺点分析**:&lt;br&gt;   - 提供了对每种方法的优缺点的全面分析，强调了准确性和鲁棒性的进展。&lt;br&gt;6. **未来方向**:&lt;br&gt;   - 讨论了提高准确性的方法，如时间感知、占用网格和端到端学习框架。&lt;br&gt;7. **合作感知**:&lt;br&gt;   - 探索了通过协作通信扩展感知范围的合作感知方法。&lt;br&gt;8. **综合视角**:&lt;br&gt;   - 旨在提供对3D物体感知当前状态和未来发展的全面理解。&lt;br&gt;9. **活跃的资源库**:&lt;br&gt;   - 建立了一个活跃的资源库，提供该领域最新进展的持续更新，链接为： [GitHub Repository](https://github.com/Fishsoup0/Autonomous-Driving-Perception)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Over the years in object detection several efficient Convolutional Neural
Networks (CNN) networks, such as DenseNet201, InceptionV3, ResNet152v2,
SEresNet152, VGG19, Xception gained significant attention due to their
performance. Moreover, CNN paradigms have expanded to transfer learning and
ensemble models from original CNN architectures. Research studies suggest that
transfer learning and ensemble models are capable of increasing the accuracy of
deep learning (DL) models. However, very few studies have conducted
comprehensive experiments utilizing these techniques in detecting and
localizing blood malignancies. Realizing the gap, this study conducted three
experiments; in the first experiment -- six original CNNs were used, in the
second experiment -- transfer learning and, in the third experiment a novel
ensemble model DIX (DenseNet201, InceptionV3, and Xception) was developed to
detect and classify blood cancer. The statistical result suggests that DIX
outperformed the original and transfer learning performance, providing an
accuracy of 99.12%. However, this study also provides a negative result in the
case of transfer learning, as the transfer learning did not increase the
accuracy of the original CNNs. Like many other cancers, blood cancer diseases
require timely identification for effective treatment plans and increased
survival possibilities. The high accuracy in detecting and categorization blood
cancer detection using CNN suggests that the CNN model is promising in blood
cancer disease detection. This research is significant in the fields of
biomedical engineering, computer-aided disease diagnosis, and ML-based disease
detection.</description>
      <guid isPermaLink="false">2409.06689v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>A Comprehensive Review of 3D Object Detection in Autonomous Driving: Technological Advances and Future Directions</title>
      <link>http://arxiv.org/abs/2408.16530v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;1. **重要性**:&lt;br&gt;   - 3D物体感知在自动驾驶系统中至关重要，提供了环境意识。&lt;br&gt;2. **发展变化**:&lt;br&gt;   - 随着感知任务的演变，出现了多种变体，导致来自工业和学术界的不同见解。&lt;br&gt;3. **调研不足**:&lt;br&gt;   - 当前缺乏全面的调查，汇总和总结这些感知任务及其发展。&lt;br&gt;4. **传统方法总结**:&lt;br&gt;   - 论文总结了传统的3D物体检测方法，重点关注基于相机、LiDAR和融合检测技术。&lt;br&gt;5. **优缺点分析**:&lt;br&gt;   - 提供了对每种方法的优缺点的全面分析，强调了准确性和鲁棒性的进展。&lt;br&gt;6. **未来方向**:&lt;br&gt;   - 讨论了提高准确性的方法，如时间感知、占用网格和端到端学习框架。&lt;br&gt;7. **合作感知**:&lt;br&gt;   - 探索了通过协作通信扩展感知范围的合作感知方法。&lt;br&gt;8. **综合视角**:&lt;br&gt;   - 旨在提供对3D物体感知当前状态和未来发展的全面理解。&lt;br&gt;9. **活跃的资源库**:&lt;br&gt;   - 建立了一个活跃的资源库，提供该领域最新进展的持续更新，链接为： [GitHub Repository](https://github.com/Fishsoup0/Autonomous-Driving-Perception)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/fishsoup0/autonomous-driving-perception&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, 3D object perception has become a crucial component in the
development of autonomous driving systems, providing essential environmental
awareness. However, as perception tasks in autonomous driving evolve, their
variants have increased, leading to diverse insights from industry and
academia. Currently, there is a lack of comprehensive surveys that collect and
summarize these perception tasks and their developments from a broader
perspective. This review extensively summarizes traditional 3D object detection
methods, focusing on camera-based, LiDAR-based, and fusion detection
techniques. We provide a comprehensive analysis of the strengths and
limitations of each approach, highlighting advancements in accuracy and
robustness. Furthermore, we discuss future directions, including methods to
improve accuracy such as temporal perception, occupancy grids, and end-to-end
learning frameworks. We also explore cooperative perception methods that extend
the perception range through collaborative communication. By providing a
holistic view of the current state and future developments in 3D object
perception, we aim to offer a more comprehensive understanding of perception
tasks for autonomous driving. Additionally, we have established an active
repository to provide continuous updates on the latest advancements in this
field, accessible at:
https://github.com/Fishsoup0/Autonomous-Driving-Perception.</description>
      <guid isPermaLink="false">2408.16530v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Human Insights Driven Latent Space for Different Driving Perspectives: A Unified Encoder for Efficient Multi-Task Inference</title>
      <link>http://arxiv.org/abs/2409.10095v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 自主驾驶有潜力通过减少人为错误和缓解交通拥堵来提高道路安全性和交通效率。&lt;br&gt;&lt;h4&gt;2. 核心挑战&lt;/h4&gt;   - 准确估计转向角度是实现自主驾驶潜力的关键，关系到有效的车辆导航和控制。&lt;br&gt;&lt;h4&gt;3. 技术进展&lt;/h4&gt;   - 最近深度学习的突破使得可以直接从原始摄像头输入中估计转向角度。&lt;br&gt;&lt;h4&gt;4. 数据限制&lt;/h4&gt;   - 可用的导航数据有限，可能阻碍最优特征学习，影响系统在复杂驾驶场景中的表现。&lt;br&gt;&lt;h4&gt;5. 提出方法&lt;/h4&gt;   - 本文提出了一种共享编码器，针对城市导航中关键的多个计算机视觉任务进行训练，包括深度、姿态、3D场景流估计，以及语义、实例、全景和运动分割。&lt;br&gt;&lt;h4&gt;6. 视觉信息整合&lt;/h4&gt;   - 通过整合人类在导航中使用的多样视觉信息，该统一编码器可能增强转向角度的估计。&lt;br&gt;&lt;h4&gt;7. 多任务学习&lt;/h4&gt;   - 为实现单一编码器内的有效多任务学习，提出了一个多尺度特征网络来改善深度学习。&lt;br&gt;&lt;h4&gt;8. 知识蒸馏&lt;/h4&gt;   - 采用从多骨干模型进行知识蒸馏，这些模型在导航任务上进行预训练，以稳定训练并提升性能。&lt;br&gt;&lt;h4&gt;9. 研究发现&lt;/h4&gt;   - 经过多样视觉任务训练的共享骨干能够提供整体感知能力。&lt;br&gt;&lt;h4&gt;10. 性能比较&lt;/h4&gt;    - 在转向角度估计方面，我们的性能与现有方法相当，同时通过多任务学习整合人类般的感知能力，为自主驾驶系统的进步提供了重要潜力。&lt;br&gt;&lt;h4&gt;11. 更多信息&lt;/h4&gt;    - 详细信息和预训练模型可在指定链接获取。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous driving holds great potential to transform road safety and traffic
efficiency by minimizing human error and reducing congestion. A key challenge
in realizing this potential is the accurate estimation of steering angles,
which is essential for effective vehicle navigation and control. Recent
breakthroughs in deep learning have made it possible to estimate steering
angles directly from raw camera inputs. However, the limited available
navigation data can hinder optimal feature learning, impacting the system's
performance in complex driving scenarios. In this paper, we propose a shared
encoder trained on multiple computer vision tasks critical for urban
navigation, such as depth, pose, and 3D scene flow estimation, as well as
semantic, instance, panoptic, and motion segmentation. By incorporating diverse
visual information used by humans during navigation, this unified encoder might
enhance steering angle estimation. To achieve effective multi-task learning
within a single encoder, we introduce a multi-scale feature network for pose
estimation to improve depth learning. Additionally, we employ knowledge
distillation from a multi-backbone model pretrained on these navigation tasks
to stabilize training and boost performance. Our findings demonstrate that a
shared backbone trained on diverse visual tasks is capable of providing overall
perception capabilities. While our performance in steering angle estimation is
comparable to existing methods, the integration of human-like perception
through multi-task learning holds significant potential for advancing
autonomous driving systems. More details and the pretrained model are available
at https://hi-computervision.github.io/uni-encoder/.</description>
      <guid isPermaLink="false">2409.10095v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>A study on Deep Convolutional Neural Networks, Transfer Learning and Ensemble Model for Breast Cancer Detection</title>
      <link>http://arxiv.org/abs/2409.06699v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 在深度学习中，迁移学习和集成模型在计算机辅助疾病诊断中展现了良好的前景，但其应用仍相对有限。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 集成模型的发展往往是临时的，忽视了冗余层，且在处理不平衡数据集和数据增强不足方面存在问题。&lt;br&gt;&lt;h4&gt;3. 特定领域的挑战&lt;/h4&gt;   - 尽管引入了显著的深度卷积神经网络（D-CNN）用于乳腺癌的检测与分类，但关于现有CNN架构的比较研究很少。&lt;br&gt;&lt;h4&gt;4. 研究目标&lt;/h4&gt;   - 本研究旨在比较不同D-CNN模型（包括原始CNN、迁移学习和集成模型）在乳腺癌检测中的表现。&lt;br&gt;&lt;h4&gt;5. 比较模型&lt;/h4&gt;   - 该研究比较了六种基于CNN的深度学习架构（SE-ResNet152、MobileNetV2、VGG19、ResNet18、InceptionV3和DenseNet-121）、迁移学习和集成模型在乳腺癌检测中的性能。&lt;br&gt;&lt;h4&gt;6. 实验结果&lt;/h4&gt;   - 在这些模型的比较中，集成模型在乳腺癌检测和分类中提供了最高的准确率，达到99.94%。&lt;br&gt;&lt;h4&gt;7. 迁移学习的负面结果&lt;/h4&gt;   - 研究发现，迁移学习未能提高原始模型（如SE-ResNet152等）的准确性，显示出迁移学习在此场景中的局限性。&lt;br&gt;&lt;h4&gt;8. 结论与意义&lt;/h4&gt;   - 使用CNN在乳腺癌检测和分类中表现出高准确率，表明CNN模型在乳腺癌疾病检测中具有良好潜力。&lt;br&gt;   - 该研究在生物医学工程、计算机辅助疾病诊断以及基于机器学习的疾病检测领域具有重要意义。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In deep learning, transfer learning and ensemble models have shown promise in
improving computer-aided disease diagnosis. However, applying the transfer
learning and ensemble model is still relatively limited. Moreover, the ensemble
model's development is ad-hoc, overlooks redundant layers, and suffers from
imbalanced datasets and inadequate augmentation. Lastly, significant Deep
Convolutional Neural Networks (D-CNNs) have been introduced to detect and
classify breast cancer. Still, very few comparative studies were conducted to
investigate the accuracy and efficiency of existing CNN architectures.
Realising the gaps, this study compares the performance of D-CNN, which
includes the original CNN, transfer learning, and an ensemble model, in
detecting breast cancer. The comparison study of this paper consists of
comparison using six CNN-based deep learning architectures (SE-ResNet152,
MobileNetV2, VGG19, ResNet18, InceptionV3, and DenseNet-121), a transfer
learning, and an ensemble model on breast cancer detection. Among the
comparison of these models, the ensemble model provides the highest detection
and classification accuracy of 99.94% for breast cancer detection and
classification. However, this study also provides a negative result in the case
of transfer learning, as the transfer learning did not increase the accuracy of
the original SE-ResNet152, MobileNetV2, VGG19, ResNet18, InceptionV3, and
DenseNet-121 model. The high accuracy in detecting and categorising breast
cancer detection using CNN suggests that the CNN model is promising in breast
cancer disease detection. This research is significant in biomedical
engineering, computer-aided disease diagnosis, and ML-based disease detection.</description>
      <guid isPermaLink="false">2409.06699v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>FAPP: Fast and Adaptive Perception and Planning for UAVs in Dynamic Cluttered Environments</title>
      <link>http://arxiv.org/abs/2312.08743v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 无人机（UAV）在复杂环境中的障碍物避让是一项重大挑战。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 现有的无人机避障技术主要关注完全静态环境或仅有少量动态物体的静态环境。&lt;br&gt;&lt;h4&gt;3. 研究目标&lt;/h4&gt;   - 本文旨在研究无人机在动态杂乱环境中的障碍物避让，这种环境中动态物体占主导地位。&lt;br&gt;&lt;h4&gt;4. 挑战&lt;/h4&gt;   - 动态杂乱环境对感知和规划提出了重大挑战，多个动态物体的运动各异，使得使用单一运动模型来估计和预测其运动变得极其困难。&lt;br&gt;&lt;h4&gt;5. 方法提出&lt;/h4&gt;   - 本文提出了一种名为快速自适应感知与规划（FAPP）的方法，针对复杂动态杂乱环境中的无人机飞行。&lt;br&gt;&lt;h4&gt;6. 点云分割策略&lt;/h4&gt;   - 提出了一种新颖高效的点云分割策略，用于区分静态物体和动态物体。&lt;br&gt;&lt;h4&gt;7. 运动预测&lt;/h4&gt;   - 针对不同运动的多个动态物体，提出了一种自适应估计方法，结合协方差适应性，快速准确地预测其运动。&lt;br&gt;&lt;h4&gt;8. 轨迹优化算法&lt;/h4&gt;   - 提出的轨迹优化算法高效，能够避开快速移动的物体。&lt;br&gt;&lt;h4&gt;9. 自适应重新规划方法&lt;/h4&gt;   - 当轨迹优化无法找到可行解时，提出了一种自适应重新规划方法，这在动态杂乱环境中非常常见。&lt;br&gt;&lt;h4&gt;10. 验证结果&lt;/h4&gt;    - 在模拟和真实世界实验中进行了广泛验证，证明了所提系统在高度动态和杂乱环境中的有效性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2023-12-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Obstacle avoidance for Unmanned Aerial Vehicles (UAVs) in cluttered
environments is significantly challenging. Existing obstacle avoidance for UAVs
either focuses on fully static environments or static environments with only a
few dynamic objects. In this paper, we take the initiative to consider the
obstacle avoidance of UAVs in dynamic cluttered environments in which dynamic
objects are the dominant objects. This type of environment poses significant
challenges to both perception and planning. Multiple dynamic objects possess
various motions, making it extremely difficult to estimate and predict their
motions using one motion model. The planning must be highly efficient to avoid
cluttered dynamic objects. This paper proposes Fast and Adaptive Perception and
Planning (FAPP) for UAVs flying in complex dynamic cluttered environments. A
novel and efficient point cloud segmentation strategy is proposed to
distinguish static and dynamic objects. To address multiple dynamic objects
with different motions, an adaptive estimation method with covariance
adaptation is proposed to quickly and accurately predict their motions. Our
proposed trajectory optimization algorithm is highly efficient, enabling it to
avoid fast objects. Furthermore, an adaptive re-planning method is proposed to
address the case when the trajectory optimization cannot find a feasible
solution, which is common for dynamic cluttered environments. Extensive
validations in both simulation and real-world experiments demonstrate the
effectiveness of our proposed system for highly dynamic and cluttered
environments.</description>
      <guid isPermaLink="false">2312.08743v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Sybil Detection using Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2409.08631v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 1 figure, 6 tables&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究主题&lt;/h4&gt;   - 本文提出了SYBILGAT，一种使用图注意力网络（GAT）进行社交网络中Sybil检测的新方法。&lt;br&gt;&lt;h4&gt;2. 传统方法的局限性&lt;/h4&gt;   - 传统的Sybil检测方法主要依赖于网络的结构特性，但在面对大量攻击边缘时，效果不佳，且无法同时利用已知的Sybil节点和诚实节点。&lt;br&gt;&lt;h4&gt;3. 方法创新&lt;/h4&gt;   - SYBILGAT通过在聚合过程中动态分配注意力权重来克服这些限制，从而提高检测性能。&lt;br&gt;&lt;h4&gt;4. 实验设计&lt;/h4&gt;   - 进行了广泛的实验，涵盖了多种场景，包括在抽样子图中的预训练、合成网络和针对性攻击下的网络。&lt;br&gt;&lt;h4&gt;5. 实验结果&lt;/h4&gt;   - 结果表明，SYBILGAT在高攻击复杂性和攻击边缘数量增加的场景中显著优于现有最先进的算法。&lt;br&gt;&lt;h4&gt;6. 性能稳健性&lt;/h4&gt;   - 该方法在不同的网络模型和规模下表现出稳健的性能，即使检测任务变得更加困难。&lt;br&gt;&lt;h4&gt;7. 实际应用&lt;/h4&gt;   - 成功将该模型应用于一个包含超过269k节点和6.8M边的真实Twitter图。&lt;br&gt;&lt;h4&gt;8. 方法的灵活性和通用性&lt;/h4&gt;   - SYBILGAT的灵活性和通用性使其成为抵御在线社交网络中Sybil攻击的有前景工具，且仅依赖结构信息。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents SYBILGAT, a novel approach to Sybil detection in social
networks using Graph Attention Networks (GATs). Traditional methods for Sybil
detection primarily leverage structural properties of networks; however, they
tend to struggle with a large number of attack edges and are often unable to
simultaneously utilize both known Sybil and honest nodes. Our proposed method
addresses these limitations by dynamically assigning attention weights to
different nodes during aggregations, enhancing detection performance. We
conducted extensive experiments in various scenarios, including pretraining in
sampled subgraphs, synthetic networks, and networks under targeted attacks. The
results show that SYBILGAT significantly outperforms the state-of-the-art
algorithms, particularly in scenarios with high attack complexity and when the
number of attack edges increases. Our approach shows robust performance across
different network models and sizes, even as the detection task becomes more
challenging. We successfully applied the model to a real-world Twitter graph
with more than 269k nodes and 6.8M edges. The flexibility and generalizability
of SYBILGAT make it a promising tool to defend against Sybil attacks in online
social networks with only structural information.</description>
      <guid isPermaLink="false">2409.08631v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Transfer Learning from Simulated to Real Scenes for Monocular 3D Object Detection</title>
      <link>http://arxiv.org/abs/2408.15637v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages. Accepted for ECVA European Conference on Computer Vision
  2024 (ECCV'24)&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 从单目图像中准确检测3D物体在动态路边场景中仍然是一项具有挑战性的任务，主要由于相机视角变化和不可预测的场景条件。&lt;br&gt;&lt;h4&gt;2. 研究方法&lt;/h4&gt;   - 本文提出了一种两阶段训练策略来应对这些挑战。&lt;br&gt;&lt;h4&gt;3. 阶段一：初始训练&lt;/h4&gt;   - 首先在大型合成数据集RoadSense3D上训练模型，该数据集提供了多样化的场景，以促进稳健的特征学习。&lt;br&gt;&lt;h4&gt;4. 阶段二：微调&lt;/h4&gt;   - 随后，在多个真实世界数据集上对模型进行微调，以增强其适应实际条件的能力。&lt;br&gt;&lt;h4&gt;5. 实验结果&lt;/h4&gt;   - 在Cube R-CNN模型的实验中，检测性能显著提升：&lt;br&gt;     - 在TUM Traffic A9 Highway数据集上的平均精度从0.26提升至12.76。&lt;br&gt;     - 在DAIR-V2X-I数据集上的平均精度从2.09提升至6.60。&lt;br&gt;&lt;h4&gt;6. 资源可用性&lt;/h4&gt;   - 代码、数据和定性视频结果可在项目网站上获取：[RoadSense3D](https://roadsense3d.github.io)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurately detecting 3D objects from monocular images in dynamic roadside
scenarios remains a challenging problem due to varying camera perspectives and
unpredictable scene conditions. This paper introduces a two-stage training
strategy to address these challenges. Our approach initially trains a model on
the large-scale synthetic dataset, RoadSense3D, which offers a diverse range of
scenarios for robust feature learning. Subsequently, we fine-tune the model on
a combination of real-world datasets to enhance its adaptability to practical
conditions. Experimental results of the Cube R-CNN model on challenging public
benchmarks show a remarkable improvement in detection performance, with a mean
average precision rising from 0.26 to 12.76 on the TUM Traffic A9 Highway
dataset and from 2.09 to 6.60 on the DAIR-V2X-I dataset when performing
transfer learning. Code, data, and qualitative video results are available on
the project website: https://roadsense3d.github.io.</description>
      <guid isPermaLink="false">2408.15637v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>LeMON: Learning to Learn Multi-Operator Networks</title>
      <link>http://arxiv.org/abs/2408.16168v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 单一操作学习涉及训练深度神经网络以学习特定的操作符，而多操作学习则通过操作符嵌入结构在多个操作符的数据上训练单一神经网络。&lt;br&gt;&lt;h4&gt;2. 多操作学习的优势&lt;/h4&gt;   - 多操作学习能够在一个模型中预测多种操作符，从而提高灵活性和适用性。&lt;br&gt;&lt;h4&gt;3. 研究目标&lt;/h4&gt;   - 本文提出了使用多操作学习解决偏微分方程（PDEs）的预训练和微调策略。&lt;br&gt;&lt;h4&gt;4. 关键方面&lt;/h4&gt;   - 通过增加用于预训练的操作符家族数量，可以将PDE基础模型微调至涉及新PDE的下游任务，即使样本数量有限，也能超越单一操作神经网络的性能。&lt;br&gt;&lt;h4&gt;5. 模型能力&lt;/h4&gt;   - 使用多种PDE家族数据预训练的多操作学习模型，能够在微调后仅用有限的新家族操作符进行预测，甚至能作为无数据的PDE求解器。&lt;br&gt;&lt;h4&gt;6. 零样本预测&lt;/h4&gt;   - 提出的训练和微调方法能在没有样本的情况下进行新操作符的零样本预测。&lt;br&gt;&lt;h4&gt;7. 元学习算法&lt;/h4&gt;   - 引入了与PDE无关的元学习算法，以改善模型对各种PDE的适应性，提供更好的参数初始化过程。&lt;br&gt;&lt;h4&gt;8. 计算资源优化&lt;/h4&gt;   - 探索低秩适应方法，以降低计算成本，同时提高求解器的准确性，满足计算资源有限的应用需求。&lt;br&gt;&lt;h4&gt;9. 扩展性研究&lt;/h4&gt;   - 通过考察与操作符家族数量相关的扩展规律，确立并强调其在PDE求解任务中广泛适应的潜力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/jingminsun/lemon_prose&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Single-operator learning involves training a deep neural network to learn a
specific operator, whereas recent work in multi-operator learning uses an
operator embedding structure to train a single neural network on data from
multiple operators. Thus, multi-operator learning is capable of predicting a
range of operators within one model. In this work, we propose pretraining and
fine-tuning strategies for solving PDEs using multi-operator learning. One key
aspect is that by increasing the number of families of operators used in
pretraining, a PDE foundation model can be fine-tuned to downstream tasks
involving new PDEs with a limited number of samples, thus outperforming single
operator neural networks. Specifically, a multi-operator learning model
pre-trained with data from diverse PDE families can predict unseen operators
after fine-tuning with only a limited number of operators from the new family,
enabling them to serve as a data-free PDE solver. We also show that the
proposed training and fine-tuning method is able to predict new operators in
zero-shot prediction without samples. Additionally, we introduce a PDE-agnostic
meta-learning algorithm to improve the adaptability of the model to various
PDEs by providing a better parameter initialization process. To address the
needs of applications with limited computing resources, we explore low-rank
adaptation methods that reduce computational costs while enhancing solver
accuracy. Lastly, by examining the scaling law with respect to the number of
operator families, we establish and highlight its potential for broad
adaptation in PDE-solving tasks.</description>
      <guid isPermaLink="false">2408.16168v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Point Cloud Segmentation Using Transfer Learning with RandLA-Net: A Case Study on Urban Areas</title>
      <link>http://arxiv.org/abs/2312.11880v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 城市环境具有复杂结构和多样特征，使得点云数据的准确分割成为一项具有挑战性的任务。&lt;br&gt;&lt;h4&gt;2. 研究目的&lt;/h4&gt;   - 本文对RandLA-Net（一种最先进的神经网络架构）在城市大规模点云数据的3D分割应用进行了全面研究。&lt;br&gt;&lt;h4&gt;3. 研究区域&lt;/h4&gt;   - 研究集中在中国三个主要城市：成都、九江和深圳，利用它们的独特特征来提高分割性能。&lt;br&gt;&lt;h4&gt;4. 数据问题&lt;/h4&gt;   - 针对这些特定城市区域标注数据的有限性，采用了迁移学习技术。&lt;br&gt;&lt;h4&gt;5. 迁移学习实施&lt;/h4&gt;   - 将来自Sensat Urban和Toronto 3D数据集的学习权重转移，以初始化RandLA-Net模型。&lt;br&gt;   - 进行了类别重映射，以适应目标城市区域，确保分割结果的准确性。&lt;br&gt;&lt;h4&gt;6. 实验结果&lt;/h4&gt;   - 实验结果表明，所提方法在3D点云分割中每个区域的F1分数均超过80%。&lt;br&gt;&lt;h4&gt;7. 迁移学习的价值&lt;/h4&gt;   - 迁移学习策略在克服数据稀缺问题上发挥了关键作用，为城市点云分析提供了稳健的解决方案。&lt;br&gt;&lt;h4&gt;8. 研究贡献&lt;/h4&gt;   - 研究成果推动了点云分割方法的发展，尤其是在快速发展的中国城市环境中。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2023-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Urban environments are characterized by complex structures and diverse
features, making accurate segmentation of point cloud data a challenging task.
This paper presents a comprehensive study on the application of RandLA-Net, a
state-of-the-art neural network architecture, for the 3D segmentation of
large-scale point cloud data in urban areas. The study focuses on three major
Chinese cities, namely Chengdu, Jiaoda, and Shenzhen, leveraging their unique
characteristics to enhance segmentation performance.
  To address the limited availability of labeled data for these specific urban
areas, we employed transfer learning techniques. We transferred the learned
weights from the Sensat Urban and Toronto 3D datasets to initialize our
RandLA-Net model. Additionally, we performed class remapping to adapt the model
to the target urban areas, ensuring accurate segmentation results.
  The experimental results demonstrate the effectiveness of the proposed
approach achieving over 80\% F1 score for each areas in 3D point cloud
segmentation. The transfer learning strategy proves to be crucial in overcoming
data scarcity issues, providing a robust solution for urban point cloud
analysis. The findings contribute to the advancement of point cloud
segmentation methods, especially in the context of rapidly evolving Chinese
urban areas.</description>
      <guid isPermaLink="false">2312.11880v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Redesigning graph filter-based GNNs to relax the homophily assumption</title>
      <link>http://arxiv.org/abs/2409.08676v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 图神经网络（GNNs）已成为处理不规则数据的重要方法，通常假设数据结构由同质图（homophilic graph）表示。&lt;br&gt;&lt;h4&gt;2. 现有问题&lt;/h4&gt;   - 近期研究表明，许多相关应用涉及异质数据（heterophilic data），在此情况下，GNNs的性能显著下降。&lt;br&gt;&lt;h4&gt;3. 研究目标&lt;/h4&gt;   - 本文提出一种简单而有效的架构，旨在缓解同质性假设的局限性。&lt;br&gt;&lt;h4&gt;4. 架构创新&lt;/h4&gt;   - 重新解释了卷积GNN中图滤波器的作用，形成了一个更通用的架构，同时比基于滤波器组的GNNs具有更强的归纳偏置。&lt;br&gt;&lt;h4&gt;5. 卷积层设计&lt;/h4&gt;   - 提出的卷积层增强了架构的表达能力，使其能够从同质和异质数据中学习，并防止过平滑（oversmoothing）问题。&lt;br&gt;&lt;h4&gt;6. 理论支持&lt;/h4&gt;   - 从理论角度出发，证明了所提出架构的置换等变性（permutation equivariance）。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 在同质和异质数据集上，与多种先进基线比较，所提出的GNNs表现优越，展示了其良好的潜力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) have become a workhorse approach for learning
from data defined over irregular domains, typically by implicitly assuming that
the data structure is represented by a homophilic graph. However, recent works
have revealed that many relevant applications involve heterophilic data where
the performance of GNNs can be notably compromised. To address this challenge,
we present a simple yet effective architecture designed to mitigate the
limitations of the homophily assumption. The proposed architecture reinterprets
the role of graph filters in convolutional GNNs, resulting in a more general
architecture while incorporating a stronger inductive bias than GNNs based on
filter banks. The proposed convolutional layer enhances the expressive capacity
of the architecture enabling it to learn from both homophilic and heterophilic
data and preventing the issue of oversmoothing. From a theoretical standpoint,
we show that the proposed architecture is permutation equivariant. Finally, we
show that the proposed GNNs compares favorably relative to several
state-of-the-art baselines in both homophilic and heterophilic datasets,
showcasing its promising potential.</description>
      <guid isPermaLink="false">2409.08676v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>RIDE: Boosting 3D Object Detection for LiDAR Point Clouds via Rotation-Invariant Analysis</title>
      <link>http://arxiv.org/abs/2408.15643v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 旋转鲁棒性在点云分析中备受关注，但在3D物体检测中仍然面临重大挑战。&lt;br&gt;   - 现有检测器在任意旋转下往往无法产生预期输出，主要由于旋转鲁棒性不足。&lt;br&gt;&lt;h4&gt;2. 研究目标&lt;/h4&gt;   - 本文提出RIDE，一个开创性的3D激光雷达（LiDAR）点云物体检测器，旨在实现旋转不变性。&lt;br&gt;&lt;h4&gt;3. 核心思想&lt;/h4&gt;   - 设计旋转不变的特征，从LiDAR场景中提取，并有效地将其融入现有的3D检测器中。&lt;br&gt;&lt;h4&gt;4. 特征提取器设计&lt;/h4&gt;   - 设计了一个双特征提取器，提取两种特征：&lt;br&gt;     - **物体感知特征**：对旋转敏感，但能很好地保留几何信息。&lt;br&gt;     - **旋转不变特征**：在一定程度上丧失几何信息，但对旋转具有鲁棒性。&lt;br&gt;&lt;h4&gt;5. 特征互补&lt;/h4&gt;   - 这两种特征相互补充，使得解码的3D提案能够抵御任意旋转。&lt;br&gt;&lt;h4&gt;6. 兼容性&lt;/h4&gt;   - RIDE与现有的一阶段和两阶段3D检测器兼容，易于集成，能够提升检测性能和旋转鲁棒性。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 在标准基准上进行的广泛实验表明，集成RIDE后，平均精度均值（mAP）和旋转鲁棒性显著提升：&lt;br&gt;     - 在KITTI数据集上，mAP提升5.6%，旋转鲁棒性提升53%。&lt;br&gt;     - 在nuScenes数据集上，分别提升5.1%和28%。&lt;br&gt;&lt;h4&gt;8. 代码发布&lt;/h4&gt;   - 相关代码将很快公开。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rotation robustness property has drawn much attention to point cloud
analysis, whereas it still poses a critical challenge in 3D object detection.
When subjected to arbitrary rotation, most existing detectors fail to produce
expected outputs due to the poor rotation robustness. In this paper, we present
RIDE, a pioneering exploration of Rotation-Invariance for the 3D
LiDAR-point-based object DEtector, with the key idea of designing
rotation-invariant features from LiDAR scenes and then effectively
incorporating them into existing 3D detectors. Specifically, we design a
bi-feature extractor that extracts (i) object-aware features though sensitive
to rotation but preserve geometry well, and (ii) rotation-invariant features,
which lose geometric information to a certain extent but are robust to
rotation. These two kinds of features complement each other to decode 3D
proposals that are robust to arbitrary rotations. Particularly, our RIDE is
compatible and easy to plug into the existing one-stage and two-stage 3D
detectors, and boosts both detection performance and rotation robustness.
Extensive experiments on the standard benchmarks showcase that the mean average
precision (mAP) and rotation robustness can be significantly boosted by
integrating with our RIDE, with +5.6% mAP and 53% rotation robustness
improvement on KITTI, +5.1% and 28% improvement correspondingly on nuScenes.
The code will be available soon.</description>
      <guid isPermaLink="false">2408.15643v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Robust Bird's Eye View Segmentation by Adapting DINOv2</title>
      <link>http://arxiv.org/abs/2409.10228v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ECCV 2024 - 2nd Workshop on Vision-Centric Autonomous Driving (VCAD)&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 从多台摄像头图像中提取鸟瞰图（BEV）表示为自动驾驶提供了一种经济高效、可扩展的替代方案，取代基于激光雷达的解决方案。&lt;br&gt;&lt;h4&gt;2. 现有问题&lt;/h4&gt;   - 现有的BEV方法在不同损坏情况下（如亮度变化、天气变化或相机故障）性能显著下降。&lt;br&gt;&lt;h4&gt;3. 研究目标&lt;/h4&gt;   - 为提高BEV感知的鲁棒性，提出将大型视觉基础模型DINOv2适配到BEV估计中，采用低秩适配（LoRA）技术。&lt;br&gt;&lt;h4&gt;4. 方法概述&lt;/h4&gt;   - 该方法基于DINOv2强大的表示空间，将其适配到BEV任务中，构建于最先进的框架SimpleBEV上。&lt;br&gt;&lt;h4&gt;5. 实验结果&lt;/h4&gt;   - 实验表明，在各种损坏情况下，BEV感知的鲁棒性得到了增强，模型规模和输入分辨率增加带来了更大的收益。&lt;br&gt;&lt;h4&gt;6. 参数效率&lt;/h4&gt;   - 展示了适配后表示的有效性，表现为所需可学习参数更少，以及训练过程中的更快收敛性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Extracting a Bird's Eye View (BEV) representation from multiple camera images
offers a cost-effective, scalable alternative to LIDAR-based solutions in
autonomous driving. However, the performance of the existing BEV methods drops
significantly under various corruptions such as brightness and weather changes
or camera failures. To improve the robustness of BEV perception, we propose to
adapt a large vision foundational model, DINOv2, to BEV estimation using Low
Rank Adaptation (LoRA). Our approach builds on the strong representation space
of DINOv2 by adapting it to the BEV task in a state-of-the-art framework,
SimpleBEV. Our experiments show increased robustness of BEV perception under
various corruptions, with increasing gains from scaling up the model and the
input resolution. We also showcase the effectiveness of the adapted
representations in terms of fewer learnable parameters and faster convergence
during training.</description>
      <guid isPermaLink="false">2409.10228v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Meta-Learning Empowered Graph Neural Networks for Radio Resource Management</title>
      <link>http://arxiv.org/abs/2408.16239v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 本文关注动态无线网络中的无线资源管理（RRM）问题，这些网络包含多个共享相同频谱资源的通信链路。&lt;br&gt;&lt;h4&gt;2. 目标&lt;/h4&gt;   - 旨在实现高网络吞吐量，同时确保所有链路之间的公平性。&lt;br&gt;&lt;h4&gt;3. 问题形式化&lt;/h4&gt;   - 形成了一个具有每用户最低速率约束的鲁棒功率优化问题。&lt;br&gt;&lt;h4&gt;4. 对偶问题&lt;/h4&gt;   - 导出了相应的拉格朗日对偶问题，并使用神经网络对所有变量进行参数化，这可以在无监督方式下进行训练，因为其可接受的对偶间隙是可以证明的。&lt;br&gt;&lt;h4&gt;5. 方法开发&lt;/h4&gt;   - 采用图神经网络（GNNs）作为参数化方法，开发了一种元学习方法，表现出快速适应性和对不同网络配置的可扩展性。&lt;br&gt;&lt;h4&gt;6. 元学习目标&lt;/h4&gt;   - 通过合并不同网络配置的拉格朗日函数来公式化元学习的目标，并利用一种称为Reptile的一阶元学习算法来获得元参数。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 数值结果验证了该方法能够有效提高整体吞吐量，并确保最低速率性能。&lt;br&gt;&lt;h4&gt;8. 快速适应性&lt;/h4&gt;   - 进一步表明，使用元参数作为初始化后，该方法能够快速适应新的无线网络配置，并减少所需的训练数据样本数量。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we consider a radio resource management (RRM) problem in the
dynamic wireless networks, comprising multiple communication links that share
the same spectrum resource. To achieve high network throughput while ensuring
fairness across all links, we formulate a resilient power optimization problem
with per-user minimum-rate constraints. We obtain the corresponding Lagrangian
dual problem and parameterize all variables with neural networks, which can be
trained in an unsupervised manner due to the provably acceptable duality gap.
We develop a meta-learning approach with graph neural networks (GNNs) as
parameterization that exhibits fast adaptation and scalability to varying
network configurations. We formulate the objective of meta-learning by
amalgamating the Lagrangian functions of different network configurations and
utilize a first-order meta-learning algorithm, called Reptile, to obtain the
meta-parameters. Numerical results verify that our method can efficiently
improve the overall throughput and ensure the minimum rate performance. We
further demonstrate that using the meta-parameters as initialization, our
method can achieve fast adaptation to new wireless network configurations and
reduce the number of required training data samples.</description>
      <guid isPermaLink="false">2408.16239v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>INTRA: Interaction Relationship-aware Weakly Supervised Affordance Grounding</title>
      <link>http://arxiv.org/abs/2409.06210v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - Affordance（可供性）指物体固有的潜在交互能力。&lt;br&gt;   - 感知可供性使智能体能够高效地在新环境中导航和交互。&lt;br&gt;&lt;h4&gt;2. 现有方法问题&lt;/h4&gt;   - 弱监督可供性定位通过外部图像教导智能体可供性概念，避免了昂贵的像素级标注。&lt;br&gt;   - 现有方法需要配对的外部和自我中心图像数据集，而这带来了挑战。&lt;br&gt;   - 针对单一物体的多样可供性定位复杂。&lt;br&gt;&lt;h4&gt;3. 新方法提出&lt;/h4&gt;   - 本文提出了INTeraction Relationship-aware弱监督可供性定位（INTRA）。&lt;br&gt;   - INTRA将问题重构为表示学习，通过对比学习仅使用外部图像，识别交互的独特特征，消除了对配对数据集的需求。&lt;br&gt;&lt;h4&gt;4. 方法特点&lt;/h4&gt;   - 利用视觉-语言模型嵌入，灵活地执行可供性定位，支持任何文本。&lt;br&gt;   - 设计了文本条件的可供性图生成，反映交互关系以用于对比学习，并通过文本同义词增强提高了鲁棒性。&lt;br&gt;&lt;h4&gt;5. 实验结果&lt;/h4&gt;   - 在AGD20K、IIT-AFF、CAD和UMD等多样数据集上，方法超越了现有技术。&lt;br&gt;   - 实验结果表明，该方法在合成图像/插图上具有显著的领域可扩展性，并能够对新交互和物体执行可供性定位。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Affordance denotes the potential interactions inherent in objects. The
perception of affordance can enable intelligent agents to navigate and interact
with new environments efficiently. Weakly supervised affordance grounding
teaches agents the concept of affordance without costly pixel-level
annotations, but with exocentric images. Although recent advances in weakly
supervised affordance grounding yielded promising results, there remain
challenges including the requirement for paired exocentric and egocentric image
dataset, and the complexity in grounding diverse affordances for a single
object. To address them, we propose INTeraction Relationship-aware weakly
supervised Affordance grounding (INTRA). Unlike prior arts, INTRA recasts this
problem as representation learning to identify unique features of interactions
through contrastive learning with exocentric images only, eliminating the need
for paired datasets. Moreover, we leverage vision-language model embeddings for
performing affordance grounding flexibly with any text, designing
text-conditioned affordance map generation to reflect interaction relationship
for contrastive learning and enhancing robustness with our text synonym
augmentation. Our method outperformed prior arts on diverse datasets such as
AGD20K, IIT-AFF, CAD and UMD. Additionally, experimental results demonstrate
that our method has remarkable domain scalability for synthesized images /
illustrations and is capable of performing affordance grounding for novel
interactions and objects.</description>
      <guid isPermaLink="false">2409.06210v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>A Bayesian framework for active object recognition, pose estimation and shape transfer learning through touch</title>
      <link>http://arxiv.org/abs/2409.06912v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 触觉感知是机器人感知的重要方面，类似于人类如何通过触觉探索和理解世界。&lt;br&gt;   - 在非结构化环境中，机器人可能会遇到已知和新颖的物体，因此需要一种方法来处理这两类物体。&lt;br&gt;&lt;h4&gt;2. 方法概述&lt;/h4&gt;   - 本研究结合了粒子滤波器（PF）和高斯过程隐式表面（GPIS），构建了一个统一的贝叶斯框架。&lt;br&gt;&lt;h4&gt;3. 框架功能&lt;/h4&gt;   - 该框架能够区分已知和新颖物体，执行物体识别，估计已知物体的姿态，并以主动学习的方式重构未知物体的形状。&lt;br&gt;&lt;h4&gt;4. 知识转移&lt;/h4&gt;   - 通过将GPIS先验与粒子滤波中的最大似然估计（MLE）形状结合，可以将已知物体形状的知识转移到新形状的学习上。&lt;br&gt;&lt;h4&gt;5. 探索程序&lt;/h4&gt;   - 提出了一种带有全局形状估计的探索程序，以指导主动数据采集，并在获取到足够信息后结束探索。&lt;br&gt;&lt;h4&gt;6. 性能评估&lt;/h4&gt;   - 通过对已知和新颖物体的模拟评估所提贝叶斯框架，初始化时采用随机姿态。&lt;br&gt;&lt;h4&gt;7. 结果展示&lt;/h4&gt;   - 结果表明，利用全局形状估计的探索程序比基于快速探索随机树（RRT）的局部探索程序实现了更快的探索。&lt;br&gt;&lt;h4&gt;8. 总体结论&lt;/h4&gt;   - 所提出的框架在物体识别、姿态估计和形状重构方面有效且高效。&lt;br&gt;   - 此外，学习到的形状可以作为新的先验，有效用于未来的物体识别和姿态估计。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As humans can explore and understand the world through the sense of touch,
tactile sensing is also an important aspect of robotic perception. In
unstructured environments, robots can encounter both known and novel objects,
this calls for a method to address both known and novel objects. In this study,
we combine a particle filter (PF) and Gaussian process implicit surface (GPIS)
in a unified Bayesian framework. The framework can differentiate between known
and novel objects, perform object recognition, estimate pose for known objects,
and reconstruct shapes for unknown objects, in an active learning fashion. By
grounding the selection of the GPIS prior with the
maximum-likelihood-estimation (MLE) shape from the PF, the knowledge about
known objects' shapes can be transferred to learn novel shapes. An exploration
procedure with global shape estimation is proposed to guide active data
acquisition and conclude the exploration when sufficient information is
obtained. The performance of the proposed Bayesian framework is evaluated
through simulations on known and novel objects, initialized with random poses.
The results show that the proposed exploration procedure, utilizing global
shape estimation, achieves faster exploration than a local exploration
procedure based on rapidly explore random tree (RRT). Overall, our results
indicate that the proposed framework is effective and efficient in object
recognition, pose estimation and shape reconstruction. Moreover, we show that a
learned shape can be included as a new prior and used effectively for future
object recognition and pose estimation.</description>
      <guid isPermaLink="false">2409.06912v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Multi-modality Affinity Inference for Weakly Supervised 3D Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2312.16578v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 3D点云语义分割在多个应用中具有广泛用途。&lt;br&gt;   - 最近提出了弱监督点云分割方法，旨在利用场景级标签来减少昂贵且繁琐的手动标注过程。&lt;br&gt;&lt;h4&gt;2. 现有问题&lt;/h4&gt;   - 这些方法未能有效利用RGB-D扫描中丰富的几何信息（如形状和尺度）和外观信息（如颜色和纹理）。&lt;br&gt;   - 当前方法未能充分利用从特征提取网络推断出的点亲和性，而这对从弱场景级标签中学习至关重要。&lt;br&gt;   - 之前的工作忽略了点云数据长尾分布在弱监督3D语义分割中的不利影响。&lt;br&gt;&lt;h4&gt;3. 新方法提出&lt;/h4&gt;   - 本文提出了一种简单而有效的场景级弱监督点云分割方法，新增了多模态点亲和推断模块。&lt;br&gt;&lt;h4&gt;4. 方法特点&lt;/h4&gt;   - 提出的点亲和性特征结合了多种模态（如点云和RGB），并通过标准化分类器权重来缓解长尾分布的负面影响，无需类别分布的先验知识。&lt;br&gt;&lt;h4&gt;5. 实验验证&lt;/h4&gt;   - 在ScanNet和S3DIS基准数据集上进行的广泛实验验证了所提方法的有效性，结果表明其相较于现有最先进方法提高了约4%到6%的mIoU（平均交并比）。&lt;br&gt;&lt;h4&gt;6. 代码发布&lt;/h4&gt;   - 相关代码已在 [GitHub](https://github.com/Sunny599/AAAI24-3DWSSG-MMA) 上公开。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2023-12-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/sunny599/aaai24-3dwssg-mma&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D point cloud semantic segmentation has a wide range of applications.
Recently, weakly supervised point cloud segmentation methods have been
proposed, aiming to alleviate the expensive and laborious manual annotation
process by leveraging scene-level labels. However, these methods have not
effectively exploited the rich geometric information (such as shape and scale)
and appearance information (such as color and texture) present in RGB-D scans.
Furthermore, current approaches fail to fully leverage the point affinity that
can be inferred from the feature extraction network, which is crucial for
learning from weak scene-level labels. Additionally, previous work overlooks
the detrimental effects of the long-tailed distribution of point cloud data in
weakly supervised 3D semantic segmentation. To this end, this paper proposes a
simple yet effective scene-level weakly supervised point cloud segmentation
method with a newly introduced multi-modality point affinity inference module.
The point affinity proposed in this paper is characterized by features from
multiple modalities (e.g., point cloud and RGB), and is further refined by
normalizing the classifier weights to alleviate the detrimental effects of
long-tailed distribution without the need of the prior of category
distribution. Extensive experiments on the ScanNet and S3DIS benchmarks verify
the effectiveness of our proposed method, which outperforms the
state-of-the-art by ~4% to ~6% mIoU. Codes are released at
https://github.com/Sunny599/AAAI24-3DWSSG-MMA.</description>
      <guid isPermaLink="false">2312.16578v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>NV-LIO: LiDAR-Inertial Odometry using Normal Vectors Towards Robust SLAM in Multifloor Environments</title>
      <link>http://arxiv.org/abs/2405.12563v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to IEEE Robotics &amp; Automation Letters&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 近年来，开发了许多激光雷达惯性测程（LIO）算法，这些算法在多种环境中表现良好。&lt;br&gt;&lt;h4&gt;2. 现有问题&lt;/h4&gt;   - 大多数算法主要在开放的户外环境中经过验证，但在封闭的室内环境中常常面临挑战。&lt;br&gt;   - 在室内环境中，由于激光雷达扫描的快速变化以及重复的结构特征（如墙壁和楼梯），可靠的点云配准变得困难，尤其是在多层建筑中。&lt;br&gt;&lt;h4&gt;3. 新方法提出&lt;/h4&gt;   - 本文提出了NV-LIO，一个基于法向量的LIO框架，旨在实现室内多层结构的同时定位与地图构建（SLAM）。&lt;br&gt;&lt;h4&gt;4. 方法特点&lt;/h4&gt;   - 该方法从激光雷达扫描中提取法向量，并利用法向量进行对应搜索，以增强点云配准的性能。&lt;br&gt;   - 分析法向量方向的分布，以确保稳健的配准，并检查退化情况以调整匹配的不确定性。&lt;br&gt;&lt;h4&gt;5. 环闭合模块&lt;/h4&gt;   - 实施了基于视点的环闭合模块，以避免被墙壁阻挡的错误对应。&lt;br&gt;&lt;h4&gt;6. 实验验证&lt;/h4&gt;   - 通过公共数据集和自有数据集验证所提出的方法。&lt;br&gt;&lt;h4&gt;7. 社区贡献&lt;/h4&gt;   - 为了贡献给社区，相关代码将公开在 [GitHub](https://github.com/dhchung/nv_lio)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/dhchung/nv_lio&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Over the last few decades, numerous LiDAR-inertial odometry (LIO) algorithms
have been developed, demonstrating satisfactory performance across diverse
environments. Most of these algorithms have predominantly been validated in
open outdoor environments, however they often encounter challenges in confined
indoor settings. In such indoor environments, reliable point cloud registration
becomes problematic due to the rapid changes in LiDAR scans and repetitive
structural features like walls and stairs, particularly in multifloor
buildings. In this paper, we present NV-LIO, a normal vector based LIO
framework, designed for simultaneous localization and mapping (SLAM) in indoor
environments with multifloor structures. Our approach extracts the normal
vectors from the LiDAR scans and utilizes them for correspondence search to
enhance the point cloud registration performance. To ensure robust
registration, the distribution of the normal vector directions is analyzed, and
situations of degeneracy are examined to adjust the matching uncertainty.
Additionally, a viewpoint based loop closure module is implemented to avoid
wrong correspondences that are blocked by the walls. The propsed method is
validated through public datasets and our own dataset. To contribute to the
community, the code will be made public on https://github.com/dhchung/nv_lio.</description>
      <guid isPermaLink="false">2405.12563v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Prior Learning in Introspective VAEs</title>
      <link>http://arxiv.org/abs/2408.13805v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 变分自编码器（VAEs）是一种流行的无监督学习和数据生成框架。许多方法已被提出以改进VAEs，特别是引入对抗性目标和先验学习机制。&lt;br&gt;&lt;h4&gt;2. 现有方法&lt;/h4&gt;   - 其中一个显著的实例是最近引入的自省变分自编码器（Introspective VAEs），旨在确保不合理样本被分配低的似然性。&lt;br&gt;&lt;h4&gt;3. 研究重点&lt;/h4&gt;   - 本研究聚焦于软自省变分自编码器（Soft-IntroVAE, S-IntroVAE），探讨将多模态和可学习先验纳入该框架的意义。&lt;br&gt;&lt;h4&gt;4. 先验学习机制&lt;/h4&gt;   - 将先验视为第三方，与解码器共同训练，展示了这种合作训练方式对先验学习的有效性，并且与传统S-IntroVAE共享纳什均衡。&lt;br&gt;&lt;h4&gt;5. 理论发展&lt;/h4&gt;   - 基于对S-IntroVAE中最优ELBO的修改，开发了理论驱动的正则化方法：&lt;br&gt;     - **自适应方差裁剪**：在学习先验时稳定训练。&lt;br&gt;     - **责任正则化**：防止无效先验模式的形成。&lt;br&gt;&lt;h4&gt;6. 实验验证&lt;/h4&gt;   - 在二维密度估计基准和图像生成设置（包括F-MNIST和CIFAR-10数据集）上进行了一系列针对性的实验，证明了在生成和表征学习中，S-IntroVAE中的先验学习的益处。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Variational Autoencoders (VAEs) are a popular framework for unsupervised
learning and data generation. A plethora of methods have been proposed focusing
on improving VAEs, with the incorporation of adversarial objectives and the
integration of prior learning mechanisms being prominent directions. When it
comes to the former, an indicative instance is the recently introduced family
of Introspective VAEs aiming at ensuring that a low likelihood is assigned to
unrealistic samples. In this study, we focus on the Soft-IntroVAE (S-IntroVAE)
and investigate the implication of incorporating a multimodal and learnable
prior into this framework. Namely, we formulate the prior as a third player and
show that when trained in cooperation with the decoder constitutes an effective
way for prior learning, which shares the Nash Equilibrium with the vanilla
S-IntroVAE. Furthermore, based on a modified formulation of the optimal ELBO in
S-IntroVAE, we develop theoretically motivated regularizations, that is (i)
adaptive variance clipping to stabilize training when learning the prior and
(ii) responsibility regularization to discourage the formation of inactive
prior mode. Finally, we perform a series of targeted experiments on a 2D
density estimation benchmark and in an image generation setting comprised of
the (F)-MNIST and CIFAR-10 datasets demonstrating the benefit of prior learning
in S-IntroVAE in generation and representation learning.</description>
      <guid isPermaLink="false">2408.13805v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>SAUC: Sparsity-Aware Uncertainty Calibration for Spatiotemporal Prediction with Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2409.08766v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Paper accepted by ACM SIGSPATIAL 2024&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 量化不确定性对于鲁棒和可靠的预测至关重要，但现有时空深度学习主要关注确定性预测，忽视了预测中的固有不确定性。&lt;br&gt;&lt;h4&gt;2. 数据挑战&lt;/h4&gt;   - 高粒度的时空数据集通常稀疏，这在预测和不确定性量化中带来了额外挑战。&lt;br&gt;&lt;h4&gt;3. 新方法提出&lt;/h4&gt;   - 本文提出了一种新颖的后处理稀疏感知不确定性校准（SAUC）框架，旨在对零值和非零值的预测不确定性进行校准。&lt;br&gt;&lt;h4&gt;4. 方法开发&lt;/h4&gt;   - 首先，将最先进的确定性时空图神经网络（ST-GNNs）修改为概率性模型，进入预校准阶段。&lt;br&gt;   - 然后，使用分位数方法对概率性ST-GNN进行零值和非零值的校准。&lt;br&gt;&lt;h4&gt;5. 实验结果&lt;/h4&gt;   - 通过广泛的实验证明，SAUC能够有效拟合稀疏数据的方差，并在两个真实世界的时空数据集上实现泛化，适应不同的粒度。&lt;br&gt;   - 具体而言，实证实验显示，在稀疏交通事故和城市犯罪预测中，零值条目的校准误差减少了20%。&lt;br&gt;&lt;h4&gt;6. 贡献与意义&lt;/h4&gt;   - 本研究展示了SAUC框架的理论和实证价值，弥补了不确定性量化与时空预测之间的重要空白。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3678717.3691241&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Quantifying uncertainty is crucial for robust and reliable predictions.
However, existing spatiotemporal deep learning mostly focuses on deterministic
prediction, overlooking the inherent uncertainty in such prediction.
Particularly, highly-granular spatiotemporal datasets are often sparse, posing
extra challenges in prediction and uncertainty quantification. To address these
issues, this paper introduces a novel post-hoc Sparsity-awar Uncertainty
Calibration (SAUC) framework, which calibrates uncertainty in both zero and
non-zero values. To develop SAUC, we firstly modify the state-of-the-art
deterministic spatiotemporal Graph Neural Networks (ST-GNNs) to probabilistic
ones in the pre-calibration phase. Then we calibrate the probabilistic ST-GNNs
for zero and non-zero values using quantile approaches.Through extensive
experiments, we demonstrate that SAUC can effectively fit the variance of
sparse data and generalize across two real-world spatiotemporal datasets at
various granularities. Specifically, our empirical experiments show a 20\%
reduction in calibration errors in zero entries on the sparse traffic accident
and urban crime prediction. Overall, this work demonstrates the theoretical and
empirical values of the SAUC framework, thus bridging a significant gap between
uncertainty quantification and spatiotemporal prediction.</description>
      <guid isPermaLink="false">2409.08766v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>PolarBEVDet: Exploring Polar Representation for Multi-View 3D Object Detection in Bird's-Eye-View</title>
      <link>http://arxiv.org/abs/2408.16200v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 6 figures&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 基于LSS的多视角3D物体检测为自动驾驶提供了一种经济且易于部署的解决方案。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 目前所有基于LSS的方法都将多视角图像特征转换为笛卡尔鸟瞰图（BEV）表示，这种方法未考虑图像信息的非均匀分布，且未充分利用视角对称性。&lt;br&gt;&lt;h4&gt;3. 新方法提出&lt;/h4&gt;   - 本文提出采用极坐标BEV表示替代笛卡尔BEV表示，以适应图像信息分布并保留视角对称性。&lt;br&gt;&lt;h4&gt;4. 模块设计&lt;/h4&gt;   - 为实现这一目标，设计了三个模块：&lt;br&gt;     - **极视图变换器**：生成极坐标BEV表示。&lt;br&gt;     - **极时融合模块**：用于融合历史极坐标BEV特征。&lt;br&gt;     - **极检测头**：预测物体的极参数表示。&lt;br&gt;&lt;h4&gt;5. 附加改进&lt;/h4&gt;   - 设计了一个二维辅助检测头和一个空间注意力增强模块，以改善透视图和BEV中的特征提取质量。&lt;br&gt;&lt;h4&gt;6. 整合与实验结果&lt;/h4&gt;   - 将上述改进集成到一个新颖的多视角3D物体检测器PolarBEVDet中。实验结果表明，PolarBEVDet在nuScenes数据集上表现优越。&lt;br&gt;&lt;h4&gt;7. 代码可用性&lt;/h4&gt;   - 相关代码可在GitHub上获取，链接为 [PolarBEVDet](https://github.com/Yzichen/PolarBEVDet.git)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, LSS-based multi-view 3D object detection provides an economical and
deployment-friendly solution for autonomous driving. However, all the existing
LSS-based methods transform multi-view image features into a Cartesian
Bird's-Eye-View(BEV) representation, which does not take into account the
non-uniform image information distribution and hardly exploits the view
symmetry. In this paper, in order to adapt the image information distribution
and preserve the view symmetry by regular convolution, we propose to employ the
polar BEV representation to substitute the Cartesian BEV representation. To
achieve this, we elaborately tailor three modules: a polar view transformer to
generate the polar BEV representation, a polar temporal fusion module for
fusing historical polar BEV features and a polar detection head to predict the
polar-parameterized representation of the object. In addition, we design a 2D
auxiliary detection head and a spatial attention enhancement module to improve
the quality of feature extraction in perspective view and BEV, respectively.
Finally, we integrate the above improvements into a novel multi-view 3D object
detector, PolarBEVDet. Experiments on nuScenes show that PolarBEVDet achieves
the superior performance. The code is available at
https://github.com/Yzichen/PolarBEVDet.git.</description>
      <guid isPermaLink="false">2408.16200v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>oboVox Far Field Speaker Recognition: A Novel Data Augmentation Approach with Pretrained Models</title>
      <link>http://arxiv.org/abs/2409.10240v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 2 figures&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究目标&lt;/h4&gt;   - 本研究旨在解决说话人识别中的挑战，采用一种新颖的数据增强技术，通过向注册文件添加噪声来提高识别效果。&lt;br&gt;&lt;h4&gt;2. 技术优势&lt;/h4&gt;   - 该增强技术有效对齐了测试文件和注册文件的来源，从而提高了可比性。&lt;br&gt;&lt;h4&gt;3. 模型使用&lt;/h4&gt;   - 使用了多种预训练模型，其中ResNet模型表现最佳，达到了0.84的决策成本函数（DCF）和13.44%的等错误率（EER）。&lt;br&gt;&lt;h4&gt;4. 增强效果&lt;/h4&gt;   - 经过数据增强，该模型的结果显著提高，得到0.75的DCF和12.79%的EER。&lt;br&gt;&lt;h4&gt;5. 比较分析&lt;/h4&gt;   - 比较分析显示，ResNet在性能上优于其他模型，如ECPA、梅尔频谱、Payonnet和Titanet large。&lt;br&gt;&lt;h4&gt;6. 贡献与应用&lt;/h4&gt;   - 结果和不同的数据增强方案为本文中RoboVox的远场说话人识别的成功做出了贡献。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this study, we address the challenge of speaker recognition using a novel
data augmentation technique of adding noise to enrollment files. This technique
efficiently aligns the sources of test and enrollment files, enhancing
comparability. Various pre-trained models were employed, with the resnet model
achieving the highest DCF of 0.84 and an EER of 13.44. The augmentation
technique notably improved these results to 0.75 DCF and 12.79 EER for the
resnet model. Comparative analysis revealed the superiority of resnet over
models such as ECPA, Mel-spectrogram, Payonnet, and Titanet large. Results,
along with different augmentation schemes, contribute to the success of RoboVox
far-field speaker recognition in this paper</description>
      <guid isPermaLink="false">2409.10240v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Meta-Learning to Communicate: Fast End-to-End Training for Fading Channels</title>
      <link>http://arxiv.org/abs/1910.09945v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  submitted for conference publication&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 在可用信道模型的情况下，可以将如何在衰落噪声信道上进行通信视为对编码器、信道和解码器级联的自编码器进行无监督训练。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 当前的方法需要针对每个新信道从头开始训练，限制了其适用性。&lt;br&gt;&lt;h4&gt;3. 先前工作的尝试&lt;/h4&gt;   - 之前的研究考虑了在多个信道上进行联合训练，以寻找一对在多个信道上表现良好的编码器和解码器，模仿非相干传输方案的操作。&lt;br&gt;&lt;h4&gt;4. 新方法提出&lt;/h4&gt;   - 本文提出通过元学习克服联合训练的局限性：而不是为所有信道训练一个通用模型，元学习寻找一个共同的初始化向量，以便在任何信道上快速训练。&lt;br&gt;&lt;h4&gt;5. 方法验证&lt;/h4&gt;   - 通过数值结果验证了该方法，显示出显著的训练速度提升，使用随机梯度下降（SGD）仅需一次迭代即可获得有效的编码器和解码器。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2019-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/ICASSP40776.2020.9053252&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/kclip/meta-autoencoder&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; When a channel model is available, learning how to communicate on fading
noisy channels can be formulated as the (unsupervised) training of an
autoencoder consisting of the cascade of encoder, channel, and decoder. An
important limitation of the approach is that training should be generally
carried out from scratch for each new channel. To cope with this problem, prior
works considered joint training over multiple channels with the aim of finding
a single pair of encoder and decoder that works well on a class of channels. As
a result, joint training ideally mimics the operation of non-coherent
transmission schemes. In this paper, we propose to obviate the limitations of
joint training via meta-learning: Rather than training a common model for all
channels, meta-learning finds a common initialization vector that enables fast
training on any channel. The approach is validated via numerical results,
demonstrating significant training speed-ups, with effective encoders and
decoders obtained with as little as one iteration of Stochastic Gradient
Descent.</description>
      <guid isPermaLink="false">1910.09945v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>BLens: Contrastive Captioning of Binary Functions using Ensemble Embedding</title>
      <link>http://arxiv.org/abs/2409.07889v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23 pages, 5 figures. Tristan Benoit and Yunru Wang have made equally
  significant contributions to this work&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 函数名称对人类逆向工程师极为重要，推动了基于机器学习的方法来预测剥离二进制文件中的函数名称。&lt;br&gt;&lt;h4&gt;2. 现有方法&lt;/h4&gt;   - 当前许多研究使用变换器，将代码与函数名称之间的联系比作机器翻译。&lt;br&gt;&lt;h4&gt;3. 面临的挑战&lt;/h4&gt;   - 函数命名模型在推广到与训练集完全无关的项目时仍然面临困难。&lt;br&gt;&lt;h4&gt;4. 新方法提出&lt;/h4&gt;   - 本文提出了一种全新的方法，通过将自动图像描述的进展转移到二进制逆向工程领域，使二进制函数的不同部分与其名称的部分相对应。&lt;br&gt;&lt;h4&gt;5. 方法名称及结构&lt;/h4&gt;   - 提出了**BLens**，该方法将多个二进制函数嵌入组合成新的集成表示，通过对比学习方法与名称表示的潜在空间对齐，并使用专门为函数名称设计的变换器架构生成函数名称。&lt;br&gt;&lt;h4&gt;6. 实验结果&lt;/h4&gt;   - 实验表明，BLens显著优于现有最先进的技术。在常规的按二进制文件分割设置中，F1得分达到0.77，相比之下，现有方法为0.67。在强调可推广性的跨项目设置中，F1得分为0.46，而现有方法为0.29。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Function names can greatly aid human reverse engineers, which has spurred
development of machine learning-based approaches to predicting function names
in stripped binaries. Much current work in this area now uses transformers,
applying a metaphor of machine translation from code to function names. Still,
function naming models face challenges in generalizing to projects completely
unrelated to the training set. In this paper, we take a completely new approach
by transferring advances in automated image captioning to the domain of binary
reverse engineering, such that different parts of a binary function can be
associated with parts of its name. We propose BLens, which combines multiple
binary function embeddings into a new ensemble representation, aligns it with
the name representation latent space via a contrastive learning approach, and
generates function names with a transformer architecture tailored for function
names. In our experiments, we demonstrate that BLens significantly outperforms
the state of the art. In the usual setting of splitting per binary, we achieve
an $F_1$ score of 0.77 compared to 0.67. Moreover, in the cross-project
setting, which emphasizes generalizability, we achieve an $F_1$ score of 0.46
compared to 0.29.</description>
      <guid isPermaLink="false">2409.07889v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Meta-UAD: A Meta-Learning Scheme for User-level Network Traffic Anomaly Detection</title>
      <link>http://arxiv.org/abs/2408.17031v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under reviewing. arXiv admin note: substantial text overlap with
  arXiv:2408.14884&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 用户级网络流量中的准确异常检测对网络安全至关重要。&lt;br&gt;&lt;h4&gt;2. 现有问题&lt;/h4&gt;   - 现有模型通常被动检测特定异常类别，依赖于大量标记的训练样本，而用户级网络流量包含许多新的异常类别，这些类别的标记样本稀少，并且数据存在不平衡、自相似和数据需求量大的特性。&lt;br&gt;&lt;h4&gt;3. 新方法介绍&lt;/h4&gt;   - 本文提出了**Meta-UAD**，一种针对用户级网络流量异常检测的元学习方案。&lt;br&gt;&lt;h4&gt;4. 特征提取&lt;/h4&gt;   - Meta-UAD使用CICFlowMeter提取81个流级统计特征，并通过累积重要性排名移除一些无效特征。&lt;br&gt;&lt;h4&gt;5. 训练结构&lt;/h4&gt;   - 该方法采用元学习训练结构，从K-way-M-shot分类任务集合中学习，能够通过少量迭代步骤，利用预训练模型适应任何新类别，尽管样本较少。&lt;br&gt;&lt;h4&gt;6. 性能评估&lt;/h4&gt;   - 在两个公共数据集上评估了该方案，结果表明，与现有模型相比，Meta-UAD在F1-score上有15% - 43%的提升，进一步证明了其优越性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accuracy anomaly detection in user-level network traffic is crucial for
network security. Compared with existing models that passively detect specific
anomaly classes with large labeled training samples, user-level network traffic
contains sizeable new anomaly classes with few labeled samples and has an
imbalance, self-similar, and data-hungry nature. Motivation on those
limitations, in this paper, we propose \textit{Meta-UAD}, a Meta-learning
scheme for User-level network traffic Anomaly Detection. Meta-UAD uses the
CICFlowMeter to extract 81 flow-level statistical features and remove some
invalid ones using cumulative importance ranking. Meta-UAD adopts a
meta-learning training structure and learns from the collection of K-way-M-shot
classification tasks, which can use a pre-trained model to adapt any new class
with few samples by few iteration steps. We evaluate our scheme on two public
datasets. Compared with existing models, the results further demonstrate the
superiority of Meta-UAD with 15{\%} - 43{\%} gains in F1-score.</description>
      <guid isPermaLink="false">2408.17031v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Sifting the debris: Patterns in the SNR population with unsupervised ML methods</title>
      <link>http://arxiv.org/abs/2409.06383v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in A&amp;A. 17 pages, 11 figures&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 超新星遗迹（SNRs）携带大量机械和辐射能量，对星系的结构、动力学和化学演化产生重大影响。&lt;br&gt;&lt;h4&gt;2. 现状与挑战&lt;/h4&gt;   - 迄今为止，已有超过300个SNR在银河系中被发现，展示出多种观测特征。然而，现有的分类方案主要基于其射电形态。&lt;br&gt;&lt;h4&gt;3. 新方法介绍&lt;/h4&gt;   - 本文提出了一种新颖的无监督深度学习流程，旨在分析银河系SNR群体的代表性子样本（约占总数的50%），寻找其多波长特征与物理属性之间的联系。&lt;br&gt;&lt;h4&gt;4. 流程分为两个阶段&lt;/h4&gt;   - **阶段一**：表示学习阶段，使用卷积自编码器处理来自红外和射电连续观测（如WISE 22μm、Hi-GAL 70μm和SMGPS 30 cm）的图像，并在低维潜在空间中生成紧凑表示。&lt;br&gt;   - **阶段二**：聚类阶段，寻找潜在空间中的有意义聚类，并与SNR及其环境的物理属性联系起来。&lt;br&gt;&lt;h4&gt;5. 结果与方法优势&lt;/h4&gt;   - 结果表明，结合中间均匀流形近似和投影（UMAP）将自编码的嵌入映射到更易于聚类的流形，能够找到可靠的聚类。&lt;br&gt;&lt;h4&gt;6. 聚类特征分析&lt;/h4&gt;   - 尽管许多源被分类为离群点，但大多数聚类与特征相关，如红外辐射分布、射电壳和脉冲星风云雾的存在，以及尘埃丝的存在。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1051/0004-6361/202451096&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Supernova remnants (SNRs) carry vast amounts of mechanical and radiative
energy that heavily influence the structural, dynamical, and chemical evolution
of galaxies. To this day, more than 300 SNRs have been discovered in the Milky
Way, exhibiting a wide variety of observational features. However, existing
classification schemes are mainly based on their radio morphology. In this
work, we introduce a novel unsupervised deep learning pipeline to analyse a
representative subsample of the Galactic SNR population ($\sim$ 50% of the
total) with the aim of finding a connection between their multi-wavelength
features and their physical properties. The pipeline involves two stages: (1) a
representation learning stage, consisting of a convolutional autoencoder that
feeds on imagery from infrared and radio continuum surveys (WISE 22$\mu$m,
Hi-GAL 70 $\mu$m and SMGPS 30 cm) and produces a compact representation in a
lower-dimensionality latent space; and (2) a clustering stage that seeks
meaningful clusters in the latent space that can be linked to the physical
properties of the SNRs and their surroundings. Our results suggest that this
approach, when combined with an intermediate uniform manifold approximation and
projection (UMAP) reprojection of the autoencoded embeddings into a more
clusterable manifold, enables us to find reliable clusters. Despite a large
number of sources being classified as outliers, most clusters relate to the
presence of distinctive features, such as the distribution of infrared
emission, the presence of radio shells and pulsar wind nebulae, and the
existence of dust filaments.</description>
      <guid isPermaLink="false">2409.06383v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Distributed Convolutional Neural Network Training on Mobile and Edge Clusters</title>
      <link>http://arxiv.org/abs/2409.09083v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 深度和卷积神经网络（DNNs/CNNs）的训练传统上是在强大的CPU和GPU服务器上进行的。&lt;br&gt;&lt;h4&gt;2. 边缘计算的优势&lt;/h4&gt;   - 最近的研究努力将机器学习任务完全本地化到边缘设备，带来了降低延迟和增强隐私的优势，但也需要在资源受限的设备上工作。&lt;br&gt;&lt;h4&gt;3. 现有方法的局限性&lt;/h4&gt;   - 针对移动和边缘设备的推理和训练方法（如剪枝、量化或增量和迁移学习）通常需要在准确性上进行权衡。&lt;br&gt;&lt;h4&gt;4. 分布式推理研究&lt;/h4&gt;   - 虽然已有多项研究探索在移动和边缘集群上分布推理操作，但关于边缘的分布式训练文献较少。&lt;br&gt;&lt;h4&gt;5. 现有方法的缺陷&lt;/h4&gt;   - 现有方法普遍需要一个中央的、可能强大的边缘或云服务器进行协调或卸载。&lt;br&gt;&lt;h4&gt;6. 新方法介绍&lt;/h4&gt;   - 本文提出了一种专门在移动和边缘设备上进行分布式CNN训练的方法，特别适用于特征图主导的CNN初始层。&lt;br&gt;&lt;h4&gt;7. 技术细节&lt;/h4&gt;   - 该方法通过切分前向推理和反向传播操作，在设备之间进行分配，以最大化局部性并暴露通信和内存感知并行性。&lt;br&gt;&lt;h4&gt;8. 层分组概念&lt;/h4&gt;   - 引入层分组的概念，根据计算和通信的权衡进一步优化性能。&lt;br&gt;&lt;h4&gt;9. 性能验证&lt;/h4&gt;   - 在2-6个四核Raspberry Pi 3设备的集群上，训练物体检测CNN相较于单核实现了2x-15x的速度提升，同时每个设备的内存使用量减少了最多8x，且没有牺牲准确性。&lt;br&gt;&lt;h4&gt;10. 分组优化效果&lt;/h4&gt;   - 层分组根据参考配置和批量大小提供了最多1.5x的速度提升。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The training of deep and/or convolutional neural networks (DNNs/CNNs) is
traditionally done on servers with powerful CPUs and GPUs. Recent efforts have
emerged to localize machine learning tasks fully on the edge. This brings
advantages in reduced latency and increased privacy, but necessitates working
with resource-constrained devices. Approaches for inference and training in
mobile and edge devices based on pruning, quantization or incremental and
transfer learning require trading off accuracy. Several works have explored
distributing inference operations on mobile and edge clusters instead. However,
there is limited literature on distributed training on the edge. Existing
approaches all require a central, potentially powerful edge or cloud server for
coordination or offloading. In this paper, we describe an approach for
distributed CNN training exclusively on mobile and edge devices. Our approach
is beneficial for the initial CNN layers that are feature map dominated. It is
based on partitioning forward inference and back-propagation operations among
devices through tiling and fusing to maximize locality and expose communication
and memory-aware parallelism. We also introduce the concept of layer grouping
to further fine-tune performance based on computation and communication
trade-off. Results show that for a cluster of 2-6 quad-core Raspberry Pi3
devices, training of an object-detection CNN provides a 2x-15x speedup with
respect to a single core and up to 8x reduction in memory usage per device, all
without sacrificing accuracy. Grouping offers up to 1.5x speedup depending on
the reference profile and batch size.</description>
      <guid isPermaLink="false">2409.09083v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Automated placement of stereotactic injections using a laser scan of the skull</title>
      <link>http://arxiv.org/abs/1410.5914v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 立体定向靶向是一种常用于小鼠及其他动物大脑注射的技术。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 当前最常见的靶向方法使用颅骨的参考点（如bregma和lambda），但其精度受颅骨曲率、个体差异以及颅骨标志与大脑位置之间的不完全对应等因素的限制。&lt;br&gt;&lt;h4&gt;3. 新方法介绍&lt;/h4&gt;   - 本文提出了一种软件工具，通过对小鼠颅骨进行3D激光扫描，并使用点云匹配算法将其注册到参考颅骨上。&lt;br&gt;&lt;h4&gt;4. 技术细节&lt;/h4&gt;   - 该软件利用变换参数精确定位玻璃微管，以进行示踪剂注射。&lt;br&gt;&lt;h4&gt;5. 性能验证&lt;/h4&gt;   - 软件能够以低于100微米的误差注册样本颅骨，并在小鼠注射中实现约500微米的误差。&lt;br&gt;&lt;h4&gt;6. 应用前景&lt;/h4&gt;   - 结果表明，使用颅骨扫描注册方法在自动化示踪剂注射的立体定向靶向中具有广泛的应用潜力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2014-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Stereotactic targeting is a commonly used technique for performing injections
in the brains of mice and other animals. The most common method for targeting
stereoscopic injections uses the skull indentations bregma and lambda as
reference points and is limited in its precision by factors such as skull
curvature and individual variation, as well as an incomplete correspondence
between skull landmarks and brain locations. In this software tool, a 3D laser
scan of the mouse skull is taken in vitro and registered onto a reference skull
using a point cloud matching algorithm, and the parameters of the
transformation are used to position a glass pipette to place tracer injections.
The software was capable of registering sample skulls with less than 100 micron
error, and was able to target an injection in a mouse with error of roughly 500
microns. These results indicate that using skull scan registration has the
potential to be widely applicable in automating stereotactic targeting of
tracer injections.</description>
      <guid isPermaLink="false">1410.5914v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Symbol as Points: Panoptic Symbol Spotting via Point-based Representation</title>
      <link>http://arxiv.org/abs/2401.10556v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICLR 2024&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究问题&lt;/h4&gt;   - 本文研究全景符号检测问题，旨在从计算机辅助设计（CAD）图纸中识别和解析可计数的物体实例（如窗户、门、桌子等）和不可计数的物体（如墙、扶手等）。&lt;br&gt;&lt;h4&gt;2. 现有方法&lt;/h4&gt;   - 现有方法通常包括将矢量图形栅格化为图像，并使用基于图像的方法进行符号检测，或直接构建图形并利用图神经网络进行符号识别。&lt;br&gt;&lt;h4&gt;3. 新方法&lt;/h4&gt;   - 本文采用不同的方法，将图形原语视为一组局部连接的2D点，并使用点云分割方法进行处理。&lt;br&gt;&lt;h4&gt;4. 技术细节&lt;/h4&gt;   - 使用点变换器提取原语特征，并添加类似于mask2former的检测头以预测最终输出。&lt;br&gt;&lt;h4&gt;5. 增强特征&lt;/h4&gt;   - 为了更好地利用原语的局部连接信息并增强其可区分性，提出了注意力与连接模块（ACM）和对比连接学习方案（CCL）。&lt;br&gt;&lt;h4&gt;6. 插值机制&lt;/h4&gt;   - 提出了KNN插值机制，用于检测头的掩膜注意力模块，以更好地处理原语掩膜的下采样，该机制是原语级别的，与图像的像素级别相对。&lt;br&gt;&lt;h4&gt;7. 效果验证&lt;/h4&gt;   - 方法命名为SymPoint，简单有效，在FloorPlanCAD数据集上，相较于最新的GAT-CADNet方法，PQ和RQ分别提高了9.6%和10.4%。&lt;br&gt;&lt;h4&gt;8. 可用资源&lt;/h4&gt;   - 源代码和模型将可在 [GitHub](https://github.com/nicehuster/SymPoint) 上获取。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-01-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/nicehuster/sympoint&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work studies the problem of panoptic symbol spotting, which is to spot
and parse both countable object instances (windows, doors, tables, etc.) and
uncountable stuff (wall, railing, etc.) from computer-aided design (CAD)
drawings. Existing methods typically involve either rasterizing the vector
graphics into images and using image-based methods for symbol spotting, or
directly building graphs and using graph neural networks for symbol
recognition. In this paper, we take a different approach, which treats graphic
primitives as a set of 2D points that are locally connected and use point cloud
segmentation methods to tackle it. Specifically, we utilize a point transformer
to extract the primitive features and append a mask2former-like spotting head
to predict the final output. To better use the local connection information of
primitives and enhance their discriminability, we further propose the attention
with connection module (ACM) and contrastive connection learning scheme (CCL).
Finally, we propose a KNN interpolation mechanism for the mask attention module
of the spotting head to better handle primitive mask downsampling, which is
primitive-level in contrast to pixel-level for the image. Our approach, named
SymPoint, is simple yet effective, outperforming recent state-of-the-art method
GAT-CADNet by an absolute increase of 9.6% PQ and 10.4% RQ on the FloorPlanCAD
dataset. The source code and models will be available at
https://github.com/nicehuster/SymPoint.</description>
      <guid isPermaLink="false">2401.10556v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Deep-PE: A Learning-Based Pose Evaluator for Point Cloud Registration</title>
      <link>http://arxiv.org/abs/2405.16085v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages, 16 figures&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 在点云配准领域，最常见的姿态评估方法基于统计，通过最大化一致对应点的数量来识别最佳变换。&lt;br&gt;&lt;h4&gt;2. 问题陈述&lt;/h4&gt;   - 当点云重叠率较低时，配准召回率显著下降，尽管在特征描述符设计和建立对应关系方面做出了努力。&lt;br&gt;&lt;h4&gt;3. 新方法介绍&lt;/h4&gt;   - 本文提出Deep-PE，一种轻量级的基于学习的姿态评估器，旨在提高姿态选择的准确性，特别是在低重叠的点云场景中。&lt;br&gt;&lt;h4&gt;4. 网络模块&lt;/h4&gt;   - Deep-PE网络包含两个主要模块：&lt;br&gt;     - **姿态感知注意力（PAA）模块**：模拟和学习在不同候选姿态下点云的对齐状态。&lt;br&gt;     - **姿态置信度预测（PCP）模块**：预测成功配准的可能性。&lt;br&gt;&lt;h4&gt;5. 学习能力&lt;/h4&gt;   - 这两个模块促进了局部和全局对齐先验的学习。&lt;br&gt;&lt;h4&gt;6. 实验验证&lt;/h4&gt;   - 在多个基准测试中进行广泛测试，验证了Deep-PE的有效性。&lt;br&gt;&lt;h4&gt;7. 性能提升&lt;/h4&gt;   - 在3DLoMatch数据集上，特别是在低重叠率情况下，Deep-PE在使用手工设计的FPFH和基于学习的FCGF描述符时，注册召回率分别比最先进的方法提高至少8%和11%。&lt;br&gt;&lt;h4&gt;8. 创新贡献&lt;/h4&gt;   - 据我们所知，这是首次利用深度学习选择最佳姿态而无需显式输入对应关系的研究。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-05-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the realm of point cloud registration, the most prevalent pose evaluation
approaches are statistics-based, identifying the optimal transformation by
maximizing the number of consistent correspondences. However, registration
recall decreases significantly when point clouds exhibit a low overlap rate,
despite efforts in designing feature descriptors and establishing
correspondences. In this paper, we introduce Deep-PE, a lightweight,
learning-based pose evaluator designed to enhance the accuracy of pose
selection, especially in challenging point cloud scenarios with low overlap.
Our network incorporates a Pose-Aware Attention (PAA) module to simulate and
learn the alignment status of point clouds under various candidate poses,
alongside a Pose Confidence Prediction (PCP) module that predicts the
likelihood of successful registration. These two modules facilitate the
learning of both local and global alignment priors. Extensive tests across
multiple benchmarks confirm the effectiveness of Deep-PE. Notably, on 3DLoMatch
with a low overlap rate, Deep-PE significantly outperforms state-of-the-art
methods by at least 8% and 11% in registration recall under handcrafted FPFH
and learning-based FCGF descriptors, respectively. To the best of our
knowledge, this is the first study to utilize deep learning to select the
optimal pose without the explicit need for input correspondences.</description>
      <guid isPermaLink="false">2405.16085v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Analysis of Diagnostics (Part II): Prevalence, Linear Independence, and Unsupervised Learning</title>
      <link>http://arxiv.org/abs/2408.16035v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 这是一个两部分系列论文的第二篇，使用诊断测试来理解流行率（即类中元素数量）、不确定性量化（UQ）和分类理论之间的关系。&lt;br&gt;&lt;h4&gt;2. 第一部分概述&lt;/h4&gt;   - 第一部分关注监督机器学习（ML），建立了流行率与相对条件概率之间的对偶关系。&lt;br&gt;   - 主要思想是通过最小化流行率加权的经验风险函数来训练一系列判别分类器。&lt;br&gt;&lt;h4&gt;3. 输出解释&lt;/h4&gt;   - 生成的输出可解释为相对概率水平集，从而提供类标签的不确定性估计。&lt;br&gt;   - 该过程还表明某些判别和生成模型是等效的。&lt;br&gt;&lt;h4&gt;4. 第二部分研究方向&lt;/h4&gt;   - 第二部分探讨这些结果在无监督学习任务中的扩展，借助线性代数的概念。&lt;br&gt;&lt;h4&gt;5. 不纯种群的参数化&lt;/h4&gt;   - 观察到，对于其类未知的样本，具有不纯种群的分布可以通过流行率参数化。&lt;br&gt;&lt;h4&gt;6. 线性独立种群的引入&lt;/h4&gt;   - 引入线性独立种群的概念，这些种群具有不同但未知的流行率值。&lt;br&gt;&lt;h4&gt;7. 分类器同构的识别&lt;/h4&gt;   - 确定了以不纯和纯种群为基础定义的分类器之间的同构关系。&lt;br&gt;&lt;h4&gt;8. 非线性方程系统&lt;/h4&gt;   - 在某些情况下，这导致一个非线性方程系统，其解可得线性独立种群的流行率值。&lt;br&gt;&lt;h4&gt;9. 无监督学习的实现&lt;/h4&gt;   - 完全实现无监督学习，作为对监督学习的推广。&lt;br&gt;&lt;h4&gt;10. 方法应用实例&lt;/h4&gt;    - 在合成数据和仅限研究用途的SARS-CoV-2酶联免疫吸附测定（ELISA）中展示了该方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This is the second manuscript in a two-part series that uses diagnostic
testing to understand the connection between prevalence (i.e. number of
elements in a class), uncertainty quantification (UQ), and classification
theory. Part I considered the context of supervised machine learning (ML) and
established a duality between prevalence and the concept of relative
conditional probability. The key idea of that analysis was to train a family of
discriminative classifiers by minimizing a sum of prevalence-weighted empirical
risk functions. The resulting outputs can be interpreted as relative
probability level-sets, which thereby yield uncertainty estimates in the class
labels. This procedure also demonstrated that certain discriminative and
generative ML models are equivalent. Part II considers the extent to which
these results can be extended to tasks in unsupervised learning through
recourse to ideas in linear algebra. We first observe that the distribution of
an impure population, for which the class of a corresponding sample is unknown,
can be parameterized in terms of a prevalence. This motivates us to introduce
the concept of linearly independent populations, which have different but
unknown prevalence values. Using this, we identify an isomorphism between
classifiers defined in terms of impure and pure populations. In certain cases,
this also leads to a nonlinear system of equations whose solution yields the
prevalence values of the linearly independent populations, fully realizing
unsupervised learning as a generalization of supervised learning. We illustrate
our methods in the context of synthetic data and a research-use-only SARS-CoV-2
enzyme-linked immunosorbent assay (ELISA).</description>
      <guid isPermaLink="false">2408.16035v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Searching for chemo-kinematic structures in the Milky Way halo with deep clustering algorithms</title>
      <link>http://arxiv.org/abs/2409.11429v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 根据λCDM模型，星系是通过层次聚合构建块形成的。我们的银河系是研究聚合事件残余的理想场所，特别是通过研究其晕星族的化学和运动学特性。&lt;br&gt;&lt;h4&gt;2. 化学标记的局限性&lt;/h4&gt;   - 由于星际晕的低密度，这里最适合进行化学标记，但仅依靠化学标记往往效果不佳，原因包括化学丰度的不确定性和不同星群之间化学特性的重叠。&lt;br&gt;&lt;h4&gt;3. 结合化学与运动学&lt;/h4&gt;   - 为了克服这些问题，可以结合化学和运动学特性。&lt;br&gt;&lt;h4&gt;4. 算法开发&lt;/h4&gt;   - 本论文开发了一种名为CREEK的机器学习算法，结合了由两个大型公共光谱调查（Gaia-ESO和APOGEE）观察到的晕星的轨道和化学特性。&lt;br&gt;&lt;h4&gt;5. 数据选择&lt;/h4&gt;   - 从APOGEE和Gaia-ESO调查中选择了晕星，基于其速度和金属丰度计算了它们的轨道参数。&lt;br&gt;&lt;h4&gt;6. 运动学处理&lt;/h4&gt;   - 将选定的数据传递给一个Siamese神经网络，根据运动学相似性建立星星之间的链接。&lt;br&gt;&lt;h4&gt;7. 化学特性处理&lt;/h4&gt;   - 构建的图通过图神经网络（GNN）自编码器进行处理，输入为选定的丰度。丰度的选择旨在最大化同一星团内星星之间的同质性，同时确保不同星团之间的独特性，优先选择误差较小的元素。&lt;br&gt;&lt;h4&gt;8. 计算化学空间&lt;/h4&gt;   - GNN自编码器计算所有连接星星的丰度均值，基于链接数量加权，并将化学空间映射到潜在空间中的更有效表示。&lt;br&gt;&lt;h4&gt;9. 结构恢复&lt;/h4&gt;   - 最后，应用OPTICS算法于潜在空间，基于星星的化学相似性提供分组结果。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; According to the lambda CDM scenario, galaxies are formed through the
hierarchical accretion of building blocks. Our Galaxy is a privileged place to
look for the remnants of accretion events through the study of the chemical and
kinematic properties of its halo stellar populations. Due to its low density,
the stellar halo holds the most favorable conditions for chemical tagging.
However, chemical tagging alone often yields weak results due to both
uncertainties in chemical abundances and to overlapping chemical properties
among different populations. To overcome this problem, the use of chemical and
kinematic properties can be combined. In this Thesis, we developed a machine
learning algorithm, named the CREEK, which combines orbital and chemical
properties of halo stars observed by two large public spectroscopic surveys,
Gaia-ESO and APOGEE. The CREEK operates as follows: 1)Data selection: We
selected halo stars from the APOGEE and Gaia-ESO surveys based both on their
velocity and metallicity and we computed their orbital parameters. 2)Using
kinematics: The selected data were passed to a Siamese Neural Network that
established links between stars based on their kinematic similarities. 3)Using
chemistry: The graph was passed through a Graph Neural Network (GNN)
auto-encoder that took as input the selected abundances. The abundances were
chosen to maximize homogeneity within stars from the same cluster while
ensuring distinctiveness between stars from different clusters. Additionally,
we prioritised elements with smallest errors. The GNN auto-encoder computed a
mean of the abundances of all connected stars, weighted on the number of links
of each star and mapped the chemical space into a more efficient representation
in the latent space. 4)Recovering structures: Finally, OPTICS was applied to
the latent space, providing groups based on the chemical similarities of the
stars.</description>
      <guid isPermaLink="false">2409.11429v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Space3D-Bench: Spatial 3D Question Answering Benchmark</title>
      <link>http://arxiv.org/abs/2408.16662v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 回答有关环境空间属性的问题对现有的语言和视觉基础模型提出了挑战，主要是由于对3D世界中物体关系的理解不足。&lt;br&gt;&lt;h4&gt;2. 现有数据集的局限&lt;/h4&gt;   - 多个3D问答数据集被提出，尽管提供了多样化的问题，但它们分别关注特定的3D推理方面或数据模态有限。&lt;br&gt;&lt;h4&gt;3. 新数据集的提出&lt;/h4&gt;   - 本文提出了Space3D-Bench，一个包含1000个与Replica数据集场景相关的通用空间问题和答案的集合。&lt;br&gt;&lt;h4&gt;4. 数据模态的多样性&lt;/h4&gt;   - Space3D-Bench提供多种数据模态，包括点云、RGB-D图像、导航网格和3D物体检测。&lt;br&gt;&lt;h4&gt;5. 问题分类体系&lt;/h4&gt;   - 为确保问题覆盖广泛的3D目标，提出了一个受地理信息系统启发的室内空间问题分类法，并相应地平衡数据集。&lt;br&gt;&lt;h4&gt;6. 评估系统&lt;/h4&gt;   - 提供了一种评估系统，根据预定义的真实答案对自然语言响应进行评分，利用视觉语言模型对文本和图像的理解来比较响应与真实文本信息或相关视觉数据。&lt;br&gt;&lt;h4&gt;7. 基线模型的介绍&lt;/h4&gt;   - 引入了一种基线模型RAG3D-Chat，将基础模型的世界理解与丰富的上下文检索相结合，在提出的数据集上取得了67%的准确率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Answering questions about the spatial properties of the environment poses
challenges for existing language and vision foundation models due to a lack of
understanding of the 3D world notably in terms of relationships between
objects. To push the field forward, multiple 3D Q&amp;A datasets were proposed
which, overall, provide a variety of questions, but they individually focus on
particular aspects of 3D reasoning or are limited in terms of data modalities.
To address this, we present Space3D-Bench - a collection of 1000 general
spatial questions and answers related to scenes of the Replica dataset which
offers a variety of data modalities: point clouds, posed RGB-D images,
navigation meshes and 3D object detections. To ensure that the questions cover
a wide range of 3D objectives, we propose an indoor spatial questions taxonomy
inspired by geographic information systems and use it to balance the dataset
accordingly. Moreover, we provide an assessment system that grades natural
language responses based on predefined ground-truth answers by leveraging a
Vision Language Model's comprehension of both text and images to compare the
responses with ground-truth textual information or relevant visual data.
Finally, we introduce a baseline called RAG3D-Chat integrating the world
understanding of foundation models with rich context retrieval, achieving an
accuracy of 67% on the proposed dataset.</description>
      <guid isPermaLink="false">2408.16662v3</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Exploring Prediction Targets in Masked Pre-Training for Speech Foundation Models</title>
      <link>http://arxiv.org/abs/2409.10788v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to ICASSP 2025&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 语音基础模型，如HuBERT及其变体，经过大量未标记语音的预训练，用于各种下游任务。&lt;br&gt;&lt;h4&gt;2. 训练目标&lt;/h4&gt;   - 这些模型使用掩码预测目标，模型学习根据未掩码的上下文预测掩码输入段的信息。&lt;br&gt;&lt;h4&gt;3. 预测目标的影响&lt;/h4&gt;   - 预测目标的选择会影响下游任务的性能：&lt;br&gt;     - 编码韵律的目标对与说话者相关的任务有利。&lt;br&gt;     - 编码语音学的目标更适合内容相关的任务。&lt;br&gt;&lt;h4&gt;4. 详细程度的差异&lt;/h4&gt;   - 预测目标在编码细节程度上有所不同：&lt;br&gt;     - 编码细微声学细节的目标对去噪任务有益。&lt;br&gt;     - 编码更高层次抽象的目标更适合内容相关的任务。&lt;br&gt;&lt;h4&gt;5. 研究空白&lt;/h4&gt;   - 尽管预测目标的重要性显而易见，但影响它们的设计选择尚未得到充分研究。&lt;br&gt;&lt;h4&gt;6. 研究目的&lt;/h4&gt;   - 本文探讨设计选择及其对下游任务性能的影响。&lt;br&gt;&lt;h4&gt;7. 研究结果&lt;/h4&gt;   - 结果表明，HuBERT常用的设计选择可能是次优的。&lt;br&gt;&lt;h4&gt;8. 创新方法&lt;/h4&gt;   - 提出新方法以创建更具信息量的预测目标，并通过在多个下游任务中的改进展示其有效性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Speech foundation models, such as HuBERT and its variants, are pre-trained on
large amounts of unlabeled speech for various downstream tasks. These models
use a masked prediction objective, where the model learns to predict
information about masked input segments from the unmasked context. The choice
of prediction targets in this framework can influence performance on downstream
tasks. For example, targets that encode prosody are beneficial for
speaker-related tasks, while targets that encode phonetics are more suited for
content-related tasks. Additionally, prediction targets can vary in the level
of detail they encode; targets that encode fine-grained acoustic details are
beneficial for denoising tasks, while targets that encode higher-level
abstractions are more suited for content-related tasks. Despite the importance
of prediction targets, the design choices that affect them have not been
thoroughly studied. This work explores the design choices and their impact on
downstream task performance. Our results indicate that the commonly used design
choices for HuBERT can be suboptimal. We propose novel approaches to create
more informative prediction targets and demonstrate their effectiveness through
improvements across various downstream tasks.</description>
      <guid isPermaLink="false">2409.10788v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Real-Time Machine Learning Strategies for a New Kind of Neuroscience Experiments</title>
      <link>http://arxiv.org/abs/2409.01280v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This article is accepted for publication in the 2024 European Signal
  Processing Conference (EUSIPCO)&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 神经系统的功能和失调与神经状态的时间演变相关。&lt;br&gt;&lt;h4&gt;2. 当前限制&lt;/h4&gt;   - 显示神经系统因果作用的限制主要源于缺乏能够实时探测大脑内部状态的工具，这限制了基础和临床神经科学实验的范围。&lt;br&gt;&lt;h4&gt;3. 技术进展&lt;/h4&gt;   - 最近，实时机器学习技术的进展，特别是在分析神经时间序列作为非线性随机动态系统方面，开始填补这一空白。&lt;br&gt;&lt;h4&gt;4. 新技术的优势&lt;/h4&gt;   - 这些技术能够即时解读和与神经系统互动，提供新的神经计算洞见。&lt;br&gt;&lt;h4&gt;5. 面临的挑战&lt;/h4&gt;   - 仍然存在若干重大挑战，包括：&lt;br&gt;     - 收敛速率慢&lt;br&gt;     - 高维数据复杂性&lt;br&gt;     - 结构噪声&lt;br&gt;     - 非可识别性&lt;br&gt;     - 缺乏针对神经动态的归纳偏见&lt;br&gt;&lt;h4&gt;6. 克服挑战的重要性&lt;/h4&gt;   - 克服这些挑战对于实现实时神经数据分析、因果调查神经计算和高级扰动基础的脑机接口至关重要。&lt;br&gt;&lt;h4&gt;7. 论文贡献&lt;/h4&gt;   - 本文提供了对该领域当前状态的全面视角，着重于持续存在的问题，并概述了潜在的发展路径。&lt;br&gt;&lt;h4&gt;8. 强调的研究方向&lt;/h4&gt;   - 强调大型综合神经科学计划的重要性和元学习在克服这些挑战中的作用。&lt;br&gt;&lt;h4&gt;9. 未来前景&lt;/h4&gt;   - 这些方法代表了有前景的研究方向，可能重新定义神经科学实验和脑机接口的格局，有助于深入理解大脑功能和治疗神经系统疾病的突破。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Function and dysfunctions of neural systems are tied to the temporal
evolution of neural states. The current limitations in showing their causal
role stem largely from the absence of tools capable of probing the brain's
internal state in real-time. This gap restricts the scope of experiments vital
for advancing both fundamental and clinical neuroscience. Recent advances in
real-time machine learning technologies, particularly in analyzing neural time
series as nonlinear stochastic dynamical systems, are beginning to bridge this
gap. These technologies enable immediate interpretation of and interaction with
neural systems, offering new insights into neural computation. However, several
significant challenges remain. Issues such as slow convergence rates,
high-dimensional data complexities, structured noise, non-identifiability, and
a general lack of inductive biases tailored for neural dynamics are key
hurdles. Overcoming these challenges is crucial for the full realization of
real-time neural data analysis for the causal investigation of neural
computation and advanced perturbation based brain machine interfaces. In this
paper, we provide a comprehensive perspective on the current state of the
field, focusing on these persistent issues and outlining potential paths
forward. We emphasize the importance of large-scale integrative neuroscience
initiatives and the role of meta-learning in overcoming these challenges. These
approaches represent promising research directions that could redefine the
landscape of neuroscience experiments and brain-machine interfaces,
facilitating breakthroughs in understanding brain function, and treatment of
neurological disorders.</description>
      <guid isPermaLink="false">2409.01280v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Fair Meta-Learning: Learning How to Learn Fairly</title>
      <link>http://arxiv.org/abs/1911.04336v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  arXiv admin note: substantial text overlap with arXiv:1908.09092&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 针对公平性相关任务的数据集可能缺乏示例或在敏感属性的特定标签上存在偏见。&lt;br&gt;&lt;h4&gt;2. 研究目的&lt;/h4&gt;   - 展示基于权重的元学习方法在这种情况下的有效性。&lt;br&gt;&lt;h4&gt;3. 模型训练方法&lt;/h4&gt;   - 对于可以通过梯度下降训练的模型，证明某些参数配置可以使模型在少量梯度步骤和最小数据下优化，同时保持公平性和准确性。&lt;br&gt;&lt;h4&gt;4. 方法改进&lt;/h4&gt;   - 将流行的MAML（Model-Agnostic Meta-Learning）算法适应为Fair-MAML，通过加入公平性正则化项来实现。&lt;br&gt;&lt;h4&gt;5. 实际应用&lt;/h4&gt;   - Fair-MAML允许从少量示例中训练公平的机器学习模型，前提是有相关任务的数据可用。&lt;br&gt;&lt;h4&gt;6. 实验验证&lt;/h4&gt;   - 通过与相关基线的比较，实证展示该技术的价值。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2019-11-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Data sets for fairness relevant tasks can lack examples or be biased
according to a specific label in a sensitive attribute. We demonstrate the
usefulness of weight based meta-learning approaches in such situations. For
models that can be trained through gradient descent, we demonstrate that there
are some parameter configurations that allow models to be optimized from a few
number of gradient steps and with minimal data which are both fair and
accurate. To learn such weight sets, we adapt the popular MAML algorithm to
Fair-MAML by the inclusion of a fairness regularization term. In practice,
Fair-MAML allows practitioners to train fair machine learning models from only
a few examples when data from related tasks is available. We empirically
exhibit the value of this technique by comparing to relevant baselines.</description>
      <guid isPermaLink="false">1911.04336v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Taylor-Sensus Network: Embracing Noise to Enlighten Uncertainty for Scientific Data</title>
      <link>http://arxiv.org/abs/2409.07942v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 不确定性估计在科学数据的机器学习中至关重要。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限&lt;/h4&gt;   - 当前的不确定性估计方法主要关注模型的固有不确定性，忽视了数据中噪声的明确建模。&lt;br&gt;   - 噪声估计方法通常依赖于时间或空间依赖性，这在结构化科学数据中往往缺乏。&lt;br&gt;&lt;h4&gt;3. 提出的新方法&lt;/h4&gt;   - 为解决这些挑战，提出了Taylor-Sensus Network（TSNet）。&lt;br&gt;&lt;h4&gt;4. 创新点&lt;/h4&gt;   - TSNet创新性地使用泰勒级数展开来建模复杂的异方差噪声，并提出了一个深度泰勒模块来处理噪声分布。&lt;br&gt;&lt;h4&gt;5. 模块设计&lt;/h4&gt;   - 包括一个噪声感知对比学习模块和一个数据密度感知模块，分别用于处理随机（aleatoric）和认知（epistemic）不确定性。&lt;br&gt;&lt;h4&gt;6. 不确定性整合&lt;/h4&gt;   - 采用不确定性组合算子来整合这些不确定性。&lt;br&gt;&lt;h4&gt;7. 损失函数&lt;/h4&gt;   - 使用新颖的异方差均方误差损失进行网络训练。&lt;br&gt;&lt;h4&gt;8. 实验结果&lt;/h4&gt;   - TSNet在实验中表现优于主流和最先进的方法，突显了其在科学研究中的潜力和噪声抵抗能力。&lt;br&gt;&lt;h4&gt;9. 开源计划&lt;/h4&gt;   - 该方法将开放源代码，以促进“AI for Science”社区的发展。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Uncertainty estimation is crucial in scientific data for machine learning.
Current uncertainty estimation methods mainly focus on the model's inherent
uncertainty, while neglecting the explicit modeling of noise in the data.
Furthermore, noise estimation methods typically rely on temporal or spatial
dependencies, which can pose a significant challenge in structured scientific
data where such dependencies among samples are often absent. To address these
challenges in scientific research, we propose the Taylor-Sensus Network
(TSNet). TSNet innovatively uses a Taylor series expansion to model complex,
heteroscedastic noise and proposes a deep Taylor block for aware noise
distribution. TSNet includes a noise-aware contrastive learning module and a
data density perception module for aleatoric and epistemic uncertainty.
Additionally, an uncertainty combination operator is used to integrate these
uncertainties, and the network is trained using a novel heteroscedastic mean
square error loss. TSNet demonstrates superior performance over mainstream and
state-of-the-art methods in experiments, highlighting its potential in
scientific research and noise resistance. It will be open-source to facilitate
the community of "AI for Science".</description>
      <guid isPermaLink="false">2409.07942v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Deep Learning Techniques for Hand Vein Biometrics: A Comprehensive Review</title>
      <link>http://arxiv.org/abs/2409.07128v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 生物识别认证因其安全和高效的身份验证方法而受到广泛关注。&lt;br&gt;&lt;h4&gt;2. 手静脉生物识别的优势&lt;/h4&gt;   - 包括指静脉、掌静脉和手背静脉识别，具有高准确性、低伪造风险和非侵入性等独特优势。&lt;br&gt;   - 手中的静脉模式复杂且个体独特，成为理想的生物识别标识符。&lt;br&gt;&lt;h4&gt;3. 用户便利性与卫生&lt;/h4&gt;   - 手静脉识别是非接触式的，相比指纹或虹膜识别，提升了用户便利性和卫生条件。&lt;br&gt;&lt;h4&gt;4. 安全性&lt;/h4&gt;   - 静脉位于体内，不易受损或改变，提高了生物识别系统的安全性和可靠性。&lt;br&gt;&lt;h4&gt;5. 综述内容&lt;/h4&gt;   - 本文回顾了应用于指静脉、掌静脉和手背静脉识别的深度学习技术的最新进展。&lt;br&gt;   - 涵盖手静脉生物识别的基本原理，汇总公开数据集，并讨论评估三种模式的最先进指标。&lt;br&gt;&lt;h4&gt;6. 方法与技术&lt;/h4&gt;   - 提供了指、掌、手背及多模态静脉技术的建议方法，讨论了最佳表现、数据增强技术及有效的迁移学习方法以及相关的预训练深度学习模型。&lt;br&gt;&lt;h4&gt;7. 研究挑战与未来方向&lt;/h4&gt;   - 讨论了面临的研究挑战，并概述了未来方向和前景，鼓励研究人员改进现有方法并提出创新技术。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Biometric authentication has garnered significant attention as a secure and
efficient method of identity verification. Among the various modalities, hand
vein biometrics, including finger vein, palm vein, and dorsal hand vein
recognition, offer unique advantages due to their high accuracy, low
susceptibility to forgery, and non-intrusiveness. The vein patterns within the
hand are highly complex and distinct for each individual, making them an ideal
biometric identifier. Additionally, hand vein recognition is contactless,
enhancing user convenience and hygiene compared to other modalities such as
fingerprint or iris recognition. Furthermore, the veins are internally located,
rendering them less susceptible to damage or alteration, thus enhancing the
security and reliability of the biometric system. The combination of these
factors makes hand vein biometrics a highly effective and secure method for
identity verification. This review paper delves into the latest advancements in
deep learning techniques applied to finger vein, palm vein, and dorsal hand
vein recognition. It encompasses all essential fundamentals of hand vein
biometrics, summarizes publicly available datasets, and discusses
state-of-the-art metrics used for evaluating the three modes. Moreover, it
provides a comprehensive overview of suggested approaches for finger, palm,
dorsal, and multimodal vein techniques, offering insights into the best
performance achieved, data augmentation techniques, and effective transfer
learning methods, along with associated pretrained deep learning models.
Additionally, the review addresses research challenges faced and outlines
future directions and perspectives, encouraging researchers to enhance existing
methods and propose innovative techniques.</description>
      <guid isPermaLink="false">2409.07128v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Teaching Robots to Do Object Assembly using Multi-modal 3D Vision</title>
      <link>http://arxiv.org/abs/1601.06473v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究动机&lt;/h4&gt;   - 本文的动机是开发一个智能系统，利用多模态视觉实现下一代机械组装。&lt;br&gt;&lt;h4&gt;2. 系统分为两个阶段&lt;/h4&gt;   - **第一阶段**：人类教导机器人组装结构。&lt;br&gt;   - **第二阶段**：机器人通过AI规划寻找物体，抓取并进行组装。&lt;br&gt;&lt;h4&gt;3. 系统的关键部分&lt;/h4&gt;   - 3D视觉检测的精度对于系统至关重要。&lt;br&gt;&lt;h4&gt;4. 多模态方法的应用&lt;/h4&gt;   - 在教学阶段使用AR标记，因为人类可以主动控制过程。&lt;br&gt;   - 在机器人执行阶段，使用点云匹配和几何约束，以避免意外噪声。&lt;br&gt;&lt;h4&gt;5. 实验验证&lt;/h4&gt;   - 进行实验以检验所提方法的精度和正确性。&lt;br&gt;&lt;h4&gt;6. 实际应用性&lt;/h4&gt;   - 开发的方法与基于图模型的运动规划相结合，已在工业机器人上实施，适用于现实场景。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2016-01-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The motivation of this paper is to develop a smart system using multi-modal
vision for next-generation mechanical assembly. It includes two phases where in
the first phase human beings teach the assembly structure to a robot and in the
second phase the robot finds objects and grasps and assembles them using AI
planning. The crucial part of the system is the precision of 3D visual
detection and the paper presents multi-modal approaches to meet the
requirements: AR markers are used in the teaching phase since human beings can
actively control the process. Point cloud matching and geometric constraints
are used in the robot execution phase to avoid unexpected noises. Experiments
are performed to examine the precision and correctness of the approaches. The
study is practical: The developed approaches are integrated with graph
model-based motion planning, implemented on an industrial robots and applicable
to real-world scenarios.</description>
      <guid isPermaLink="false">1601.06473v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Bridging Domain Gap of Point Cloud Representations via Self-Supervised Geometric Augmentation</title>
      <link>http://arxiv.org/abs/2409.06956v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 6 figures, 5 tables&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 最近，语义点云分析的进展主要受到合成数据（如ModelNet和ShapeNet）的推动，这些数据通常是完整的、对齐良好的且无噪声的。&lt;br&gt;&lt;h4&gt;2. 合成点云的局限&lt;/h4&gt;   - 这些理想的合成点云在几何角度上变化有限，在许多3D视觉任务（如点云分类）中表现良好。&lt;br&gt;&lt;h4&gt;3. 无监督领域适应（UDA）中的挑战&lt;/h4&gt;   - 针对合成点云设计的表示学习难以从不完整和有噪声的点云中捕捉领域不变的几何模式。&lt;br&gt;&lt;h4&gt;4. 新方案的提出&lt;/h4&gt;   - 为解决这一问题，提出了一种新方案，通过两个自监督几何增强任务来正则化表示学习，以实现跨领域的几何不变性。&lt;br&gt;&lt;h4&gt;5. 预训练任务&lt;/h4&gt;   - 提出了预测增强样本平移距离的新预训练任务，以缓解由于遮挡和噪声造成的点云质心偏移。&lt;br&gt;&lt;h4&gt;6. 关系自监督学习的创新&lt;/h4&gt;   - 首次在几何增强点云上以级联方式整合关系自监督学习，利用增强变体与其他样本之间的内在关系作为跨领域几何特征的额外约束。&lt;br&gt;&lt;h4&gt;7. 实验验证&lt;/h4&gt;   - 在PointDA-10数据集上的实验表明，所提方法的有效性，达到了最先进的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent progress of semantic point clouds analysis is largely driven by
synthetic data (e.g., the ModelNet and the ShapeNet), which are typically
complete, well-aligned and noisy free. Therefore, representations of those
ideal synthetic point clouds have limited variations in the geometric
perspective and can gain good performance on a number of 3D vision tasks such
as point cloud classification. In the context of unsupervised domain adaptation
(UDA), representation learning designed for synthetic point clouds can hardly
capture domain invariant geometric patterns from incomplete and noisy point
clouds. To address such a problem, we introduce a novel scheme for induced
geometric invariance of point cloud representations across domains, via
regularizing representation learning with two self-supervised geometric
augmentation tasks. On one hand, a novel pretext task of predicting translation
distances of augmented samples is proposed to alleviate centroid shift of point
clouds due to occlusion and noises. On the other hand, we pioneer an
integration of the relational self-supervised learning on
geometrically-augmented point clouds in a cascade manner, utilizing the
intrinsic relationship of augmented variants and other samples as extra
constraints of cross-domain geometric features. Experiments on the PointDA-10
dataset demonstrate the effectiveness of the proposed method, achieving the
state-of-the-art performance.</description>
      <guid isPermaLink="false">2409.06956v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>L-PR: Exploiting LiDAR Fiducial Marker for Unordered Low Overlap Multiview Point Cloud Registration</title>
      <link>http://arxiv.org/abs/2406.03298v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 点云配准是计算机视觉和机器人技术中许多应用的前提。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限&lt;/h4&gt;   - 大多数现有方法专注于高重叠的两点云之间的配准。虽然有一些方法针对低重叠情况，但在退化场景中表现不佳。&lt;br&gt;&lt;h4&gt;3. 提出的新框架&lt;/h4&gt;   - 本文介绍了一种新框架，称为L-PR，旨在利用LiDAR基准标记注册无序低重叠的多视点点云。&lt;br&gt;&lt;h4&gt;4. LiDAR基准标记&lt;/h4&gt;   - 这些标记与流行的AprilTag和ArUco标记相同，是薄纸片，不会影响环境的3D几何形状。&lt;br&gt;&lt;h4&gt;5. 改进的检测方法&lt;/h4&gt;   - 首先提出了一种改进的自适应阈值标记检测方法，以便在点云视角剧烈变化时提供稳健的检测结果。&lt;br&gt;&lt;h4&gt;6. MAP问题的公式化&lt;/h4&gt;   - 将无序多视点点云配准问题公式化为最大后验（MAP）问题，并开发了一个包含两级图的框架来解决它。&lt;br&gt;&lt;h4&gt;7. 第一层图&lt;/h4&gt;   - 第一层图构建为加权图，旨在从无序集合中有效且最优地推断扫描姿态的初始值。&lt;br&gt;&lt;h4&gt;8. 第二层图&lt;/h4&gt;   - 第二层图构建为因子图。通过全局优化图上的变量（包括扫描姿态、标记姿态和标记角点位置），解决MAP问题。&lt;br&gt;&lt;h4&gt;9. 实验验证&lt;/h4&gt;   - 进行了定性和定量实验，证明所提方法超越了之前的最先进方法，并展示L-PR可以作为低成本且高效的3D资产收集和训练数据收集工具。&lt;br&gt;&lt;h4&gt;10. 新数据集的收集&lt;/h4&gt;    - 特别地，使用L-PR收集了一个新数据集Livox-3DMatch，并将其纳入最先进学习方法SGHR的训练中，从而在多个基准上显著提升了SGHR的表现。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-06-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/yorklyb/LiDAR-SFM&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud registration is a prerequisite for many applications in computer
vision and robotics. Most existing methods focus on pairwise registration of
two point clouds with high overlap. Although there have been some methods for
low overlap cases, they struggle in degraded scenarios. This paper introduces a
novel framework dubbed L-PR, designed to register unordered low overlap
multiview point clouds leveraging LiDAR fiducial markers. We refer to them as
LiDAR fiducial markers, but they are the same as the popular AprilTag and ArUco
markers, thin sheets of paper that do not affect the 3D geometry of the
environment. We first propose an improved adaptive threshold marker detection
method to provide robust detection results when the viewpoints among point
clouds change dramatically. Then, we formulate the unordered multiview point
cloud registration problem as a maximum a-posteriori (MAP) problem and develop
a framework consisting of two levels of graphs to address it. The first-level
graph, constructed as a weighted graph, is designed to efficiently and
optimally infer initial values of scan poses from the unordered set. The
second-level graph is constructed as a factor graph. By globally optimizing the
variables on the graph, including scan poses, marker poses, and marker corner
positions, we tackle the MAP problem. We conduct both qualitative and
quantitative experiments to demonstrate that the proposed method surpasses
previous state-of-the-art (SOTA) methods and to showcase that L-PR can serve as
a low-cost and efficient tool for 3D asset collection and training data
collection. In particular, we collect a new dataset named Livox-3DMatch using
L-PR and incorporate it into the training of the SOTA learning-based method,
SGHR, which brings evident improvements for SGHR on various benchmarks.</description>
      <guid isPermaLink="false">2406.03298v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Rotation-Invariant Completion Network</title>
      <link>http://arxiv.org/abs/2308.11979v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, accepted to PRCV 2023 (The 6th Chinese Conference on
  Pattern Recognition and Computer Vision)&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 现实世界的点云通常存在不完整性，并表现出不同的姿态。&lt;br&gt;&lt;h4&gt;2. 现有问题&lt;/h4&gt;   - 当前的点云补全方法在重建训练集中一致姿态的完整点云时表现良好，但在处理具有多样姿态的点云时效果不佳。&lt;br&gt;&lt;h4&gt;3. 提出的新方法&lt;/h4&gt;   - 本研究提出了一个名为旋转不变补全网络（RICNet）的网络，包含两个部分：双管道补全网络（DPCNet）和增强模块。&lt;br&gt;&lt;h4&gt;4. DPCNet的功能&lt;/h4&gt;   - DPCNet 首先生成一个粗略的完整点云。其特征提取模块能够提取一致的特征，无论输入的点云是否经历旋转或平移。&lt;br&gt;&lt;h4&gt;5. 增强模块的作用&lt;/h4&gt;   - 随后，增强模块对最终生成的点云的细粒度细节进行精细化处理。&lt;br&gt;&lt;h4&gt;6. 旋转不变性&lt;/h4&gt;   - RICNet 在特征提取中实现了更好的旋转不变性，并结合了人造物体的结构关系。&lt;br&gt;&lt;h4&gt;7. 性能评估&lt;/h4&gt;   - 为了评估 RICNet 和现有方法在不同姿态点云上的性能，研究对 MVP 数据集中的点云应用了随机变换，并进行实验。&lt;br&gt;&lt;h4&gt;8. 实验结果&lt;/h4&gt;   - 实验结果表明，RICNet 在补全性能上优于现有方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2023-08-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1007/978-981-99-8432-9_10&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-world point clouds usually suffer from incompleteness and display
different poses. While current point cloud completion methods excel in
reproducing complete point clouds with consistent poses as seen in the training
set, their performance tends to be unsatisfactory when handling point clouds
with diverse poses. We propose a network named Rotation-Invariant Completion
Network (RICNet), which consists of two parts: a Dual Pipeline Completion
Network (DPCNet) and an enhancing module. Firstly, DPCNet generates a coarse
complete point cloud. The feature extraction module of DPCNet can extract
consistent features, no matter if the input point cloud has undergone rotation
or translation. Subsequently, the enhancing module refines the fine-grained
details of the final generated point cloud. RICNet achieves better rotation
invariance in feature extraction and incorporates structural relationships in
man-made objects. To assess the performance of RICNet and existing methods on
point clouds with various poses, we applied random transformations to the point
clouds in the MVP dataset and conducted experiments on them. Our experiments
demonstrate that RICNet exhibits superior completion performance compared to
existing methods.</description>
      <guid isPermaLink="false">2308.11979v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Dynamic Prototype Adaptation with Distillation for Few-shot Point Cloud Segmentation</title>
      <link>http://arxiv.org/abs/2401.16051v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in 3DV2024, code is available at
  https://github.com/jliu4ai/DPA&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究目标&lt;/h4&gt;   - 少量样本点云分割旨在生成对以前未见类别的每个点的掩膜，仅使用最小数量的标注点云作为参考。&lt;br&gt;&lt;h4&gt;2. 现有方法的挑战&lt;/h4&gt;   - 现有的原型基础方法依赖支持原型来指导查询点云的分割，但在支持原型与查询特征之间存在显著对象变化时会遇到困难。&lt;br&gt;&lt;h4&gt;3. 提出的新方法&lt;/h4&gt;   - 本研究提出了动态原型适应（DPA），该方法明确学习每个查询点云的任务特定原型，以应对对象变化问题。&lt;br&gt;&lt;h4&gt;4. 适应机制&lt;/h4&gt;   - DPA通过原型校正实现适应，将支持中的原型与查询特征分布对齐，并通过原型到查询的注意机制，从查询点云中提取任务特定上下文。&lt;br&gt;&lt;h4&gt;5. 知识转移&lt;/h4&gt;   - 引入了原型蒸馏正则化项，促进在适应过程中早期原型与其更深层次的对应物之间的知识转移。&lt;br&gt;&lt;h4&gt;6. 迭代应用&lt;/h4&gt;   - 通过反复应用这些适应，生成任务特定的原型，从而对查询点云进行准确的掩膜预测。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 在两个流行基准测试上进行的广泛实验表明，DPA在性能上显著超越了现有方法，例如在 S3DIS 和 ScanNet 数据集的 2-way 1-shot 设置下，分别提高了 7.43% 和 6.39%。&lt;br&gt;&lt;h4&gt;8. 代码可用性&lt;/h4&gt;   - 代码可在 GitHub 上获取，链接为 [https://github.com/jliu4ai/DPA](https://github.com/jliu4ai/DPA)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-01-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/jliu4ai/dpa&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Few-shot point cloud segmentation seeks to generate per-point masks for
previously unseen categories, using only a minimal set of annotated point
clouds as reference. Existing prototype-based methods rely on support
prototypes to guide the segmentation of query point clouds, but they encounter
challenges when significant object variations exist between the support
prototypes and query features. In this work, we present dynamic prototype
adaptation (DPA), which explicitly learns task-specific prototypes for each
query point cloud to tackle the object variation problem. DPA achieves the
adaptation through prototype rectification, aligning vanilla prototypes from
support with the query feature distribution, and prototype-to-query attention,
extracting task-specific context from query point clouds. Furthermore, we
introduce a prototype distillation regularization term, enabling knowledge
transfer between early-stage prototypes and their deeper counterparts during
adaption. By iteratively applying these adaptations, we generate task-specific
prototypes for accurate mask predictions on query point clouds. Extensive
experiments on two popular benchmarks show that DPA surpasses state-of-the-art
methods by a significant margin, e.g., 7.43\% and 6.39\% under the 2-way 1-shot
setting on S3DIS and ScanNet, respectively. Code is available at
https://github.com/jliu4ai/DPA.</description>
      <guid isPermaLink="false">2401.16051v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>sEMG-Driven Physics-Informed Gated Recurrent Networks for Modeling Upper Limb Multi-Joint Movement Dynamics</title>
      <link>http://arxiv.org/abs/2408.16599v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 外骨骼和康复系统通过先进的人机界面（HMI）增强人类力量和恢复能力。&lt;br&gt;&lt;h4&gt;2. 现有问题&lt;/h4&gt;   - 物理信息神经网络（PINNs）的实时应用受到固定输入长度和替代模型的限制。&lt;br&gt;&lt;h4&gt;3. 新方法引入&lt;/h4&gt;   - 本研究提出了一种新型的物理信息门控递归网络（PiGRN），旨在使用表面肌电图（sEMG）数据预测多关节扭矩。&lt;br&gt;&lt;h4&gt;4. 模型结构&lt;/h4&gt;   - PiGRN 模型使用门控递归单元（GRU）将时间序列的 sEMG 输入转换为多关节运动学和外部载荷，然后将其整合到运动方程中，以确保符合物理法则。&lt;br&gt;&lt;h4&gt;5. 实验验证&lt;/h4&gt;   - 对五名参与者在进行肘部屈伸任务时采集的 sEMG 数据进行实验验证，结果显示 PiGRN 模型能够准确预测 10 种不熟悉动作的关节扭矩。&lt;br&gt;&lt;h4&gt;6. 性能指标&lt;/h4&gt;   - 模型的均方根误差（RMSE）值在 4.02% 到 11.40% 之间，相关系数范围为 0.87 到 0.98，表明预测效果良好。&lt;br&gt;&lt;h4&gt;7. 应用潜力&lt;/h4&gt;   - 这些发现突显了 PiGRN 在实时外骨骼和康复应用中的潜力。&lt;br&gt;&lt;h4&gt;8. 未来研究方向&lt;/h4&gt;   - 未来的研究将探讨更为多样化的数据集，改善肌肉骨骼模型，并研究无监督学习方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Exoskeletons and rehabilitation systems offer great potential for enhancing
human strength and recovery through advanced human-machine interfaces (HMIs)
that adapt to movement dynamics. However, the real-time application of
physics-informed neural networks (PINNs) is limited by their reliance on fixed
input lengths and surrogate models. This study introduces a novel
physics-informed Gated Recurrent Network (PiGRN) designed to predict
multi-joint torques using surface electromyography (sEMG) data. The PiGRN model
employs a Gated Recurrent Unit (GRU) to convert time-series sEMG inputs into
multi-joint kinematics and external loads, which are then integrated into an
equation of motion to ensure consistency with physical laws. Experimental
validation with sEMG data from five participants performing elbow
flexion-extension tasks showed that the PiGRN model accurately predicted joint
torques for 10 unfamiliar movements, with RMSE values between 4.02\% and
11.40\% and correlation coefficients ranging from 0.87 to 0.98. These findings
highlight the PiGRN's potential for real-time exoskeleton and rehabilitation
applications. Future research will explore more diverse datasets, improve
musculoskeletal models, and investigate unsupervised learning methods.</description>
      <guid isPermaLink="false">2408.16599v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Operational Wind Speed Forecasts for Chile's Electric Power Sector Using a Hybrid ML Model</title>
      <link>http://arxiv.org/abs/2409.09263v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 随着智利电力部门朝向可再生能源未来发展，准确预测可再生能源的发电至关重要，以便有效管理电网操作。&lt;br&gt;&lt;h4&gt;2. 挑战&lt;/h4&gt;   - 可再生能源（如风能和太阳能）的发电高度变化，与化石燃料源相比，给电力管理带来了操作上的困难，延迟了清洁能源的可用性。&lt;br&gt;&lt;h4&gt;3. 研究目标&lt;/h4&gt;   - 量化风能和太阳能的间歇性发电对智利热电厂的影响。&lt;br&gt;&lt;h4&gt;4. 混合风速预测方法&lt;/h4&gt;   - 提出了一种混合风速预测方法，结合了两种定制的机器学习模型，专门针对智利的需求。&lt;br&gt;&lt;h4&gt;5. 模型介绍&lt;/h4&gt;   - 第一个模型基于 TiDE，这是一种用于短期预测的多层感知器（MLP）模型。&lt;br&gt;   - 第二个模型基于图神经网络，称为 GraphCast，适用于中期预测，时间范围可达 10 天。&lt;br&gt;&lt;h4&gt;6. 性能提升&lt;/h4&gt;   - 该混合方法在短期预测中比最准确的操作性确定性系统提高了 4-21%，在中期预测中提高了 5-23%。&lt;br&gt;&lt;h4&gt;7. 环境影响&lt;/h4&gt;   - 此方法可以直接降低风能发电对热电厂的升降、削减发电和系统整体排放的影响，有助于智利的可持续发展。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As Chile's electric power sector advances toward a future powered by
renewable energy, accurate forecasting of renewable generation is essential for
managing grid operations. The integration of renewable energy sources is
particularly challenging due to the operational difficulties of managing their
power generation, which is highly variable compared to fossil fuel sources,
delaying the availability of clean energy. To mitigate this, we quantify the
impact of increasing intermittent generation from wind and solar on thermal
power plants in Chile and introduce a hybrid wind speed forecasting methodology
which combines two custom ML models for Chile. The first model is based on
TiDE, an MLP-based ML model for short-term forecasts, and the second is based
on a graph neural network, GraphCast, for medium-term forecasts up to 10 days.
Our hybrid approach outperforms the most accurate operational deterministic
systems by 4-21% for short-term forecasts and 5-23% for medium-term forecasts
and can directly lower the impact of wind generation on thermal ramping,
curtailment, and system-level emissions in Chile.</description>
      <guid isPermaLink="false">2409.09263v3</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>MakeWay: Object-Aware Costmaps for Proactive Indoor Navigation Using LiDAR</title>
      <link>http://arxiv.org/abs/2408.17034v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 11 figures&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究主题&lt;/h4&gt;   - 本文介绍了一种基于 LiDAR 的机器人导航系统，使用新型的面向对象的可用性驱动成本图。&lt;br&gt;&lt;h4&gt;2. 3D 物体检测&lt;/h4&gt;   - 系统利用 3D 物体检测网络在 LiDAR 关键帧中识别感兴趣的物体，并通过迭代最近点（ICP）算法精细化其 3D 位姿。&lt;br&gt;&lt;h4&gt;3. 跟踪与数据关联&lt;/h4&gt;   - 采用卡尔曼滤波器和匈牙利算法进行数据关联，跟踪识别到的物体。&lt;br&gt;&lt;h4&gt;4. 对象位姿更新&lt;/h4&gt;   - 系统在新关联检测到的物体上更新现有物体位姿，并为未匹配的检测创建新的物体地图。&lt;br&gt;&lt;h4&gt;5. 可用性驱动的成本图&lt;/h4&gt;   - 基于维护的物体级映射系统，创建可用性驱动的物体成本图，以实现路径规划中的主动避碰。&lt;br&gt;&lt;h4&gt;6. 数据标注技术&lt;/h4&gt;   - 针对室内语义 LiDAR 数据的稀缺性，提出了一种自动化标注技术，利用 CAD 模型数据库进行准确的真实标注，包括边界框、位置、方向和点语义。&lt;br&gt;&lt;h4&gt;7. 评估与效果&lt;/h4&gt;   - 在模拟和真实机器人平台上进行的广泛评估显示，使用物体可用性成本图的主动避碰有效提升了机器人导航的安全性和效率。&lt;br&gt;&lt;h4&gt;8. 实时操作&lt;/h4&gt;   - 系统能够实时在机载设备上运行，并计划公开发布代码和数据供公众使用。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we introduce a LiDAR-based robot navigation system, based on
novel object-aware affordance-based costmaps. Utilizing a 3D object detection
network, our system identifies objects of interest in LiDAR keyframes, refines
their 3D poses with the Iterative Closest Point (ICP) algorithm, and tracks
them via Kalman filters and the Hungarian algorithm for data association. It
then updates existing object poses with new associated detections and creates
new object maps for unmatched detections. Using the maintained object-level
mapping system, our system creates affordance-driven object costmaps for
proactive collision avoidance in path planning. Additionally, we address the
scarcity of indoor semantic LiDAR data by introducing an automated labeling
technique. This method utilizes a CAD model database for accurate ground-truth
annotations, encompassing bounding boxes, positions, orientations, and
point-wise semantics of each object in LiDAR sequences. Our extensive
evaluations, conducted in both simulated and real-world robot platforms,
highlights the effectiveness of proactive object avoidance by using object
affordance costmaps, enhancing robotic navigation safety and efficiency. The
system can operate in real-time onboard and we intend to release our code and
data for public use.</description>
      <guid isPermaLink="false">2408.17034v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Multiplex Graph Contrastive Learning with Soft Negatives</title>
      <link>http://arxiv.org/abs/2409.08010v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究主题&lt;/h4&gt;   - 图对比学习（GCL）旨在从图结构数据中学习包含最大一致信息的节点或图表示。&lt;br&gt;&lt;h4&gt;2. 当前挑战&lt;/h4&gt;   - 尽管节点级对比模式占主导地位，但一些研究开始探索不同尺度间的一致性，这些方法往往会丢失一致信息并受到干扰特征的影响。&lt;br&gt;&lt;h4&gt;3. 新方法引入&lt;/h4&gt;   - 本文提出了 MUX-GCL，一种新颖的跨尺度对比学习范式，利用多重表示作为有效的补丁。&lt;br&gt;&lt;h4&gt;4. 噪声最小化&lt;/h4&gt;   - 这种学习模式能够最小化干扰噪声，同时使用位置亲和性进行相应的对比策略，进一步避免信息损失，通过纠正跨尺度的假负对。&lt;br&gt;&lt;h4&gt;5. 实验验证&lt;/h4&gt;   - 大量下游实验表明，MUX-GCL 在公共数据集上取得了多个最先进的结果。&lt;br&gt;&lt;h4&gt;6. 理论分析&lt;/h4&gt;   - 理论分析进一步证明了新的目标函数是原始输入特征与输出嵌入的互信息的更严格下界，这为该范式提供了合理性支持。&lt;br&gt;&lt;h4&gt;7. 代码可用性&lt;/h4&gt;   - 相关代码可在 GitHub 上获取：[MUX-GCL/Code](https://github.com/MUX-GCL/Code)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/mux-gcl/code&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Contrastive Learning (GCL) seeks to learn nodal or graph
representations that contain maximal consistent information from
graph-structured data. While node-level contrasting modes are dominating, some
efforts commence to explore consistency across different scales. Yet, they tend
to lose consistent information and be contaminated by disturbing features.
Here, we introduce MUX-GCL, a novel cross-scale contrastive learning paradigm
that utilizes multiplex representations as effective patches. While this
learning mode minimizes contaminating noises, a commensurate contrasting
strategy using positional affinities further avoids information loss by
correcting false negative pairs across scales. Extensive downstream experiments
demonstrate that MUX-GCL yields multiple state-of-the-art results on public
datasets. Our theoretical analysis further guarantees the new objective
function as a stricter lower bound of mutual information of raw input features
and output embeddings, which rationalizes this paradigm. Code is available at
https://github.com/MUX-GCL/Code.</description>
      <guid isPermaLink="false">2409.08010v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Transfer-based Adversarial Poisoning Attacks for Online (MIMO-)Deep Receviers</title>
      <link>http://arxiv.org/abs/2409.02430v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 14 figures&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 最近，使用深度神经网络（DNN）的无线接收器设计（称为深度接收器）受到广泛关注，以确保在复杂信道环境中的可靠通信。&lt;br&gt;&lt;h4&gt;2. 在线学习的应用&lt;/h4&gt;   - 为了快速适应动态信道，采用在线学习通过空中数据（例如导频）更新深度接收器的权重。&lt;br&gt;&lt;h4&gt;3. 安全性挑战&lt;/h4&gt;   - 然而，神经模型的脆弱性和无线信道的开放性使这些系统容易受到恶意攻击。因此，理解攻击方法对于设计稳健的接收器至关重要。&lt;br&gt;&lt;h4&gt;4. 攻击方法的提出&lt;/h4&gt;   - 本文提出了一种基于迁移的对抗性污染攻击方法，针对在线接收器进行攻击。&lt;br&gt;&lt;h4&gt;5. 攻击机制&lt;/h4&gt;   - 在不知道攻击目标的情况下，将对抗扰动注入到导频中，从而污染在线深度接收器，削弱其适应动态信道和非线性效应的能力。&lt;br&gt;&lt;h4&gt;6. 具体攻击目标&lt;/h4&gt;   - 攻击方法特别针对使用在线元学习的深度软干扰消除（DeepSIC）模型，该模型是经典的模型驱动深度接收器，结合了无线领域知识。&lt;br&gt;&lt;h4&gt;7. 适应能力&lt;/h4&gt;   - DeepSIC能够高效适应时变信道，仅需少量导频，在多输入多输出（MIMO）场景中实现最佳性能。&lt;br&gt;&lt;h4&gt;8. 应用动机&lt;/h4&gt;   - 该深度接收器在无线通信领域有多种应用，这激励了我们对针对其攻击方法的研究。&lt;br&gt;&lt;h4&gt;9. 攻击效果验证&lt;/h4&gt;   - 通过在合成线性、合成非线性、静态和 COST 2100 信道上进行模拟，展示了攻击的有效性。&lt;br&gt;&lt;h4&gt;10. 实验结果&lt;/h4&gt;    - 模拟结果表明，所提污染攻击在快速变化场景中显著降低了在线接收器的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, the design of wireless receivers using deep neural networks (DNNs),
known as deep receivers, has attracted extensive attention for ensuring
reliable communication in complex channel environments. To adapt quickly to
dynamic channels, online learning has been adopted to update the weights of
deep receivers with over-the-air data (e.g., pilots). However, the fragility of
neural models and the openness of wireless channels expose these systems to
malicious attacks. To this end, understanding these attack methods is essential
for robust receiver design. In this paper, we propose a transfer-based
adversarial poisoning attack method for online receivers.Without knowledge of
the attack target, adversarial perturbations are injected to the pilots,
poisoning the online deep receiver and impairing its ability to adapt to
dynamic channels and nonlinear effects. In particular, our attack method
targets Deep Soft Interference Cancellation (DeepSIC)[1] using online
meta-learning. As a classical model-driven deep receiver, DeepSIC incorporates
wireless domain knowledge into its architecture. This integration allows it to
adapt efficiently to time-varying channels with only a small number of pilots,
achieving optimal performance in a multi-input and multi-output (MIMO)
scenario.The deep receiver in this scenario has a number of applications in the
field of wireless communication, which motivates our study of the attack
methods targeting it.Specifically, we demonstrate the effectiveness of our
attack in simulations on synthetic linear, synthetic nonlinear, static, and
COST 2100 channels. Simulation results indicate that the proposed poisoning
attack significantly reduces the performance of online receivers in rapidly
changing scenarios.</description>
      <guid isPermaLink="false">2409.02430v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Real-Time Object Tracking via Meta-Learning: Efficient Model Adaptation and One-Shot Channel Pruning</title>
      <link>http://arxiv.org/abs/1911.11170v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 5 figures, AAAI 2020 accepted&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究目标&lt;/h4&gt;   - 提出一种新颖的元学习框架，用于实时目标跟踪，具有效率的模型适应和通道剪枝。&lt;br&gt;&lt;h4&gt;2. 框架功能&lt;/h4&gt;   - 在目标跟踪过程中，该框架能够在少量梯度下降迭代中微调模型参数，同时使用第一帧的目标真实值进行网络通道剪枝。&lt;br&gt;&lt;h4&gt;3. 学习问题的定义&lt;/h4&gt;   - 将该学习问题形式化为元学习任务，训练一个元跟踪器，通过精心设计的跟踪模拟更新其元参数，包括初始权重、学习率和剪枝掩码。&lt;br&gt;&lt;h4&gt;4. 性能提升&lt;/h4&gt;   - 集成的元跟踪器显著提高了跟踪性能，加速了在线学习的收敛，并减少了特征计算的成本。&lt;br&gt;&lt;h4&gt;5. 实验评估&lt;/h4&gt;   - 在标准数据集上的实验评估表明，该方法在准确性和速度方面优于现有的最先进方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2019-11-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a novel meta-learning framework for real-time object tracking with
efficient model adaptation and channel pruning. Given an object tracker, our
framework learns to fine-tune its model parameters in only a few iterations of
gradient-descent during tracking while pruning its network channels using the
target ground-truth at the first frame. Such a learning problem is formulated
as a meta-learning task, where a meta-tracker is trained by updating its
meta-parameters for initial weights, learning rates, and pruning masks through
carefully designed tracking simulations. The integrated meta-tracker greatly
improves tracking performance by accelerating the convergence of online
learning and reducing the cost of feature computation. Experimental evaluation
on the standard datasets demonstrates its outstanding accuracy and speed
compared to the state-of-the-art methods.</description>
      <guid isPermaLink="false">1911.11170v3</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Deep Neural Network-Based Sign Language Recognition: A Comprehensive Approach Using Transfer Learning with Explainability</title>
      <link>http://arxiv.org/abs/2409.07426v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 为促进包容性并确保依赖手语的人的有效沟通，手语识别（SLR）至关重要。&lt;br&gt;&lt;h4&gt;2. 手语识别的意义&lt;/h4&gt;   - 手语识别与多种技术无缝结合，提高了聋人社区的可访问性，便利他们使用数字平台、视频通话和通信设备。&lt;br&gt;&lt;h4&gt;3. 研究目标&lt;/h4&gt;   - 提出一种新颖的解决方案，利用深度神经网络（DNN）完全自动化手语识别。&lt;br&gt;&lt;h4&gt;4. 方法论&lt;/h4&gt;   - 本方法整合复杂的预处理技术，以优化整体性能。&lt;br&gt;   - 使用 ResNet、Inception、Xception 和 VGG 架构选择性分类手语图像。&lt;br&gt;&lt;h4&gt;5. 架构构建&lt;/h4&gt;   - 准备了 DNN 架构，并与预处理架构合并。&lt;br&gt;&lt;h4&gt;6. 后处理阶段&lt;/h4&gt;   - 在后处理阶段，使用基于合作博弈理论的 SHAP 深度解释器量化特定特征对机器学习模型输出的影响。&lt;br&gt;&lt;h4&gt;7. 数据集&lt;/h4&gt;   - 使用不丹手语（BSL）数据集进行模型的训练和测试。&lt;br&gt;&lt;h4&gt;8. 实验结果&lt;/h4&gt;   - 在不丹手语数据集上训练时，整体 ResNet50 和 DNN 模型表现出最佳准确率，达到了 98.90%。&lt;br&gt;&lt;h4&gt;9. 模型评估&lt;/h4&gt;   - 使用 SHAP 方法评估模型提供信息清晰度的能力。&lt;br&gt;&lt;h4&gt;10. 应用潜力&lt;/h4&gt;    - 由于其显著的鲁棒性和可靠性，所提方法可以用于开发完全自动化的手语识别系统。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; To promote inclusion and ensuring effective communication for those who rely
on sign language as their main form of communication, sign language recognition
(SLR) is crucial. Sign language recognition (SLR) seamlessly incorporates with
diverse technology, enhancing accessibility for the deaf community by
facilitating their use of digital platforms, video calls, and communication
devices. To effectively solve this problem, we suggest a novel solution that
uses a deep neural network to fully automate sign language recognition. This
methodology integrates sophisticated preprocessing methodologies to optimise
the overall performance. The architectures resnet, inception, xception, and vgg
are utilised to selectively categorise images of sign language. We prepared a
DNN architecture and merged it with the pre-processing architectures. In the
post-processing phase, we utilised the SHAP deep explainer, which is based on
cooperative game theory, to quantify the influence of specific features on the
output of a machine learning model. Bhutanese-Sign-Language (BSL) dataset was
used for training and testing the suggested technique. While training on
Bhutanese-Sign-Language (BSL) dataset, overall ResNet50 with the DNN model
performed better accuracy which is 98.90%. Our model's ability to provide
informational clarity was assessed using the SHAP (SHapley Additive
exPlanations) method. In part to its considerable robustness and reliability,
the proposed methodological approach can be used to develop a fully automated
system for sign language recognition.</description>
      <guid isPermaLink="false">2409.07426v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Has Multimodal Learning Delivered Universal Intelligence in Healthcare? A Comprehensive Survey</title>
      <link>http://arxiv.org/abs/2408.12880v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages, 6 figures&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 人工智能的快速发展不断重塑智能医疗和保健领域。&lt;br&gt;&lt;h4&gt;2. 多模态学习的重要性&lt;/h4&gt;   - 作为一种重要技术，多模态学习因其数据互补性、全面建模形式和巨大的应用潜力而受到越来越多的关注。&lt;br&gt;&lt;h4&gt;3. 研究现状&lt;/h4&gt;   - 许多研究者专注于这一领域，进行广泛研究并构建丰富的智能系统。&lt;br&gt;&lt;h4&gt;4. 核心问题&lt;/h4&gt;   - 自然产生的一个开放问题是：多模态学习是否在医疗领域实现了普遍智能？&lt;br&gt;&lt;h4&gt;5. 分析视角&lt;/h4&gt;   - 为回答这一问题，采用三种独特视角进行整体分析。&lt;br&gt;&lt;h4&gt;6. 现状调查&lt;/h4&gt;   - 从数据集、任务导向方法和通用基础模型的角度，对医疗多模态学习的当前进展进行全面调查。&lt;br&gt;&lt;h4&gt;7. 深入讨论&lt;/h4&gt;   - 基于上述调查，从五个问题探讨先进技术在医疗中的真实影响，涵盖数据、技术、性能和伦理等方面。&lt;br&gt;&lt;h4&gt;8. 研究结论&lt;/h4&gt;   - 当前技术尚未实现普遍智能，未来仍有显著的探索空间。&lt;br&gt;&lt;h4&gt;9. 未来方向&lt;/h4&gt;   - 最后，基于以上评估和讨论，指出十个可能的探索方向，以实现医疗领域的普遍智能目标。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/deepreasoning/aihealth&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid development of artificial intelligence has constantly reshaped the
field of intelligent healthcare and medicine. As a vital technology, multimodal
learning has increasingly garnered interest due to data complementarity,
comprehensive modeling form, and great application potential. Currently,
numerous researchers are dedicating their attention to this field, conducting
extensive studies and constructing abundant intelligent systems. Naturally, an
open question arises that has multimodal learning delivered universal
intelligence in healthcare? To answer the question, we adopt three unique
viewpoints for a holistic analysis. Firstly, we conduct a comprehensive survey
of the current progress of medical multimodal learning from the perspectives of
datasets, task-oriented methods, and universal foundation models. Based on
them, we further discuss the proposed question from five issues to explore the
real impacts of advanced techniques in healthcare, from data and technologies
to performance and ethics. The answer is that current technologies have NOT
achieved universal intelligence and there remains a significant journey to
undertake. Finally, in light of the above reviews and discussions, we point out
ten potential directions for exploration towards the goal of universal
intelligence in healthcare.</description>
      <guid isPermaLink="false">2408.12880v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Implicit Reasoning in Deep Time Series Forecasting</title>
      <link>http://arxiv.org/abs/2409.10840v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 最近，时间序列基础模型在来自多个领域的时间序列零-shot 预测中表现出色。&lt;br&gt;&lt;h4&gt;2. 研究问题&lt;/h4&gt;   - 目前尚不清楚这些模型的成功是源于对时间动态的真正理解，还是仅仅是对训练数据的记忆。&lt;br&gt;&lt;h4&gt;3. 研究缺口&lt;/h4&gt;   - 尽管对语言模型中的隐性推理进行了研究，但时间序列模型的类似评估仍然较少。&lt;br&gt;&lt;h4&gt;4. 研究目标&lt;/h4&gt;   - 本文旨在初步评估深度时间序列预测模型的推理能力。&lt;br&gt;&lt;h4&gt;5. 研究发现&lt;/h4&gt;   - 发现某些线性、基于多层感知机（MLP）和基于补丁的 Transformer 模型在系统性设计的离散外场景中能够有效泛化。&lt;br&gt;&lt;h4&gt;6. 推理能力的暗示&lt;/h4&gt;   - 这些结果表明，这些模型具备超出简单模式记忆的潜在推理能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, time series foundation models have shown promising zero-shot
forecasting performance on time series from a wide range of domains. However,
it remains unclear whether their success stems from a true understanding of
temporal dynamics or simply from memorizing the training data. While implicit
reasoning in language models has been studied, similar evaluations for time
series models have been largely unexplored. This work takes an initial step
toward assessing the reasoning abilities of deep time series forecasting
models. We find that certain linear, MLP-based, and patch-based Transformer
models generalize effectively in systematically orchestrated
out-of-distribution scenarios, suggesting underexplored reasoning capabilities
beyond simple pattern memorization.</description>
      <guid isPermaLink="false">2409.10840v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Correspondence Free Multivector Cloud Registration using Conformal Geometric Algebra</title>
      <link>http://arxiv.org/abs/2406.11732v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究创新&lt;/h4&gt;   - 本文首次提出了一种新颖的理论方法，用于在符合几何代数中解决无对应的多向量云注册问题。&lt;br&gt;&lt;h4&gt;2. 正交自同构&lt;/h4&gt;   - 该方法形成了一种正交自同构，超越了典型的向量空间，涵盖了整个符合几何代数，同时尊重多向量的分级。&lt;br&gt;&lt;h4&gt;3. 注册过程&lt;/h4&gt;   - 注册可以视为属于 \(SO(4,1)\) 的正交变换（即：缩放、平移、旋转），这是一组特殊的正交变换。&lt;br&gt;&lt;h4&gt;4. 无需直接访问输入多向量&lt;/h4&gt;   - 注册过程无需直接访问输入多向量，而是使用符合模型提供的几何对象（多向量）。&lt;br&gt;&lt;h4&gt;5. 几何对象的获取&lt;/h4&gt;   - 这些几何对象通过解决多线性特征值问题来获得，从而找到特征多向量集，避免了在注册过程中的对应问题。&lt;br&gt;&lt;h4&gt;6. 等变特性&lt;/h4&gt;   - 该方法提供了输入多向量和特征多向量之间的旋转和位移等变特性。&lt;br&gt;&lt;h4&gt;7. 实验评估&lt;/h4&gt;   - 在常用的点云注册数据集上进行了实验评估，验证了该方法的有效性，特别是在高噪声水平下的歧义问题。&lt;br&gt;&lt;h4&gt;8. 代码可用性&lt;/h4&gt;   - 相关代码可在 GitHub 上找到，链接为 [RegistrationGA](https://github.com/Numerical-Geometric-Algebra/RegistrationGA)。&lt;br&gt;&lt;h4&gt;9. 投稿信息&lt;/h4&gt;   - 本研究已提交至《计算机视觉国际期刊》，目前正在审稿中。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-06-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/numerical-geometric-algebra/registrationga&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present, for the first time, a novel theoretical approach to address the
problem of correspondence free multivector cloud registration in conformal
geometric algebra. Such formalism achieves several favorable properties.
Primarily, it forms an orthogonal automorphism that extends beyond the typical
vector space to the entire conformal geometric algebra while respecting the
multivector grading. Concretely, the registration can be viewed as an
orthogonal transformation (\it i.e., scale, translation, rotation) belonging to
$SO(4,1)$ - group of special orthogonal transformations in conformal geometric
algebra. We will show that such formalism is able to: $(i)$ perform the
registration without directly accessing the input multivectors. Instead, we use
primitives or geometric objects provided by the conformal model - the
multivectors, $(ii)$ the geometric objects are obtained by solving a
multilinear eigenvalue problem to find sets of eigenmultivectors. In this way,
we can explicitly avoid solving the correspondences in the registration
process. Most importantly, this offers rotation and translation equivariant
properties between the input multivectors and the eigenmultivectors.
Experimental evaluation is conducted in datasets commonly used in point cloud
registration, to testify the usefulness of the approach with emphasis to
ambiguities arising from high levels of noise. The code is available at
https://github.com/Numerical-Geometric-Algebra/RegistrationGA . This work was
submitted to the International Journal of Computer Vision and is currently
under review.</description>
      <guid isPermaLink="false">2406.11732v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Current Symmetry Group Equivariant Convolution Frameworks for Representation Learning</title>
      <link>http://arxiv.org/abs/2409.07327v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  31 pages, 4 figures&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 欧几里得深度学习在处理具有不规则、曲面和复杂拓扑的真实世界信号时往往不足。&lt;br&gt;&lt;h4&gt;2. 几何特性的重要性&lt;/h4&gt;   - 理解特征空间的几何特性对于获取鲁棒且紧凑的特征表示至关重要，这些表示应能抵御非平凡的几何变换，而传统卷积神经网络（CNN）无法有效应对。&lt;br&gt;&lt;h4&gt;3. 对称性的认识&lt;/h4&gt;   - 识别旋转、平移、置换或缩放对称性可以帮助学习到具有等变性质的表示。&lt;br&gt;&lt;h4&gt;4. 几何深度学习的进展&lt;/h4&gt;   - 与其不变性对应物相比，基于几何深度学习的框架在计算机视觉和机器学习任务中取得了显著进展。&lt;br&gt;&lt;h4&gt;5. 对称群等变深度学习模型&lt;/h4&gt;   - 强调对称群等变深度学习模型的重要性，这些模型通过利用群论和对称性实现了图、三维形状和非欧几里得空间上的类似卷积的操作。&lt;br&gt;&lt;h4&gt;6. 卷积分类&lt;/h4&gt;   - 将这些卷积方法分为常规卷积、可操控卷积和基于偏微分方程（PDE）的卷积，并深入分析其输入空间和后续表示的内在对称性。&lt;br&gt;&lt;h4&gt;7. 数学联系&lt;/h4&gt;   - 概述了群卷积或消息聚合操作与等变性概念之间的数学联系。&lt;br&gt;&lt;h4&gt;8. 数据集与应用&lt;/h4&gt;   - 突出展示了各种数据集及其应用范围、局限性，并提出对未来研究方向的深刻见解。&lt;br&gt;&lt;h4&gt;9. 研究价值&lt;/h4&gt;   - 本报告为这一新兴学科提供了宝贵的参考，旨在刺激进一步研究。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Euclidean deep learning is often inadequate for addressing real-world signals
where the representation space is irregular and curved with complex topologies.
Interpreting the geometric properties of such feature spaces has become
paramount in obtaining robust and compact feature representations that remain
unaffected by nontrivial geometric transformations, which vanilla CNNs cannot
effectively handle. Recognizing rotation, translation, permutation, or scale
symmetries can lead to equivariance properties in the learned representations.
This has led to notable advancements in computer vision and machine learning
tasks under the framework of geometric deep learning, as compared to their
invariant counterparts. In this report, we emphasize the importance of symmetry
group equivariant deep learning models and their realization of
convolution-like operations on graphs, 3D shapes, and non-Euclidean spaces by
leveraging group theory and symmetry. We categorize them as regular, steerable,
and PDE-based convolutions and thoroughly examine the inherent symmetries of
their input spaces and ensuing representations. We also outline the
mathematical link between group convolutions or message aggregation operations
and the concept of equivariance. The report also highlights various datasets,
their application scopes, limitations, and insightful observations on future
directions to serve as a valuable reference and stimulate further research in
this emerging discipline.</description>
      <guid isPermaLink="false">2409.07327v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Denoising Linear Models with Permuted Data</title>
      <link>http://arxiv.org/abs/1704.07461v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To appear in part at ISIT 2017, Aachen&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 多变量线性回归模型在带有随机数据和加性高斯噪声的情况下，出现在各种对应估计和匹配问题中。&lt;br&gt;&lt;h4&gt;2. 研究重点&lt;/h4&gt;   - 本文关注这一问题的去噪方面，提供了最小最大误差率的特征描述，精确到对数因子级别。&lt;br&gt;&lt;h4&gt;3. 估计器性能分析&lt;/h4&gt;   - 分析了两种计算效率高的估计器版本的性能，确立了它们在广泛输入参数范围内的一致性。&lt;br&gt;&lt;h4&gt;4. 无噪声问题的算法&lt;/h4&gt;   - 提供了一种针对无噪声问题的精确算法，并展示其在图像点云匹配任务中的表现。&lt;br&gt;&lt;h4&gt;5. 扩展分析&lt;/h4&gt;   - 研究分析还扩展到含有离群点的数据集，表明其广泛适用性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2017-04-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The multivariate linear regression model with shuffled data and additive
Gaussian noise arises in various correspondence estimation and matching
problems. Focusing on the denoising aspect of this problem, we provide a
characterization the minimax error rate that is sharp up to logarithmic
factors. We also analyze the performance of two versions of a computationally
efficient estimator, and establish their consistency for a large range of input
parameters. Finally, we provide an exact algorithm for the noiseless problem
and demonstrate its performance on an image point-cloud matching task. Our
analysis also extends to datasets with outliers.</description>
      <guid isPermaLink="false">1704.07461v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Compositional Semantic Mix for Domain Adaptation in Point Cloud Segmentation</title>
      <link>http://arxiv.org/abs/2308.14619v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  TPAMI. arXiv admin note: text overlap with arXiv:2207.09778&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 深度学习模型在三维点云语义分割中，当训练和测试数据来自不同传感器或环境时，表现出有限的泛化能力，主要由于领域转移（domain shift）。&lt;br&gt;&lt;h4&gt;2. 领域适应方法&lt;/h4&gt;   - 可以通过模拟传感器噪声、开发领域无关生成器或训练点云补全网络来减轻这种领域转移，但这些方法通常针对范围视图图或需要多模态输入。&lt;br&gt;&lt;h4&gt;3. 图像领域的对比&lt;/h4&gt;   - 在图像领域，领域适应可以通过样本混合来实现，强调输入数据操作，而不是使用不同的适应模块。&lt;br&gt;&lt;h4&gt;4. 提出的新方法&lt;/h4&gt;   - 本研究提出了一种用于点云领域适应的组合语义混合方法，这是一种基于语义和几何样本混合的无监督领域适应技术。&lt;br&gt;&lt;h4&gt;5. 网络架构&lt;/h4&gt;   - 介绍了一种双分支对称网络架构，能够同时处理来自源领域（例如合成数据）和目标领域（例如真实世界）的点云。&lt;br&gt;&lt;h4&gt;6. 分支操作&lt;/h4&gt;   - 每个分支在单一领域内运行，通过整合来自另一领域的选定数据片段，并利用源标签和目标（伪）标签衍生的语义信息。&lt;br&gt;&lt;h4&gt;7. 半监督学习&lt;/h4&gt;   - 方法还可以利用有限的人类点级注释（半监督）来进一步提升性能。&lt;br&gt;&lt;h4&gt;8. 实验评估&lt;/h4&gt;   - 在合成到真实和真实到真实的场景中使用LiDAR数据集评估该方法。&lt;br&gt;&lt;h4&gt;9. 性能比较&lt;/h4&gt;   - 结果表明，该方法在无监督和半监督设置中显著优于现有的最先进方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2023-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/saltoricristiano/cosmix-uda&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep-learning models for 3D point cloud semantic segmentation exhibit limited
generalization capabilities when trained and tested on data captured with
different sensors or in varying environments due to domain shift. Domain
adaptation methods can be employed to mitigate this domain shift, for instance,
by simulating sensor noise, developing domain-agnostic generators, or training
point cloud completion networks. Often, these methods are tailored for range
view maps or necessitate multi-modal input. In contrast, domain adaptation in
the image domain can be executed through sample mixing, which emphasizes input
data manipulation rather than employing distinct adaptation modules. In this
study, we introduce compositional semantic mixing for point cloud domain
adaptation, representing the first unsupervised domain adaptation technique for
point cloud segmentation based on semantic and geometric sample mixing. We
present a two-branch symmetric network architecture capable of concurrently
processing point clouds from a source domain (e.g. synthetic) and point clouds
from a target domain (e.g. real-world). Each branch operates within one domain
by integrating selected data fragments from the other domain and utilizing
semantic information derived from source labels and target (pseudo) labels.
Additionally, our method can leverage a limited number of human point-level
annotations (semi-supervised) to further enhance performance. We assess our
approach in both synthetic-to-real and real-to-real scenarios using LiDAR
datasets and demonstrate that it significantly outperforms state-of-the-art
methods in both unsupervised and semi-supervised settings.</description>
      <guid isPermaLink="false">2308.14619v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Region-Transformer: Self-Attention Region Based Class-Agnostic Point Cloud Segmentation</title>
      <link>http://arxiv.org/abs/2403.01407v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 5 figures, 3 tables&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 点云分割有助于理解特定结构和物体的环境，可分为类特定和类无关两种方式。&lt;br&gt;&lt;h4&gt;2. 研究目标&lt;/h4&gt;   - 提出一种新的基于区域的变换模型——Region-Transformer，用于进行类无关的点云分割。&lt;br&gt;&lt;h4&gt;3. 模型方法&lt;/h4&gt;   - 该模型采用区域生长方法和自注意力机制，迭代地通过添加或移除点来扩展或收缩区域。&lt;br&gt;&lt;h4&gt;4. 训练数据&lt;/h4&gt;   - 模型在仅包含实例标签的模拟点云上进行训练，避免使用语义标签。&lt;br&gt;&lt;h4&gt;5. 现有技术的局限性&lt;/h4&gt;   - 尽管注意力机制在许多点云分割方法中取得成功，但尚未在区域生长方法中结合使用以探索性能提升。&lt;br&gt;&lt;h4&gt;6. 创新点&lt;/h4&gt;   - 这是首次将自注意力机制应用于区域生长方法，能够利用邻近点的局部上下文信息。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 实验表明，Region-Transformer模型在室内数据集的聚类指标上优于以往的类无关和类特定方法。&lt;br&gt;&lt;h4&gt;8. 模型的泛化能力&lt;/h4&gt;   - 该模型在大规模场景中表现良好。&lt;br&gt;&lt;h4&gt;9. 主要优势&lt;/h4&gt;   - 通过自注意力捕获长距离依赖关系，训练时不需要语义标签，并且适用于可变数量的物体。&lt;br&gt;&lt;h4&gt;10. 应用前景&lt;/h4&gt;    - Region-Transformer模型在机器人技术、数字双胞胎和自主车辆等领域具有广泛的应用潜力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.5220/0012424500003660&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud segmentation, which helps us understand the environment of
specific structures and objects, can be performed in class-specific and
class-agnostic ways. We propose a novel region-based transformer model called
Region-Transformer for performing class-agnostic point cloud segmentation. The
model utilizes a region-growth approach and self-attention mechanism to
iteratively expand or contract a region by adding or removing points. It is
trained on simulated point clouds with instance labels only, avoiding semantic
labels. Attention-based networks have succeeded in many previous methods of
performing point cloud segmentation. However, a region-growth approach with
attention-based networks has yet to be used to explore its performance gain. To
our knowledge, we are the first to use a self-attention mechanism in a
region-growth approach. With the introduction of self-attention to
region-growth that can utilize local contextual information of neighborhood
points, our experiments demonstrate that the Region-Transformer model
outperforms previous class-agnostic and class-specific methods on indoor
datasets regarding clustering metrics. The model generalizes well to
large-scale scenes. Key advantages include capturing long-range dependencies
through self-attention, avoiding the need for semantic labels during training,
and applicability to a variable number of objects. The Region-Transformer model
represents a promising approach for flexible point cloud segmentation with
applications in robotics, digital twinning, and autonomous vehicles.</description>
      <guid isPermaLink="false">2403.01407v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Hold Me Tight: Stable Encoder-Decoder Design for Speech Enhancement</title>
      <link>http://arxiv.org/abs/2408.17358v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at INTERSPEECH 2024&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 一维卷积层常用于音频信号编码，与固定的时频表示相比，它们能够适应输入数据的局部特征。&lt;br&gt;&lt;h4&gt;2. 现有问题&lt;/h4&gt;   - 在原始音频上使用一维滤波器训练困难，且常常出现不稳定性。&lt;br&gt;&lt;h4&gt;3. 提出的解决方案&lt;/h4&gt;   - 本文通过混合解决方案，结合理论驱动和数据驱动的方法，来解决上述问题。&lt;br&gt;&lt;h4&gt;4. 音频预处理&lt;/h4&gt;   - 首先，通过听觉滤波器组对音频信号进行预处理，确保学习编码器的频率定位良好。&lt;br&gt;&lt;h4&gt;5. 无监督学习目标&lt;/h4&gt;   - 使用框架理论的结果定义一个无监督学习目标，促进能量守恒和完美重构。&lt;br&gt;&lt;h4&gt;6. 混合压缩谱范数&lt;/h4&gt;   - 将混合压缩谱范数作为学习目标，应用于编码器系数。&lt;br&gt;&lt;h4&gt;7. 模型应用&lt;/h4&gt;   - 在低复杂度的编码器-掩码-解码器模型中应用这些解决方案。&lt;br&gt;&lt;h4&gt;8. 性能提升&lt;/h4&gt;   - 这些方法显著改善了语音增强中的感知语音质量评估（PESQ）结果。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/felixperfler/stable-hybrid-auditory-filterbanks&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Convolutional layers with 1-D filters are often used as frontend to encode
audio signals. Unlike fixed time-frequency representations, they can adapt to
the local characteristics of input data. However, 1-D filters on raw audio are
hard to train and often suffer from instabilities. In this paper, we address
these problems with hybrid solutions, i.e., combining theory-driven and
data-driven approaches. First, we preprocess the audio signals via a auditory
filterbank, guaranteeing good frequency localization for the learned encoder.
Second, we use results from frame theory to define an unsupervised learning
objective that encourages energy conservation and perfect reconstruction.
Third, we adapt mixed compressed spectral norms as learning objectives to the
encoder coefficients. Using these solutions in a low-complexity
encoder-mask-decoder model significantly improves the perceptual evaluation of
speech quality (PESQ) in speech enhancement.</description>
      <guid isPermaLink="false">2408.17358v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Improved Physics-Informed Neural Network based AC Power Flow for Distribution Networks</title>
      <link>http://arxiv.org/abs/2409.09466v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 功率流分析在电力系统的控制和操作中扮演着重要角色。&lt;br&gt;&lt;h4&gt;2. 传统方法的局限性&lt;/h4&gt;   - 传统解法的高计算负担促使向数据驱动的方法转变，利用数字计量数据的可用性。&lt;br&gt;&lt;h4&gt;3. 数据驱动方法的挑战&lt;/h4&gt;   - 例如深度学习等数据驱动方法尚未获得操作员的信任，因为它们对基础物理模型无感知，并且在观测有限的情况下表现较差。&lt;br&gt;&lt;h4&gt;4. 提出的解决方案&lt;/h4&gt;   - 本文提出了一种新的物理信息模型，旨在解决上述挑战。&lt;br&gt;&lt;h4&gt;5. 新型损失函数&lt;/h4&gt;   - 开发了一种新颖的物理信息损失函数，用于训练旨在进行功率流模拟的深度神经网络。&lt;br&gt;&lt;h4&gt;6. 损失函数特点&lt;/h4&gt;   - 该损失函数不仅基于理论的交流电功率流方程，还考虑了实际的线路损耗，提升了损失准确性和学习潜力。&lt;br&gt;&lt;h4&gt;7. 模型应用&lt;/h4&gt;   - 使用提出的模型训练图神经网络（GNN），并在一个小型3总线测试案例上进行评估。&lt;br&gt;&lt;h4&gt;8. 比较实验&lt;/h4&gt;   - 将该模型与另一种不考虑物理损耗的物理信息GNN和无模型技术进行比较。&lt;br&gt;&lt;h4&gt;9. 验证结果&lt;/h4&gt;   - 验证结果显示，提出的模型在所有性能指标上均优于传统的物理信息网络。&lt;br&gt;&lt;h4&gt;10. 模型的预测能力&lt;/h4&gt;    - 更值得注意的是，该模型在训练样本集之外的场景测试中表现出强大的预测能力，这在无模型技术中是一个显著的缺陷。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Power flow analysis plays a critical role in the control and operation of
power systems. The high computational burden of traditional solution methods
led to a shift towards data-driven approaches, exploiting the availability of
digital metering data. However, data-driven approaches, such as deep learning,
have not yet won the trust of operators as they are agnostic to the underlying
physical model and have poor performances in regimes with limited
observability. To address these challenges, this paper proposes a new,
physics-informed model. More specifically, a novel physics-informed loss
function is developed that can be used to train (deep) neural networks aimed at
power flow simulation. The loss function is not only based on the theoretical
AC power flow equations that govern the problem but also incorporates real
physical line losses, resulting in higher loss accuracy and increased learning
potential. The proposed model is used to train a Graph Neural Network (GNN) and
is evaluated on a small 3-bus test case both against another physics-informed
GNN that does not incorporate physical losses and against a model-free
technique. The validation results show that the proposed model outperforms the
conventional physics-informed network on all used performance metrics. Even
more interesting is that the model shows strong prediction capabilities when
tested on scenarios outside the training sample set, something that is a
substantial deficiency of model-free techniques.</description>
      <guid isPermaLink="false">2409.09466v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>UNIT: Unsupervised Online Instance Segmentation through Time</title>
      <link>http://arxiv.org/abs/2409.07887v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 在线物体分割和跟踪在激光雷达点云中使自主代理能够理解其周围环境并做出安全决策。&lt;br&gt;&lt;h4&gt;2. 问题描述&lt;/h4&gt;   - 手动标注这些任务的成本非常高，因此需要寻找替代方法。&lt;br&gt;&lt;h4&gt;3. 研究目标&lt;/h4&gt;   - 本文针对无监督的类无关在线实例分割和跟踪问题进行研究。&lt;br&gt;&lt;h4&gt;4. 方法论&lt;/h4&gt;   - 利用实例分割骨干网络，提出一种新的训练方案，使得在线跟踪对象成为可能。&lt;br&gt;&lt;h4&gt;5. 伪标签的使用&lt;/h4&gt;   - 网络通过伪标签进行训练，消除对手动标注的需求。&lt;br&gt;&lt;h4&gt;6. 评估方法&lt;/h4&gt;   - 使用适用于时间实例分割的指标进行评估，这些指标需要时间一致的实例标签。&lt;br&gt;&lt;h4&gt;7. 标签构建&lt;/h4&gt;   - 在缺乏时间一致标签的情况下，利用数据集中可用的3D边界框和语义标签构建这些标签。&lt;br&gt;&lt;h4&gt;8. 实验比较&lt;/h4&gt;   - 将我们的方法与强基线进行比较，结果表明在两个不同的户外激光雷达数据集上表现优越。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Online object segmentation and tracking in Lidar point clouds enables
autonomous agents to understand their surroundings and make safe decisions.
Unfortunately, manual annotations for these tasks are prohibitively costly. We
tackle this problem with the task of class-agnostic unsupervised online
instance segmentation and tracking. To that end, we leverage an instance
segmentation backbone and propose a new training recipe that enables the online
tracking of objects. Our network is trained on pseudo-labels, eliminating the
need for manual annotations. We conduct an evaluation using metrics adapted for
temporal instance segmentation. Computing these metrics requires
temporally-consistent instance labels. When unavailable, we construct these
labels using the available 3D bounding boxes and semantic labels in the
dataset. We compare our method against strong baselines and demonstrate its
superiority across two different outdoor Lidar datasets.</description>
      <guid isPermaLink="false">2409.07887v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>CP-VoteNet: Contrastive Prototypical VoteNet for Few-Shot Point Cloud Object Detection</title>
      <link>http://arxiv.org/abs/2408.17036v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by PRCV 2024&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究主题&lt;/h4&gt;   - 少样本点云三维物体检测（FS3D）旨在从点云中识别和定位新类别的物体，利用从标注基础类别学习到的知识以及新类别的少量注释。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 目前的研究主要采用原型学习方法，但性能仍未令人满意。现有方法中，原型仅受到松散约束，缺乏对点云空间内语义和几何关联的细致理解。&lt;br&gt;&lt;h4&gt;3. 提出的解决方案&lt;/h4&gt;   - 本文提出利用语义和几何子空间内的固有对比关系，以学习更精细和可泛化的原型表示。&lt;br&gt;&lt;h4&gt;4. 对比语义挖掘&lt;/h4&gt;   - 首先引入对比语义挖掘，允许网络通过在训练批次内构建正负样本对，提取具有区分性的类别特征。&lt;br&gt;&lt;h4&gt;5. 几何层次的对比关系&lt;/h4&gt;   - 进一步提出在原始层面施加对比关系，因为表示局部模式的点特征可以聚类为几何组件。&lt;br&gt;&lt;h4&gt;6. 特征编码的可转移性&lt;/h4&gt;   - 通过精细的原始几何结构，显著增强从基础类别到新类别的特征编码的可转移性。&lt;br&gt;&lt;h4&gt;7. 新模型的提出&lt;/h4&gt;   - 以上设计和见解促成了我们提出的新模型——对比原型VoteNet（CP-VoteNet）。&lt;br&gt;&lt;h4&gt;8. 实验验证&lt;/h4&gt;   - 在两个FS3D基准（FS-ScanNet和FS-SUNRGBD）上进行的广泛实验表明，CP-VoteNet在不同FS3D设置下显著超越当前最先进的方法。&lt;br&gt;&lt;h4&gt;9. 消融研究&lt;/h4&gt;   - 进一步的消融研究验证了我们设计的合理性和有效性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Few-shot point cloud 3D object detection (FS3D) aims to identify and localise
objects of novel classes from point clouds, using knowledge learnt from
annotated base classes and novel classes with very few annotations. Thus far,
this challenging task has been approached using prototype learning, but the
performance remains far from satisfactory. We find that in existing methods,
the prototypes are only loosely constrained and lack of fine-grained awareness
of the semantic and geometrical correlation embedded within the point cloud
space. To mitigate these issues, we propose to leverage the inherent
contrastive relationship within the semantic and geometrical subspaces to learn
more refined and generalisable prototypical representations. To this end, we
first introduce contrastive semantics mining, which enables the network to
extract discriminative categorical features by constructing positive and
negative pairs within training batches. Meanwhile, since point features
representing local patterns can be clustered into geometric components, we
further propose to impose contrastive relationship at the primitive level.
Through refined primitive geometric structures, the transferability of feature
encoding from base to novel classes is significantly enhanced. The above
designs and insights lead to our novel Contrastive Prototypical VoteNet
(CP-VoteNet). Extensive experiments on two FS3D benchmarks FS-ScanNet and
FS-SUNRGBD demonstrate that CP-VoteNet surpasses current state-of-the-art
methods by considerable margins across different FS3D settings. Further
ablation studies conducted corroborate the rationale and effectiveness of our
designs.</description>
      <guid isPermaLink="false">2408.17036v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Music auto-tagging in the long tail: A few-shot approach</title>
      <link>http://arxiv.org/abs/2409.07730v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published in Audio Engineering Society NY Show 2024 as a Peer
  Reviewed (Category 1) paper; typos corrected&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 在数字音乐领域，使用标签高效组织和检索音乐对音乐目录拥有者至关重要。&lt;br&gt;&lt;h4&gt;2. 人工标记与自动标记&lt;/h4&gt;   - 人工标记由专家完成，劳动强度大但准确性较高；而通过监督学习进行的自动标记已接近令人满意的准确性，但受限于预定义的标签集。&lt;br&gt;&lt;h4&gt;3. 少样本学习的潜力&lt;/h4&gt;   - 少样本学习为超越这一小范围的预定义标签提供了可行解决方案，允许模型仅从少量人类提供的示例中学习标签含义，并随后自主应用这些标签。&lt;br&gt;&lt;h4&gt;4. 研究方法&lt;/h4&gt;   - 本文提出将少样本学习方法整合到多标签音乐自动标记中，使用来自预训练模型的特征作为输入，结合轻量级线性分类器（线性探针）。&lt;br&gt;&lt;h4&gt;5. 实验内容&lt;/h4&gt;   - 研究不同流行的预训练特征以及不同的少样本参数化，涉及不同数量的类别和每类样本数。&lt;br&gt;&lt;h4&gt;6. 实验结果&lt;/h4&gt;   - 实验表明，使用预训练特征的简单模型能够在训练数据显著减少（如每个标签仅20个样本）的情况下，实现接近最先进模型的性能。&lt;br&gt;&lt;h4&gt;7. 与领先模型的比较&lt;/h4&gt;   - 当在整个训练数据集上训练时，线性探针的表现与领先模型竞争力强。&lt;br&gt;&lt;h4&gt;8. 研究意义&lt;/h4&gt;   - 结果表明，这种基于迁移学习的少样本方法能够有效解决仅用有限标记数据自动分配长尾标签的问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the realm of digital music, using tags to efficiently organize and
retrieve music from extensive databases is crucial for music catalog owners.
Human tagging by experts is labor-intensive but mostly accurate, whereas
automatic tagging through supervised learning has approached satisfying
accuracy but is restricted to a predefined set of training tags. Few-shot
learning offers a viable solution to expand beyond this small set of predefined
tags by enabling models to learn from only a few human-provided examples to
understand tag meanings and subsequently apply these tags autonomously. We
propose to integrate few-shot learning methodology into multi-label music
auto-tagging by using features from pre-trained models as inputs to a
lightweight linear classifier, also known as a linear probe. We investigate
different popular pre-trained features, as well as different few-shot
parametrizations with varying numbers of classes and samples per class. Our
experiments demonstrate that a simple model with pre-trained features can
achieve performance close to state-of-the-art models while using significantly
less training data, such as 20 samples per tag. Additionally, our linear probe
performs competitively with leading models when trained on the entire training
dataset. The results show that this transfer learning-based few-shot approach
could effectively address the issue of automatically assigning long-tail tags
with only limited labeled data.</description>
      <guid isPermaLink="false">2409.07730v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>A Broader Study of Cross-Domain Few-Shot Learning</title>
      <link>http://arxiv.org/abs/1912.07200v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ECCV 2020. Website: https://www.learning-with-limited-labels.com/&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 最近的少样本学习进展主要依赖于用于元学习的标注数据，尤其是基础类别和新类别来自同一领域。&lt;br&gt;&lt;h4&gt;2. 问题提出&lt;/h4&gt;   - 在许多应用中，收集用于元学习的数据是不可行的，这导致了跨领域少样本学习问题，其中基础类别和新类别之间存在较大差异。&lt;br&gt;&lt;h4&gt;3. 现有研究的局限性&lt;/h4&gt;   - 现有对跨领域少样本学习的研究主要集中在自然图像上，这些图像之间仍然具有较高的视觉相似性。&lt;br&gt;&lt;h4&gt;4. 研究创新&lt;/h4&gt;   - 本文提出了“跨领域少样本学习的更广泛研究”（BSCD-FSL）基准，包含来自多种图像采集方法的图像数据。&lt;br&gt;&lt;h4&gt;5. 数据集组成&lt;/h4&gt;   - 基准数据集包括自然图像（如作物疾病图像），以及与自然图像差异逐渐增大的图像（如卫星图像、皮肤病学图像和放射学图像）。&lt;br&gt;&lt;h4&gt;6. 实验评估&lt;/h4&gt;   - 在提出的基准上进行了广泛实验，以评估最先进的元学习方法、迁移学习方法以及新方法的跨领域少样本学习。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 实验结果显示，最先进的元学习方法意外地被早期的元学习方法超越，所有元学习方法的表现都比简单的微调低12.8%的平均准确率。&lt;br&gt;&lt;h4&gt;8. 观察到的趋势&lt;/h4&gt;   - 之前在专门针对跨领域少样本学习的方法中观察到的性能提升在这个更具挑战性的基准中消失。&lt;br&gt;&lt;h4&gt;9. 数据集相似性影响&lt;/h4&gt;   - 所有方法的准确性与数据集与自然图像的相似性相关，验证了该基准在更好地代表实践中数据多样性方面的价值，并为未来的研究提供指导。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2019-12-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent progress on few-shot learning largely relies on annotated data for
meta-learning: base classes sampled from the same domain as the novel classes.
However, in many applications, collecting data for meta-learning is infeasible
or impossible. This leads to the cross-domain few-shot learning problem, where
there is a large shift between base and novel class domains. While
investigations of the cross-domain few-shot scenario exist, these works are
limited to natural images that still contain a high degree of visual
similarity. No work yet exists that examines few-shot learning across different
imaging methods seen in real world scenarios, such as aerial and medical
imaging. In this paper, we propose the Broader Study of Cross-Domain Few-Shot
Learning (BSCD-FSL) benchmark, consisting of image data from a diverse
assortment of image acquisition methods. This includes natural images, such as
crop disease images, but additionally those that present with an increasing
dissimilarity to natural images, such as satellite images, dermatology images,
and radiology images. Extensive experiments on the proposed benchmark are
performed to evaluate state-of-art meta-learning approaches, transfer learning
approaches, and newer methods for cross-domain few-shot learning. The results
demonstrate that state-of-art meta-learning methods are surprisingly
outperformed by earlier meta-learning approaches, and all meta-learning methods
underperform in relation to simple fine-tuning by 12.8% average accuracy.
Performance gains previously observed with methods specialized for cross-domain
few-shot learning vanish in this more challenging benchmark. Finally, accuracy
of all methods tend to correlate with dataset similarity to natural images,
verifying the value of the benchmark to better represent the diversity of data
seen in practice and guiding future research.</description>
      <guid isPermaLink="false">1912.07200v3</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Expediting and Elevating Large Language Model Reasoning via Hidden Chain-of-Thought Decoding</title>
      <link>http://arxiv.org/abs/2409.08561v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究主题&lt;/h4&gt;   - 大语言模型（LLMs）在需要推理和多步问题解决的任务中表现出色，特别是通过链式思维（CoT）提示。&lt;br&gt;&lt;h4&gt;2. 存在的问题&lt;/h4&gt;   - 生成完整的CoT过程会导致输出序列显著增加，从而增加计算成本和推理延迟。&lt;br&gt;&lt;h4&gt;3. 提出的解决方案&lt;/h4&gt;   - 本文提出了一种新方法，通过语义对齐来压缩CoT过程，实现更高效的解码，同时保留CoT推理的优点。&lt;br&gt;&lt;h4&gt;4. 辅助模型的引入&lt;/h4&gt;   - 引入一个辅助CoT模型，学习将完整思维过程压缩为与原始CoT输出语义对齐的紧凑特殊令牌表示。&lt;br&gt;&lt;h4&gt;5. 集成到HCoT模型&lt;/h4&gt;   - 该压缩表示随后集成到隐链思维（HCoT）模型的输入中。&lt;br&gt;&lt;h4&gt;6. 训练过程&lt;/h4&gt;   - 训练过程分为两个阶段：第一阶段优化CoT模型，以生成与真实CoT输出对齐的压缩令牌表示，使用对比损失；第二阶段在冻结CoT模型参数的情况下，微调HCoT模型，以生成基于前缀指令和CoT模型压缩表示的准确后续预测。&lt;br&gt;&lt;h4&gt;7. 实验验证&lt;/h4&gt;   - 在数学推理、代理调用和问答等三个具有挑战性的领域进行了广泛实验，结果表明，语义压缩方法的性能与完整CoT基线相当甚至更优，同时解码时间显著加快，至少提高1.5倍。&lt;br&gt;&lt;h4&gt;8. 对比学习的效果&lt;/h4&gt;   - 引入对比学习目标进一步提升了压缩表示的质量，改善了CoT提示和任务准确性。&lt;br&gt;&lt;h4&gt;9. 研究意义&lt;/h4&gt;   - 本研究为在广泛应用中更高效地利用LLMs的多步推理能力铺平了道路。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) have demonstrated remarkable capabilities in
tasks requiring reasoning and multi-step problem-solving through the use of
chain-of-thought (CoT) prompting. However, generating the full CoT process
results in significantly longer output sequences, leading to increased
computational costs and latency during inference. To address this challenge, we
propose a novel approach to compress the CoT process through semantic
alignment, enabling more efficient decoding while preserving the benefits of
CoT reasoning. Our method introduces an auxiliary CoT model that learns to
generate and compress the full thought process into a compact special token
representation semantically aligned with the original CoT output. This
compressed representation is then integrated into the input of the Hidden
Chain-of-Thought (HCoT) model. The training process follows a two-stage
procedure: First, the CoT model is optimized to generate the compressed token
representations aligned with the ground-truth CoT outputs using a contrastive
loss. Subsequently, with the CoT model parameters frozen, the HCoT model is
fine-tuned to generate accurate subsequent predictions conditioned on the
prefix instruction and the compressed CoT representations from the CoT model.
Extensive experiments across three challenging domains - mathematical
reasoning, agent invocation, and question answering - demonstrate that our
semantic compression approach achieves competitive or improved performance
compared to the full CoT baseline, while providing significant speedups of at
least 1.5x in decoding time. Moreover, incorporating contrastive learning
objectives further enhances the quality of the compressed representations,
leading to better CoT prompting and improved task accuracy. Our work paves the
way for more efficient exploitation of multi-step reasoning capabilities in
LLMs across a wide range of applications.</description>
      <guid isPermaLink="false">2409.08561v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Prompt Obfuscation for Large Language Models</title>
      <link>http://arxiv.org/abs/2409.11026v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究主题&lt;/h4&gt;   - 系统提示包含详细指令，可以将基础大语言模型（LLM）转变为工具和服务，且开销较小。&lt;br&gt;&lt;h4&gt;2. 重要性&lt;/h4&gt;   - 由于其对实用性的重大影响，系统提示被视为知识产权，类似于软件产品的代码。&lt;br&gt;&lt;h4&gt;3. 安全隐患&lt;/h4&gt;   - 系统提示容易被提取，尤其是通过提示注入（prompt injection）技术。&lt;br&gt;&lt;h4&gt;4. 缺乏有效对策&lt;/h4&gt;   - 当前没有有效的反制措施来防止系统提示被窃取，所有保护措施均可被精心设计的提示注入绕过。&lt;br&gt;&lt;h4&gt;5. 提出的新方法&lt;/h4&gt;   - 本文提出了一种替代传统系统提示的方法，称为提示混淆（prompt obfuscation），以防止系统提示的提取，同时保持系统的实用性，且开销较小。&lt;br&gt;&lt;h4&gt;6. 核心思路&lt;/h4&gt;   - 找到原始系统提示的表示形式，使其具有相同的功能，但混淆后的系统提示不包含任何可推断原始提示的信息。&lt;br&gt;&lt;h4&gt;7. 实现方法&lt;/h4&gt;   - 实施基于优化的方法，以找到在保持功能性的同时的混淆提示表示。&lt;br&gt;&lt;h4&gt;8. 评估效果&lt;/h4&gt;   - 通过八种不同的指标评估方法，比较使用原始与混淆系统提示的系统性能，结果表明混淆版本与原始版本的性能始终相当。&lt;br&gt;&lt;h4&gt;9. 攻击测试&lt;/h4&gt;   - 进行了三种不同的去混淆攻击，结果显示在获得混淆提示和LLM后，无法一致提取有意义的信息。&lt;br&gt;&lt;h4&gt;10. 结论&lt;/h4&gt;    - 总体而言，提示混淆被证明是一种有效的方法，可以保护知识产权，同时保持与原始系统提示相同的实用性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; System prompts that include detailed instructions to describe the task
performed by the underlying large language model (LLM) can easily transform
foundation models into tools and services with minimal overhead. Because of
their crucial impact on the utility, they are often considered intellectual
property, similar to the code of a software product. However, extracting system
prompts is easily possible by using prompt injection. As of today, there is no
effective countermeasure to prevent the stealing of system prompts and all
safeguarding efforts could be evaded with carefully crafted prompt injections
that bypass all protection mechanisms.In this work, we propose an alternative
to conventional system prompts. We introduce prompt obfuscation to prevent the
extraction of the system prompt while maintaining the utility of the system
itself with only little overhead. The core idea is to find a representation of
the original system prompt that leads to the same functionality, while the
obfuscated system prompt does not contain any information that allows
conclusions to be drawn about the original system prompt. We implement an
optimization-based method to find an obfuscated prompt representation while
maintaining the functionality. To evaluate our approach, we investigate eight
different metrics to compare the performance of a system using the original and
the obfuscated system prompts, and we show that the obfuscated version is
constantly on par with the original one. We further perform three different
deobfuscation attacks and show that with access to the obfuscated prompt and
the LLM itself, we are not able to consistently extract meaningful information.
Overall, we showed that prompt obfuscation can be an effective method to
protect intellectual property while maintaining the same utility as the
original system prompt.</description>
      <guid isPermaLink="false">2409.11026v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition</title>
      <link>http://arxiv.org/abs/2408.12895v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ACM Multimedia 2024&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究主题&lt;/h4&gt;   - 多模态情感识别（ERC）是一项典型的多模态学习任务，旨在同时利用多种数据模态。&lt;br&gt;&lt;h4&gt;2. 现有研究的挑战&lt;/h4&gt;   - 以往研究在有效的多模态ERC中面临处理模态不平衡和优化跨模态学习的挑战。&lt;br&gt;&lt;h4&gt;3. 提出的框架&lt;/h4&gt;   - 本文提出了一个新框架，名为Ada2I，包含两个不可分割的模块：自适应特征加权（AFW）和自适应模态加权（AMW），分别用于特征层面和模态层面的平衡。&lt;br&gt;&lt;h4&gt;4. 交互利用&lt;/h4&gt;   - 该框架利用了模态间和模态内的交互来实现平衡。&lt;br&gt;&lt;h4&gt;5. 优化策略&lt;/h4&gt;   - 引入了一种精细化的差异比率，作为训练优化策略的一部分，这是一种简单但有效的度量，用于评估模型在同时处理多个模态时的学习过程整体差异。&lt;br&gt;&lt;h4&gt;6. 实验结果&lt;/h4&gt;   - 实验结果验证了Ada2I的有效性，其在三个基准数据集上的表现超过了现有的基线方法，特别是在解决模态不平衡方面。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal Emotion Recognition in Conversations (ERC) is a typical multimodal
learning task in exploiting various data modalities concurrently. Prior studies
on effective multimodal ERC encounter challenges in addressing modality
imbalances and optimizing learning across modalities. Dealing with these
problems, we present a novel framework named Ada2I, which consists of two
inseparable modules namely Adaptive Feature Weighting (AFW) and Adaptive
Modality Weighting (AMW) for feature-level and modality-level balancing
respectively via leveraging both Inter- and Intra-modal interactions.
Additionally, we introduce a refined disparity ratio as part of our training
optimization strategy, a simple yet effective measure to assess the overall
discrepancy of the model's learning process when handling multiple modalities
simultaneously. Experimental results validate the effectiveness of Ada2I with
state-of-the-art performance compared to baselines on three benchmark datasets,
particularly in addressing modality imbalances.</description>
      <guid isPermaLink="false">2408.12895v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Boosting Generalizability towards Zero-Shot Cross-Dataset Single-Image Indoor Depth by Meta-Initialization</title>
      <link>http://arxiv.org/abs/2409.02486v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  IROS 2024. The version supersedes 2305.07269. arXiv admin note: text
  overlap with arXiv:2305.07269&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究主题&lt;/h4&gt;   - 室内机器人依赖深度信息进行导航和障碍物检测，单图像深度估计广泛用于辅助感知。&lt;br&gt;&lt;h4&gt;2. 现有问题&lt;/h4&gt;   - 目前大多数室内单图像深度预测方法较少关注模型对未见数据集的泛化能力，忽视了系统部署的真实场景鲁棒性。&lt;br&gt;&lt;h4&gt;3. 研究方法&lt;/h4&gt;   - 本文采用基于梯度的元学习，以提高在零样本跨数据集推理中的泛化能力。&lt;br&gt;&lt;h4&gt;4. 任务边界的挑战&lt;/h4&gt;   - 与已研究的图像分类元学习不同，深度值是连续的，缺乏明确的任务边界，且室内环境中对象排列和场景组成变化很大。&lt;br&gt;&lt;h4&gt;5. 细粒度任务定义&lt;/h4&gt;   - 提出了细粒度任务，将每个RGB-D小批量视为元学习框架中的一个任务。&lt;br&gt;&lt;h4&gt;6. 实验结果&lt;/h4&gt;   - 在有限数据上，所提方法显著提高了先验性能（最大提高27.8% RMSE）。&lt;br&gt;&lt;h4&gt;7. 微调效果&lt;/h4&gt;   - 在元学习初始化的基础上进行微调，性能始终优于不使用元学习方法的基线。&lt;br&gt;&lt;h4&gt;8. 验证协议&lt;/h4&gt;   - 为了验证泛化能力，提出了零样本跨数据集协议，证明了元初始化带来的更高泛化能力，作为现有深度估计方法的简单而有效的插件。&lt;br&gt;&lt;h4&gt;9. 研究意义&lt;/h4&gt;   - 本研究在深度估计与元学习的交叉领域，可能推动两者向实际机器人和机器感知应用的更近一步发展。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Indoor robots rely on depth to perform tasks like navigation or obstacle
detection, and single-image depth estimation is widely used to assist
perception. Most indoor single-image depth prediction focuses less on model
generalizability to unseen datasets, concerned with in-the-wild robustness for
system deployment. This work leverages gradient-based meta-learning to gain
higher generalizability on zero-shot cross-dataset inference. Unlike the
most-studied meta-learning of image classification associated with explicit
class labels, no explicit task boundaries exist for continuous depth values
tied to highly varying indoor environments regarding object arrangement and
scene composition. We propose fine-grained task that treats each RGB-D
mini-batch as a task in our meta-learning formulation. We first show that our
method on limited data induces a much better prior (max 27.8% in RMSE). Then,
finetuning on meta-learned initialization consistently outperforms baselines
without the meta approach. Aiming at generalization, we propose zero-shot
cross-dataset protocols and validate higher generalizability induced by our
meta-initialization, as a simple and useful plugin to many existing depth
estimation methods. The work at the intersection of depth and meta-learning
potentially drives both research to step closer to practical robotic and
machine perception usage.</description>
      <guid isPermaLink="false">2409.02486v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Multi-scale spatiotemporal representation learning for EEG-based emotion recognition</title>
      <link>http://arxiv.org/abs/2409.07589v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究主题&lt;/h4&gt;   - 基于EEG的情感识别在脑机接口领域具有重要潜力。&lt;br&gt;&lt;h4&gt;2. 主要挑战&lt;/h4&gt;   - 从脑电图（EEG）信号中提取具有区分性的时空特征是一个关键挑战。&lt;br&gt;&lt;h4&gt;3. 现有研究的不足&lt;/h4&gt;   - 当前研究常依赖于特定领域的时频特征，分别分析时间依赖性和空间特征，忽视了局部与全局关系及时空动态之间的互动。&lt;br&gt;&lt;h4&gt;4. 提出的新网络&lt;/h4&gt;   - 本文提出了一种新颖的网络，称为多尺度倒Mamba（MS-iMamba），由多尺度时间块（MSTB）和时间-空间融合块（TSFB）组成。&lt;br&gt;&lt;h4&gt;5. 多尺度时间块（MSTB）&lt;/h4&gt;   - MSTB旨在捕捉不同尺度子序列中的局部细节和全局时间依赖性。&lt;br&gt;&lt;h4&gt;6. 时间-空间融合块（TSFB）&lt;/h4&gt;   - TSFB采用倒Mamba结构，关注动态时间依赖性与空间特征之间的互动。&lt;br&gt;&lt;h4&gt;7. 主要优势&lt;/h4&gt;   - MS-iMamba能够利用重构的多尺度EEG序列，充分利用时间特征与空间特征之间的相互作用，而无需特定领域的时频特征提取。&lt;br&gt;&lt;h4&gt;8. 实验结果&lt;/h4&gt;   - 在DEAP、DREAMER和SEED数据集上的实验结果表明，MS-iMamba分别达到94.86%、94.94%和91.36%的分类准确率，仅使用四通道EEG信号，优于现有的最先进方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; EEG-based emotion recognition holds significant potential in the field of
brain-computer interfaces. A key challenge lies in extracting discriminative
spatiotemporal features from electroencephalogram (EEG) signals. Existing
studies often rely on domain-specific time-frequency features and analyze
temporal dependencies and spatial characteristics separately, neglecting the
interaction between local-global relationships and spatiotemporal dynamics. To
address this, we propose a novel network called Multi-Scale Inverted Mamba
(MS-iMamba), which consists of Multi-Scale Temporal Blocks (MSTB) and
Temporal-Spatial Fusion Blocks (TSFB). Specifically, MSTBs are designed to
capture both local details and global temporal dependencies across different
scale subsequences. The TSFBs, implemented with an inverted Mamba structure,
focus on the interaction between dynamic temporal dependencies and spatial
characteristics. The primary advantage of MS-iMamba lies in its ability to
leverage reconstructed multi-scale EEG sequences, exploiting the interaction
between temporal and spatial features without the need for domain-specific
time-frequency feature extraction. Experimental results on the DEAP, DREAMER,
and SEED datasets demonstrate that MS-iMamba achieves classification accuracies
of 94.86%, 94.94%, and 91.36%, respectively, using only four-channel EEG
signals, outperforming state-of-the-art methods.</description>
      <guid isPermaLink="false">2409.07589v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>LiDAR and Inertial Fusion for Pose Estimation by Non-linear Optimization</title>
      <link>http://arxiv.org/abs/1710.07104v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 13 figures, submitted to ICRA 2018&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究主题&lt;/h4&gt;   - 基于3D点云的姿态估计可能会受到降级影响，例如在扫描块或重复环境中。&lt;br&gt;&lt;h4&gt;2. 提出的解决方案&lt;/h4&gt;   - 本文提出了一种融合3D旋转LiDAR和IMU的方案，以估计传感器主体的自我运动（ego-motion）。&lt;br&gt;&lt;h4&gt;3. 核心思想&lt;/h4&gt;   - 通过非线性优化方法来优化两种传感器的姿态和状态。&lt;br&gt;&lt;h4&gt;4. IMU测量的应用&lt;/h4&gt;   - 将一系列IMU测量作为相对约束，通过预积分来最小化状态误差，并借助激光姿态估计和非线性优化算法。&lt;br&gt;&lt;h4&gt;5. 优化的IMU输出&lt;/h4&gt;   - 优化后的IMU姿态输出可为后续的点云匹配提供更好的初始值。&lt;br&gt;&lt;h4&gt;6. 评估方法&lt;/h4&gt;   - 方法在模拟和实际测试中进行了评估，并与最先进的方法进行了比较。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 结果表明，所提方法在降级情况下仍能提供更好的姿态估计性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2017-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pose estimation purely based on 3D point-cloud could suffer from degradation,
e.g. scan blocks or scans in repetitive environments. To deal with this
problem, we propose an approach for fusing 3D spinning LiDAR and IMU to
estimate the ego-motion of the sensor body. The main idea of our work is to
optimize the poses and states of two kinds of sensors with non-linear
optimization methods. On the one hand, a bunch of IMU measurements are
considered as a relative constraint using pre-integration and the state errors
can be minimized with the help of laser pose estimation and non-linear
optimization algorithms; on the other hand, the optimized IMU pose outputs can
provide a better initial for the subsequent point-cloud matching. The method is
evaluated under both simulation and real tests with comparison to the
state-of-the-art. The results show that the proposed method can provide better
pose estimation performance even in the degradation cases.</description>
      <guid isPermaLink="false">1710.07104v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>DR-Pose: A Two-stage Deformation-and-Registration Pipeline for Category-level 6D Object Pose Estimation</title>
      <link>http://arxiv.org/abs/2309.01925v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Camera-ready version accepted to IROS 2023&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究主题&lt;/h4&gt;   - 类别级对象姿态估计涉及从预定义类别中估计对象的6D姿态和3D度量大小。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 最近的方法利用类别形状先验信息来提高姿态估计的准确性，但单阶段网络设计和训练方式导致性能亚优化，因为管道中有两个不同的任务。&lt;br&gt;&lt;h4&gt;3. 提出的解决方案&lt;/h4&gt;   - 本文讨论了两阶段管道相较于单阶段设计的优势，提出了一种称为DR-Pose的两阶段变形与配准管道。&lt;br&gt;&lt;h4&gt;4. 管道结构&lt;/h4&gt;   - DR-Pose包括两个阶段：补全辅助变形阶段和缩放配准阶段。&lt;br&gt;&lt;h4&gt;5. 第一阶段&lt;/h4&gt;   - 第一阶段使用点云补全方法生成目标对象的未见部分，指导后续对形状先验的变形。&lt;br&gt;&lt;h4&gt;6. 第二阶段&lt;/h4&gt;   - 第二阶段设计了一种新颖的配准网络，用于提取对姿态敏感的特征，并基于第一阶段的变形结果预测对象部分点云在典范空间中的表示。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - DR-Pose在CAMERA25和REAL275基准测试上表现优于最先进的基于形状先验的方法。&lt;br&gt;&lt;h4&gt;8. 代码获取&lt;/h4&gt;   - 相关代码可在GitHub上获取，链接为 [DR-Pose GitHub](https://github.com/Zray26/DR-Pose.git)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2023-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/zray26/dr-pose&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Category-level object pose estimation involves estimating the 6D pose and the
3D metric size of objects from predetermined categories. While recent
approaches take categorical shape prior information as reference to improve
pose estimation accuracy, the single-stage network design and training manner
lead to sub-optimal performance since there are two distinct tasks in the
pipeline. In this paper, the advantage of two-stage pipeline over single-stage
design is discussed. To this end, we propose a two-stage deformation-and
registration pipeline called DR-Pose, which consists of completion-aided
deformation stage and scaled registration stage. The first stage uses a point
cloud completion method to generate unseen parts of target object, guiding
subsequent deformation on the shape prior. In the second stage, a novel
registration network is designed to extract pose-sensitive features and predict
the representation of object partial point cloud in canonical space based on
the deformation results from the first stage. DR-Pose produces superior results
to the state-of-the-art shape prior-based methods on both CAMERA25 and REAL275
benchmarks. Codes are available at https://github.com/Zray26/DR-Pose.git.</description>
      <guid isPermaLink="false">2309.01925v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Refining Segmentation On-the-Fly: An Interactive Framework for Point Cloud Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2403.06401v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 点云配准的重要性&lt;/h4&gt;   - 点云配准是计算机视觉和机器人领域的一个基础任务。&lt;br&gt;&lt;h4&gt;2. 变换器方法的发展&lt;/h4&gt;   - 最近的变换器（transformer）方法在点云配准中表现出色，但存在一些限制。&lt;br&gt;&lt;h4&gt;3. 标准注意力机制的问题&lt;/h4&gt;   - 现有方法中的标准注意力机制整合了许多低相关的点，难以优先关注稀疏但重要的点。&lt;br&gt;&lt;h4&gt;4. 效率与复杂性问题&lt;/h4&gt;   - 这种低效导致局部结构建模能力有限，并且计算复杂度为平方级别。&lt;br&gt;&lt;h4&gt;5. 提出PTT方法&lt;/h4&gt;   - 为了解决这些问题，提出了点树变换器（Point Tree Transformer, PTT），一种新颖的点云配准变换器方法，能够高效提取全面的局部和全局特征，同时保持线性计算复杂度。&lt;br&gt;&lt;h4&gt;6. 层次特征树构建&lt;/h4&gt;   - PTT通过粗到细的方式从点云构建层次特征树，提升特征提取效率。&lt;br&gt;&lt;h4&gt;7. 新型注意力机制&lt;/h4&gt;   - 引入了新颖的点树注意力（Point Tree Attention, PTA）机制，遵循树结构促进关注区域向显著点的逐步收敛。&lt;br&gt;&lt;h4&gt;8. 层次选择关键点&lt;/h4&gt;   - 每层树选择具有最高注意力分数的关键点子集，后续层集中关注来自所选点集子点的相关区域。&lt;br&gt;&lt;h4&gt;9. 语义信息的整合&lt;/h4&gt;   - 特征提取过程中还结合了捕捉高层语义信息的粗点特征，促进局部结构建模和多尺度信息的逐步整合。&lt;br&gt;&lt;h4&gt;10. 实验结果&lt;/h4&gt;    - Extensive experiments conducted on the 3DMatch, ModelNet40, and KITTI datasets demonstrate that our method achieves superior performance over the state-of-the-art methods.&lt;br&gt;    &lt;br&gt;&lt;h4&gt;11. 整体贡献&lt;/h4&gt;    - PTA使模型能够集中关注关键局部结构并提取详细的局部信息，同时保持线性计算复杂度，显著提升了点云配准的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing interactive point cloud segmentation approaches primarily focus on
the object segmentation, which aim to determine which points belong to the
object of interest guided by user interactions. This paper concentrates on an
unexplored yet meaningful task, i.e., interactive point cloud semantic
segmentation, which assigns high-quality semantic labels to all points in a
scene with user corrective clicks. Concretely, we presents the first
interactive framework for point cloud semantic segmentation, named InterPCSeg,
which seamlessly integrates with off-the-shelf semantic segmentation networks
without offline re-training, enabling it to run in an on-the-fly manner. To
achieve online refinement, we treat user interactions as sparse training
examples during the test-time. To address the instability caused by the sparse
supervision, we design a stabilization energy to regulate the test-time
training process. For objective and reproducible evaluation, we develop an
interaction simulation scheme tailored for the interactive point cloud semantic
segmentation task. We evaluate our framework on the S3DIS and ScanNet datasets
with off-the-shelf segmentation networks, incorporating interactions from both
the proposed interaction simulator and real users. Quantitative and qualitative
experimental results demonstrate the efficacy of our framework in refining the
semantic segmentation results with user interactions. The source code will be
publicly available.</description>
      <guid isPermaLink="false">2403.06401v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Point Tree Transformer for Point Cloud Registration</title>
      <link>http://arxiv.org/abs/2406.17530v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 点云配准的重要性&lt;/h4&gt;   - 点云配准是计算机视觉和机器人领域的一个基础任务。&lt;br&gt;&lt;h4&gt;2. 变换器方法的发展&lt;/h4&gt;   - 最近的变换器（transformer）方法在点云配准中表现出色，但存在一些限制。&lt;br&gt;&lt;h4&gt;3. 标准注意力机制的问题&lt;/h4&gt;   - 现有方法中的标准注意力机制整合了许多低相关的点，难以优先关注稀疏但重要的点。&lt;br&gt;&lt;h4&gt;4. 效率与复杂性问题&lt;/h4&gt;   - 这种低效导致局部结构建模能力有限，并且计算复杂度为平方级别。&lt;br&gt;&lt;h4&gt;5. 提出PTT方法&lt;/h4&gt;   - 为了解决这些问题，提出了点树变换器（Point Tree Transformer, PTT），一种新颖的点云配准变换器方法，能够高效提取全面的局部和全局特征，同时保持线性计算复杂度。&lt;br&gt;&lt;h4&gt;6. 层次特征树构建&lt;/h4&gt;   - PTT通过粗到细的方式从点云构建层次特征树，提升特征提取效率。&lt;br&gt;&lt;h4&gt;7. 新型注意力机制&lt;/h4&gt;   - 引入了新颖的点树注意力（Point Tree Attention, PTA）机制，遵循树结构促进关注区域向显著点的逐步收敛。&lt;br&gt;&lt;h4&gt;8. 层次选择关键点&lt;/h4&gt;   - 每层树选择具有最高注意力分数的关键点子集，后续层集中关注来自所选点集子点的相关区域。&lt;br&gt;&lt;h4&gt;9. 语义信息的整合&lt;/h4&gt;   - 特征提取过程中还结合了捕捉高层语义信息的粗点特征，促进局部结构建模和多尺度信息的逐步整合。&lt;br&gt;&lt;h4&gt;10. 实验结果&lt;/h4&gt;    - Extensive experiments conducted on the 3DMatch, ModelNet40, and KITTI datasets demonstrate that our method achieves superior performance over the state-of-the-art methods.&lt;br&gt;    &lt;br&gt;&lt;h4&gt;11. 整体贡献&lt;/h4&gt;    - PTA使模型能够集中关注关键局部结构并提取详细的局部信息，同时保持线性计算复杂度，显著提升了点云配准的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-06-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud registration is a fundamental task in the fields of computer
vision and robotics. Recent developments in transformer-based methods have
demonstrated enhanced performance in this domain. However, the standard
attention mechanism utilized in these methods often integrates many
low-relevance points, thereby struggling to prioritize its attention weights on
sparse yet meaningful points. This inefficiency leads to limited local
structure modeling capabilities and quadratic computational complexity. To
overcome these limitations, we propose the Point Tree Transformer (PTT), a
novel transformer-based approach for point cloud registration that efficiently
extracts comprehensive local and global features while maintaining linear
computational complexity. The PTT constructs hierarchical feature trees from
point clouds in a coarse-to-dense manner, and introduces a novel Point Tree
Attention (PTA) mechanism, which follows the tree structure to facilitate the
progressive convergence of attended regions towards salient points.
Specifically, each tree layer selectively identifies a subset of key points
with the highest attention scores. Subsequent layers focus attention on areas
of significant relevance, derived from the child points of the selected point
set. The feature extraction process additionally incorporates coarse point
features that capture high-level semantic information, thus facilitating local
structure modeling and the progressive integration of multiscale information.
Consequently, PTA empowers the model to concentrate on crucial local structures
and derive detailed local information while maintaining linear computational
complexity. Extensive experiments conducted on the 3DMatch, ModelNet40, and
KITTI datasets demonstrate that our method achieves superior performance over
the state-of-the-art methods.</description>
      <guid isPermaLink="false">2406.17530v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Augmentation: An Unsupervised Learning Approach for Keyword Spotting in Speech Technology</title>
      <link>http://arxiv.org/abs/2409.00356v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted by the ICPR2024&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 本文关注关键词检测（KWS）中的一个持续挑战：获取大量标注数据进行训练。&lt;br&gt;&lt;h4&gt;2. 数据获取难题&lt;/h4&gt;   - 难以获得大量正样本，且在关键词变化时，收集新目标样本的过程繁琐。&lt;br&gt;&lt;h4&gt;3. 新方法介绍&lt;/h4&gt;   - 提出了一种结合无监督对比学习和独特增强技术的新方法，使神经网络能够在未标注数据集上训练。&lt;br&gt;&lt;h4&gt;4. 性能提升潜力&lt;/h4&gt;   - 该方法有可能在下游任务中改善性能，尤其是在标注数据集有限的情况下。&lt;br&gt;&lt;h4&gt;5. 特征表示理论&lt;/h4&gt;   - 提出相同关键词的语音表达，即使在速度或音量变化下，也应使用相似的高层特征表示。&lt;br&gt;&lt;h4&gt;6. 无监督学习方法&lt;/h4&gt;   - 介绍了一种基于语音增强的无监督学习方法，利用瓶颈层特征与音频重构信息之间的相似性进行辅助训练。&lt;br&gt;&lt;h4&gt;7. 压缩卷积架构&lt;/h4&gt;   - 提出了一种压缩卷积架构，解决KWS任务中的冗余和非信息性信息问题，使模型能够同时学习局部特征并关注长期信息。&lt;br&gt;&lt;h4&gt;8. 实验结果&lt;/h4&gt;   - 在Google Speech Commands V2数据集上取得了强劲的性能。&lt;br&gt;&lt;h4&gt;9. 方法启发&lt;/h4&gt;   - 受近期在手语检测和口语术语检测方面的进展启发，强调了对比学习方法在KWS中的潜力，以及基于示例的口语术语检测策略的优势。&lt;br&gt;&lt;h4&gt;10. 新视角贡献&lt;/h4&gt;    - 提出的CAB-KWS为KWS领域提供了新的视角，展示了减少数据收集工作和提高系统鲁棒性的有效方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper addresses the persistent challenge in Keyword Spotting (KWS), a
fundamental component in speech technology, regarding the acquisition of
substantial labeled data for training. Given the difficulty in obtaining large
quantities of positive samples and the laborious process of collecting new
target samples when the keyword changes, we introduce a novel approach
combining unsupervised contrastive learning and a unique augmentation-based
technique. Our method allows the neural network to train on unlabeled data
sets, potentially improving performance in downstream tasks with limited
labeled data sets. We also propose that similar high-level feature
representations should be employed for speech utterances with the same keyword
despite variations in speed or volume. To achieve this, we present a speech
augmentation-based unsupervised learning method that utilizes the similarity
between the bottleneck layer feature and the audio reconstructing information
for auxiliary training. Furthermore, we propose a compressed convolutional
architecture to address potential redundancy and non-informative information in
KWS tasks, enabling the model to simultaneously learn local features and focus
on long-term information. This method achieves strong performance on the Google
Speech Commands V2 Dataset. Inspired by recent advancements in sign spotting
and spoken term detection, our method underlines the potential of our
contrastive learning approach in KWS and the advantages of Query-by-Example
Spoken Term Detection strategies. The presented CAB-KWS provide new
perspectives in the field of KWS, demonstrating effective ways to reduce data
collection efforts and increase the system's robustness.</description>
      <guid isPermaLink="false">2409.00356v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Predicting building types and functions at transnational scale</title>
      <link>http://arxiv.org/abs/2409.09692v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 建筑特定知识的重要性&lt;/h4&gt;   - 建筑类型和功能信息对许多能源应用至关重要，但在欧洲许多地区缺乏包含这些信息的全面数据集。&lt;br&gt;&lt;h4&gt;2. 研究目的&lt;/h4&gt;   - 本研究首次探讨基于开放GIS数据集在欧洲范围内预测建筑类型和功能类的可行性。&lt;br&gt;&lt;h4&gt;3. 数据来源&lt;/h4&gt;   - 使用来自OpenStreetMap（OSM）的建筑数据训练图神经网络（GNN）分类器，涵盖欧盟、挪威、瑞士和英国。&lt;br&gt;&lt;h4&gt;4. 训练方法&lt;/h4&gt;   - 利用局部子图高效进行大规模图的训练。&lt;br&gt;&lt;h4&gt;5. 分类结果&lt;/h4&gt;   - 图变换模型在将建筑分类为9个类别时，获得高达0.754的Cohen's kappa系数；在将建筑分类为住宅和非住宅类时，获得0.844的高系数。&lt;br&gt;&lt;h4&gt;6. 核心贡献&lt;/h4&gt;   - **第一**，展示了使用多源数据集（包含2D建筑形状、土地使用、城市化程度和国家信息，以及OSM标签作为真值）进行跨国建筑分类的可能性。&lt;br&gt;   - **第二**，结果表明，考虑建筑邻里上下文信息的GNN模型在预测性能上优于仅考虑单个建筑的模型。&lt;br&gt;   - **第三**，展示了在局部子图上训练GNN比在标准GNN上训练能提高建筑分类的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Building-specific knowledge such as building type and function information is
important for numerous energy applications. However, comprehensive datasets
containing this information for individual households are missing in many
regions of Europe. For the first time, we investigate whether it is feasible to
predict building types and functional classes at a European scale based on only
open GIS datasets available across countries. We train a graph neural network
(GNN) classifier on a large-scale graph dataset consisting of OpenStreetMap
(OSM) buildings across the EU, Norway, Switzerland, and the UK. To efficiently
perform training using the large-scale graph, we utilize localized subgraphs. A
graph transformer model achieves a high Cohen's kappa coefficient of 0.754 when
classifying buildings into 9 classes, and a very high Cohen's kappa coefficient
of 0.844 when classifying buildings into the residential and non-residential
classes. The experimental results imply three core novel contributions to
literature. Firstly, we show that building classification across multiple
countries is possible using a multi-source dataset consisting of information
about 2D building shape, land use, degree of urbanization, and countries as
input, and OSM tags as ground truth. Secondly, our results indicate that GNN
models that consider contextual information about building neighborhoods
improve predictive performance compared to models that only consider individual
buildings and ignore the neighborhood. Thirdly, we show that training with GNNs
on localized subgraphs instead of standard GNNs improves performance for the
task of building classification.</description>
      <guid isPermaLink="false">2409.09692v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Bayesian Self-Training for Semi-Supervised 3D Segmentation</title>
      <link>http://arxiv.org/abs/2409.08102v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ECCV 2024&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 3D分割的重要性&lt;/h4&gt;   - 3D分割是计算机视觉中的一个核心问题，与许多其他密集预测任务类似，需要大量标注数据进行充分训练。&lt;br&gt;&lt;h4&gt;2. 标注数据的挑战&lt;/h4&gt;   - 对3D点云进行密集标注以进行完全监督训练过于耗费人力和成本。&lt;br&gt;&lt;h4&gt;3. 半监督训练的替代方案&lt;/h4&gt;   - 半监督训练提供了一个更实用的替代方案，仅需少量标注数据和大量未标注数据。&lt;br&gt;&lt;h4&gt;4. 研究重点&lt;/h4&gt;   - 本研究探索有效利用未标注数据，以减少因缺乏标注而产生的性能差距。&lt;br&gt;&lt;h4&gt;5. 贝叶斯自训练框架&lt;/h4&gt;   - 受贝叶斯深度学习启发，提出了一种用于半监督3D语义分割的贝叶斯自训练框架。&lt;br&gt;&lt;h4&gt;6. 伪标记生成与过滤&lt;/h4&gt;   - 通过随机推断生成初始伪标记，并根据估计的逐点不确定性进行过滤。&lt;br&gt;&lt;h4&gt;7. 扩展至实例分割&lt;/h4&gt;   - 通过构建启发式$n$-部分匹配算法，将方法扩展到半监督3D实例分割，最终扩展到密集3D视觉定位。&lt;br&gt;&lt;h4&gt;8. 实验结果&lt;/h4&gt;   - 在SemanticKITTI和ScribbleKITTI上实现了3D语义分割的最新成果，在ScanNet和S3DIS上取得了3D实例分割的优异表现。&lt;br&gt;&lt;h4&gt;9. 视觉定位的改进&lt;/h4&gt;   - 在ScanRefer上，密集3D视觉定位的表现相比仅监督的基准有显著提升。&lt;br&gt;&lt;h4&gt;10. 项目页面&lt;/h4&gt;    - 项目页面可在 [ouenal.github.io/bst/](https://ouenal.github.io/bst/) 获取。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D segmentation is a core problem in computer vision and, similarly to many
other dense prediction tasks, it requires large amounts of annotated data for
adequate training. However, densely labeling 3D point clouds to employ
fully-supervised training remains too labor intensive and expensive.
Semi-supervised training provides a more practical alternative, where only a
small set of labeled data is given, accompanied by a larger unlabeled set. This
area thus studies the effective use of unlabeled data to reduce the performance
gap that arises due to the lack of annotations. In this work, inspired by
Bayesian deep learning, we first propose a Bayesian self-training framework for
semi-supervised 3D semantic segmentation. Employing stochastic inference, we
generate an initial set of pseudo-labels and then filter these based on
estimated point-wise uncertainty. By constructing a heuristic $n$-partite
matching algorithm, we extend the method to semi-supervised 3D instance
segmentation, and finally, with the same building blocks, to dense 3D visual
grounding. We demonstrate state-of-the-art results for our semi-supervised
method on SemanticKITTI and ScribbleKITTI for 3D semantic segmentation and on
ScanNet and S3DIS for 3D instance segmentation. We further achieve substantial
improvements in dense 3D visual grounding over supervised-only baselines on
ScanRefer. Our project page is available at ouenal.github.io/bst/.</description>
      <guid isPermaLink="false">2409.08102v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Study of Dropout in PointPillars with 3D Object Detection</title>
      <link>http://arxiv.org/abs/2409.00673v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 3D物体检测的重要性&lt;/h4&gt;   - 3D物体检测对自动驾驶至关重要，利用深度学习技术解析LiDAR数据。&lt;br&gt;&lt;h4&gt;2. PointPillars架构&lt;/h4&gt;   - PointPillars是该领域一个突出的模型，以高效利用LiDAR数据而著称。&lt;br&gt;&lt;h4&gt;3. 研究重点&lt;/h4&gt;   - 本研究分析了在不同的dropout率下增强PointPillars模型性能，以应对过拟合并提高模型的泛化能力。&lt;br&gt;&lt;h4&gt;4. dropout技术&lt;/h4&gt;   - Dropout是一种正则化技术，通过在训练过程中随机省略神经元，迫使网络学习更健壮和多样化的特征。&lt;br&gt;&lt;h4&gt;5. 系统比较&lt;/h4&gt;   - 系统比较了不同增强技术对模型回归性能的影响，包括训练过程中的准确性，使用平均精度（AP）和平均方向相似性（AOS）进行测量。&lt;br&gt;&lt;h4&gt;6. 研究发现&lt;/h4&gt;   - 研究结果为优化增强技术提供了见解，促进了自动驾驶应用中3D物体检测的改进。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D object detection is critical for autonomous driving, leveraging deep
learning techniques to interpret LiDAR data. The PointPillars architecture is a
prominent model in this field, distinguished by its efficient use of LiDAR
data. This study provides an analysis of enhancing the performance of
PointPillars model under various dropout rates to address overfitting and
improve model generalization. Dropout, a regularization technique, involves
randomly omitting neurons during training, compelling the network to learn
robust and diverse features. We systematically compare the effects of different
enhancement techniques on the model's regression performance during training
and its accuracy, measured by Average Precision (AP) and Average Orientation
Similarity (AOS). Our findings offer insights into the optimal enhancements,
contributing to improved 3D object detection in autonomous driving
applications.</description>
      <guid isPermaLink="false">2409.00673v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>From Learning to Meta-Learning: Reduced Training Overhead and Complexity for Communication Systems</title>
      <link>http://arxiv.org/abs/2001.01227v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Invited to the 6G Wireless Summit 2020&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 机器学习方法概述&lt;/h4&gt;   - 机器学习方法通过使用固定的学习程序，调整模型参数，使其在给定的模型类别中。&lt;br&gt;&lt;h4&gt;2. 任务适应性&lt;/h4&gt;   - 参数适应是基于每个任务进行的，当系统配置发生变化时，需要重新训练模型。&lt;br&gt;&lt;h4&gt;3. 效率问题&lt;/h4&gt;   - 这种方法在数据和训练时间上的低效率可以通过利用领域知识来缓解，选择合适的模型类别和学习程序，这被统称为归纳偏置（inductive bias）。&lt;br&gt;&lt;h4&gt;4. 归纳偏置的挑战&lt;/h4&gt;   - 然而，将先验知识编码为归纳偏置通常较为困难，特别是在黑箱模型类别（如神经网络）中。&lt;br&gt;&lt;h4&gt;5. 元学习的优势&lt;/h4&gt;   - 元学习提供了一种自动化选择归纳偏置的方法，通过利用来自与未来未知任务相关的任务的数据或主动观察。&lt;br&gt;&lt;h4&gt;6. 元学习的潜力&lt;/h4&gt;   - 采用经过元学习训练的归纳偏置，机器学习模型的训练可以在减少训练数据和/或时间复杂度的情况下进行。&lt;br&gt;&lt;h4&gt;7. 应用领域&lt;/h4&gt;   - 本文高层次地介绍了元学习及其在通信系统中的应用。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2020-01-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/6GSUMMIT49458.2020.9083856&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/kclip/meta-autoencoder&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine learning methods adapt the parameters of a model, constrained to lie
in a given model class, by using a fixed learning procedure based on data or
active observations. Adaptation is done on a per-task basis, and retraining is
needed when the system configuration changes. The resulting inefficiency in
terms of data and training time requirements can be mitigated, if domain
knowledge is available, by selecting a suitable model class and learning
procedure, collectively known as inductive bias. However, it is generally
difficult to encode prior knowledge into an inductive bias, particularly with
black-box model classes such as neural networks. Meta-learning provides a way
to automatize the selection of an inductive bias. Meta-learning leverages data
or active observations from tasks that are expected to be related to future,
and a priori unknown, tasks of interest. With a meta-trained inductive bias,
training of a machine learning model can be potentially carried out with
reduced training data and/or time complexity. This paper provides a high-level
introduction to meta-learning with applications to communication systems.</description>
      <guid isPermaLink="false">2001.01227v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Down-Sampling Inter-Layer Adapter for Parameter and Computation Efficient Ultra-Fine-Grained Image Recognition</title>
      <link>http://arxiv.org/abs/2409.11051v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ECCV 2024 Workshop on Efficient Deep Learning for
  Foundation Models (EFM). Main: 13 pages, 3 figures, 2 tables. Appendix: 3
  pages, 1 table. Total: 16 pages, 3 figures, 4 tables&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究领域&lt;/h4&gt;   - 超细粒度图像识别（UFGIR）旨在对类间差异极小的对象进行分类，例如区分同一物种内的不同品种，这与细粒度图像识别（FGIR）中的物种级分类不同。&lt;br&gt;&lt;h4&gt;2. 任务难度&lt;/h4&gt;   - 该任务的难度因每个类别样本稀缺而加剧。&lt;br&gt;&lt;h4&gt;3. 新方法引入&lt;/h4&gt;   - 提出了一种新方法，采用下采样的层间适配器，在参数高效的设置中进行，其中主体参数被冻结，仅微调一小部分附加模块。&lt;br&gt;&lt;h4&gt;4. 集成双分支下采样&lt;/h4&gt;   - 通过集成双分支下采样，显著减少了所需的参数和浮点运算（FLOPs），使得方法高效。&lt;br&gt;&lt;h4&gt;5. 实验结果&lt;/h4&gt;   - 在十个数据集上的全面实验表明，所提方法在准确性与成本性能方面表现优异，突显其在资源受限环境中的实际应用潜力。&lt;br&gt;&lt;h4&gt;6. 性能提升&lt;/h4&gt;   - 与其他方法相比，在参数高效的设置中，平均准确率提高至少6.8%，同时训练参数比当前最先进的UFGIR方法减少至少123倍，FLOPs平均减少30%。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/arkel23/DownSamplingInterLayerAdapter&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ultra-fine-grained image recognition (UFGIR) categorizes objects with
extremely small differences between classes, such as distinguishing between
cultivars within the same species, as opposed to species-level classification
in fine-grained image recognition (FGIR). The difficulty of this task is
exacerbated due to the scarcity of samples per category. To tackle these
challenges we introduce a novel approach employing down-sampling inter-layer
adapters in a parameter-efficient setting, where the backbone parameters are
frozen and we only fine-tune a small set of additional modules. By integrating
dual-branch down-sampling, we significantly reduce the number of parameters and
floating-point operations (FLOPs) required, making our method highly efficient.
Comprehensive experiments on ten datasets demonstrate that our approach obtains
outstanding accuracy-cost performance, highlighting its potential for practical
applications in resource-constrained environments. In particular, our method
increases the average accuracy by at least 6.8\% compared to other methods in
the parameter-efficient setting while requiring at least 123x less trainable
parameters compared to current state-of-the-art UFGIR methods and reducing the
FLOPs by 30\% in average compared to other methods.</description>
      <guid isPermaLink="false">2409.11051v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Transfer Learning Applied to Computer Vision Problems: Survey on Current Progress, Limitations, and Opportunities</title>
      <link>http://arxiv.org/abs/2409.07736v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 8 figures&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 计算机视觉的挑战&lt;/h4&gt;   - 计算机视觉领域面临挑战，早期依赖手工特征和基于规则的算法，导致准确性有限。&lt;br&gt;&lt;h4&gt;2. 机器学习的引入&lt;/h4&gt;   - 机器学习的出现推动了进展，尤其是迁移学习（Transfer Learning, TL），有效解决了多种计算机视觉问题。&lt;br&gt;&lt;h4&gt;3. 迁移学习的优势&lt;/h4&gt;   - TL通过重用预训练模型，减少了对数据和计算资源的需求，同时能提供几乎相同的准确性，使其成为计算机视觉中的重要技术。&lt;br&gt;&lt;h4&gt;4. 研究重点&lt;/h4&gt;   - 本研究集中在迁移学习的发展及其在计算机视觉应用中解决实际问题的方式。&lt;br&gt;&lt;h4&gt;5. 讨论内容&lt;/h4&gt;   - 论文讨论了最近的发展、迁移学习的局限性以及未来的机会。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The field of Computer Vision (CV) has faced challenges. Initially, it relied
on handcrafted features and rule-based algorithms, resulting in limited
accuracy. The introduction of machine learning (ML) has brought progress,
particularly Transfer Learning (TL), which addresses various CV problems by
reusing pre-trained models. TL requires less data and computing while
delivering nearly equal accuracy, making it a prominent technique in the CV
landscape. Our research focuses on TL development and how CV applications use
it to solve real-world problems. We discuss recent developments, limitations,
and opportunities.</description>
      <guid isPermaLink="false">2409.07736v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Few-shot Multi-Task Learning of Linear Invariant Features with Meta Subspace Pursuit</title>
      <link>http://arxiv.org/abs/2409.02708v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 数据稀缺问题&lt;/h4&gt;   - 数据稀缺对现代机器学习和人工智能构成严重威胁，因为其实际成功通常依赖于大规模数据集的可用性。&lt;br&gt;&lt;h4&gt;2. 解决策略&lt;/h4&gt;   - 一种有效的解决方案是首先利用其他具有相似性的数据信息，在研究设计阶段进行准备，然后在分析阶段采用多任务或元学习框架。&lt;br&gt;&lt;h4&gt;3. 研究重点&lt;/h4&gt;   - 本文聚焦于多任务（或多源）线性模型，其各任务之间的系数共享一个不变的低秩成分，这是近期多任务或元学习文献中的一个常见结构假设。&lt;br&gt;&lt;h4&gt;4. 新算法提案&lt;/h4&gt;   - 在这一假设下，提出了一种新算法，称为Meta Subspace Pursuit（简称Meta-SP），该算法能够有效学习不同任务共享的不变子空间。&lt;br&gt;&lt;h4&gt;5. 理论保证&lt;/h4&gt;   - 在多任务或元学习的简化设置下，建立了所提方法的算法保证和统计保证。&lt;br&gt;&lt;h4&gt;6. 实验比较&lt;/h4&gt;   - 进行了大量数值实验，将Meta-SP与多种竞争方法进行比较，包括流行的模型无关元学习算法如ANIL。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 实验表明，Meta-SP在多个方面的表现优于竞争方法，展示了其优越性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Data scarcity poses a serious threat to modern machine learning and
artificial intelligence, as their practical success typically relies on the
availability of big datasets. One effective strategy to mitigate the issue of
insufficient data is to first harness information from other data sources
possessing certain similarities in the study design stage, and then employ the
multi-task or meta learning framework in the analysis stage. In this paper, we
focus on multi-task (or multi-source) linear models whose coefficients across
tasks share an invariant low-rank component, a popular structural assumption
considered in the recent multi-task or meta learning literature. Under this
assumption, we propose a new algorithm, called Meta Subspace Pursuit
(abbreviated as Meta-SP), that provably learns this invariant subspace shared
by different tasks. Under this stylized setup for multi-task or meta learning,
we establish both the algorithmic and statistical guarantees of the proposed
method. Extensive numerical experiments are conducted, comparing Meta-SP
against several competing methods, including popular, off-the-shelf
model-agnostic meta learning algorithms such as ANIL. These experiments
demonstrate that Meta-SP achieves superior performance over the competing
methods in various aspects.</description>
      <guid isPermaLink="false">2409.02708v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>PointSSC: A Cooperative Vehicle-Infrastructure Point Cloud Benchmark for Semantic Scene Completion</title>
      <link>http://arxiv.org/abs/2309.12708v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICRA2024, oral &amp; poster&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究目标&lt;/h4&gt;   - 语义场景补全（SSC）旨在为复杂的3D场景生成空间占用和语义标签。&lt;br&gt;&lt;h4&gt;2. 现有问题&lt;/h4&gt;   - 大多数现有的SSC模型集中在体积表示上，这在处理大规模户外场景时效率较低。&lt;br&gt;&lt;h4&gt;3. 点云优势&lt;/h4&gt;   - 点云提供了一种轻量级的替代方案，但现有基准缺乏带有语义标签的户外点云场景。&lt;br&gt;&lt;h4&gt;4. PointSSC介绍&lt;/h4&gt;   - 引入PointSSC，这是首个用于语义场景补全的协作车辆-基础设施点云基准，场景具有远程感知和最小遮挡特性。&lt;br&gt;&lt;h4&gt;5. 自动标注流程&lt;/h4&gt;   - 开发了一个自动标注管道，利用Semantic Segment Anything高效地分配语义。&lt;br&gt;&lt;h4&gt;6. 基准测试方法&lt;/h4&gt;   - 提出了一种基于LiDAR的模型，采用空间感知变换器进行全局和局部特征提取，并设有联合补全与分割的协作模块。&lt;br&gt;&lt;h4&gt;7. 研究意义&lt;/h4&gt;   - PointSSC为推动现实世界导航的语义点云补全技术提供了一个具有挑战性的测试平台。&lt;br&gt;&lt;h4&gt;8. 资源可用性&lt;/h4&gt;   - 代码和数据集可在 [https://github.com/yyxssm/PointSSC](https://github.com/yyxssm/PointSSC) 获取。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2023-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/yyxssm/pointssc&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semantic Scene Completion (SSC) aims to jointly generate space occupancies
and semantic labels for complex 3D scenes. Most existing SSC models focus on
volumetric representations, which are memory-inefficient for large outdoor
spaces. Point clouds provide a lightweight alternative but existing benchmarks
lack outdoor point cloud scenes with semantic labels. To address this, we
introduce PointSSC, the first cooperative vehicle-infrastructure point cloud
benchmark for semantic scene completion. These scenes exhibit long-range
perception and minimal occlusion. We develop an automated annotation pipeline
leveraging Semantic Segment Anything to efficiently assign semantics. To
benchmark progress, we propose a LiDAR-based model with a Spatial-Aware
Transformer for global and local feature extraction and a Completion and
Segmentation Cooperative Module for joint completion and segmentation. PointSSC
provides a challenging testbed to drive advances in semantic point cloud
completion for real-world navigation. The code and datasets are available at
https://github.com/yyxssm/PointSSC.</description>
      <guid isPermaLink="false">2309.12708v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>3DFeat-Net: Weakly Supervised Local 3D Features for Point Cloud Registration</title>
      <link>http://arxiv.org/abs/1807.09413v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 6 figures. Accepted in ECCV 2018&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 方法概述&lt;/h4&gt;   - 提出了3DFeat-Net，一个学习3D特征检测器和描述符的网络，用于点云匹配，采用弱监督学习。&lt;br&gt;&lt;h4&gt;2. 与现有方法的区别&lt;/h4&gt;   - 不同于许多现有研究，该方法不需要手动标注匹配点集群。&lt;br&gt;&lt;h4&gt;3. 学习机制&lt;/h4&gt;   - 利用对齐和注意力机制，从带有GPS/INS标记的3D点云中学习特征对应，而无需明确指定这些对应关系。&lt;br&gt;&lt;h4&gt;4. 数据集创建&lt;/h4&gt;   - 创建了训练和基准的户外Lidar数据集，用于验证方法的有效性。&lt;br&gt;&lt;h4&gt;5. 实验结果&lt;/h4&gt;   - 实验结果表明，3DFeat-Net在这些重力对齐的数据集上取得了最先进的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2018-07-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1007/978-3-030-01267-0_37&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/yewzijian/3DFeatNet&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we propose the 3DFeat-Net which learns both 3D feature
detector and descriptor for point cloud matching using weak supervision. Unlike
many existing works, we do not require manual annotation of matching point
clusters. Instead, we leverage on alignment and attention mechanisms to learn
feature correspondences from GPS/INS tagged 3D point clouds without explicitly
specifying them. We create training and benchmark outdoor Lidar datasets, and
experiments show that 3DFeat-Net obtains state-of-the-art performance on these
gravity-aligned datasets.</description>
      <guid isPermaLink="false">1807.09413v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Incremental Multiview Point Cloud Registration</title>
      <link>http://arxiv.org/abs/2407.05021v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 新方法介绍&lt;/h4&gt;   - 本文提出了一种新颖的多视角点云配准方法，与以往的全局配准方案不同，采用了增量式流程。&lt;br&gt;&lt;h4&gt;2. 增量式配准&lt;/h4&gt;   - 该方法逐步将扫描数据对齐到一个规范坐标系统中，以提高配准的精度和效率。&lt;br&gt;&lt;h4&gt;3. 灵感来源&lt;/h4&gt;   - 方法借鉴了基于图像的3D重建技术，首先构建稀疏扫描图，进行扫描检索和几何验证。&lt;br&gt;&lt;h4&gt;4. 配准流程&lt;/h4&gt;   - 增量配准流程包括初始化、下一扫描选择与配准、轨迹创建与继续，以及束调整（Bundle Adjustment）。&lt;br&gt;&lt;h4&gt;5. 检测器无关的匹配器&lt;/h4&gt;   - 对于无检测器的匹配器，增加了轨迹精细化过程，以构建粗略的多视角配准并通过调整关键点位置来优化模型。&lt;br&gt;&lt;h4&gt;6. 实验结果&lt;/h4&gt;   - 实验结果表明，该框架在三个基准数据集上的表现优于现有的多视角配准方法。&lt;br&gt;&lt;h4&gt;7. 代码可用性&lt;/h4&gt;   - 相关代码已公开，供研究者使用，网址为 [https://github.com/Choyaa/IncreMVR](https://github.com/Choyaa/IncreMVR)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-07-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/choyaa/incremvr&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we present a novel approach for multiview point cloud
registration. Different from previous researches that typically employ a global
scheme for multiview registration, we propose to adopt an incremental pipeline
to progressively align scans into a canonical coordinate system. Specifically,
drawing inspiration from image-based 3D reconstruction, our approach first
builds a sparse scan graph with scan retrieval and geometric verification.
Then, we perform incremental registration via initialization, next scan
selection and registration, Track create and continue, and Bundle Adjustment.
Additionally, for detector-free matchers, we incorporate a Track refinement
process. This process primarily constructs a coarse multiview registration and
refines the model by adjusting the positions of the keypoints on the Track.
Experiments demonstrate that the proposed framework outperforms existing
multiview registration methods on three benchmark datasets. The code is
available at https://github.com/Choyaa/IncreMVR.</description>
      <guid isPermaLink="false">2407.05021v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>3DRef: 3D Dataset and Benchmark for Reflection Detection in RGB and Lidar Data</title>
      <link>http://arxiv.org/abs/2403.06538v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 反射表面挑战&lt;/h4&gt;   - 反射表面对机器人和自主系统中的可靠3D映射和感知构成持续挑战。&lt;br&gt;&lt;h4&gt;2. 现有数据集的局限性&lt;/h4&gt;   - 目前的反射数据集和基准主要限于稀疏的2D数据，无法充分满足需求。&lt;br&gt;&lt;h4&gt;3. 新数据集的介绍&lt;/h4&gt;   - 本文介绍了首个大规模的3D反射检测数据集，包含超过50,000个对齐样本，涵盖多返回Lidar、RGB图像及2D/3D语义标签，覆盖多样的室内环境和反射表面。&lt;br&gt;&lt;h4&gt;4. 精确的标注机制&lt;/h4&gt;   - 采用纹理化的3D真实模型，支持自动点云标注，从而提供精确的真实注释。&lt;br&gt;&lt;h4&gt;5. 详细的基准评估&lt;/h4&gt;   - 评估三种Lidar点云分割方法以及当前最先进的图像分割网络，专门针对玻璃和镜子检测进行测试。&lt;br&gt;&lt;h4&gt;6. 数据集的优势&lt;/h4&gt;   - 提供全面的测试平台，具有精确的全局对齐、多模态数据及多样的反射物体和材料，推动反射检测的研究进展。&lt;br&gt;&lt;h4&gt;7. 公开可用性&lt;/h4&gt;   - 数据集已公开，供研究者访问，网址为 [http://3dref.github.io](http://3dref.github.io)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reflective surfaces present a persistent challenge for reliable 3D mapping
and perception in robotics and autonomous systems. However, existing reflection
datasets and benchmarks remain limited to sparse 2D data. This paper introduces
the first large-scale 3D reflection detection dataset containing more than
50,000 aligned samples of multi-return Lidar, RGB images, and 2D/3D semantic
labels across diverse indoor environments with various reflections. Textured 3D
ground truth meshes enable automatic point cloud labeling to provide precise
ground truth annotations. Detailed benchmarks evaluate three Lidar point cloud
segmentation methods, as well as current state-of-the-art image segmentation
networks for glass and mirror detection. The proposed dataset advances
reflection detection by providing a comprehensive testbed with precise global
alignment, multi-modal data, and diverse reflective objects and materials. It
will drive future research towards reliable reflection detection. The dataset
is publicly available at http://3dref.github.io</description>
      <guid isPermaLink="false">2403.06538v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Ensemble with Conditional Feature Fusion for Dysgraphia Diagnosis in Children from Handwriting Samples</title>
      <link>http://arxiv.org/abs/2408.13754v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 发展性书写障碍的定义&lt;/h4&gt;   - 发展性书写障碍是一种神经系统疾病，影响儿童的写作能力。&lt;br&gt;&lt;h4&gt;2. 研究背景&lt;/h4&gt;   - 近年来，研究者们越来越多地探索机器学习方法，以支持基于离线和在线书写的书写障碍诊断。&lt;br&gt;&lt;h4&gt;3. 先前研究的局限性&lt;/h4&gt;   - 大多数先前研究分别分析在线和离线书写，未能探索两者之间的关系，这可能导致不理想的结果。&lt;br&gt;&lt;h4&gt;4. 新的研究方法&lt;/h4&gt;   - 为解决这一局限，提出了一种新颖的多模态机器学习方法，利用在线和离线书写数据。&lt;br&gt;&lt;h4&gt;5. 数据集的创建&lt;/h4&gt;   - 创建了一个新数据集，通过转换现有的在线手写数据集，生成相应的离线手写图像。&lt;br&gt;&lt;h4&gt;6. 分析类型的选择&lt;/h4&gt;   - 在多模态分析中，仅考虑不同类型的单词数据（简单单词、伪单词和困难单词）。&lt;br&gt;&lt;h4&gt;7. 分类器训练&lt;/h4&gt;   - 分别在在线和离线特征上训练支持向量机（SVM）和XGBoost分类器，并实现多模态特征融合和软投票集成。&lt;br&gt;&lt;h4&gt;8. 新颖的集成方法&lt;/h4&gt;   - 提出了带条件特征融合的新型集成方法，智能结合在线和离线分类器的预测，在置信度分数低于阈值时选择性地进行特征融合。&lt;br&gt;&lt;h4&gt;9. 实验结果&lt;/h4&gt;   - 该方法实现了88.8%的准确率，超越了单一模态的SVM模型12-14%，现有方法8-9%，以及传统的多模态方法（软投票集成和特征融合）3%和5%。&lt;br&gt;&lt;h4&gt;10. 研究贡献&lt;/h4&gt;    - 该方法为开发准确且高效的书写障碍诊断工具做出贡献，仅需一实例的多模态单词/伪单词数据即可确定书写障碍。&lt;br&gt;&lt;h4&gt;11. 未来展望&lt;/h4&gt;    - 本研究强调了多模态学习在提升书写障碍诊断中的潜力，为可及和实用的诊断工具铺平了道路。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Developmental dysgraphia is a neurological disorder that hinders children's
writing skills. In recent years, researchers have increasingly explored machine
learning methods to support the diagnosis of dysgraphia based on offline and
online handwriting. In most previous studies, the two types of handwriting have
been analysed separately, which does not necessarily lead to promising results.
In this way, the relationship between online and offline data cannot be
explored. To address this limitation, we propose a novel multimodal machine
learning approach utilizing both online and offline handwriting data. We
created a new dataset by transforming an existing online handwritten dataset,
generating corresponding offline handwriting images. We considered only
different types of word data (simple word, pseudoword &amp; difficult word) in our
multimodal analysis. We trained SVM and XGBoost classifiers separately on
online and offline features as well as implemented multimodal feature fusion
and soft-voted ensemble. Furthermore, we proposed a novel ensemble with
conditional feature fusion method which intelligently combines predictions from
online and offline classifiers, selectively incorporating feature fusion when
confidence scores fall below a threshold. Our novel approach achieves an
accuracy of 88.8%, outperforming SVMs for single modalities by 12-14%, existing
methods by 8-9%, and traditional multimodal approaches (soft-vote ensemble and
feature fusion) by 3% and 5%, respectively. Our methodology contributes to the
development of accurate and efficient dysgraphia diagnosis tools, requiring
only a single instance of multimodal word/pseudoword data to determine the
handwriting impairment. This work highlights the potential of multimodal
learning in enhancing dysgraphia diagnosis, paving the way for accessible and
practical diagnostic tools.</description>
      <guid isPermaLink="false">2408.13754v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Flexible Diffusion Scopes with Parameterized Laplacian for Heterophilic Graph Learning</title>
      <link>http://arxiv.org/abs/2409.09888v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  arXiv admin note: substantial text overlap with arXiv:2403.01475&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. GNN的局限性&lt;/h4&gt;   - 图神经网络（GNN）在捕捉长程和全球拓扑信息方面受限于传统图拉普拉斯算子，导致在某些数据集上的表现不尽人意，尤其是在异质图上。&lt;br&gt;&lt;h4&gt;2. 新参数化拉普拉斯矩阵的提出&lt;/h4&gt;   - 为了解决这一局限，提出了一类新的参数化拉普拉斯矩阵，该矩阵在控制节点间扩散距离方面提供了更大的灵活性。&lt;br&gt;&lt;h4&gt;3. 扩散距离与谱距离的关系&lt;/h4&gt;   - 证明了图中的扩散距离与谱距离存在顺序保持关系，这一结果表明参数化拉普拉斯可以加速长程信息的扩散。&lt;br&gt;&lt;h4&gt;4. 参数的灵活性&lt;/h4&gt;   - 拉普拉斯中的参数使得扩散范围具有灵活性，从而适应性地捕获图中的长程信息。&lt;br&gt;&lt;h4&gt;5. 拓扑引导重连机制&lt;/h4&gt;   - 基于理论结果，提出了拓扑引导的重连机制，以捕获异质图的有用长程邻域信息。&lt;br&gt;&lt;h4&gt;6. 新GNN的提出&lt;/h4&gt;   - 提出了两种具有灵活扩散范围的GNN：参数化扩散基图卷积网络（PD-GCN）和图注意力网络（PD-GAT）。&lt;br&gt;&lt;h4&gt;7. 实验验证&lt;/h4&gt;   - 通过合成实验，揭示了新拉普拉斯参数与不同图同质性水平下参数化GNN性能之间的高相关性，验证了新提出的GNN能够调整参数以适应不同异质图的全球信息捕获。&lt;br&gt;&lt;h4&gt;8. 性能优越性&lt;/h4&gt;   - 在7个真实基准数据集中，PD-GCN和PD-GAT在6个数据集上超越了最先进模型（SOTA），进一步确认了其优越性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The ability of Graph Neural Networks (GNNs) to capture long-range and global
topology information is limited by the scope of conventional graph Laplacian,
leading to unsatisfactory performance on some datasets, particularly on
heterophilic graphs. To address this limitation, we propose a new class of
parameterized Laplacian matrices, which provably offers more flexibility in
controlling the diffusion distance between nodes than the conventional graph
Laplacian, allowing long-range information to be adaptively captured through
diffusion on graph. Specifically, we first prove that the diffusion distance
and spectral distance on graph have an order-preserving relationship. With this
result, we demonstrate that the parameterized Laplacian can accelerate the
diffusion of long-range information, and the parameters in the Laplacian enable
flexibility of the diffusion scopes. Based on the theoretical results, we
propose topology-guided rewiring mechanism to capture helpful long-range
neighborhood information for heterophilic graphs. With this mechanism and the
new Laplacian, we propose two GNNs with flexible diffusion scopes: namely the
Parameterized Diffusion based Graph Convolutional Networks (PD-GCN) and Graph
Attention Networks (PD-GAT). Synthetic experiments reveal the high correlations
between the parameters of the new Laplacian and the performance of
parameterized GNNs under various graph homophily levels, which verifies that
our new proposed GNNs indeed have the ability to adjust the parameters to
adaptively capture the global information for different levels of heterophilic
graphs. They also outperform the state-of-the-art (SOTA) models on 6 out of 7
real-world benchmark datasets, which further confirms their superiority.</description>
      <guid isPermaLink="false">2409.09888v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Mental-Gen: A Brain-Computer Interface-Based Interactive Method for Interior Space Generative Design</title>
      <link>http://arxiv.org/abs/2409.00962v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 8 figures&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 室内空间设计的重要性&lt;/h4&gt;   - 室内空间设计对居民的日常生活有显著影响，但设计过程通常存在高障碍和复杂的推理，造成用户表达需求时的语义损失。&lt;br&gt;&lt;h4&gt;2. 研究方法的提出&lt;/h4&gt;   - 本研究提出了“心理生成设计方法”（Mental-Gen），旨在在神经层面解读用户的空间设计意图，并通过生成AI模型进行表达。&lt;br&gt;&lt;h4&gt;3. 无监督学习的应用&lt;/h4&gt;   - 采用无监督学习方法检测用户对不同空间特征的脑电波反应的相似性，以评估脑机接口（BCI）命令的可行性。&lt;br&gt;&lt;h4&gt;4. 生成AI模型的训练&lt;/h4&gt;   - 针对每个有价值的设计命令，训练和优化生成AI模型。&lt;br&gt;&lt;h4&gt;5. 命令预测过程&lt;/h4&gt;   - 采用脑机接口研究中的运动想象范式进行命令预测，通过训练支持向量机（SVM）模型，根据脑电图（EEG）特征预测不同空间特征的设计命令。&lt;br&gt;&lt;h4&gt;6. 研究结果&lt;/h4&gt;   - 结果表明，Mental-Gen方法能够有效解读通过脑电波信号传达的设计意图，帮助用户通过想象的命令实现令人满意的室内空间设计。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Interior space design significantly influences residents' daily lives.
However, the process often presents high barriers and complex reasoning for
users, leading to semantic losses in articulating comprehensive requirements
and communicating them to designers. This study proposes the Mental-Gen design
method, which focuses on interpreting users' spatial design intentions at
neural level and expressing them through generative AI models. We employed
unsupervised learning methods to detect similarities in users' brainwave
responses to different spatial features, assess the feasibility of BCI
commands. We trained and refined generative AI models for each valuable design
command. The command prediction process adopted the motor imagery paradigm from
BCI research. We trained Support Vector Machine (SVM) models to predict design
commands for different spatial features based on EEG features. The results
indicate that the Mental-Gen method can effectively interpret design intentions
through brainwave signals, assisting users in achieving satisfactory interior
space designs using imagined commands.</description>
      <guid isPermaLink="false">2409.00962v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>The JPEG Pleno Learning-based Point Cloud Coding Standard: Serving Man and Machine</title>
      <link>http://arxiv.org/abs/2409.08130v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  28 pages, 12 figures, submitted to IEEE Access&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 点云编码的重要性&lt;/h4&gt;   - 高效的点云编码在虚拟现实、自动驾驶和数字双胞胎系统等多个应用中变得越来越关键，丰富和互动的3D数据表示可以极大地影响功能性。&lt;br&gt;&lt;h4&gt;2. 深度学习的应用&lt;/h4&gt;   - 深度学习作为一种强大的工具，提供了比传统编码方法更高效的点云压缩技术，同时允许在压缩域中进行有效的计算机视觉任务。&lt;br&gt;&lt;h4&gt;3. JPEG Pleno标准的发布&lt;/h4&gt;   - JPEG最近最终确定了基于学习的点云编码（PCC）标准，旨在利用深度学习模型高效地进行静态点云的有损编码，兼顾人类可视化和机器处理。&lt;br&gt;&lt;h4&gt;4. 几何和颜色编码处理&lt;/h4&gt;   - 几何数据直接以原始3D形态通过稀疏卷积神经网络进行处理，而颜色数据则投影到2D图像上，并使用基于学习的JPEG AI标准进行编码。&lt;br&gt;&lt;h4&gt;5. 论文的目标&lt;/h4&gt;   - 本文旨在提供JPEG PCC标准的完整技术描述，并对其性能进行全面基准测试，与最先进的方法进行比较，突出其主要优缺点。&lt;br&gt;&lt;h4&gt;6. 压缩性能比较&lt;/h4&gt;   - 在压缩性能方面，JPEG PCC在几何编码上优于传统的MPEG PCC标准，达到显著的比特率降低。&lt;br&gt;&lt;h4&gt;7. 颜色压缩性能的挑战&lt;/h4&gt;   - 虽然颜色压缩性能相对竞争力较弱，但通过全学习型编码框架，几何和颜色的有效处理得以克服这一挑战。&lt;br&gt;&lt;h4&gt;8. 有效的压缩域处理&lt;/h4&gt;   - 该框架的优势在于能够在压缩域中进行有效处理，提升整体性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Efficient point cloud coding has become increasingly critical for multiple
applications such as virtual reality, autonomous driving, and digital twin
systems, where rich and interactive 3D data representations may functionally
make the difference. Deep learning has emerged as a powerful tool in this
domain, offering advanced techniques for compressing point clouds more
efficiently than conventional coding methods while also allowing effective
computer vision tasks performed in the compressed domain thus, for the first
time, making available a common compressed visual representation effective for
both man and machine. Taking advantage of this potential, JPEG has recently
finalized the JPEG Pleno Learning-based Point Cloud Coding (PCC) standard
offering efficient lossy coding of static point clouds, targeting both human
visualization and machine processing by leveraging deep learning models for
geometry and color coding. The geometry is processed directly in its original
3D form using sparse convolutional neural networks, while the color data is
projected onto 2D images and encoded using the also learning-based JPEG AI
standard. The goal of this paper is to provide a complete technical description
of the JPEG PCC standard, along with a thorough benchmarking of its performance
against the state-of-the-art, while highlighting its main strengths and
weaknesses. In terms of compression performance, JPEG PCC outperforms the
conventional MPEG PCC standards, especially in geometry coding, achieving
significant rate reductions. Color compression performance is less competitive
but this is overcome by the power of a full learning-based coding framework for
both geometry and color and the associated effective compressed domain
processing.</description>
      <guid isPermaLink="false">2409.08130v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Decoupled and Interactive Regression Modeling for High-performance One-stage 3D Object Detection</title>
      <link>http://arxiv.org/abs/2409.00690v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 一阶段3D目标检测的局限性&lt;/h4&gt;   - 不充分的边界框建模在回归任务中限制了一阶段3D目标检测的性能。&lt;br&gt;&lt;h4&gt;2. 问题原因&lt;/h4&gt;   - 研究发现主要原因有两个：&lt;br&gt;     - **中心偏移预测的局限性**：有限的中心偏移预测严重影响了边界框的定位，许多响应最高的位置与物体中心显著偏离。&lt;br&gt;     - **低质量样本的影响**：回归任务中忽视的低质量样本对边界框预测产生重大影响，因为这些样本导致不可靠的质量（IoU）校正。&lt;br&gt;&lt;h4&gt;3. 提出的新方法&lt;/h4&gt;   - 为了解决这些问题，本文提出了解耦和交互回归建模（DIRM）用于一阶段检测。&lt;br&gt;&lt;h4&gt;4. 解耦属性回归（DAR）&lt;/h4&gt;   - 实施解耦属性回归，以通过自适应多样本分配策略促进中心属性的长回归范围建模，深度解耦边界框属性。&lt;br&gt;&lt;h4&gt;5. 交互质量预测（IQP）&lt;/h4&gt;   - 为了增强低质量结果的IoU预测的可靠性，交互质量预测将擅长建模负样本的分类任务与质量预测结合，实现联合优化。&lt;br&gt;&lt;h4&gt;6. 实验验证&lt;/h4&gt;   - 在Waymo和ONCE数据集上进行的大量实验表明，DIRM显著提高了多种最先进方法的性能，并且额外的推理延迟最小。&lt;br&gt;&lt;h4&gt;7. 成果&lt;/h4&gt;   - DIRM在Waymo和ONCE数据集上实现了最先进的检测性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Inadequate bounding box modeling in regression tasks constrains the
performance of one-stage 3D object detection. Our study reveals that the
primary reason lies in two aspects: (1) The limited center-offset prediction
seriously impairs the bounding box localization since many highest response
positions significantly deviate from object centers. (2) The low-quality sample
ignored in regression tasks significantly impacts the bounding box prediction
since it produces unreliable quality (IoU) rectification. To tackle these
problems, we propose Decoupled and Interactive Regression Modeling (DIRM) for
one-stage detection. Specifically, Decoupled Attribute Regression (DAR) is
implemented to facilitate long regression range modeling for the center
attribute through an adaptive multi-sample assignment strategy that deeply
decouples bounding box attributes. On the other hand, to enhance the
reliability of IoU predictions for low-quality results, Interactive Quality
Prediction (IQP) integrates the classification task, proficient in modeling
negative samples, with quality prediction for joint optimization. Extensive
experiments on Waymo and ONCE datasets demonstrate that DIRM significantly
improves the performance of several state-of-the-art methods with minimal
additional inference latency. Notably, DIRM achieves state-of-the-art detection
performance on both the Waymo and ONCE datasets.</description>
      <guid isPermaLink="false">2409.00690v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Vision foundation models: can they be applied to astrophysics data?</title>
      <link>http://arxiv.org/abs/2409.11175v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 4 figures, submitted to Foundation Models for Science
  Workshop at NeurIPS 2024&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 视觉基础模型的潜力&lt;/h4&gt;   - 视觉基础模型在许多多媒体应用中展现出显著潜力，但在自然科学中应用较少。&lt;br&gt;&lt;h4&gt;2. 数据不匹配问题&lt;/h4&gt;   - 主要原因是领域特定的科学数据与基础模型通常使用的训练数据之间存在不匹配，导致分布偏移。&lt;br&gt;&lt;h4&gt;3. 科学数据的特征&lt;/h4&gt;   - 科学数据在结构和特征上通常有显著差异，研究人员常常面临仅有几百或几千张图像的有限标记数据的挑战。&lt;br&gt;&lt;h4&gt;4. 定制化适应方法&lt;/h4&gt;   - 为了有效适应基础模型，需要在预处理、数据增强和训练技术上采取定制化方法。&lt;br&gt;&lt;h4&gt;5. 模型的特异性&lt;/h4&gt;   - 每个视觉基础模型具有独特的优缺点，这些优缺点受模型架构、训练流程和训练数据集的影响。&lt;br&gt;&lt;h4&gt;6. 评估在天体物理学中的应用&lt;/h4&gt;   - 本文评估了多种视觉基础模型在天体物理学数据（特别是光学和射电天文学图像）中的应用。&lt;br&gt;&lt;h4&gt;7. 光学星系图像的分类准确性&lt;/h4&gt;   - 结果显示，使用特定基础模型提取的特征在光学星系图像分类中，提高了准确性，相较于传统的监督训练方法有明显提升。&lt;br&gt;&lt;h4&gt;8. 射电图像的物体检测&lt;/h4&gt;   - 这些模型在射电图像的物体检测任务中表现出与传统方法相当或更好的性能。&lt;br&gt;&lt;h4&gt;9. 射电星系图像的分类性能&lt;/h4&gt;   - 然而，它们在射电星系图像分类中的表现总体较差，通常不如传统监督训练的结果。&lt;br&gt;&lt;h4&gt;10. 模型选择的重要性&lt;/h4&gt;    - 这些发现表明，选择适合的视觉基础模型用于天体物理应用时，需要仔细考虑模型特性与下游任务的具体要求之间的对齐。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision foundation models, which have demonstrated significant potential in
many multimedia applications, are often underutilized in the natural sciences.
This is primarily due to mismatches between the nature of domain-specific
scientific data and the typical training data used for foundation models,
leading to distribution shifts. Scientific data often differ substantially in
structure and characteristics; researchers frequently face the challenge of
optimizing model performance with limited labeled data of only a few hundred or
thousand images. To adapt foundation models effectively requires customized
approaches in preprocessing, data augmentation, and training techniques.
Additionally, each vision foundation model exhibits unique strengths and
limitations, influenced by differences in architecture, training procedures,
and the datasets used for training. In this work, we evaluate the application
of various vision foundation models to astrophysics data, specifically images
from optical and radio astronomy. Our results show that using features
extracted by specific foundation models improves the classification accuracy of
optical galaxy images compared to conventional supervised training. Similarly,
these models achieve equivalent or better performance in object detection tasks
with radio images. However, their performance in classifying radio galaxy
images is generally poor and often inferior to traditional supervised training
results. These findings suggest that selecting suitable vision foundation
models for astrophysics applications requires careful consideration of the
model characteristics and alignment with the specific requirements of the
downstream tasks.</description>
      <guid isPermaLink="false">2409.11175v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Unraveling Meta-Learning: Understanding Feature Representations for Few-Shot Tasks</title>
      <link>http://arxiv.org/abs/2002.06753v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICML 2020&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 元学习算法的表现&lt;/h4&gt;   - 元学习算法生成的特征提取器在少样本分类中表现出色，达到了最先进的性能。&lt;br&gt;&lt;h4&gt;2. 文献中的知识缺口&lt;/h4&gt;   - 尽管元学习方法丰富，但对其生成的特征提取器为何表现优异的理解仍然有限。&lt;br&gt;&lt;h4&gt;3. 深入理解元学习机制&lt;/h4&gt;   - 本文旨在更好地理解元学习的基本机制，以及元学习模型与经典训练模型之间的区别。&lt;br&gt;&lt;h4&gt;4. 提出并验证假设&lt;/h4&gt;   - 我们提出并验证了多个假设，解释为何元学习模型的性能优于传统模型。&lt;br&gt;&lt;h4&gt;5. 开发新正则化器&lt;/h4&gt;   - 本文开发了一种正则化器，提升了标准训练流程在少样本分类中的表现。&lt;br&gt;&lt;h4&gt;6. 性能比较&lt;/h4&gt;   - 在许多情况下，我们的训练流程在性能上超过了元学习方法，同时运行速度快了一个数量级。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2020-02-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/goldblum/FeatureClustering&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Meta-learning algorithms produce feature extractors which achieve
state-of-the-art performance on few-shot classification. While the literature
is rich with meta-learning methods, little is known about why the resulting
feature extractors perform so well. We develop a better understanding of the
underlying mechanics of meta-learning and the difference between models trained
using meta-learning and models which are trained classically. In doing so, we
introduce and verify several hypotheses for why meta-learned models perform
better. Furthermore, we develop a regularizer which boosts the performance of
standard training routines for few-shot classification. In many cases, our
routine outperforms meta-learning while simultaneously running an order of
magnitude faster.</description>
      <guid isPermaLink="false">2002.06753v3</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Acoustic identification of individual animals with hierarchical contrastive learning</title>
      <link>http://arxiv.org/abs/2409.08673v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under review; Submitted to ICASSP 2025&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 动物声学识别的定义&lt;/h4&gt;   - 声学识别个体动物（AIID）与基于音频的物种分类密切相关，但需要更细致的区分同一物种内的个体。&lt;br&gt;&lt;h4&gt;2. 任务框架&lt;/h4&gt;   - 本文将AIID视为一个层次化的多标签分类任务，提出使用层次感知损失函数来学习个体身份的稳健表示。&lt;br&gt;&lt;h4&gt;3. 保持层次关系&lt;/h4&gt;   - 所提方法旨在维护物种和分类群之间的层次关系，从而提升个体身份的识别能力。&lt;br&gt;&lt;h4&gt;4. 实验结果&lt;/h4&gt;   - 结果表明，层次嵌入不仅提高了个体层面的识别准确性，还在更高的分类层次上有效保留了层次结构。&lt;br&gt;&lt;h4&gt;5. 与非层次模型比较&lt;/h4&gt;   - 通过与非层次模型的对比，强调了在嵌入空间中强制执行层次结构的优势。&lt;br&gt;&lt;h4&gt;6. 扩展评估&lt;/h4&gt;   - 进一步扩展评估到新个体类别的分类，展示了该方法在开放集分类场景中的潜力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Acoustic identification of individual animals (AIID) is closely related to
audio-based species classification but requires a finer level of detail to
distinguish between individual animals within the same species. In this work,
we frame AIID as a hierarchical multi-label classification task and propose the
use of hierarchy-aware loss functions to learn robust representations of
individual identities that maintain the hierarchical relationships among
species and taxa. Our results demonstrate that hierarchical embeddings not only
enhance identification accuracy at the individual level but also at higher
taxonomic levels, effectively preserving the hierarchical structure in the
learned representations. By comparing our approach with non-hierarchical
models, we highlight the advantage of enforcing this structure in the embedding
space. Additionally, we extend the evaluation to the classification of novel
individual classes, demonstrating the potential of our method in open-set
classification scenarios.</description>
      <guid isPermaLink="false">2409.08673v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>A New First-Order Meta-Learning Algorithm with Convergence Guarantees</title>
      <link>http://arxiv.org/abs/2409.03682v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 任务学习的核心属性&lt;/h4&gt;   - 从其他相关任务中获取经验以学习新任务是智能系统的核心特性。&lt;br&gt;&lt;h4&gt;2. 梯度基础的元学习&lt;/h4&gt;   - 基于梯度的元学习，特别是模型无关元学习（MAML）及其变体，已成为实现这一目标的可行解决方案。&lt;br&gt;&lt;h4&gt;3. MAML的挑战&lt;/h4&gt;   - MAML面临的一个问题是计算和内存负担，特别是在计算元梯度时。&lt;br&gt;&lt;h4&gt;4. 提出新的第一阶变体&lt;/h4&gt;   - 本文提出了一种新的第一阶MAML变体，并证明其收敛到MAML目标的一个平稳点，这与其他第一阶变体不同。&lt;br&gt;&lt;h4&gt;5. 平滑性假设的分析&lt;/h4&gt;   - 我们展示了MAML目标不满足先前研究中假设的平滑性条件；相反，其平滑常数随着元梯度的范数增长。&lt;br&gt;&lt;h4&gt;6. 梯度方法的理论建议&lt;/h4&gt;   - 理论上建议使用归一化或裁剪梯度方法，而不是以往研究中使用的普通梯度方法。&lt;br&gt;&lt;h4&gt;7. 理论验证&lt;/h4&gt;   - 我们通过一个合成实验验证了我们的理论。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning new tasks by drawing on prior experience gathered from other
(related) tasks is a core property of any intelligent system. Gradient-based
meta-learning, especially MAML and its variants, has emerged as a viable
solution to accomplish this goal. One problem MAML encounters is its
computational and memory burdens needed to compute the meta-gradients. We
propose a new first-order variant of MAML that we prove converges to a
stationary point of the MAML objective, unlike other first-order variants. We
also show that the MAML objective does not satisfy the smoothness assumption
assumed in previous works; we show instead that its smoothness constant grows
with the norm of the meta-gradient, which theoretically suggests the use of
normalized or clipped-gradient methods compared to the plain gradient method
used in previous works. We validate our theory on a synthetic experiment.</description>
      <guid isPermaLink="false">2409.03682v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Reimagining Linear Probing: Kolmogorov-Arnold Networks in Transfer Learning</title>
      <link>http://arxiv.org/abs/2409.07763v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figure&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 引入Kolmogorov-Arnold网络（KAN）&lt;/h4&gt;   - 本文提出了Kolmogorov-Arnold网络作为传统线性探测方法在迁移学习中的一种增强。&lt;br&gt;&lt;h4&gt;2. 线性探测的局限性&lt;/h4&gt;   - 线性探测通常应用于预训练模型的最后一层，但其无法建模数据中的复杂关系，限制了性能。&lt;br&gt;&lt;h4&gt;3. KAN的优势&lt;/h4&gt;   - 本文建议用KAN替代线性探测层，KAN利用基于样条的表示来近似复杂函数，从而增强模型能力。&lt;br&gt;&lt;h4&gt;4. 与ResNet-50模型结合&lt;/h4&gt;   - 我们将KAN与在ImageNet上预训练的ResNet-50模型集成，并在CIFAR-10数据集上评估其性能。&lt;br&gt;&lt;h4&gt;5. 系统的超参数搜索&lt;/h4&gt;   - 进行了系统的超参数搜索，重点关注网格大小和样条度（k），旨在优化KAN的灵活性和准确性。&lt;br&gt;&lt;h4&gt;6. 实验结果&lt;/h4&gt;   - 实验结果显示，KAN在多种配置下始终优于传统的线性探测，显著提高了准确性和泛化能力。&lt;br&gt;&lt;h4&gt;7. KAN的潜力&lt;/h4&gt;   - 这些发现表明，KAN为迁移学习中的传统线性探测技术提供了更强大和适应性更强的替代方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces Kolmogorov-Arnold Networks (KAN) as an enhancement to
the traditional linear probing method in transfer learning. Linear probing,
often applied to the final layer of pre-trained models, is limited by its
inability to model complex relationships in data. To address this, we propose
substituting the linear probing layer with KAN, which leverages spline-based
representations to approximate intricate functions. In this study, we integrate
KAN with a ResNet-50 model pre-trained on ImageNet and evaluate its performance
on the CIFAR-10 dataset. We perform a systematic hyperparameter search,
focusing on grid size and spline degree (k), to optimize KAN's flexibility and
accuracy. Our results demonstrate that KAN consistently outperforms traditional
linear probing, achieving significant improvements in accuracy and
generalization across a range of configurations. These findings indicate that
KAN offers a more powerful and adaptable alternative to conventional linear
probing techniques in transfer learning.</description>
      <guid isPermaLink="false">2409.07763v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>GRE^2-MDCL: Graph Representation Embedding Enhanced via Multidimensional Contrastive Learning</title>
      <link>http://arxiv.org/abs/2409.07725v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 图表示学习的兴起&lt;/h4&gt;   - 图表示学习是一种强大的工具，有助于在将节点映射到向量表示时保持图的拓扑结构，从而支持节点分类和社区检测等下游任务。&lt;br&gt;&lt;h4&gt;2. 当前模型的挑战&lt;/h4&gt;   - 现有的大多数图神经网络模型面临需要大量标记数据的问题，这限制了它们在缺乏标记数据的实际应用中的效果。&lt;br&gt;&lt;h4&gt;3. 图对比学习的探索&lt;/h4&gt;   - 为了应对标记数据稀缺的问题，研究者们探索了图对比学习（GCL），利用增强图数据和对比学习技术。&lt;br&gt;&lt;h4&gt;4. 现有GCL方法的不足&lt;/h4&gt;   - 现有的GCL方法在有效捕捉局部和全局图结构方面存在困难，同时在节点级别和图级别表示之间的平衡也面临挑战。&lt;br&gt;&lt;h4&gt;5. 提出的新模型GRE2-MDCL&lt;/h4&gt;   - 本文提出了图表示嵌入增强多维对比学习（GRE2-MDCL），采用了一种新颖的三重网络架构，以多头注意力图神经网络为核心。&lt;br&gt;&lt;h4&gt;6. 图的增强处理&lt;/h4&gt;   - GRE2-MDCL首先使用奇异值分解（SVD）和局部自适应图神经网络（LAGNN）技术对输入图进行全局和局部增强。&lt;br&gt;&lt;h4&gt;7. 构建多维对比损失&lt;/h4&gt;   - 该模型构建了一个多维对比损失，结合跨网络、跨视图和邻居对比，以优化模型性能。&lt;br&gt;&lt;h4&gt;8. 实验证明有效性&lt;/h4&gt;   - 在基准数据集Cora、Citeseer和PubMed上的大量实验表明，GRE2-MDCL达到了最先进的性能，平均准确率分别为82.5%、72.5%和81.6%。&lt;br&gt;&lt;h4&gt;9. 可视化结果&lt;/h4&gt;   - 可视化结果显示了更紧密的类内聚合和更清晰的类间边界，凸显了该框架在提高基线GCL模型方面的有效性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph representation learning has emerged as a powerful tool for preserving
graph topology when mapping nodes to vector representations, enabling various
downstream tasks such as node classification and community detection. However,
most current graph neural network models face the challenge of requiring
extensive labeled data, which limits their practical applicability in
real-world scenarios where labeled data is scarce. To address this challenge,
researchers have explored Graph Contrastive Learning (GCL), which leverages
enhanced graph data and contrastive learning techniques. While promising,
existing GCL methods often struggle with effectively capturing both local and
global graph structures, and balancing the trade-off between nodelevel and
graph-level representations. In this work, we propose Graph Representation
Embedding Enhanced via Multidimensional Contrastive Learning (GRE2-MDCL). Our
model introduces a novel triple network architecture with a multi-head
attention GNN as the core. GRE2-MDCL first globally and locally augments the
input graph using SVD and LAGNN techniques. It then constructs a
multidimensional contrastive loss, incorporating cross-network, cross-view, and
neighbor contrast, to optimize the model. Extensive experiments on benchmark
datasets Cora, Citeseer, and PubMed demonstrate that GRE2-MDCL achieves
state-of-the-art performance, with average accuracies of 82.5%, 72.5%, and
81.6% respectively. Visualizations further show tighter intra-cluster
aggregation and clearer inter-cluster boundaries, highlighting the
effectiveness of our framework in improving upon baseline GCL models.</description>
      <guid isPermaLink="false">2409.07725v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>The Perfect Match: 3D Point Cloud Matching with Smoothed Densities</title>
      <link>http://arxiv.org/abs/1811.06879v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CVPR 2019&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 提出3DSmoothNet&lt;/h4&gt;   - 该方法是一个完整的工作流程，用于匹配3D点云。&lt;br&gt;&lt;h4&gt;2. 使用深度学习架构&lt;/h4&gt;   - 采用了西米尔（Siamese）深度学习结构和全卷积层。&lt;br&gt;&lt;h4&gt;3. 体素化平滑密度值（SDV）表示&lt;/h4&gt;   - 利用体素化的平滑密度值作为点云的表示，计算每个兴趣点的SDV，并与局部参考框架（LRF）对齐，以实现旋转不变性。&lt;br&gt;&lt;h4&gt;4. 高召回率表现&lt;/h4&gt;   - 该方法在3DMatch基准数据集上达到了94.9%的平均召回率，超越了现有最先进技术20%以上。&lt;br&gt;&lt;h4&gt;5. 低输出维度&lt;/h4&gt;   - 仅使用32个输出维度，使得在标准PC上每个特征点只需0.1毫秒进行近实时对应搜索。&lt;br&gt;&lt;h4&gt;6. 传感器和场景无关性&lt;/h4&gt;   - 由于SDV和LRF的使用以及全卷积层学习到的高度描述性特征，该方法对传感器和场景无关。&lt;br&gt;&lt;h4&gt;7. 户外激光扫描的性能&lt;/h4&gt;   - 仅用 RGB-D 室内建筑场景训练的3DSmoothNet，在户外植被的激光扫描中也达到了79.0%的平均召回率，性能是最接近的竞争对手的两倍以上。&lt;br&gt;&lt;h4&gt;8. 可用资源&lt;/h4&gt;   - 代码、数据及预训练模型可以在线获取，链接为 [GitHub - 3DSmoothNet](https://github.com/zgojcic/3DSmoothNet)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2018-11-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/zgojcic/3DSmoothNet&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose 3DSmoothNet, a full workflow to match 3D point clouds with a
siamese deep learning architecture and fully convolutional layers using a
voxelized smoothed density value (SDV) representation. The latter is computed
per interest point and aligned to the local reference frame (LRF) to achieve
rotation invariance. Our compact, learned, rotation invariant 3D point cloud
descriptor achieves 94.9% average recall on the 3DMatch benchmark data set,
outperforming the state-of-the-art by more than 20 percent points with only 32
output dimensions. This very low output dimension allows for near realtime
correspondence search with 0.1 ms per feature point on a standard PC. Our
approach is sensor- and sceneagnostic because of SDV, LRF and learning highly
descriptive features with fully convolutional layers. We show that 3DSmoothNet
trained only on RGB-D indoor scenes of buildings achieves 79.0% average recall
on laser scans of outdoor vegetation, more than double the performance of our
closest, learning-based competitors. Code, data and pre-trained models are
available online at https://github.com/zgojcic/3DSmoothNet.</description>
      <guid isPermaLink="false">1811.06879v3</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>EffiPerception: an Efficient Framework for Various Perception Tasks</title>
      <link>http://arxiv.org/abs/2403.12317v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 准确性、速度和内存之间的权衡是计算机视觉感知任务中的重要考虑因素。&lt;br&gt;&lt;h4&gt;2. 现有方法局限&lt;/h4&gt;   - 以往的方法主要集中于单一或少数几种任务，例如有效的数据增强、特征提取器和学习策略。&lt;br&gt;   - 这些方法往往是任务特定的，模型性能可能依赖于特定的感知任务或数据集。&lt;br&gt;&lt;h4&gt;3. 研究目标&lt;/h4&gt;   - 本文旨在探索通用学习模式并提高模块的鲁棒性，提出EffiPerception框架。&lt;br&gt;&lt;h4&gt;4. 框架优势&lt;/h4&gt;   - EffiPerception在多个感知任务（包括2D物体检测、3D物体检测、2D实例分割和3D点云分割）下，能够实现良好的准确性和速度表现，同时内存开销相对较低。&lt;br&gt;&lt;h4&gt;5. 框架组成&lt;/h4&gt;   - **高效特征提取器**：为每种模态提取输入特征。&lt;br&gt;   - **高效层**：可插拔的层进一步处理特征表示，聚合核心学习信息，同时修剪噪声提案。&lt;br&gt;   - **EffiOptim**：一种8位优化器，进一步降低计算成本并促进性能稳定性。&lt;br&gt;&lt;h4&gt;6. 实验验证&lt;/h4&gt;   - 在KITTI、semantic-KITTI和COCO数据集上进行的广泛实验表明，EffiPerception在四个检测和分割任务中，相较于早期成熟的方法，整体表现出显著的准确性、速度和内存性能提升。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-03-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The accuracy-speed-memory trade-off is always the priority to consider for
several computer vision perception tasks.
  Previous methods mainly focus on a single or small couple of these tasks,
such as creating effective data augmentation, feature extractor, learning
strategies, etc. These approaches, however, could be inherently task-specific:
their proposed model's performance may depend on a specific perception task or
a dataset.
  Targeting to explore common learning patterns and increasing the module
robustness, we propose the EffiPerception framework.
  It could achieve great accuracy-speed performance with relatively low memory
cost under several perception tasks: 2D Object Detection, 3D Object Detection,
2D Instance Segmentation, and 3D Point Cloud Segmentation.
  Overall, the framework consists of three parts:
  (1) Efficient Feature Extractors, which extract the input features for each
modality. (2) Efficient Layers, plug-in plug-out layers that further process
the feature representation, aggregating core learned information while pruning
noisy proposals. (3) The EffiOptim, an 8-bit optimizer to further cut down the
computational cost and facilitate performance stability.
  Extensive experiments on the KITTI, semantic-KITTI, and COCO datasets
revealed that EffiPerception could show great accuracy-speed-memory overall
performance increase within the four detection and segmentation tasks, in
comparison to earlier, well-respected methods.</description>
      <guid isPermaLink="false">2403.12317v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Fine-Tuning Generative Models as an Inference Method for Robotic Tasks</title>
      <link>http://arxiv.org/abs/2310.12862v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7th Conference on Robot Learning, 2023. Project website at
  https://www.orrkrup.com/mace&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 可适应的模型对在现实世界中操作的机器人代理具有重要意义，能够使其应对新颖和多变的环境条件。&lt;br&gt;&lt;h4&gt;2. 方法论基础&lt;/h4&gt;   - 虽然贝叶斯推理等方法是适应模型的成熟框架，本文基于近年来深度生成模型的进展，这些进展对机器人领域产生了重要影响。&lt;br&gt;&lt;h4&gt;3. 现代技术应用&lt;/h4&gt;   - 利用现代GPU加速，研究如何快速适应神经网络模型的样本生成以应对机器人任务中的观察数据。&lt;br&gt;&lt;h4&gt;4. 方法提案&lt;/h4&gt;   - 提出了一个简单且通用的方法，适用于多种深度生成模型和机器人环境。&lt;br&gt;   - 关键思想是通过将模型快速微调，使其拟合与观察到的证据匹配的生成样本，采用交叉熵方法。&lt;br&gt;&lt;h4&gt;5. 方法应用&lt;/h4&gt;   - 该方法可应用于自回归模型和变分自编码器（VAE）。&lt;br&gt;   - 展示了其在物体形状推断、逆向运动学计算和点云补全等任务中的可用性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2023-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/orrkrup/mace&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Adaptable models could greatly benefit robotic agents operating in the real
world, allowing them to deal with novel and varying conditions. While
approaches such as Bayesian inference are well-studied frameworks for adapting
models to evidence, we build on recent advances in deep generative models which
have greatly affected many areas of robotics. Harnessing modern GPU
acceleration, we investigate how to quickly adapt the sample generation of
neural network models to observations in robotic tasks. We propose a simple and
general method that is applicable to various deep generative models and robotic
environments. The key idea is to quickly fine-tune the model by fitting it to
generated samples matching the observed evidence, using the cross-entropy
method. We show that our method can be applied to both autoregressive models
and variational autoencoders, and demonstrate its usability in object shape
inference from grasping, inverse kinematics calculation, and point cloud
completion.</description>
      <guid isPermaLink="false">2310.12862v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>GaussReg: Fast 3D Registration with Gaussian Splatting</title>
      <link>http://arxiv.org/abs/2407.05254v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ECCV 2024&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 点云配准是大规模3D场景扫描和重建的基本问题。&lt;br&gt;   - 借助深度学习，配准方法取得了显著进展，几乎达到成熟阶段。&lt;br&gt;&lt;h4&gt;2. NeRF的引入&lt;/h4&gt;   - 神经辐射场（NeRF）成为最受欢迎的3D场景表示，因其强大的视图合成能力。&lt;br&gt;   - NeRF表示在大规模场景重建中也需要进行配准，但这一领域探索不足。&lt;br&gt;&lt;h4&gt;3. 挑战&lt;/h4&gt;   - 使用隐式表示建模两个场景之间的几何关系存在固有挑战。&lt;br&gt;   - 现有方法通常将隐式表示转换为显式表示以进行进一步配准。&lt;br&gt;&lt;h4&gt;4. 新方法的提出&lt;/h4&gt;   - 最近引入的高斯喷涂（Gaussian Splatting，GS）使用显式3D高斯，显著提高了渲染速度，同时保持高渲染质量。&lt;br&gt;   - 本文探讨了基于GS表示的两个场景之间的3D配准任务。&lt;br&gt;&lt;h4&gt;5. GaussReg框架&lt;/h4&gt;   - 提出了GaussReg，一种新颖的粗到细框架，快速且准确。&lt;br&gt;   - 粗略阶段遵循现有的点云配准方法，估计GS点云的粗略对齐。&lt;br&gt;&lt;h4&gt;6. 精细配准方法&lt;/h4&gt;   - 提出了图像引导的精细配准方法，从GS渲染图像以提供更详细的几何信息，实现精确对齐。&lt;br&gt;&lt;h4&gt;7. 数据集构建&lt;/h4&gt;   - 构建了一个名为ScanNet-GSReg的场景级数据集，包含来自ScanNet数据集的1379个场景，并收集了一个野外数据集GSReg。&lt;br&gt;&lt;h4&gt;8. 实验结果&lt;/h4&gt;   - 实验结果表明，所提方法在多个数据集上实现了最先进的性能。&lt;br&gt;   - GaussReg比HLoc（使用SuperPoint作为特征提取器和SuperGlue作为匹配器）快44倍，同时保持相似的准确性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-07-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud registration is a fundamental problem for large-scale 3D scene
scanning and reconstruction. With the help of deep learning, registration
methods have evolved significantly, reaching a nearly-mature stage. As the
introduction of Neural Radiance Fields (NeRF), it has become the most popular
3D scene representation as its powerful view synthesis capabilities. Regarding
NeRF representation, its registration is also required for large-scale scene
reconstruction. However, this topic extremly lacks exploration. This is due to
the inherent challenge to model the geometric relationship among two scenes
with implicit representations. The existing methods usually convert the
implicit representation to explicit representation for further registration.
Most recently, Gaussian Splatting (GS) is introduced, employing explicit 3D
Gaussian. This method significantly enhances rendering speed while maintaining
high rendering quality. Given two scenes with explicit GS representations, in
this work, we explore the 3D registration task between them. To this end, we
propose GaussReg, a novel coarse-to-fine framework, both fast and accurate. The
coarse stage follows existing point cloud registration methods and estimates a
rough alignment for point clouds from GS. We further newly present an
image-guided fine registration approach, which renders images from GS to
provide more detailed geometric information for precise alignment. To support
comprehensive evaluation, we carefully build a scene-level dataset called
ScanNet-GSReg with 1379 scenes obtained from the ScanNet dataset and collect an
in-the-wild dataset called GSReg. Experimental results demonstrate our method
achieves state-of-the-art performance on multiple datasets. Our GaussReg is 44
times faster than HLoc (SuperPoint as the feature extractor and SuperGlue as
the matcher) with comparable accuracy.</description>
      <guid isPermaLink="false">2407.05254v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Dynamic Fraud Detection: Integrating Reinforcement Learning into Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2409.09892v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 金融欺诈是通过不诚实手段获取财务利益的行为，严重扰乱金融市场秩序，对经济和社会发展造成危害，并滋生其他非法活动。&lt;br&gt;&lt;h4&gt;2. 问题提出&lt;/h4&gt;   - 随着互联网和在线支付的普及，许多欺诈和洗钱活动从线下转移到线上，给监管机构带来了巨大挑战。&lt;br&gt;   - 有效检测这些金融欺诈活动已成为亟待解决的问题。&lt;br&gt;&lt;h4&gt;3. 技术应用&lt;/h4&gt;   - 图神经网络（GNNs）是一种深度学习模型，能够利用图结构中的交互关系，广泛应用于欺诈检测领域。&lt;br&gt;&lt;h4&gt;4. 现存问题&lt;/h4&gt;   - **标签不平衡**：欺诈活动仅占交易转移的一小部分，导致欺诈检测中标签不平衡的问题。&lt;br&gt;   - **行为伪装**：欺诈者常常掩饰其行为，影响最终的预测结果。&lt;br&gt;   - **信息平衡**：现有研究忽视了邻居信息和平衡中心节点信息的重要性；当中心节点有过多邻居时，中心节点的特征可能被忽视。&lt;br&gt;   - **动态演变**：欺诈活动和模式随着时间不断变化，因此考虑图边关系的动态演变也非常重要。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Financial fraud refers to the act of obtaining financial benefits through
dishonest means. Such behavior not only disrupts the order of the financial
market but also harms economic and social development and breeds other illegal
and criminal activities. With the popularization of the internet and online
payment methods, many fraudulent activities and money laundering behaviors in
life have shifted from offline to online, posing a great challenge to
regulatory authorities. How to efficiently detect these financial fraud
activities has become an urgent issue that needs to be resolved. Graph neural
networks are a type of deep learning model that can utilize the interactive
relationships within graph structures, and they have been widely applied in the
field of fraud detection. However, there are still some issues. First,
fraudulent activities only account for a very small part of transaction
transfers, leading to an inevitable problem of label imbalance in fraud
detection. At the same time, fraudsters often disguise their behavior, which
can have a negative impact on the final prediction results. In addition,
existing research has overlooked the importance of balancing neighbor
information and central node information. For example, when the central node
has too many neighbors, the features of the central node itself are often
neglected. Finally, fraud activities and patterns are constantly changing over
time, so considering the dynamic evolution of graph edge relationships is also
very important.</description>
      <guid isPermaLink="false">2409.09892v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Quantum Multimodal Contrastive Learning Framework</title>
      <link>http://arxiv.org/abs/2408.13919v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究目标&lt;/h4&gt;   - 本文提出了一种新颖的多模态对比学习框架，利用量子编码器整合脑电图（EEG）和图像数据。&lt;br&gt;&lt;h4&gt;2. 创新尝试&lt;/h4&gt;   - 该研究首次探索在传统多模态学习框架中集成量子编码器。&lt;br&gt;&lt;h4&gt;3. 方法优势&lt;/h4&gt;   - 利用量子计算的独特属性，我们的方法增强了表示学习能力，为同时分析时间序列和视觉信息提供了强健的框架。&lt;br&gt;&lt;h4&gt;4. 效果验证&lt;/h4&gt;   - 实验表明，量子编码器能够有效捕捉EEG信号和图像特征中的复杂模式，从而促进跨模态的对比学习改进。&lt;br&gt;&lt;h4&gt;5. 应用前景&lt;/h4&gt;   - 本研究为量子计算与多模态数据分析的整合开辟了新途径，特别是在需要同时解释时间数据和视觉数据的应用中。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we propose a novel framework for multimodal contrastive
learning utilizing a quantum encoder to integrate EEG (electroencephalogram)
and image data. This groundbreaking attempt explores the integration of quantum
encoders within the traditional multimodal learning framework. By leveraging
the unique properties of quantum computing, our method enhances the
representation learning capabilities, providing a robust framework for
analyzing time series and visual information concurrently. We demonstrate that
the quantum encoder effectively captures intricate patterns within EEG signals
and image features, facilitating improved contrastive learning across
modalities. This work opens new avenues for integrating quantum computing with
multimodal data analysis, particularly in applications requiring simultaneous
interpretation of temporal and visual data.</description>
      <guid isPermaLink="false">2408.13919v3</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Multi-frequency Neural Born Iterative Method for Solving 2-D Inverse Scattering Problems</title>
      <link>http://arxiv.org/abs/2409.01315v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究目标&lt;/h4&gt;   - 本研究提出了一种基于深度学习的成像方法，用于解决多频电磁（EM）逆散射问题（ISP）。&lt;br&gt;&lt;h4&gt;2. 方法创新&lt;/h4&gt;   - 结合深度学习技术与电磁物理规律，成功开发了多频神经Born迭代方法（NeuralBIM），该方法以单频NeuralBIM的原则为指导。&lt;br&gt;&lt;h4&gt;3. 技术集成&lt;/h4&gt;   - 此方法整合了多任务学习技术与NeuralBIM高效的迭代反演过程，构建了一个强健的多频Born迭代反演模型。&lt;br&gt;&lt;h4&gt;4. 训练过程&lt;/h4&gt;   - 模型在训练中采用多任务学习方法，利用均匀不确定性自适应分配各频率数据的权重。&lt;br&gt;   - 采用无监督学习方法，受ISP的物理规律约束，训练多频NeuralBIM模型，无需对比数据和总场数据。&lt;br&gt;&lt;h4&gt;5. 效果验证&lt;/h4&gt;   - 通过合成数据和实验数据验证了多频NeuralBIM的有效性，显示出在解决ISP时在准确性和计算效率上的提升。&lt;br&gt;&lt;h4&gt;6. 模型优势&lt;/h4&gt;   - 该方法表现出强大的泛化能力和抗噪声能力。&lt;br&gt;&lt;h4&gt;7. 应用前景&lt;/h4&gt;   - 多频NeuralBIM方法探索了一种新颖的多频电磁数据反演方法，为多频数据的电磁逆散射问题提供了有效解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this work, we propose a deep learning-based imaging method for addressing
the multi-frequency electromagnetic (EM) inverse scattering problem (ISP). By
combining deep learning technology with EM physical laws, we have successfully
developed a multi-frequency neural Born iterative method (NeuralBIM), guided by
the principles of the single-frequency NeuralBIM. This method integrates
multitask learning techniques with NeuralBIM's efficient iterative inversion
process to construct a robust multi-frequency Born iterative inversion model.
During training, the model employs a multitask learning approach guided by
homoscedastic uncertainty to adaptively allocate the weights of each
frequency's data. Additionally, an unsupervised learning method, constrained by
the physical laws of ISP, is used to train the multi-frequency NeuralBIM model,
eliminating the need for contrast and total field data. The effectiveness of
the multi-frequency NeuralBIM is validated through synthetic and experimental
data, demonstrating improvements in accuracy and computational efficiency for
solving ISP. Moreover, this method exhibits strong generalization capabilities
and noise resistance. The multi-frequency NeuralBIM method explores a novel
inversion method for multi-frequency EM data and provides an effective solution
for the electromagnetic ISP of multi-frequency data.</description>
      <guid isPermaLink="false">2409.01315v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>VI3DRM:Towards meticulous 3D Reconstruction from Sparse Views via Photo-Realistic Novel View Synthesis</title>
      <link>http://arxiv.org/abs/2409.08207v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 最近，像Zero-1-2-3这样的单视图3D重建方法取得了显著成功。&lt;br&gt;   - 然而，它们对未见区域的预测高度依赖于大规模预训练扩散模型的归纳偏置。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 后续工作如DreamComposer试图通过结合额外视图使预测更可控，但由于原始潜在空间中的特征纠缠（包括光照、材料和结构等因素），结果仍不够真实。&lt;br&gt;&lt;h4&gt;3. 创新提出&lt;/h4&gt;   - 本文提出了视觉各向同性3D重建模型（VI3DRM），这是一种基于扩散的稀疏视图3D重建模型，操作在一个一致的ID和透视解耦的3D潜在空间内。&lt;br&gt;&lt;h4&gt;4. 模型优势&lt;/h4&gt;   - VI3DRM通过促进语义信息、颜色、材料属性和光照的解耦，能够生成高度真实的图像，几乎无法与真实照片区分。&lt;br&gt;&lt;h4&gt;5. 点图构建&lt;/h4&gt;   - 利用真实和合成图像，我们的方法能够准确构建点图，从而最终生成精细纹理的网格或点云。&lt;br&gt;&lt;h4&gt;6. 实验结果&lt;/h4&gt;   - 在GSO数据集的NVS任务中，VI3DRM显著超越了最先进的DreamComposer方法，达到了38.61的PSNR、0.929的SSIM和0.027的LPIPS。&lt;br&gt;&lt;h4&gt;7. 代码发布&lt;/h4&gt;   - 代码将在论文发表后提供。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, methods like Zero-1-2-3 have focused on single-view based 3D
reconstruction and have achieved remarkable success. However, their predictions
for unseen areas heavily rely on the inductive bias of large-scale pretrained
diffusion models. Although subsequent work, such as DreamComposer, attempts to
make predictions more controllable by incorporating additional views, the
results remain unrealistic due to feature entanglement in the vanilla latent
space, including factors such as lighting, material, and structure. To address
these issues, we introduce the Visual Isotropy 3D Reconstruction Model
(VI3DRM), a diffusion-based sparse views 3D reconstruction model that operates
within an ID consistent and perspective-disentangled 3D latent space. By
facilitating the disentanglement of semantic information, color, material
properties and lighting, VI3DRM is capable of generating highly realistic
images that are indistinguishable from real photographs. By leveraging both
real and synthesized images, our approach enables the accurate construction of
pointmaps, ultimately producing finely textured meshes or point clouds. On the
NVS task, tested on the GSO dataset, VI3DRM significantly outperforms
state-of-the-art method DreamComposer, achieving a PSNR of 38.61, an SSIM of
0.929, and an LPIPS of 0.027. Code will be made available upon publication.</description>
      <guid isPermaLink="false">2409.08207v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>3D Siamese Voxel-to-BEV Tracker for Sparse Point Clouds</title>
      <link>http://arxiv.org/abs/2111.04426v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2021&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 在动态环境中，3D点云中的物体跟踪仍然是一个具有挑战性的问题，主要由于LiDAR点的稀疏性。&lt;br&gt;&lt;h4&gt;2. 研究目标&lt;/h4&gt;   - 本研究提出了一种“孪生体素到鸟瞰图（BEV）跟踪器”，旨在显著提升稀疏3D点云中的跟踪性能。&lt;br&gt;&lt;h4&gt;3. 网络架构&lt;/h4&gt;   - 该系统包括两个主要部分：&lt;br&gt;     - **孪生形状感知特征学习网络**：用于捕获物体的3D形状信息，学习物体的区分特征，以便在稀疏点云中识别潜在目标。&lt;br&gt;     - **体素到BEV目标定位网络**：负责从密集的鸟瞰图特征图中进行目标定位。&lt;br&gt;&lt;h4&gt;4. 特征嵌入过程&lt;/h4&gt;   - 首先进行模板特征嵌入，将模板特征嵌入潜在目标中，然后生成一个密集的3D形状，以表征潜在目标的形状信息。&lt;br&gt;&lt;h4&gt;5. 目标定位方法&lt;/h4&gt;   - 体素到BEV目标定位网络以无锚点的方式回归目标的2D中心和$z$轴中心。&lt;br&gt;   - 通过沿$z$轴进行最大池化，压缩体素化点云以获得密集的BEV特征图，从而提高2D中心和$z$轴中心的回归效率。&lt;br&gt;&lt;h4&gt;6. 实验结果&lt;/h4&gt;   - 在KITTI和nuScenes数据集上进行了广泛评估，结果表明该方法在性能上显著超越当前最先进的方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2021-11-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/fpthink/v2b&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D object tracking in point clouds is still a challenging problem due to the
sparsity of LiDAR points in dynamic environments. In this work, we propose a
Siamese voxel-to-BEV tracker, which can significantly improve the tracking
performance in sparse 3D point clouds. Specifically, it consists of a Siamese
shape-aware feature learning network and a voxel-to-BEV target localization
network. The Siamese shape-aware feature learning network can capture 3D shape
information of the object to learn the discriminative features of the object so
that the potential target from the background in sparse point clouds can be
identified. To this end, we first perform template feature embedding to embed
the template's feature into the potential target and then generate a dense 3D
shape to characterize the shape information of the potential target. For
localizing the tracked target, the voxel-to-BEV target localization network
regresses the target's 2D center and the $z$-axis center from the dense bird's
eye view (BEV) feature map in an anchor-free manner. Concretely, we compress
the voxelized point cloud along $z$-axis through max pooling to obtain a dense
BEV feature map, where the regression of the 2D center and the $z$-axis center
can be performed more effectively. Extensive evaluation on the KITTI and
nuScenes datasets shows that our method significantly outperforms the current
state-of-the-art methods by a large margin.</description>
      <guid isPermaLink="false">2111.04426v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Entropy Loss: An Interpretability Amplifier of 3D Object Detection Network for Intelligent Driving</title>
      <link>http://arxiv.org/abs/2409.00839v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 随着交通环境的复杂性增加，智能驾驶中的安全感知变得愈发重要。&lt;br&gt;   - 传统的智能驾驶感知方法依赖深度学习，但其可解释性有限，常被描述为“黑箱”。&lt;br&gt;&lt;h4&gt;2. 创新提出&lt;/h4&gt;   - 论文引入了一种新型损失函数，称为“熵损失”（Entropy Loss），以及一种创新的训练策略。&lt;br&gt;&lt;h4&gt;3. 熵损失的构建&lt;/h4&gt;   - 熵损失是基于感知模型中特征压缩网络的功能进行公式化的。&lt;br&gt;   - 受到通信系统的启发，特征压缩网络中的信息传输过程应表现出信息量的稳步变化和信息熵的持续减少。&lt;br&gt;&lt;h4&gt;4. 模型构建&lt;/h4&gt;   - 将网络层输出建模为连续随机变量，构建一个概率模型，以量化信息量的变化。&lt;br&gt;   - 根据这些期望导出熵损失，指导网络参数的更新，从而增强网络的可解释性。&lt;br&gt;&lt;h4&gt;5. 实验结果&lt;/h4&gt;   - 实验表明，熵损失训练策略加速了训练过程。&lt;br&gt;   - 在相同的60个训练周期内，使用熵损失的3D物体检测模型在KITTI测试集上的准确性提高了最多4.47%，相比于未使用熵损失的模型，显示了该方法的有效性。&lt;br&gt;&lt;h4&gt;6. 代码发布&lt;/h4&gt;   - 实现代码可在指定的GitHub链接获取。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/yhbcode000/Eloss-Interpretability&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the increasing complexity of the traffic environment, the significance
of safety perception in intelligent driving is intensifying. Traditional
methods in the field of intelligent driving perception rely on deep learning,
which suffers from limited interpretability, often described as a "black box."
This paper introduces a novel type of loss function, termed "Entropy Loss,"
along with an innovative training strategy. Entropy Loss is formulated based on
the functionality of feature compression networks within the perception model.
Drawing inspiration from communication systems, the information transmission
process in a feature compression network is expected to demonstrate steady
changes in information volume and a continuous decrease in information entropy.
By modeling network layer outputs as continuous random variables, we construct
a probabilistic model that quantifies changes in information volume. Entropy
Loss is then derived based on these expectations, guiding the update of network
parameters to enhance network interpretability. Our experiments indicate that
the Entropy Loss training strategy accelerates the training process. Utilizing
the same 60 training epochs, the accuracy of 3D object detection models using
Entropy Loss on the KITTI test set improved by up to 4.47\% compared to models
without Entropy Loss, underscoring the method's efficacy. The implementation
code is available at
\url{https://github.com/yhbcode000/Eloss-Interpretability}.</description>
      <guid isPermaLink="false">2409.00839v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Task Arithmetic for Language Expansion in Speech Translation</title>
      <link>http://arxiv.org/abs/2409.11274v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 最近，大型语言模型（LLMs）的进展引发了对语音-文本多模态基础模型的关注，这些模型在基于指令的语音翻译（ST）上表现出色。&lt;br&gt;&lt;h4&gt;2. 现有系统的局限性&lt;/h4&gt;   - 从已有的指令调优的ST系统扩展语言对成本高，需要重新训练新旧数据集的组合。&lt;br&gt;&lt;h4&gt;3. 研究创新&lt;/h4&gt;   - 提出通过任务算术合并新语言对训练的模型和现有模型，以实现语言对扩展。&lt;br&gt;&lt;h4&gt;4. 问题识别&lt;/h4&gt;   - 直接应用任务算术进行ST时，合并模型无法正确遵循指令，导致生成错误语言的翻译。&lt;br&gt;&lt;h4&gt;5. 解决方案&lt;/h4&gt;   - 提出增强的任务算术方法，合并额外的语言控制模型，确保生成正确的目标语言标记。&lt;br&gt;&lt;h4&gt;6. 实验验证&lt;/h4&gt;   - 实验表明，语言控制模型可以消除语言混淆，从而实现语言扩展。&lt;br&gt;   - 在MuST-C和CoVoST-2实验中，分别提高了最多4.66和4.92的BLEU分数。&lt;br&gt;&lt;h4&gt;7. 方法的适用性&lt;/h4&gt;   - 证明了该任务算术框架可以扩展到没有配对ST训练数据或预训练ST模型的语言对。&lt;br&gt;   - 首先通过任务类比从机器翻译（MT）系统合成ST系统，然后将合成的ST系统与现有ST模型合并。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in large language models (LLMs) have gained interest in
speech-text multimodal foundation models, achieving strong performance on
instruction-based speech translation (ST). However, expanding language pairs
from an existing instruction-tuned ST system is costly due to the necessity of
re-training on a combination of new and previous datasets. We propose to expand
new language pairs by merging the model trained on new language pairs and the
existing model, using task arithmetic. We find that the direct application of
task arithmetic for ST causes the merged model to fail to follow instructions;
thus, generating translation in incorrect languages. To eliminate language
confusion, we propose an augmented task arithmetic method that merges an
additional language control model. It is trained to generate the correct target
language token following the instructions. Our experiments demonstrate that our
proposed language control model can achieve language expansion by eliminating
language confusion. In our MuST-C and CoVoST-2 experiments, it shows up to 4.66
and 4.92 BLEU scores improvement, respectively. In addition, we demonstrate the
use of our task arithmetic framework can expand to a language pair where
neither paired ST training data nor a pre-trained ST model is available. We
first synthesize the ST system from machine translation (MT) systems via task
analogy, then merge the synthesized ST system to the existing ST model.</description>
      <guid isPermaLink="false">2409.11274v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Learning to Learn Transferable Generative Attack for Person Re-Identification</title>
      <link>http://arxiv.org/abs/2409.04208v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 基于深度学习的人体重识别（re-id）模型广泛应用于监控系统，但这些模型容易受到对抗攻击的影响。&lt;br&gt;&lt;h4&gt;2. 现有攻击方法的局限性&lt;/h4&gt;   - 现有攻击方法仅考虑跨数据集和跨模型的可转移性，忽略了对不同领域训练模型的跨测试能力。&lt;br&gt;&lt;h4&gt;3. 研究创新&lt;/h4&gt;   - 提出Meta Transferable Generative Attack（MTGA）方法，通过元学习优化，促进生成攻击者产生高度可转移的对抗示例。&lt;br&gt;&lt;h4&gt;4. 攻击任务的设计&lt;/h4&gt;   - 在元训练和元测试攻击过程中，模拟跨模型和跨数据集的黑箱攻击任务，通过选择不同的re-id模型和数据集。&lt;br&gt;&lt;h4&gt;5. 特征干扰机制&lt;/h4&gt;   - 设计了扰动随机擦除模块，以防止攻击者仅学习破坏特定模型的特征。&lt;br&gt;&lt;h4&gt;6. 跨测试可转移性增强&lt;/h4&gt;   - 引入标准化混合策略，通过混合目标模型的多域统计来模仿多样的特征嵌入空间，从而增强攻击者的学习能力。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 大量实验表明，MTGA在跨模型和跨数据集攻击以及跨模型、数据集和测试攻击中表现优越。&lt;br&gt;   - MTGA在平均mAP下降率上分别超越现有最先进的方法21.5%和11.3%。&lt;br&gt;&lt;h4&gt;8. 代码发布计划&lt;/h4&gt;   - MTGA的代码将在论文接受后公开发布。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning-based person re-identification (re-id) models are widely
employed in surveillance systems and inevitably inherit the vulnerability of
deep networks to adversarial attacks. Existing attacks merely consider
cross-dataset and cross-model transferability, ignoring the cross-test
capability to perturb models trained in different domains. To powerfully
examine the robustness of real-world re-id models, the Meta Transferable
Generative Attack (MTGA) method is proposed, which adopts meta-learning
optimization to promote the generative attacker producing highly transferable
adversarial examples by learning comprehensively simulated transfer-based
cross-model\&amp;dataset\&amp;test black-box meta attack tasks. Specifically,
cross-model\&amp;dataset black-box attack tasks are first mimicked by selecting
different re-id models and datasets for meta-train and meta-test attack
processes. As different models may focus on different feature regions, the
Perturbation Random Erasing module is further devised to prevent the attacker
from learning to only corrupt model-specific features. To boost the attacker
learning to possess cross-test transferability, the Normalization Mix strategy
is introduced to imitate diverse feature embedding spaces by mixing
multi-domain statistics of target models. Extensive experiments show the
superiority of MTGA, especially in cross-model\&amp;dataset and
cross-model\&amp;dataset\&amp;test attacks, our MTGA outperforms the SOTA methods by
21.5\% and 11.3\% on mean mAP drop rate, respectively. The code of MTGA will be
released after the paper is accepted.</description>
      <guid isPermaLink="false">2409.04208v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Meta-learning curiosity algorithms</title>
      <link>http://arxiv.org/abs/2003.05325v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published in ICLR 2020&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究假设&lt;/h4&gt;   - 假设好奇心是进化过程中发展的机制，旨在促进智能体在早期进行有意义的探索，从而获得能够在其一生中带来高奖励的经验。&lt;br&gt;&lt;h4&gt;2. 问题形式化&lt;/h4&gt;   - 将生成好奇行为的问题视为元学习（meta-learning）问题：&lt;br&gt;     - 外层循环搜索动态适应智能体奖励信号的好奇机制。&lt;br&gt;     - 内层循环使用适应后的奖励信号进行标准强化学习。&lt;br&gt;&lt;h4&gt;3. 现有方法的局限性&lt;/h4&gt;   - 当前基于转移神经网络权重的元强化学习方法仅在非常相似的任务之间实现了泛化。&lt;br&gt;&lt;h4&gt;4. 研究创新&lt;/h4&gt;   - 提出元学习算法的方法：类似于人类在机器学习论文中设计的代码片段。&lt;br&gt;   - 利用丰富的程序语言，结合神经网络、缓冲区、最近邻模块和自定义损失函数等构建块。&lt;br&gt;&lt;h4&gt;5. 实验验证&lt;/h4&gt;   - 实证研究展示了该方法的有效性，发现了两种新颖的好奇算法。&lt;br&gt;   - 这些算法在多个领域（如网格导航、倒立摆、月球着陆器、蚂蚁和跳跃者）上的表现与人类设计的已发表好奇算法相当或更好。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2020-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/mfranzs/meta-learning-curiosity-algorithms&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We hypothesize that curiosity is a mechanism found by evolution that
encourages meaningful exploration early in an agent's life in order to expose
it to experiences that enable it to obtain high rewards over the course of its
lifetime. We formulate the problem of generating curious behavior as one of
meta-learning: an outer loop will search over a space of curiosity mechanisms
that dynamically adapt the agent's reward signal, and an inner loop will
perform standard reinforcement learning using the adapted reward signal.
However, current meta-RL methods based on transferring neural network weights
have only generalized between very similar tasks. To broaden the
generalization, we instead propose to meta-learn algorithms: pieces of code
similar to those designed by humans in ML papers. Our rich language of programs
combines neural networks with other building blocks such as buffers,
nearest-neighbor modules and custom loss functions. We demonstrate the
effectiveness of the approach empirically, finding two novel curiosity
algorithms that perform on par or better than human-designed published
curiosity algorithms in domains as disparate as grid navigation with image
inputs, acrobot, lunar lander, ant and hopper.</description>
      <guid isPermaLink="false">2003.05325v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Multi-object event graph representation learning for Video Question Answering</title>
      <link>http://arxiv.org/abs/2409.07747v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  presented at MIRU2024&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 视频问答（VideoQA）任务是根据给定视频回答问题。&lt;br&gt;   - 系统需要理解视频中对象之间的空间和时间关系，以进行因果和时间推理。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 先前的研究主要使用基于变换器的方法建模单个对象的运动，但在处理涉及多个对象的复杂场景时表现不佳（例如：“一个男孩把球扔进篮筐”）。&lt;br&gt;&lt;h4&gt;3. 研究创新&lt;/h4&gt;   - 提出了一种名为CLanG的对比语言事件图表示学习方法，以解决上述限制。&lt;br&gt;   - 该方法旨在捕捉与多个对象相关的事件表示，采用多层GNN聚类模块进行对抗图表示学习。&lt;br&gt;&lt;h4&gt;4. 对比学习机制&lt;/h4&gt;   - 实现问题文本与其相关的多对象事件图之间的对比学习，以提高模型的理解能力。&lt;br&gt;&lt;h4&gt;5. 实验结果&lt;/h4&gt;   - 在两个具有挑战性的视频问答数据集（NExT-QA和TGIF-QA-R）上，CLanG在准确性上比强基线高出最多2.2%。&lt;br&gt;   - 特别是在处理因果和时间问题时，CLanG比基线高出2.8%，突出其在多对象事件推理方面的优势。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video question answering (VideoQA) is a task to predict the correct answer to
questions posed about a given video. The system must comprehend spatial and
temporal relationships among objects extracted from videos to perform causal
and temporal reasoning. While prior works have focused on modeling individual
object movements using transformer-based methods, they falter when capturing
complex scenarios involving multiple objects (e.g., "a boy is throwing a ball
in a hoop"). We propose a contrastive language event graph representation
learning method called CLanG to address this limitation. Aiming to capture
event representations associated with multiple objects, our method employs a
multi-layer GNN-cluster module for adversarial graph representation learning,
enabling contrastive learning between the question text and its relevant
multi-object event graph. Our method outperforms a strong baseline, achieving
up to 2.2% higher accuracy on two challenging VideoQA datasets, NExT-QA and
TGIF-QA-R. In particular, it is 2.8% better than baselines in handling causal
and temporal questions, highlighting its strength in reasoning multiple
object-based events.</description>
      <guid isPermaLink="false">2409.07747v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Multi-intent Aware Contrastive Learning for Sequential Recommendation</title>
      <link>http://arxiv.org/abs/2409.08733v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 用户意图是影响用户与物品交互序列的重要潜在因素。&lt;br&gt;&lt;h4&gt;2. 现有模型问题&lt;/h4&gt;   - 目前的序列推荐模型多采用对比学习，主要依赖单一意图表示指导训练过程。&lt;br&gt;   - 这种方法过于简化了现实世界中的推荐场景，试图用单一意图表示来概括意图的多样性。&lt;br&gt;&lt;h4&gt;3. 研究目标&lt;/h4&gt;   - 提出考虑多重意图信息的序列推荐模型，以更准确地反映现实生活中的推荐场景。&lt;br&gt;&lt;h4&gt;4. 研究意义&lt;/h4&gt;   - 通过引入多意图信息，可以提高推荐模型的表现，并更好地适应用户的复杂需求。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Intent is a significant latent factor influencing user-item interaction
sequences. Prevalent sequence recommendation models that utilize contrastive
learning predominantly rely on single-intent representations to direct the
training process. However, this paradigm oversimplifies real-world
recommendation scenarios, attempting to encapsulate the diversity of intents
within the single-intent level representation. SR models considering
multi-intent information in their framework are more likely to reflect
real-life recommendation scenarios accurately.</description>
      <guid isPermaLink="false">2409.08733v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>LCD: Learned Cross-Domain Descriptors for 2D-3D Matching</title>
      <link>http://arxiv.org/abs/1911.09326v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to AAAI 2020 (Oral)&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究目标&lt;/h4&gt;   - 提出一种新方法，用于学习2D图像和3D点云匹配的局部跨域描述符。&lt;br&gt;&lt;h4&gt;2. 方法概述&lt;/h4&gt;   - 采用双自动编码器神经网络，将2D和3D输入映射到共享的潜在空间表示。&lt;br&gt;&lt;h4&gt;3. 描述符优势&lt;/h4&gt;   - 共享嵌入中的局部跨域描述符比单独在2D和3D领域训练得到的描述符更具判别力。&lt;br&gt;&lt;h4&gt;4. 数据集构建&lt;/h4&gt;   - 收集了约140万对2D-3D对应关系，涵盖不同光照条件和设置，基于公开的RGB-D场景构建新数据集。&lt;br&gt;&lt;h4&gt;5. 实验评估&lt;/h4&gt;   - 在三个主要实验中评估描述符的性能：&lt;br&gt;     - 2D-3D匹配&lt;br&gt;     - 跨域检索&lt;br&gt;     - 稀疏到密集的深度估计&lt;br&gt;&lt;h4&gt;6. 实验结果&lt;/h4&gt;   - 实验结果确认了方法的鲁棒性和竞争力，能够有效解决跨域任务，并且能泛化到单独的2D和3D任务。&lt;br&gt;&lt;h4&gt;7. 公开资源&lt;/h4&gt;   - 数据集和代码已公开发布，链接为：[LCD Dataset](https://hkust-vgd.github.io/lcd)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2019-11-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/hkust-vgd/lcd&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this work, we present a novel method to learn a local cross-domain
descriptor for 2D image and 3D point cloud matching. Our proposed method is a
dual auto-encoder neural network that maps 2D and 3D input into a shared latent
space representation. We show that such local cross-domain descriptors in the
shared embedding are more discriminative than those obtained from individual
training in 2D and 3D domains. To facilitate the training process, we built a
new dataset by collecting $\approx 1.4$ millions of 2D-3D correspondences with
various lighting conditions and settings from publicly available RGB-D scenes.
Our descriptor is evaluated in three main experiments: 2D-3D matching,
cross-domain retrieval, and sparse-to-dense depth estimation. Experimental
results confirm the robustness of our approach as well as its competitive
performance not only in solving cross-domain tasks but also in being able to
generalize to solve sole 2D and 3D tasks. Our dataset and code are released
publicly at \url{https://hkust-vgd.github.io/lcd}.</description>
      <guid isPermaLink="false">1911.09326v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>CurbNet: Curb Detection Framework Based on LiDAR Point Cloud Segmentation</title>
      <link>http://arxiv.org/abs/2403.16794v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 路缘检测在智能驾驶中至关重要，有助于确定可行驶区域。&lt;br&gt;   - 道路环境的复杂性使得路缘检测面临挑战。&lt;br&gt;&lt;h4&gt;2. 研究目标&lt;/h4&gt;   - 介绍CurbNet，一个基于点云分割的新的路缘检测框架。&lt;br&gt;&lt;h4&gt;3. 数据集开发&lt;/h4&gt;   - 针对缺乏全面的3D注释路缘数据集，开发了3D-Curb数据集，基于SemanticKITTI，成为最大的路缘点云集合。&lt;br&gt;&lt;h4&gt;4. 方法创新&lt;/h4&gt;   - 利用高度变化这一路缘的主要特征，采用丰富的3D点云进行训练。&lt;br&gt;   - 引入多尺度和通道注意力（MSCA）模块，旨在优化检测性能，解决xy平面上路缘特征分布不均和z轴高频特征依赖的问题。&lt;br&gt;&lt;h4&gt;5. 损失函数设计&lt;/h4&gt;   - 提出自适应加权损失函数组，以应对路缘点云相对于其他类别分布的不平衡。&lt;br&gt;&lt;h4&gt;6. 实验结果&lt;/h4&gt;   - 在两个主要数据集上进行广泛实验，结果表明该方法超越了现有的路缘检测和点云分割模型基准。&lt;br&gt;   - 通过后处理精炼检测结果，显著减少了路缘检测中的噪声，提高了精度4.5个点。&lt;br&gt;&lt;h4&gt;7. 鲁棒性验证&lt;/h4&gt;   - 容错实验也达到了最先进的结果，进一步验证了CurbNet的优越检测能力和强大的泛化能力。&lt;br&gt;&lt;h4&gt;8. 项目链接&lt;/h4&gt;   - 项目网站可访问：[CurbNet GitHub](https://github.com/guoyangzhao/CurbNet/)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-03-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/guoyangzhao/curbnet&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Curb detection is a crucial function in intelligent driving, essential for
determining drivable areas on the road. However, the complexity of road
environments makes curb detection challenging. This paper introduces CurbNet, a
novel framework for curb detection utilizing point cloud segmentation. To
address the lack of comprehensive curb datasets with 3D annotations, we have
developed the 3D-Curb dataset based on SemanticKITTI, currently the largest and
most diverse collection of curb point clouds. Recognizing that the primary
characteristic of curbs is height variation, our approach leverages spatially
rich 3D point clouds for training. To tackle the challenges posed by the uneven
distribution of curb features on the xy-plane and their dependence on
high-frequency features along the z-axis, we introduce the Multi-Scale and
Channel Attention (MSCA) module, a customized solution designed to optimize
detection performance. Additionally, we propose an adaptive weighted loss
function group specifically formulated to counteract the imbalance in the
distribution of curb point clouds relative to other categories. Extensive
experiments conducted on 2 major datasets demonstrate that our method surpasses
existing benchmarks set by leading curb detection and point cloud segmentation
models. Through the post-processing refinement of the detection results, we
have significantly reduced noise in curb detection, thereby improving precision
by 4.5 points. Similarly, our tolerance experiments also achieved
state-of-the-art results. Furthermore, real-world experiments and dataset
analyses mutually validate each other, reinforcing CurbNet's superior detection
capability and robust generalizability. The project website is available at:
https://github.com/guoyangzhao/CurbNet/.</description>
      <guid isPermaLink="false">2403.16794v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>VET: Visual Error Tomography for Point Cloud Completion and High-Quality Neural Rendering</title>
      <link>http://arxiv.org/abs/2311.04634v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 近年来，深度神经网络在新视图合成方面取得了重大进展。&lt;br&gt;   - 许多方法基于运动结构算法获得的（粗略）代理几何体。&lt;br&gt;&lt;h4&gt;2. 存在的问题&lt;/h4&gt;   - 代理几何中的小缺陷可以通过神经渲染修复，但薄结构或光滑区域常见的大孔或缺失部分仍会导致干扰性伪影和时间不稳定性。&lt;br&gt;&lt;h4&gt;3. 研究目标&lt;/h4&gt;   - 提出一种新颖的基于神经渲染的方法，用于检测和修复这些缺陷。&lt;br&gt;&lt;h4&gt;4. 代理几何&lt;/h4&gt;   - 使用点云作为代理，便于去除异常几何体并填补缺失几何体，而无需复杂的拓扑操作。&lt;br&gt;&lt;h4&gt;5. 方法关键&lt;/h4&gt;   - **(i)** 可微分的混合点渲染器，能够融合冗余点。&lt;br&gt;   - **(ii)** 视觉误差计算层（VET），将2D误差图提升至3D区域，以识别缺失几何的区域并生成新点。&lt;br&gt;   - **(iii)** 通过将点作为嵌套环境图添加，允许在同一管道中生成高质量的周围环境渲染。&lt;br&gt;&lt;h4&gt;6. 实验结果&lt;/h4&gt;   - 该方法显著提高了通过运动结构获得的点云的质量，进而显著提升新视图合成质量。&lt;br&gt;   - 相较于点生长技术，该方法能够有效修复大规模孔洞和缺失的薄结构。&lt;br&gt;&lt;h4&gt;7. 性能表现&lt;/h4&gt;   - 渲染质量超越了最新的其他方法，时间稳定性显著提高，且渲染能够实现实时帧率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2023-11-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3610548.3618212&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/lfranke/vet&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the last few years, deep neural networks opened the doors for big advances
in novel view synthesis. Many of these approaches are based on a (coarse) proxy
geometry obtained by structure from motion algorithms. Small deficiencies in
this proxy can be fixed by neural rendering, but larger holes or missing parts,
as they commonly appear for thin structures or for glossy regions, still lead
to distracting artifacts and temporal instability. In this paper, we present a
novel neural-rendering-based approach to detect and fix such deficiencies. As a
proxy, we use a point cloud, which allows us to easily remove outlier geometry
and to fill in missing geometry without complicated topological operations.
Keys to our approach are (i) a differentiable, blending point-based renderer
that can blend out redundant points, as well as (ii) the concept of Visual
Error Tomography (VET), which allows us to lift 2D error maps to identify
3D-regions lacking geometry and to spawn novel points accordingly. Furthermore,
(iii) by adding points as nested environment maps, our approach allows us to
generate high-quality renderings of the surroundings in the same pipeline. In
our results, we show that our approach can improve the quality of a point cloud
obtained by structure from motion and thus increase novel view synthesis
quality significantly. In contrast to point growing techniques, the approach
can also fix large-scale holes and missing thin structures effectively.
Rendering quality outperforms state-of-the-art methods and temporal stability
is significantly improved, while rendering is possible at real-time frame
rates.</description>
      <guid isPermaLink="false">2311.04634v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Generalizability of Graph Neural Network Force Fields for Predicting Solid-State Properties</title>
      <link>http://arxiv.org/abs/2409.09931v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 7 figures&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 机器学习力场（MLFFs）提供了一种计算效率高的替代方案，用于复杂分子系统的从头算（ab initio）模拟。&lt;br&gt;&lt;h4&gt;2. 研究重要性&lt;/h4&gt;   - 确保MLFFs在训练数据之外的泛化能力对其在固体材料研究中的广泛应用至关重要。&lt;br&gt;&lt;h4&gt;3. 研究目的&lt;/h4&gt;   - 调查基于图神经网络（GNN）的MLFF在未明确包含的固态现象描述能力，尤其是在Lennard-Jones Argon上训练的模型。&lt;br&gt;&lt;h4&gt;4. 性能评估&lt;/h4&gt;   - 评估MLFF在预测完美面心立方（FCC）晶体结构的声子态密度（PDOS）方面的表现，分析零温和有限温度下的情况。&lt;br&gt;   - 评估不完美晶体中的空位迁移率和能量障碍，使用直接分子动力学（MD）模拟和弦法。&lt;br&gt;&lt;h4&gt;5. 训练数据的局限性&lt;/h4&gt;   - 训练数据中未包含空位配置，突出MLFF在处理未见配置时的能力。&lt;br&gt;&lt;h4&gt;6. 研究结果&lt;/h4&gt;   - 结果表明，MLFF能够捕捉固态材料的基本属性，与参考数据有良好的一致性。&lt;br&gt;&lt;h4&gt;7. 数据工程策略&lt;/h4&gt;   - 讨论了增强MLFF泛化能力的数据工程策略。&lt;br&gt;&lt;h4&gt;8. 方法论贡献&lt;/h4&gt;   - 提出的基准测试集和评估MLFF在描述完美与不完美晶体性能的工作流程，为MLFF在复杂固态材料研究中的可靠应用奠定基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine-learned force fields (MLFFs) promise to offer a computationally
efficient alternative to ab initio simulations for complex molecular systems.
However, ensuring their generalizability beyond training data is crucial for
their wide application in studying solid materials. This work investigates the
ability of a graph neural network (GNN)-based MLFF, trained on Lennard-Jones
Argon, to describe solid-state phenomena not explicitly included during
training. We assess the MLFF's performance in predicting phonon density of
states (PDOS) for a perfect face-centered cubic (FCC) crystal structure at both
zero and finite temperatures. Additionally, we evaluate vacancy migration rates
and energy barriers in an imperfect crystal using direct molecular dynamics
(MD) simulations and the string method. Notably, vacancy configurations were
absent from the training data. Our results demonstrate the MLFF's capability to
capture essential solid-state properties with good agreement to reference data,
even for unseen configurations. We further discuss data engineering strategies
to enhance the generalizability of MLFFs. The proposed set of benchmark tests
and workflow for evaluating MLFF performance in describing perfect and
imperfect crystals pave the way for reliable application of MLFFs in studying
complex solid-state materials.</description>
      <guid isPermaLink="false">2409.09931v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>GeoNLF: Geometry guided Pose-Free Neural LiDAR Fields</title>
      <link>http://arxiv.org/abs/2407.05597v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 最近的研究将神经辐射场（NeRF）扩展到LiDAR点云合成，但大多数现有工作对预先计算的姿态依赖性强。&lt;br&gt;&lt;h4&gt;2. 问题陈述&lt;/h4&gt;   - 点云配准方法难以实现精确的全局姿态估计，而之前无姿态的NeRF忽视了全局重建中的几何一致性。&lt;br&gt;&lt;h4&gt;3. 研究目标&lt;/h4&gt;   - 探索点云的几何特征，提供显式的配准先验信息以改善重建。&lt;br&gt;&lt;h4&gt;4. 提出的方法&lt;/h4&gt;   - 提出了几何引导的神经LiDAR场（GeoNLF），这是一个混合框架，交替进行全局神经重建和纯几何姿态优化。&lt;br&gt;&lt;h4&gt;5. 问题解决方案&lt;/h4&gt;   - 针对NeRF在稀疏视图输入下容易过拟合个别帧并陷入局部最小值的问题，开发了选择性重加权策略，并引入几何约束以实现稳健优化。&lt;br&gt;&lt;h4&gt;6. 实验结果&lt;/h4&gt;   - 在NuScenes和KITTI-360数据集上进行了广泛实验，结果证明GeoNLF在新视图合成和低频大规模点云的多视图配准方面具有显著优势。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-07-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although recent efforts have extended Neural Radiance Fields (NeRF) into
LiDAR point cloud synthesis, the majority of existing works exhibit a strong
dependence on precomputed poses. However, point cloud registration methods
struggle to achieve precise global pose estimation, whereas previous pose-free
NeRFs overlook geometric consistency in global reconstruction. In light of
this, we explore the geometric insights of point clouds, which provide explicit
registration priors for reconstruction. Based on this, we propose Geometry
guided Neural LiDAR Fields(GeoNLF), a hybrid framework performing alternately
global neural reconstruction and pure geometric pose optimization. Furthermore,
NeRFs tend to overfit individual frames and easily get stuck in local minima
under sparse-view inputs. To tackle this issue, we develop a
selective-reweighting strategy and introduce geometric constraints for robust
optimization. Extensive experiments on NuScenes and KITTI-360 datasets
demonstrate the superiority of GeoNLF in both novel view synthesis and
multi-view registration of low-frequency large-scale point clouds.</description>
      <guid isPermaLink="false">2407.05597v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>FusionSAM: Latent Space driven Segment Anything Model for Multimodal Fusion and Segmentation</title>
      <link>http://arxiv.org/abs/2408.13980v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 多模态图像融合与分割通过整合来自不同传感器的数据，增强自动驾驶中的场景理解。&lt;br&gt;&lt;h4&gt;2. 现有问题&lt;/h4&gt;   - 当前模型在密集元素的分割上效率不足，缺乏全面的融合特征来指导中间过程的微调，并关注相关区域。&lt;br&gt;&lt;h4&gt;3. 方法创新&lt;/h4&gt;   - 引入Segment Anything Model (SAM)，作为一种变革性的分割方法，提供更有效的提示，具有灵活的提示编码器，相比于缺乏微调控制的变换器。&lt;br&gt;&lt;h4&gt;4. 研究缺口&lt;/h4&gt;   - SAM在自然图像的多模态融合领域尚未得到深入研究。&lt;br&gt;&lt;h4&gt;5. 提出的框架&lt;/h4&gt;   - 提出一个新颖的框架，将潜在空间令牌生成（LSTG）和融合掩模提示（FMP）模块结合，以增强SAM的多模态融合和分割能力。&lt;br&gt;&lt;h4&gt;6. 具体方法&lt;/h4&gt;   - 首先，通过向量量化获取两个模态的潜在空间特征，并将其嵌入到基于交叉注意力的跨域融合模块中，以建立模态之间的长程依赖关系。&lt;br&gt;   - 然后，使用这些全面的融合特征作为提示，引导精确的像素级分割。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 在多个公共数据集上进行广泛实验，结果表明，所提出的方法在多模态自动驾驶场景中显著优于SAM和SAM2，分割的mIoU至少提高了3.9%，超越了最新的其他方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal image fusion and segmentation enhance scene understanding in
autonomous driving by integrating data from various sensors. However, current
models struggle to efficiently segment densely packed elements in such scenes,
due to the absence of comprehensive fusion features that can guide mid-process
fine-tuning and focus attention on relevant areas. The Segment Anything Model
(SAM) has emerged as a transformative segmentation method. It provides more
effective prompts through its flexible prompt encoder, compared to transformers
lacking fine-tuned control. Nevertheless, SAM has not been extensively studied
in the domain of multimodal fusion for natural images. In this paper, we
introduce SAM into multimodal image segmentation for the first time, proposing
a novel framework that combines Latent Space Token Generation (LSTG) and Fusion
Mask Prompting (FMP) modules to enhance SAM's multimodal fusion and
segmentation capabilities. Specifically, we first obtain latent space features
of the two modalities through vector quantization and embed them into a
cross-attention-based inter-domain fusion module to establish long-range
dependencies between modalities. Then, we use these comprehensive fusion
features as prompts to guide precise pixel-level segmentation. Extensive
experiments on several public datasets demonstrate that the proposed method
significantly outperforms SAM and SAM2 in multimodal autonomous driving
scenarios, achieving at least 3.9$\%$ higher segmentation mIoU than the
state-of-the-art approaches.</description>
      <guid isPermaLink="false">2408.13980v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Neural Network Kalman filtering for 3D object tracking from linear array ultrasound data</title>
      <link>http://arxiv.org/abs/2111.09631v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 8 figures&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 许多介入性外科手术依赖医学影像来可视化和跟踪仪器。&lt;br&gt;   - 这些影像方法需要具备实时能力，并提供准确且稳健的位置信息。&lt;br&gt;&lt;h4&gt;2. 挑战&lt;/h4&gt;   - 在超声应用中，通常只有来自线性阵列的二维数据，因此在三维空间中获得准确的位置信息并不简单。&lt;br&gt;&lt;h4&gt;3. 研究方法&lt;/h4&gt;   - 首先，使用逼真的合成训练数据训练神经网络，以估计物体的出平面偏移及重建超声图像中的轴向畸变。&lt;br&gt;   - 将获得的估计与卡尔曼滤波方法结合，利用先前时间帧获得的定位估计，提高定位的稳健性，减少测量噪声的影响。&lt;br&gt;&lt;h4&gt;4. 实验与评估&lt;/h4&gt;   - 通过模拟评估所提方法的准确性，并在使用新型光学超声成像设备获得的实验数据上验证其实用性。&lt;br&gt;   - 实时提供准确且稳健的位置信息。&lt;br&gt;&lt;h4&gt;5. 结果&lt;/h4&gt;   - 对于模拟数据，出平面物体的轴向和横向坐标估计的平均误差为0.1mm；对于实验数据，平均误差为0.2mm。&lt;br&gt;   - 对于高于1mm的提升距离，三维定位最为准确，最大考虑距离为6mm，使用25mm的光圈。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2021-11-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/TUFFC.2022.3162097&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Many interventional surgical procedures rely on medical imaging to visualise
and track instruments. Such imaging methods not only need to be real-time
capable, but also provide accurate and robust positional information. In
ultrasound applications, typically only two-dimensional data from a linear
array are available, and as such obtaining accurate positional estimation in
three dimensions is non-trivial. In this work, we first train a neural network,
using realistic synthetic training data, to estimate the out-of-plane offset of
an object with the associated axial aberration in the reconstructed ultrasound
image. The obtained estimate is then combined with a Kalman filtering approach
that utilises positioning estimates obtained in previous time-frames to improve
localisation robustness and reduce the impact of measurement noise. The
accuracy of the proposed method is evaluated using simulations, and its
practical applicability is demonstrated on experimental data obtained using a
novel optical ultrasound imaging setup. Accurate and robust positional
information is provided in real-time. Axial and lateral coordinates for
out-of-plane objects are estimated with a mean error of 0.1mm for simulated
data and a mean error of 0.2mm for experimental data. Three-dimensional
localisation is most accurate for elevational distances larger than 1mm, with a
maximum distance of 6mm considered for a 25mm aperture.</description>
      <guid isPermaLink="false">2111.09631v3</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Learned Compression for Images and Point Clouds</title>
      <link>http://arxiv.org/abs/2409.08376v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  65 pages, 21 figures, Master's Thesis, defended in 2023&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 在过去十年中，深度学习在计算机视觉任务（如分类、超分辨率和风格迁移）方面取得了显著成功。&lt;br&gt;&lt;h4&gt;2. 研究目标&lt;/h4&gt;   - 将深度学习应用于数据压缩，以帮助构建下一代多媒体编解码器。&lt;br&gt;&lt;h4&gt;3. 主要贡献&lt;/h4&gt;   - **第一项贡献**：&lt;br&gt;     - 提出了一种高效的低复杂度熵模型，该模型通过将编码分布作为附加信息进行压缩和传输，动态适应特定输入的编码分布。&lt;br&gt;   &lt;br&gt;   - **第二项贡献**：&lt;br&gt;     - 提出了一种新颖的轻量级低复杂度点云编解码器，专门针对分类任务，与非专业编解码器相比，显著降低了比特率。&lt;br&gt;   - **第三项贡献**：&lt;br&gt;     - 探讨了在连续视频帧之间输入域中的运动如何在相应的卷积推导的潜在空间中体现。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/multimedialabsfu/learned-point-cloud-compression-for-classification&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Over the last decade, deep learning has shown great success at performing
computer vision tasks, including classification, super-resolution, and style
transfer. Now, we apply it to data compression to help build the next
generation of multimedia codecs. This thesis provides three primary
contributions to this new field of learned compression. First, we present an
efficient low-complexity entropy model that dynamically adapts the encoding
distribution to a specific input by compressing and transmitting the encoding
distribution itself as side information. Secondly, we propose a novel
lightweight low-complexity point cloud codec that is highly specialized for
classification, attaining significant reductions in bitrate compared to
non-specialized codecs. Lastly, we explore how motion within the input domain
between consecutive video frames is manifested in the corresponding
convolutionally-derived latent space.</description>
      <guid isPermaLink="false">2409.08376v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>MedUnA: Language guided Unsupervised Adaptation of Vision-Language Models for Medical Image Classification</title>
      <link>http://arxiv.org/abs/2409.02729v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 在医学图像分类中，监督学习面临挑战，主要是由于缺乏标记的医学图像。&lt;br&gt;&lt;h4&gt;2. 研究目标&lt;/h4&gt;   - 提出了一种新方法，利用视觉-文本对齐的视觉语言模型（VLMs）来促进无监督学习，改变传统的预训练加微调的方式。&lt;br&gt;&lt;h4&gt;3. 方法名称&lt;/h4&gt;   - 提出名为“MedUnA”（Medical Unsupervised Adaptation）的框架，包含两个阶段的训练：适配器预训练和无监督学习。&lt;br&gt;&lt;h4&gt;4. 第一阶段 - 适配器预训练&lt;/h4&gt;   - 使用大型语言模型（LLM）生成与类标签对应的描述，并通过文本编码器BioBERT处理。&lt;br&gt;   - 生成的文本嵌入与类标签对齐，训练一个轻量级的适配器，以增强文本嵌入。&lt;br&gt;&lt;h4&gt;5. 第二阶段 - 无监督学习&lt;/h4&gt;   - 将训练好的适配器与MedCLIP的视觉编码器结合。&lt;br&gt;   - 采用对比熵损失和提示调优来对齐视觉嵌入，并引入自熵最小化以增强嵌入的信心。&lt;br&gt;&lt;h4&gt;6. 评估方法&lt;/h4&gt;   - 在三种不同的数据模态上进行评估：胸部X光、眼底图像和皮肤病变图像。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 结果显示，与基线相比，MedUnA在不同数据集上显著提高了平均准确率，突显了该方法的有效性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In medical image classification, supervised learning is challenging due to
the lack of labeled medical images. Contrary to the traditional \textit{modus
operandi} of pre-training followed by fine-tuning, this work leverages the
visual-textual alignment within Vision-Language models (\texttt{VLMs}) to
facilitate the unsupervised learning.
  Specifically, we propose \underline{Med}ical \underline{Un}supervised
\underline{A}daptation (\texttt{MedUnA}), constituting two-stage training:
Adapter Pre-training, and Unsupervised Learning. In the first stage, we use
descriptions generated by a Large Language Model (\texttt{LLM}) corresponding
to class labels, which are passed through the text encoder \texttt{BioBERT}.
The resulting text embeddings are then aligned with the class labels by
training a lightweight \texttt{adapter}. We choose \texttt{\texttt{LLMs}}
because of their capability to generate detailed, contextually relevant
descriptions to obtain enhanced text embeddings.
  In the second stage, the trained \texttt{adapter} is integrated with the
visual encoder of \texttt{MedCLIP}. This stage employs a contrastive
entropy-based loss and prompt tuning to align visual embeddings. We incorporate
self-entropy minimization into the overall training objective to ensure more
confident embeddings, which are crucial for effective unsupervised learning and
alignment. We evaluate the performance of \texttt{MedUnA} on three different
kinds of data modalities - chest X-rays, eye fundus and skin lesion images. The
results demonstrate significant accuracy gain on average compared to the
baselines across different datasets, highlighting the efficacy of our approach.</description>
      <guid isPermaLink="false">2409.02729v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Data-efficient multi-fidelity training for high-fidelity machine learning interatomic potentials</title>
      <link>http://arxiv.org/abs/2409.07947v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 4 figures, 1 tables, Supplementary information included as
  ancillary file (+16 pages)&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 机器学习原子间势能（MLIPs）用于从第一性原理计算中估计势能面（PES），提供接近量子水平的精度，同时降低计算成本。&lt;br&gt;   - 高保真数据库的组装成本高，限制了MLIPs在要求高化学精度系统中的应用。&lt;br&gt;&lt;h4&gt;2. 研究目标&lt;/h4&gt;   - 提出一个利用等变图神经网络的MLIP框架，能够同时在多重保真度数据库上进行训练。&lt;br&gt;&lt;h4&gt;3. 方法创新&lt;/h4&gt;   - 该方法可以在最小的高保真数据下，准确学习高保真的势能面。&lt;br&gt;&lt;h4&gt;4. 实验对象&lt;/h4&gt;   - 该框架在Li$_6$PS$_5$Cl和In$_x$Ga$_{1-x}$N系统上进行了测试。&lt;br&gt;&lt;h4&gt;5. 实验结果&lt;/h4&gt;   - 计算结果表明，未被高保真meta-GGA数据库覆盖的几何和成分空间可以有效地从低保真GGA数据中推断，从而提高精度和分子动力学的稳定性。&lt;br&gt;&lt;h4&gt;6. 通用性&lt;/h4&gt;   - 开发了一种通用的MLIP，利用来自材料项目的GGA和meta-GGA数据，显著提升了MLIP在高精度任务中的表现，如预测晶体的能量高于壳层。&lt;br&gt;&lt;h4&gt;7. 比较分析&lt;/h4&gt;   - 证明当前的多保真学习方法比迁移学习或Δ学习更有效，并且可以应用于学习更高保真的模型，直至耦合簇水平。&lt;br&gt;&lt;h4&gt;8. 研究前景&lt;/h4&gt;   - 该方法有望通过有效扩展高保真数据集，创建高度准确的定制或通用MLIPs。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine learning interatomic potentials (MLIPs) are used to estimate
potential energy surfaces (PES) from ab initio calculations, providing near
quantum-level accuracy with reduced computational costs. However, the high cost
of assembling high-fidelity databases hampers the application of MLIPs to
systems that require high chemical accuracy. Utilizing an equivariant graph
neural network, we present an MLIP framework that trains on multi-fidelity
databases simultaneously. This approach enables the accurate learning of
high-fidelity PES with minimal high-fidelity data. We test this framework on
the Li$_6$PS$_5$Cl and In$_x$Ga$_{1-x}$N systems. The computational results
indicate that geometric and compositional spaces not covered by the
high-fidelity meta-gradient generalized approximation (meta-GGA) database can
be effectively inferred from low-fidelity GGA data, thus enhancing accuracy and
molecular dynamics stability. We also develop a general-purpose MLIP that
utilizes both GGA and meta-GGA data from the Materials Project, significantly
enhancing MLIP performance for high-accuracy tasks such as predicting energies
above hull for crystals in general. Furthermore, we demonstrate that the
present multi-fidelity learning is more effective than transfer learning or
$\Delta$-learning an d that it can also be applied to learn higher-fidelity up
to the coupled-cluster level. We believe this methodology holds promise for
creating highly accurate bespoke or universal MLIPs by effectively expanding
the high-fidelity dataset.</description>
      <guid isPermaLink="false">2409.07947v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>WarpAdam: A new Adam optimizer based on Meta-Learning approach</title>
      <link>http://arxiv.org/abs/2409.04244v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 优化算法的最佳选择对训练深度学习模型至关重要。&lt;br&gt;   - Adam优化器因其高效性和广泛适用性而受到关注。&lt;br&gt;&lt;h4&gt;2. 研究目标&lt;/h4&gt;   - 提出一种创新的优化策略，通过将元学习中的“扭曲梯度下降”概念与Adam优化器结合，以提高优化器在不同数据集上的适应性。&lt;br&gt;&lt;h4&gt;3. 方法创新&lt;/h4&gt;   - 在传统的Adam优化器中，梯度用于计算梯度均值和方差，然后更新模型参数。&lt;br&gt;   - 本方法引入一个可学习的扭曲矩阵P，用于线性变换梯度，轻微调整每次迭代中的梯度。&lt;br&gt;&lt;h4&gt;4. 适应性调整&lt;/h4&gt;   - 通过学习适当的扭曲矩阵P，方法旨在自适应地调整不同数据分布下的梯度信息，从而增强优化性能。&lt;br&gt;&lt;h4&gt;5. 研究成果&lt;/h4&gt;   - 研究通过理论分析和实证评估展示了这一新方法的潜力。&lt;br&gt;   - 在多项任务和数据集上的实验结果验证了结合“扭曲梯度下降”概念的优化器在适应性方面的优越性。&lt;br&gt;&lt;h4&gt;6. 训练策略&lt;/h4&gt;   - 探索有效的扭曲矩阵P的训练策略，并识别出此方法能够产生最佳结果的场景。&lt;br&gt;&lt;h4&gt;7. 总结&lt;/h4&gt;   - 本研究介绍了一种创新的方法，将元学习中的“扭曲梯度下降”概念与Adam优化器结合，通过引入可学习的扭曲矩阵P，旨在增强模型在不同数据分布下的泛化能力，为深度学习优化领域开辟新可能性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Optimal selection of optimization algorithms is crucial for training deep
learning models. The Adam optimizer has gained significant attention due to its
efficiency and wide applicability. However, to enhance the adaptability of
optimizers across diverse datasets, we propose an innovative optimization
strategy by integrating the 'warped gradient descend'concept from Meta Learning
into the Adam optimizer. In the conventional Adam optimizer, gradients are
utilized to compute estimates of gradient mean and variance, subsequently
updating model parameters. Our approach introduces a learnable distortion
matrix, denoted as P, which is employed for linearly transforming gradients.
This transformation slightly adjusts gradients during each iteration, enabling
the optimizer to better adapt to distinct dataset characteristics. By learning
an appropriate distortion matrix P, our method aims to adaptively adjust
gradient information across different data distributions, thereby enhancing
optimization performance. Our research showcases the potential of this novel
approach through theoretical insights and empirical evaluations. Experimental
results across various tasks and datasets validate the superiority of our
optimizer that integrates the 'warped gradient descend' concept in terms of
adaptability. Furthermore, we explore effective strategies for training the
adaptation matrix P and identify scenarios where this method can yield optimal
results. In summary, this study introduces an innovative approach that merges
the 'warped gradient descend' concept from Meta Learning with the Adam
optimizer. By introducing a learnable distortion matrix P within the optimizer,
we aim to enhance the model's generalization capability across diverse data
distributions, thus opening up new possibilities in the field of deep learning
optimization.</description>
      <guid isPermaLink="false">2409.04244v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Beyond LoRA: Exploring Efficient Fine-Tuning Techniques for Time Series Foundational Models</title>
      <link>http://arxiv.org/abs/2409.11302v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages. Under review&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 时间序列基础模型（TSFMs）在建模复杂的大规模时间序列数据（如零售、金融和交通）方面受到关注。&lt;br&gt;   - 然而，将其应用于敏感的领域（如医疗保健）仍然具有挑战性，主要是因为在特定领域任务上微调这些模型时缺乏公开可用的数据集。&lt;br&gt;&lt;h4&gt;2. 研究目标&lt;/h4&gt;   - 探索使用参数高效微调（PEFT）技术来解决医疗保健应用中的这些限制，特别是针对脓毒症患者的ICU生命体征预测。&lt;br&gt;&lt;h4&gt;3. 方法介绍&lt;/h4&gt;   - 引入并评估两种选择性PEFT技术（BitFit和LayerNorm Tuning）以及两种加法PEFT技术（VeRA和FourierFT），用于对Chronos TSFM的多个配置进行生命体征预测。&lt;br&gt;&lt;h4&gt;4. 实验结果&lt;/h4&gt;   - 比较分析显示，一些PEFT方法在参数效率和领域适应性方面优于LoRA，确立了ICU生命体征预测任务的最先进（SOTA）结果。&lt;br&gt;&lt;h4&gt;5. 关键发现&lt;/h4&gt;   - 有趣的是，FourierFT在Chronos（Tiny）变体上的应用超越了SOTA模型，仅微调2,400个参数，而基准模型微调了700K个参数。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time Series Foundation Models (TSFMs) have recently garnered attention for
their ability to model complex, large-scale time series data across domains
such as retail, finance, and transportation. However, their application to
sensitive, domain-specific fields like healthcare remains challenging,
primarily due to the difficulty of fine-tuning these models for specialized,
out-of-domain tasks with scarce publicly available datasets. In this work, we
explore the use of Parameter-Efficient Fine-Tuning (PEFT) techniques to address
these limitations, focusing on healthcare applications, particularly ICU vitals
forecasting for sepsis patients. We introduce and evaluate two selective
(BitFit and LayerNorm Tuning) and two additive (VeRA and FourierFT) PEFT
techniques on multiple configurations of the Chronos TSFM for forecasting vital
signs of sepsis patients. Our comparative analysis demonstrates that some of
these PEFT methods outperform LoRA in terms of parameter efficiency and domain
adaptation, establishing state-of-the-art (SOTA) results in ICU vital
forecasting tasks. Interestingly, FourierFT applied to the Chronos (Tiny)
variant surpasses the SOTA model while fine-tuning only 2,400 parameters
compared to the 700K parameters of the benchmark.</description>
      <guid isPermaLink="false">2409.11302v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Top-down Activity Representation Learning for Video Question Answering</title>
      <link>http://arxiv.org/abs/2409.07748v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  presented at MIRU2024&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 捕捉复杂的层次化人类活动（从原子动作到上下文事件）对于实现高性能的视频问答（VideoQA）至关重要。&lt;br&gt;   - 原子动作示例包括：拿起礼物、移动到沙发、拆开礼物；上下文事件示例包括：庆祝圣诞节。&lt;br&gt;&lt;h4&gt;2. 现有方法局限&lt;/h4&gt;   - 最近的多模态模型（如CLIP、LLaVA）已扩展以处理连续的视频序列，从而增强模型的时间推理能力。&lt;br&gt;   - 这些方法通常未能捕捉可以分解为多个原子动作且在相对较长的序列中非连续分布的上下文事件。&lt;br&gt;&lt;h4&gt;3. 研究目标&lt;/h4&gt;   - 本文旨在利用CLIP模型的空间视觉上下文表示能力，以获取视频中上下文事件的非连续视觉表示。&lt;br&gt;&lt;h4&gt;4. 方法创新&lt;/h4&gt;   - 将长时间视频序列转换为空间图像域，并为VideoQA任务微调多模态模型LLaVA。&lt;br&gt;&lt;h4&gt;5. 实验结果&lt;/h4&gt;   - 在STAR任务上取得了具有竞争力的表现，特别是在NExTQA任务上，准确率达78.4%，超过当前最先进的分数2.8个百分点。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Capturing complex hierarchical human activities, from atomic actions (e.g.,
picking up one present, moving to the sofa, unwrapping the present) to
contextual events (e.g., celebrating Christmas) is crucial for achieving
high-performance video question answering (VideoQA). Recent works have expanded
multimodal models (e.g., CLIP, LLaVA) to process continuous video sequences,
enhancing the model's temporal reasoning capabilities. However, these
approaches often fail to capture contextual events that can be decomposed into
multiple atomic actions non-continuously distributed over relatively long-term
sequences. In this paper, to leverage the spatial visual context representation
capability of the CLIP model for obtaining non-continuous visual
representations in terms of contextual events in videos, we convert long-term
video sequences into a spatial image domain and finetune the multimodal model
LLaVA for the VideoQA task. Our approach achieves competitive performance on
the STAR task, in particular, with a 78.4% accuracy score, exceeding the
current state-of-the-art score by 2.8 points on the NExTQA task.</description>
      <guid isPermaLink="false">2409.07748v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Task-Adaptive Clustering for Semi-Supervised Few-Shot Classification</title>
      <link>http://arxiv.org/abs/2003.08221v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 5 figures&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 少样本学习旨在利用少量新训练数据处理之前未见过的任务。&lt;br&gt;   - 准备（或元训练）少样本学习者时，通常需要大量标记数据，但在现实世界中，这些标记数据往往昂贵且稀缺。&lt;br&gt;&lt;h4&gt;2. 研究目的&lt;/h4&gt;   - 提出一种在半监督环境下有效的少样本学习者，即大部分训练数据为未标记数据的情况下。&lt;br&gt;&lt;h4&gt;3. 方法创新&lt;/h4&gt;   - 采用显式任务条件化的方法，在新的投影空间中进行当前任务的未标记样本聚类，该空间不同于嵌入特征空间。&lt;br&gt;&lt;h4&gt;4. 聚类空间构建&lt;/h4&gt;   - 条件聚类空间是线性构建的，目的是迅速缩小当前任务的类别质心与跨任务元训练的独立每类参考向量之间的距离。&lt;br&gt;&lt;h4&gt;5. 任务条件化控制&lt;/h4&gt;   - 在更一般的设置中，方法引入了控制元学习中任务条件化程度的概念：任务条件化的程度随着聚类空间的重复更新次数而变化。&lt;br&gt;&lt;h4&gt;6. 实验结果&lt;/h4&gt;   - 基于miniImageNet和tieredImageNet数据集的广泛仿真实验显示，所提出方法在半监督少样本分类任务中表现出最先进的性能。&lt;br&gt;&lt;h4&gt;7. 鲁棒性分析&lt;/h4&gt;   - 仿真结果表明，所提出的任务自适应聚类在增加干扰样本数量（即来自候选类外的未标记样本图像）时表现出优雅的降级。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2020-03-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Few-shot learning aims to handle previously unseen tasks using only a small
amount of new training data. In preparing (or meta-training) a few-shot
learner, however, massive labeled data are necessary. In the real world,
unfortunately, labeled data are expensive and/or scarce. In this work, we
propose a few-shot learner that can work well under the semi-supervised setting
where a large portion of training data is unlabeled. Our method employs
explicit task-conditioning in which unlabeled sample clustering for the current
task takes place in a new projection space different from the embedding feature
space. The conditioned clustering space is linearly constructed so as to
quickly close the gap between the class centroids for the current task and the
independent per-class reference vectors meta-trained across tasks. In a more
general setting, our method introduces a concept of controlling the degree of
task-conditioning for meta-learning: the amount of task-conditioning varies
with the number of repetitive updates for the clustering space. Extensive
simulation results based on the miniImageNet and tieredImageNet datasets show
state-of-the-art semi-supervised few-shot classification performance of the
proposed method. Simulation results also indicate that the proposed
task-adaptive clustering shows graceful degradation with a growing number of
distractor samples, i.e., unlabeled sample images coming from outside the
candidate classes.</description>
      <guid isPermaLink="false">2003.08221v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>RiEMann: Near Real-Time SE(3)-Equivariant Robot Manipulation without Point Cloud Segmentation</title>
      <link>http://arxiv.org/abs/2403.19460v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究介绍&lt;/h4&gt;   - 提出RiEMann，一个端到端的近实时SE(3)-等变机器人操作模仿学习框架。&lt;br&gt;   - 该框架从场景点云输入中工作。&lt;br&gt;&lt;h4&gt;2. 方法创新&lt;/h4&gt;   - 与依赖描述符场匹配的传统方法不同，RiEMann直接预测目标物体的操作姿态，无需任何物体分割。&lt;br&gt;&lt;h4&gt;3. 学习能力&lt;/h4&gt;   - RiEMann可以从5到10个演示开始学习操作任务，具备从零开始学习的能力。&lt;br&gt;   - 能够推广到未见的SE(3)变换和目标物体实例。&lt;br&gt;&lt;h4&gt;4. 抗干扰能力&lt;/h4&gt;   - 抵抗视觉干扰，能够在有干扰物体的情况下进行操作。&lt;br&gt;&lt;h4&gt;5. 实时跟踪&lt;/h4&gt;   - 能够跟随目标物体的近实时姿态变化。&lt;br&gt;&lt;h4&gt;6. 可扩展的动作空间&lt;/h4&gt;   - RiEMann的可扩展动作空间支持自定义等变动作，例如水龙头的转动方向，使得复杂物体操作成为可能。&lt;br&gt;&lt;h4&gt;7. 实验验证&lt;/h4&gt;   - 在仿真和真实世界的6自由度机器人操作实验中，RiEMann在5个类别的操作任务中进行了测试，总共25种变体。&lt;br&gt;   - 显示出RiEMann在任务成功率和预测姿态的SE(3)测地距离误差上均优于基线（误差减少68.6%）。&lt;br&gt;&lt;h4&gt;8. 性能指标&lt;/h4&gt;   - 实现了5.4帧每秒（FPS）的网络推理速度。&lt;br&gt;&lt;h4&gt;9. 资源链接&lt;/h4&gt;   - 代码和视频结果可在 [RiEMann网站](https://riemann-web.github.io/) 获取。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-03-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present RiEMann, an end-to-end near Real-time SE(3)-Equivariant Robot
Manipulation imitation learning framework from scene point cloud input.
Compared to previous methods that rely on descriptor field matching, RiEMann
directly predicts the target poses of objects for manipulation without any
object segmentation. RiEMann learns a manipulation task from scratch with 5 to
10 demonstrations, generalizes to unseen SE(3) transformations and instances of
target objects, resists visual interference of distracting objects, and follows
the near real-time pose change of the target object. The scalable action space
of RiEMann facilitates the addition of custom equivariant actions such as the
direction of turning the faucet, which makes articulated object manipulation
possible for RiEMann. In simulation and real-world 6-DOF robot manipulation
experiments, we test RiEMann on 5 categories of manipulation tasks with a total
of 25 variants and show that RiEMann outperforms baselines in both task success
rates and SE(3) geodesic distance errors on predicted poses (reduced by 68.6%),
and achieves a 5.4 frames per second (FPS) network inference speed. Code and
video results are available at https://riemann-web.github.io/.</description>
      <guid isPermaLink="false">2403.19460v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Modal Loop Closing in Unstructured Planetary Environments with Visually Enriched Submaps</title>
      <link>http://arxiv.org/abs/2105.02020v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS 2021)&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 未来的行星任务将依赖于能够自主探索和导航于非结构化环境的探测车（rover）。&lt;br&gt;   - 关键能力是识别已经访问或映射的地点。&lt;br&gt;&lt;h4&gt;2. 方法论&lt;/h4&gt;   - 利用立体摄像头提供的视觉和深度信息，从多模态的角度指导回环闭合的搜索和验证。&lt;br&gt;&lt;h4&gt;3. 子地图增强&lt;/h4&gt;   - 通过聚合立体点云创建的子地图，加入视觉关键帧进行增强。&lt;br&gt;&lt;h4&gt;4. 匹配方法&lt;/h4&gt;   - 点云匹配通过比较CSHOT描述符找到，并通过聚类进行验证。&lt;br&gt;   - 视觉匹配通过比较关键帧使用词袋模型（BoW）和ORB描述符建立。&lt;br&gt;&lt;h4&gt;5. 融合变换&lt;/h4&gt;   - 通过关键帧和点云匹配得到的相对变换被融合，以提供在图形SLAM框架中子地图之间的姿态约束。&lt;br&gt;&lt;h4&gt;6. 实验验证&lt;/h4&gt;   - 使用LRU探测车在室内实验室环境和意大利埃特纳山的行星类比环境中进行了多次测试。&lt;br&gt;&lt;h4&gt;7. 环境挑战&lt;/h4&gt;   - 测试环境中存在仅依靠关键帧或点云无法提供足够匹配的区域，展示了所提多模态方法的优势。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2021-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Future planetary missions will rely on rovers that can autonomously explore
and navigate in unstructured environments. An essential element is the ability
to recognize places that were already visited or mapped. In this work, we
leverage the ability of stereo cameras to provide both visual and depth
information, guiding the search and validation of loop closures from a
multi-modal perspective. We propose to augment submaps that are created by
aggregating stereo point clouds, with visual keyframes. Point clouds matches
are found by comparing CSHOT descriptors and validated by clustering, while
visual matches are established by comparing keyframes using Bag-of-Words (BoW)
and ORB descriptors. The relative transformations resulting from both keyframe
and point cloud matches are then fused to provide pose constraints between
submaps in our graph-based SLAM framework. Using the LRU rover, we performed
several tests in both an indoor laboratory environment as well as a challenging
planetary analog environment on Mount Etna, Italy. These environments consist
of areas where either keyframes or point clouds alone failed to provide
adequate matches demonstrating the benefit of the proposed multi-modal
approach.</description>
      <guid isPermaLink="false">2105.02020v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>LiDAR-based Person Re-identification</title>
      <link>http://arxiv.org/abs/2312.03033v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 基于摄像头的人物重识别（ReID）系统广泛应用于公共安全领域。&lt;br&gt;   - 摄像头常常无法感知人类的3D形态信息，并受到多种限制，如光照不足、复杂背景和个人隐私问题。&lt;br&gt;&lt;h4&gt;2. 研究目的&lt;/h4&gt;   - 提出一种基于LiDAR的人物重识别框架，命名为ReID3D。&lt;br&gt;&lt;h4&gt;3. 方法创新&lt;/h4&gt;   - 利用预训练策略提取3D人体形状特征。&lt;br&gt;   - 引入图形互补增强编码器（Graph-based Complementary Enhancement Encoder），用于提取全面特征。&lt;br&gt;&lt;h4&gt;4. 数据集构建&lt;/h4&gt;   - 构建了LReID，这是第一个基于LiDAR的人物ReID数据集，数据收集自多个户外场景，涵盖自然条件的变化。&lt;br&gt;&lt;h4&gt;5. 模拟数据集&lt;/h4&gt;   - 引入LReID-sync，这是一个模拟行人数据集，用于预训练编码器，任务包括点云补全和形状参数学习。&lt;br&gt;&lt;h4&gt;6. 实验结果&lt;/h4&gt;   - 在LReID上进行的广泛实验表明，ReID3D实现了94.0的Rank-1准确率，显示出LiDAR在解决人物ReID任务中的巨大潜力。&lt;br&gt;&lt;h4&gt;7. 研究贡献&lt;/h4&gt;   - 据我们所知，这是首个提出基于LiDAR的人物重识别解决方案的研究。&lt;br&gt;&lt;h4&gt;8. 代码与数据集发布&lt;/h4&gt;   - 研究中的代码和数据集将很快发布。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2023-12-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/gwxuan/reid3d&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Camera-based person re-identification (ReID) systems have been widely applied
in the field of public security. However, cameras often lack the perception of
3D morphological information of human and are susceptible to various
limitations, such as inadequate illumination, complex background, and personal
privacy. In this paper, we propose a LiDAR-based ReID framework, ReID3D, that
utilizes pre-training strategy to retrieve features of 3D body shape and
introduces Graph-based Complementary Enhancement Encoder for extracting
comprehensive features. Due to the lack of LiDAR datasets, we build LReID, the
first LiDAR-based person ReID dataset, which is collected in several outdoor
scenes with variations in natural conditions. Additionally, we introduce
LReID-sync, a simulated pedestrian dataset designed for pre-training encoders
with tasks of point cloud completion and shape parameter learning. Extensive
experiments on LReID show that ReID3D achieves exceptional performance with a
rank-1 accuracy of 94.0, highlighting the significant potential of LiDAR in
addressing person ReID tasks. To the best of our knowledge, we are the first to
propose a solution for LiDAR-based ReID. The code and datasets will be released
soon.</description>
      <guid isPermaLink="false">2312.03033v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Deep Graph Anomaly Detection: A Survey and New Perspectives</title>
      <link>http://arxiv.org/abs/2409.09957v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  24 pages, 6 figures, and 7 tables&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 图异常检测（GAD）旨在识别异常图实例（包括节点、边、子图或整图）。&lt;br&gt;   - 近年来，GAD在多个应用领域引起了越来越多的关注。&lt;br&gt;&lt;h4&gt;2. 深度学习方法&lt;/h4&gt;   - 深度学习，尤其是图神经网络（GNN），被认为是GAD的一种有前景的方法。&lt;br&gt;   - GNN能够有效捕捉图数据中的复杂结构和节点属性。&lt;br&gt;&lt;h4&gt;3. 研究必要性&lt;/h4&gt;   - 由于提出了大量基于GNN的GAD方法，有必要总结现有研究中的方法论和发现。&lt;br&gt;   - 这样可以帮助识别有效的模型设计，以解决当前GAD领域的开放问题。&lt;br&gt;&lt;h4&gt;4. 综述目标&lt;/h4&gt;   - 本文旨在提供对GAD的深度学习方法的全面回顾。&lt;br&gt;   - 现有的GAD综述往往集中于任务特定的讨论，缺乏对方法技术洞察和局限性的理解。&lt;br&gt;&lt;h4&gt;5. 问题复杂性与挑战&lt;/h4&gt;   - 文章首先讨论GAD中的问题复杂性及其带来的挑战。&lt;br&gt;&lt;h4&gt;6. 系统性回顾&lt;/h4&gt;   - 从三种新颖的视角（GNN骨干设计、GAD的代理任务设计和图异常度量）对现有深度GAD方法进行系统回顾。&lt;br&gt;&lt;h4&gt;7. 分类体系&lt;/h4&gt;   - 提出了一个包含13个细分方法类别的分类体系，以深入分析模型设计及其能力。&lt;br&gt;&lt;h4&gt;8. 实验与验证&lt;/h4&gt;   - 汇总了广泛使用的GAD数据集和经验比较，以便于实验和验证。&lt;br&gt;&lt;h4&gt;9. 未来研究方向&lt;/h4&gt;   - 讨论了多个开放问题，以激励未来的高质量研究。&lt;br&gt;&lt;h4&gt;10. 资源链接&lt;/h4&gt;    - 提供了一个持续更新的资源库，包含数据集、算法代码链接和经验比较，网址为 [GitHub - Awesome-Deep-Graph-Anomaly-Detection](https://github.com/mala-lab/Awesome-Deep-Graph-Anomaly-Detection)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/mala-lab/awesome-deep-graph-anomaly-detection&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph anomaly detection (GAD), which aims to identify unusual graph instances
(nodes, edges, subgraphs, or graphs), has attracted increasing attention in
recent years due to its significance in a wide range of applications. Deep
learning approaches, graph neural networks (GNNs) in particular, have been
emerging as a promising paradigm for GAD, owing to its strong capability in
capturing complex structure and/or node attributes in graph data. Considering
the large number of methods proposed for GNN-based GAD, it is of paramount
importance to summarize the methodologies and findings in the existing GAD
studies, so that we can pinpoint effective model designs for tackling open GAD
problems. To this end, in this work we aim to present a comprehensive review of
deep learning approaches for GAD. Existing GAD surveys are focused on
task-specific discussions, making it difficult to understand the technical
insights of existing methods and their limitations in addressing some unique
challenges in GAD. To fill this gap, we first discuss the problem complexities
and their resulting challenges in GAD, and then provide a systematic review of
current deep GAD methods from three novel perspectives of methodology,
including GNN backbone design, proxy task design for GAD, and graph anomaly
measures. To deepen the discussions, we further propose a taxonomy of 13
fine-grained method categories under these three perspectives to provide more
in-depth insights into the model designs and their capabilities. To facilitate
the experiments and validation, we also summarize a collection of widely-used
GAD datasets and empirical comparison. We further discuss multiple open
problems to inspire more future high-quality research. A continuously updated
repository for datasets, links to the codes of algorithms, and empirical
comparison is available at
https://github.com/mala-lab/Awesome-Deep-Graph-Anomaly-Detection.</description>
      <guid isPermaLink="false">2409.09957v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>SGOR: Outlier Removal by Leveraging Semantic and Geometric Information for Robust Point Cloud Registration</title>
      <link>http://arxiv.org/abs/2407.06297v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IROS 2024&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究目的&lt;/h4&gt;   - 本文介绍了一种新颖的异常值去除方法，充分利用几何和语义信息，以实现稳健的配准。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 当前的基于语义的配准方法仅用于点对点或实例语义对应生成，存在两个主要问题：&lt;br&gt;     - **依赖性问题**：这些方法高度依赖语义的正确性，在语义不正确或稀疏的情况下表现不佳。&lt;br&gt;     - **局限性问题**：语义的使用仅限于对应生成，在几何信息较弱的场景中表现不佳。&lt;br&gt;&lt;h4&gt;3. 解决方案&lt;/h4&gt;   - **地面分割和松散语义一致性**：&lt;br&gt;     - 提出二次地面分割和基于区域投票的松散语义一致性，减少对单点语义的依赖，从而提高对语义正确性的鲁棒性。&lt;br&gt;   - **语义-几何一致性**：&lt;br&gt;     - 提出用于异常值去除的语义-几何一致性，充分利用语义信息，显著提高对应的质量。&lt;br&gt;&lt;h4&gt;4. 假设验证机制&lt;/h4&gt;   - 提出两阶段假设验证，解决弱几何场景中不正确变换选择的问题。&lt;br&gt;&lt;h4&gt;5. 实验结果&lt;/h4&gt;   - 在户外数据集中，所提方法展示出优越的性能，注册召回率提高了22.5个百分点，并在各种条件下展现出更好的鲁棒性。&lt;br&gt;&lt;h4&gt;6. 代码可用性&lt;/h4&gt;   - 提供了相关代码，以便于其他研究者的使用和验证。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-07-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/GuiyuZhao/SGOR&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we introduce a new outlier removal method that fully leverages
geometric and semantic information, to achieve robust registration. Current
semantic-based registration methods only use semantics for point-to-point or
instance semantic correspondence generation, which has two problems. First,
these methods are highly dependent on the correctness of semantics. They
perform poorly in scenarios with incorrect semantics and sparse semantics.
Second, the use of semantics is limited only to the correspondence generation,
resulting in bad performance in the weak geometry scene. To solve these
problems, on the one hand, we propose secondary ground segmentation and loose
semantic consistency based on regional voting. It improves the robustness to
semantic correctness by reducing the dependence on single-point semantics. On
the other hand, we propose semantic-geometric consistency for outlier removal,
which makes full use of semantic information and significantly improves the
quality of correspondences. In addition, a two-stage hypothesis verification is
proposed, which solves the problem of incorrect transformation selection in the
weak geometry scene. In the outdoor dataset, our method demonstrates superior
performance, boosting a 22.5 percentage points improvement in registration
recall and achieving better robustness under various conditions. Our code is
available.</description>
      <guid isPermaLink="false">2407.06297v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Foundation Models for Music: A Survey</title>
      <link>http://arxiv.org/abs/2408.14340v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 近年来，基础模型（FMs），如大型语言模型（LLMs）和潜在扩散模型（LDMs），对音乐等多个领域产生了深远影响。&lt;br&gt;&lt;h4&gt;2. 综述目的&lt;/h4&gt;   - 本文全面审查音乐领域的最先进（SOTA）预训练模型和基础模型，包括表示学习、生成学习和多模态学习。&lt;br&gt;&lt;h4&gt;3. 音乐的重要性&lt;/h4&gt;   - 首先探讨音乐在各种产业中的重要性，并追溯AI在音乐领域的发展历程。&lt;br&gt;&lt;h4&gt;4. 基础模型的探索&lt;/h4&gt;   - 通过分析基础模型针对的多模态，发现许多音乐表示在基础模型开发中尚未得到充分探索。&lt;br&gt;&lt;h4&gt;5. 方法的局限性&lt;/h4&gt;   - 强调以往方法在多样化音乐应用中的缺乏灵活性，以及基础模型在音乐理解、生成和医疗应用中的潜力。&lt;br&gt;&lt;h4&gt;6. 模型细节探讨&lt;/h4&gt;   - 深入探讨模型预训练范式、架构选择、标记化、微调方法和可控性等重要主题，指出如指令调优、上下文学习、规模法则及涌现能力和长序列建模等问题应得到更好的探索。&lt;br&gt;&lt;h4&gt;7. 音乐代理的洞察&lt;/h4&gt;   - 专门部分提供有关音乐代理的见解，并彻底分析预训练和下游任务所需的数据集和评估。&lt;br&gt;&lt;h4&gt;8. 伦理考量&lt;/h4&gt;   - 强调伦理问题的重要性，倡导未来音乐领域的基础模型研究应更多关注可解释性、透明度、人类责任和版权等问题。&lt;br&gt;&lt;h4&gt;9. 未来挑战和趋势&lt;/h4&gt;   - 提供对音乐领域基础模型的未来挑战和趋势的见解，旨在塑造人类与AI在音乐领域协作的轨迹。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/nicolaus625/fm4music&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, foundation models (FMs) such as large language models (LLMs)
and latent diffusion models (LDMs) have profoundly impacted diverse sectors,
including music. This comprehensive review examines state-of-the-art (SOTA)
pre-trained models and foundation models in music, spanning from representation
learning, generative learning and multimodal learning. We first contextualise
the significance of music in various industries and trace the evolution of AI
in music. By delineating the modalities targeted by foundation models, we
discover many of the music representations are underexplored in FM development.
Then, emphasis is placed on the lack of versatility of previous methods on
diverse music applications, along with the potential of FMs in music
understanding, generation and medical application. By comprehensively exploring
the details of the model pre-training paradigm, architectural choices,
tokenisation, finetuning methodologies and controllability, we emphasise the
important topics that should have been well explored, like instruction tuning
and in-context learning, scaling law and emergent ability, as well as
long-sequence modelling etc. A dedicated section presents insights into music
agents, accompanied by a thorough analysis of datasets and evaluations
essential for pre-training and downstream tasks. Finally, by underscoring the
vital importance of ethical considerations, we advocate that following research
on FM for music should focus more on such issues as interpretability,
transparency, human responsibility, and copyright issues. The paper offers
insights into future challenges and trends on FMs for music, aiming to shape
the trajectory of human-AI collaboration in the music realm.</description>
      <guid isPermaLink="false">2408.14340v3</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Fair Minimum Representation Clustering via Integer Programming</title>
      <link>http://arxiv.org/abs/2409.02963v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  arXiv admin note: text overlap with arXiv:2302.03151&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 聚类是一种无监督学习任务，旨在将数据划分为一组簇。这些簇在许多应用中对应于现实世界的构造（如选区、播放列表、电视频道）。&lt;br&gt;&lt;h4&gt;2. 最小代表性要求&lt;/h4&gt;   - 在某些情况下，这些组只有在达到最小代表性水平时（例如，50%才能选举所需候选人）才能获得益处。&lt;br&gt;&lt;h4&gt;3. 研究目标&lt;/h4&gt;   - 本文研究k-means和k-medians聚类问题，增加了每个组（如人口统计组）在至少一定数量的簇中必须具有最小代表性的约束。&lt;br&gt;&lt;h4&gt;4. 问题表述&lt;/h4&gt;   - 通过混合整数优化框架对问题进行公式化。&lt;br&gt;&lt;h4&gt;5. 算法提出&lt;/h4&gt;   - 提出了一种交替最小化算法，称为**MiniReL**，该算法直接整合了公平性约束。&lt;br&gt;&lt;h4&gt;6. 计算复杂性&lt;/h4&gt;   - 尽管将公平性标准纳入算法导致了NP-hard的分配问题，但提供了计算方法，使得该算法在处理大数据集时仍然可行。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 数值结果表明，该方法能够创建更公平的簇，同时在标准基准数据集上几乎不增加聚类成本。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Clustering is an unsupervised learning task that aims to partition data into
a set of clusters. In many applications, these clusters correspond to
real-world constructs (e.g., electoral districts, playlists, TV channels) whose
benefit can only be attained by groups when they reach a minimum level of
representation (e.g., 50\% to elect their desired candidate). In this paper, we
study the k-means and k-medians clustering problems with the additional
constraint that each group (e.g., demographic group) must have a minimum level
of representation in at least a given number of clusters. We formulate the
problem through a mixed-integer optimization framework and present an
alternating minimization algorithm, called MiniReL, that directly incorporates
the fairness constraints. While incorporating the fairness criteria leads to an
NP-Hard assignment problem within the algorithm, we provide computational
approaches that make the algorithm practical even for large datasets. Numerical
results show that the approach is able to create fairer clusters with
practically no increase in the clustering cost across standard benchmark
datasets.</description>
      <guid isPermaLink="false">2409.02963v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>PTTR: Relational 3D Point Cloud Object Tracking with Transformer</title>
      <link>http://arxiv.org/abs/2112.02857v5</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 在点云序列中，3D物体跟踪旨在根据模板点云预测当前搜索点云中物体的位置和方向。&lt;br&gt;&lt;h4&gt;2. 方法动机&lt;/h4&gt;   - 受到变压器（transformers）成功的启发，提出了**点跟踪变压器（PTTR）**，能有效地以粗到细的方式预测高质量的3D跟踪结果。&lt;br&gt;&lt;h4&gt;3. 方法结构&lt;/h4&gt;   - PTTR包含三个新颖的设计：&lt;br&gt;     &lt;h4&gt;1. 关系感知采样（Relation-Aware Sampling）&lt;/h4&gt;        - 采用关系感知采样，保留与给定模板相关的点，而非随机采样。&lt;br&gt;     &lt;h4&gt;2. 点关系变压器（Point Relation Transformer, PRT）&lt;/h4&gt;        - 由自注意力模块和交叉注意力模块组成。全球自注意力操作捕捉长距离依赖性，增强搜索区域和模板的编码点特征。然后通过交叉注意力匹配两组点特征生成粗略跟踪结果。&lt;br&gt;     &lt;h4&gt;3. 预测精 Refinement 模块&lt;/h4&gt;        - 基于粗略跟踪结果，采用新颖的预测精 Refinement 模块以获得最终的精细预测。&lt;br&gt;&lt;h4&gt;4. 基准数据集&lt;/h4&gt;   - 创建了基于Waymo开放数据集的大规模点云单对象跟踪基准。&lt;br&gt;&lt;h4&gt;5. 实验结果&lt;/h4&gt;   - 通过大量实验表明，PTTR在准确性和效率上实现了优越的点云跟踪性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2021-12-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/jasonkks/pttr&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In a point cloud sequence, 3D object tracking aims to predict the location
and orientation of an object in the current search point cloud given a template
point cloud. Motivated by the success of transformers, we propose Point
Tracking TRansformer (PTTR), which efficiently predicts high-quality 3D
tracking results in a coarse-to-fine manner with the help of transformer
operations. PTTR consists of three novel designs. 1) Instead of random
sampling, we design Relation-Aware Sampling to preserve relevant points to
given templates during subsampling. 2) Furthermore, we propose a Point Relation
Transformer (PRT) consisting of a self-attention and a cross-attention module.
The global self-attention operation captures long-range dependencies to enhance
encoded point features for the search area and the template, respectively.
Subsequently, we generate the coarse tracking results by matching the two sets
of point features via cross-attention. 3) Based on the coarse tracking results,
we employ a novel Prediction Refinement Module to obtain the final refined
prediction. In addition, we create a large-scale point cloud single object
tracking benchmark based on the Waymo Open Dataset. Extensive experiments show
that PTTR achieves superior point cloud tracking in both accuracy and
efficiency.</description>
      <guid isPermaLink="false">2112.02857v5</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>SPARK: Self-supervised Personalized Real-time Monocular Face Capture</title>
      <link>http://arxiv.org/abs/2409.07984v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  SIGGRAPH Asia 2024 Conference Paper. Project page:
  https://kelianb.github.io/SPARK/&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 前馈单目人脸捕捉方法旨在从单张人脸图像重建姿态面孔。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 当前最先进的方法能够实时回归参数化3D人脸模型，但仅提供粗略的面部形状估计，限制了在精确3D重建任务（如老化、换脸、数字化妆等）中的应用。&lt;br&gt;&lt;h4&gt;3. 提出的新方法&lt;/h4&gt;   - 本文提出一种高精度3D人脸捕捉方法，利用一组无约束的视频作为先验信息。&lt;br&gt;&lt;h4&gt;4. 方法框架&lt;/h4&gt;   - 采用两阶段方法：&lt;br&gt;     - **第一阶段**：重建个体的详细3D面部头像，捕捉精确的几何形状和外观。&lt;br&gt;     - **第二阶段**：使用预训练的单目人脸重建方法的编码器，替换其解码器为个性化模型，并在视频集合上进行迁移学习。&lt;br&gt;&lt;h4&gt;5. 自监督学习目标&lt;/h4&gt;   - 利用预估的图像形成模型，获得更精确的自监督学习目标，提升表情和姿态对齐的效果。&lt;br&gt;&lt;h4&gt;6. 实时参数回归能力&lt;/h4&gt;   - 训练得到的编码器能够从先前未见的图像中高效回归姿态和表情参数。&lt;br&gt;&lt;h4&gt;7. 高保真网格推断&lt;/h4&gt;   - 结合个性化的几何模型，实现更准确和高保真的网格推断。&lt;br&gt;&lt;h4&gt;8. 评估结果&lt;/h4&gt;   - 通过广泛的定性和定量评估，展示了最终模型相较于现有最先进方法的优越性，并证明其在未见姿态、表情和光照条件下的泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3680528.3687704&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Feedforward monocular face capture methods seek to reconstruct posed faces
from a single image of a person. Current state of the art approaches have the
ability to regress parametric 3D face models in real-time across a wide range
of identities, lighting conditions and poses by leveraging large image datasets
of human faces. These methods however suffer from clear limitations in that the
underlying parametric face model only provides a coarse estimation of the face
shape, thereby limiting their practical applicability in tasks that require
precise 3D reconstruction (aging, face swapping, digital make-up, ...). In this
paper, we propose a method for high-precision 3D face capture taking advantage
of a collection of unconstrained videos of a subject as prior information. Our
proposal builds on a two stage approach. We start with the reconstruction of a
detailed 3D face avatar of the person, capturing both precise geometry and
appearance from a collection of videos. We then use the encoder from a
pre-trained monocular face reconstruction method, substituting its decoder with
our personalized model, and proceed with transfer learning on the video
collection. Using our pre-estimated image formation model, we obtain a more
precise self-supervision objective, enabling improved expression and pose
alignment. This results in a trained encoder capable of efficiently regressing
pose and expression parameters in real-time from previously unseen images,
which combined with our personalized geometry model yields more accurate and
high fidelity mesh inference. Through extensive qualitative and quantitative
evaluation, we showcase the superiority of our final model as compared to
state-of-the-art baselines, and demonstrate its generalization ability to
unseen pose, expression and lighting.</description>
      <guid isPermaLink="false">2409.07984v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Continual Learning in 3D Point Clouds: Employing Spectral Techniques for Exemplar Selection</title>
      <link>http://arxiv.org/abs/2409.08388v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 引入一种新颖的持续学习框架，专注于3D物体分类（CL3D）。&lt;br&gt;&lt;h4&gt;2. 方法概述&lt;/h4&gt;   - 该方法基于从每个类别中选择原型，使用谱聚类（spectral clustering）进行处理。&lt;br&gt;&lt;h4&gt;3. 数据类型&lt;/h4&gt;   - 对于非欧几里得数据（如点云），谱聚类可应用，只要能够定义样本对之间的距离度量。&lt;br&gt;&lt;h4&gt;4. 距离度量的重要性&lt;/h4&gt;   - 选择合适的距离度量使得能够利用3D几何特征来识别每个类别的代表性原型。&lt;br&gt;&lt;h4&gt;5. 聚类空间的探索&lt;/h4&gt;   - 探讨在输入空间（3D点）、局部特征空间（1024维点）和全局特征空间中的聚类有效性。&lt;br&gt;&lt;h4&gt;6. 实验结果&lt;/h4&gt;   - 在ModelNet40、ShapeNet和ScanNet数据集上进行实验，通过仅使用输入空间特征达到最先进的准确性。&lt;br&gt;&lt;h4&gt;7. 特征组合的优势&lt;/h4&gt;   - 利用输入、局部和全局特征的组合，提升了ModelNet和ShapeNet的最先进性能，同时使用的内存仅为竞争方法的一半。&lt;br&gt;&lt;h4&gt;8. ScanNet数据集表现&lt;/h4&gt;   - 在ScanNet这一具有挑战性的数据集上，方法提高了4.1%的准确率，同时内存使用仅为竞争对手的28%，展示了方法的可扩展性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce a novel framework for Continual Learning in 3D object
classification (CL3D). Our approach is based on the selection of prototypes
from each class using spectral clustering. For non-Euclidean data such as point
clouds, spectral clustering can be employed as long as one can define a
distance measure between pairs of samples. Choosing the appropriate distance
measure enables us to leverage 3D geometric characteristics to identify
representative prototypes for each class. We explore the effectiveness of
clustering in the input space (3D points), local feature space
(1024-dimensional points), and global feature space. We conduct experiments on
the ModelNet40, ShapeNet, and ScanNet datasets, achieving state-of-the-art
accuracy exclusively through the use of input space features. By leveraging the
combined input, local, and global features, we have improved the
state-of-the-art on ModelNet and ShapeNet, utilizing nearly half the memory
used by competing approaches. For the challenging ScanNet dataset, our method
enhances accuracy by 4.1% while consuming just 28% of the memory used by our
competitors, demonstrating the scalability of our approach.</description>
      <guid isPermaLink="false">2409.08388v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Learning vs Retrieval: The Role of In-Context Examples in Regression with LLMs</title>
      <link>http://arxiv.org/abs/2409.04318v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 生成性大语言模型（LLMs）具备上下文学习（ICL）的能力，但其工作机制仍然是一个主要的研究问题，相关实验结果并不总是一致。&lt;br&gt;&lt;h4&gt;2. 研究目标&lt;/h4&gt;   - 本文提出一个评估ICL机制的框架，认为ICL机制是内部知识检索与从上下文示例学习的结合，主要聚焦于回归任务。&lt;br&gt;&lt;h4&gt;3. 实验设计&lt;/h4&gt;   - 首先证明LLMs能够在真实世界的数据集上执行回归任务，然后设计实验来测量LLMs在内部知识检索与从上下文示例学习之间的相对程度。&lt;br&gt;&lt;h4&gt;4. 研究假设&lt;/h4&gt;   - 这一过程位于检索和学习两个极端之间的一个光谱上。&lt;br&gt;&lt;h4&gt;5. 分析因素&lt;/h4&gt;   - 提供深入分析，探讨不同因素（如对任务的先前知识以及上下文示例提供的信息类型和丰富性）如何影响这些机制的触发程度。&lt;br&gt;&lt;h4&gt;6. 实验方法&lt;/h4&gt;   - 使用三种LLMs和多个数据集，以验证研究结果的稳健性。&lt;br&gt;&lt;h4&gt;7. 研究贡献&lt;/h4&gt;   - 研究结果为如何设计提示以利用上下文示例进行元学习和知识检索提供了启示，具体取决于所解决的问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/HLR/LvsR-LLM&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generative Large Language Models (LLMs) are capable of being in-context
learners. However, the underlying mechanism of in-context learning (ICL) is
still a major research question, and experimental research results about how
models exploit ICL are not always consistent. In this work, we propose a
framework for evaluating in-context learning mechanisms, which we claim are a
combination of retrieving internal knowledge and learning from in-context
examples by focusing on regression tasks. First, we show that LLMs can perform
regression on real-world datasets and then design experiments to measure the
extent to which the LLM retrieves its internal knowledge versus learning from
in-context examples. We argue that this process lies on a spectrum between
these two extremes. We provide an in-depth analysis of the degrees to which
these mechanisms are triggered depending on various factors, such as prior
knowledge about the tasks and the type and richness of the information provided
by the in-context examples. We employ three LLMs and utilize multiple datasets
to corroborate the robustness of our findings. Our results shed light on how to
engineer prompts to leverage meta-learning from in-context examples and foster
knowledge retrieval depending on the problem being addressed.</description>
      <guid isPermaLink="false">2409.04318v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Molecular Graph Representation Learning via Structural Similarity Information</title>
      <link>http://arxiv.org/abs/2409.08580v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 图神经网络（GNNs）广泛应用于分子图的特征表示学习，增强特征表示的表达能力对于GNNs的有效性至关重要。&lt;br&gt;&lt;h4&gt;2. 当前研究的局限性&lt;/h4&gt;   - 现有研究主要关注个体分子的结构特征，常常忽视分子之间的结构相似性，而这种相似性包含了丰富的分子性质与结构特征之间的关系信息。&lt;br&gt;&lt;h4&gt;3. 提出的解决方案&lt;/h4&gt;   - 引入**分子结构相似性模式GNN（MSSM-GNN）**，这是一种新颖的分子图表示学习方法，能够从全局视角捕捉分子间的结构相似性信息。&lt;br&gt;&lt;h4&gt;4. 方法细节&lt;/h4&gt;   - 设计了一种特殊的图，利用图核算法定量表示分子之间的相似性。然后，采用GNNs从分子图中学习特征表示，以增强性质预测的准确性。&lt;br&gt;&lt;h4&gt;5. 实验结果&lt;/h4&gt;   - 在一系列小规模和大规模的分子数据集上进行的实验表明，MSSM-GNN模型在性能上持续超越了十一种最先进的基线模型。&lt;br&gt;&lt;h4&gt;6. 代码可用性&lt;/h4&gt;   - 相关代码可在GitHub上获取，链接为 [MSSM-GNN](https://github.com/yaoyao-yaoyao-cell/MSSM-GNN)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1007/978-3-031-70352-2_21&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/yaoyao-yaoyao-cell/mssm-gnn&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have been widely employed for feature
representation learning in molecular graphs. Therefore, it is crucial to
enhance the expressiveness of feature representation to ensure the
effectiveness of GNNs. However, a significant portion of current research
primarily focuses on the structural features within individual molecules, often
overlooking the structural similarity between molecules, which is a crucial
aspect encapsulating rich information on the relationship between molecular
properties and structural characteristics. Thus, these approaches fail to
capture the rich semantic information at the molecular structure level. To
bridge this gap, we introduce the \textbf{Molecular Structural Similarity Motif
GNN (MSSM-GNN)}, a novel molecular graph representation learning method that
can capture structural similarity information among molecules from a global
perspective. In particular, we propose a specially designed graph that
leverages graph kernel algorithms to represent the similarity between molecules
quantitatively. Subsequently, we employ GNNs to learn feature representations
from molecular graphs, aiming to enhance the accuracy of property prediction by
incorporating additional molecular representation information. Finally, through
a series of experiments conducted on both small-scale and large-scale molecular
datasets, we demonstrate that our model consistently outperforms eleven
state-of-the-art baselines. The codes are available at
https://github.com/yaoyao-yaoyao-cell/MSSM-GNN.</description>
      <guid isPermaLink="false">2409.08580v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Transfer Meta-Learning: Information-Theoretic Bounds and Information Meta-Risk Minimization</title>
      <link>http://arxiv.org/abs/2011.02872v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 元学习通过观察多个相关任务的数据，自动推断归纳偏差。归纳偏差通过超参数编码，这些超参数决定模型类别或训练算法的某些方面（如初始化或学习率）。&lt;br&gt;&lt;h4&gt;2. 元学习假设&lt;/h4&gt;   - 元学习假设学习任务属于同一任务环境，并且在元训练和元测试期间任务来自相同的环境。然而，这在实际中可能并不成立。&lt;br&gt;&lt;h4&gt;3. 引入的新问题&lt;/h4&gt;   - 本文引入转移元学习的问题，其中在元测试期间，任务来自一个可能与元训练期间观察到的源任务环境不同的目标任务环境。&lt;br&gt;&lt;h4&gt;4. 转移元泛化间隙&lt;/h4&gt;   - 提出了新颖的信息论上界，度量元训练损失与目标任务环境中随机选择的新任务的平均损失之间的差异，即转移元泛化间隙。&lt;br&gt;&lt;h4&gt;5. 第一个界限&lt;/h4&gt;   - 第一个界限是关于平均转移元泛化间隙的，利用源任务和目标任务的数据分布之间的KL散度捕获源与目标任务环境之间的转变。&lt;br&gt;&lt;h4&gt;6. 第二和第三个界限&lt;/h4&gt;   - 第二个界限为PAC-Bayesian界限，第三个为单抽样界限，这两个界限通过源任务和目标任务分布之间的对数似然比来解释环境转变。&lt;br&gt;&lt;h4&gt;7. 转移元学习解决方案&lt;/h4&gt;   - 提出了两种转移元学习解决方案。第一种称为经验元风险最小化（EMRM），我们推导了平均最优性间隙的界限。第二种被称为信息元风险最小化（IMRM），通过最小化PAC-Bayesian界限获得。&lt;br&gt;&lt;h4&gt;8. 实验结果&lt;/h4&gt;   - 实验表明，IMRM在性能上有潜力超越EMRM。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2020-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Meta-learning automatically infers an inductive bias by observing data from a
number of related tasks. The inductive bias is encoded by hyperparameters that
determine aspects of the model class or training algorithm, such as
initialization or learning rate. Meta-learning assumes that the learning tasks
belong to a task environment, and that tasks are drawn from the same task
environment both during meta-training and meta-testing. This, however, may not
hold true in practice. In this paper, we introduce the problem of transfer
meta-learning, in which tasks are drawn from a target task environment during
meta-testing that may differ from the source task environment observed during
meta-training. Novel information-theoretic upper bounds are obtained on the
transfer meta-generalization gap, which measures the difference between the
meta-training loss, available at the meta-learner, and the average loss on
meta-test data from a new, randomly selected, task in the target task
environment. The first bound, on the average transfer meta-generalization gap,
captures the meta-environment shift between source and target task environments
via the KL divergence between source and target data distributions. The second,
PAC-Bayesian bound, and the third, single-draw bound, account for this shift
via the log-likelihood ratio between source and target task distributions.
Furthermore, two transfer meta-learning solutions are introduced. For the
first, termed Empirical Meta-Risk Minimization (EMRM), we derive bounds on the
average optimality gap. The second, referred to as Information Meta-Risk
Minimization (IMRM), is obtained by minimizing the PAC-Bayesian bound. IMRM is
shown via experiments to potentially outperform EMRM.</description>
      <guid isPermaLink="false">2011.02872v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>DSCLAP: Domain-Specific Contrastive Language-Audio Pre-Training</title>
      <link>http://arxiv.org/abs/2409.09289v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 分析真实世界的多模态信号对于智能语音助手（IVAs）至关重要且具有挑战性。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 主流方法通过预训练的音频模型和文本模型在各种下游任务上取得了显著性能，但这些模型是独立预训练的，并且通常在与目标领域不同的任务上进行训练，导致下游任务中模态表示的次优。&lt;br&gt;&lt;h4&gt;3. 数据收集的困难&lt;/h4&gt;   - 在许多领域，收集足够的语言-音频配对非常困难，且转录原始音频需要高专业技能，使得联合预训练变得困难或甚至不可行。&lt;br&gt;&lt;h4&gt;4. 提出的解决方案&lt;/h4&gt;   - 提出DSCLAP，一个简单有效的框架，能够仅使用原始音频信号进行语言-音频预训练。&lt;br&gt;&lt;h4&gt;5. 框架工作原理&lt;/h4&gt;   - DSCLAP通过自动语音识别（ASR）系统将原始音频信号转换为文本，并结合对比学习目标和语言-音频匹配目标来对齐音频与ASR转录。&lt;br&gt;&lt;h4&gt;6. 预训练数据&lt;/h4&gt;   - 在12,107小时的车载领域音频上预训练DSCLAP。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 在两个下游任务上的实证结果表明，尽管概念上简单，DSCLAP在所有指标上显著优于基线模型，展示了在特定领域IVAs应用中的巨大潜力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Analyzing real-world multimodal signals is an essential and challenging task
for intelligent voice assistants (IVAs). Mainstream approaches have achieved
remarkable performance on various downstream tasks of IVAs with pre-trained
audio models and text models. However, these models are pre-trained
independently and usually on tasks different from target domains, resulting in
sub-optimal modality representations for downstream tasks. Moreover, in many
domains, collecting enough language-audio pairs is extremely hard, and
transcribing raw audio also requires high professional skills, making it
difficult or even infeasible to joint pre-training. To address these
painpoints, we propose DSCLAP, a simple and effective framework that enables
language-audio pre-training with only raw audio signal input. Specifically,
DSCLAP converts raw audio signals into text via an ASR system and combines a
contrastive learning objective and a language-audio matching objective to align
the audio and ASR transcriptions. We pre-train DSCLAP on 12,107 hours of
in-vehicle domain audio. Empirical results on two downstream tasks show that
while conceptually simple, DSCLAP significantly outperforms the baseline models
in all metrics, showing great promise for domain-specific IVAs applications.</description>
      <guid isPermaLink="false">2409.09289v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>LPT++: Efficient Training on Mixture of Long-tailed Experts</title>
      <link>http://arxiv.org/abs/2409.11323v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Extended version of arXiv:2210.01033&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究目的&lt;/h4&gt;   - 介绍LPT++，一个针对长尾分类的综合框架，结合了参数高效微调（PEFT）和可学习的模型集成。&lt;br&gt;&lt;h4&gt;2. 框架增强&lt;/h4&gt;   - LPT++通过整合三个核心组件来增强冻结的视觉变换器（ViTs）。&lt;br&gt;&lt;h4&gt;3. 核心组件&lt;/h4&gt;   - **长尾适应模块**：这是一个通用的适应模块，聚合长尾提示和视觉适配器，以适应预训练模型到目标域，同时提高其区分能力。&lt;br&gt;   - **长尾专家框架**：该框架结合了混合专家（MoE）评分器，能够自适应地计算来自视觉模型和视觉-语言（VL）模型专家的置信分数重加权系数，从而生成更准确的预测。&lt;br&gt;   - **三阶段训练框架**：每个关键模块单独学习，形成一个稳定有效的长尾分类训练范式。&lt;br&gt;&lt;h4&gt;4. 简化版本&lt;/h4&gt;   - 提出了LPT的简化版本，仅集成视觉预训练ViT和长尾提示，形成单一模型方法。这有助于清晰展示长尾提示的工作原理，并在没有VL预训练模型的情况下实现可比性能。&lt;br&gt;&lt;h4&gt;5. 实验结果&lt;/h4&gt;   - 实验表明，LPT++仅需约1%的额外可训练参数，便能在准确性上与所有对比方法相媲美。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce LPT++, a comprehensive framework for long-tailed classification
that combines parameter-efficient fine-tuning (PEFT) with a learnable model
ensemble. LPT++ enhances frozen Vision Transformers (ViTs) through the
integration of three core components. The first is a universal long-tailed
adaptation module, which aggregates long-tailed prompts and visual adapters to
adapt the pretrained model to the target domain, meanwhile improving its
discriminative ability. The second is the mixture of long-tailed experts
framework with a mixture-of-experts (MoE) scorer, which adaptively calculates
reweighting coefficients for confidence scores from both visual-only and
visual-language (VL) model experts to generate more accurate predictions.
Finally, LPT++ employs a three-phase training framework, wherein each critical
module is learned separately, resulting in a stable and effective long-tailed
classification training paradigm. Besides, we also propose the simple version
of LPT++ namely LPT, which only integrates visual-only pretrained ViT and
long-tailed prompts to formulate a single model method. LPT can clearly
illustrate how long-tailed prompts works meanwhile achieving comparable
performance without VL pretrained models. Experiments show that, with only ~1%
extra trainable parameters, LPT++ achieves comparable accuracy against all the
counterparts.</description>
      <guid isPermaLink="false">2409.11323v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>OpenNeRF: Open Set 3D Neural Scene Segmentation with Pixel-Wise Features and Rendered Novel Views</title>
      <link>http://arxiv.org/abs/2404.03650v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICLR 2024, Project page: https://opennerf.github.io&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 大型视觉语言模型（VLMs），如CLIP，能够以零样本方式从图像中分割任意概念，实现开放集图像分割。&lt;br&gt;&lt;h4&gt;2. 开放集与闭合集的区别&lt;/h4&gt;   - 传统的闭合集假设指模型只能从预定义的训练集中分割类别，而开放集则允许分割未见过的类别。&lt;br&gt;&lt;h4&gt;3. 3D场景中的开放集分割&lt;/h4&gt;   - 最近的研究开始关注3D场景中的开放集分割。这些方法受限于传统的闭合集3D卷积方法，处理点云或多边形网格。&lt;br&gt;&lt;h4&gt;4. 3D表示的局限性&lt;/h4&gt;   - 点云和3D网格通常分辨率低，且重建的3D场景几何结构可能无法很好地映射到用于计算像素对齐CLIP特征的2D图像序列上。&lt;br&gt;&lt;h4&gt;5. 提出的解决方案&lt;/h4&gt;   - 本文提出了OpenNeRF，能够自然地在已定位图像上操作，并直接在NeRF中编码VLM特征。&lt;br&gt;&lt;h4&gt;6. 与现有方法的比较&lt;/h4&gt;   - OpenNeRF与LERF在精神上相似，但使用像素级VLM特征（而非全局CLIP特征），结果导致架构复杂度降低，且无需额外的DINO正则化。&lt;br&gt;&lt;h4&gt;7. 功能优势&lt;/h4&gt;   - OpenNeRF还利用NeRF的能力来渲染新视图，并从初始已定位图像中未充分观察到的区域提取开放集VLM特征。&lt;br&gt;&lt;h4&gt;8. 实验结果&lt;/h4&gt;   - 在Replica数据集上的3D点云分割任务中，OpenNeRF的性能优于近期的开放词汇方法，如LERF和OpenScene，至少提高了4.9 mIoU。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-04-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large visual-language models (VLMs), like CLIP, enable open-set image
segmentation to segment arbitrary concepts from an image in a zero-shot manner.
This goes beyond the traditional closed-set assumption, i.e., where models can
only segment classes from a pre-defined training set. More recently, first
works on open-set segmentation in 3D scenes have appeared in the literature.
These methods are heavily influenced by closed-set 3D convolutional approaches
that process point clouds or polygon meshes. However, these 3D scene
representations do not align well with the image-based nature of the
visual-language models. Indeed, point cloud and 3D meshes typically have a
lower resolution than images and the reconstructed 3D scene geometry might not
project well to the underlying 2D image sequences used to compute pixel-aligned
CLIP features. To address these challenges, we propose OpenNeRF which naturally
operates on posed images and directly encodes the VLM features within the NeRF.
This is similar in spirit to LERF, however our work shows that using pixel-wise
VLM features (instead of global CLIP features) results in an overall less
complex architecture without the need for additional DINO regularization. Our
OpenNeRF further leverages NeRF's ability to render novel views and extract
open-set VLM features from areas that are not well observed in the initial
posed images. For 3D point cloud segmentation on the Replica dataset, OpenNeRF
outperforms recent open-vocabulary methods such as LERF and OpenScene by at
least +4.9 mIoU.</description>
      <guid isPermaLink="false">2404.03650v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Stein ICP for Uncertainty Estimation in Point Cloud Matching</title>
      <link>http://arxiv.org/abs/2106.03287v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 7 figures, Robotics and Automation Letters&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 超图通过引入超边（hyperedges）克服了传统图的局限性。超边可以连接任意数量的节点，而传统的图边仅连接两个节点。&lt;br&gt;&lt;h4&gt;2. 超图神经网络（HGNNs）&lt;/h4&gt;   - HGNNs的消息传递机制为顶点-超边-顶点的形式，这使得HGNNs能够捕捉和利用比传统图神经网络（GNNs）更丰富和复杂的结构信息。&lt;br&gt;&lt;h4&gt;3. 重叠子图的概念&lt;/h4&gt;   - 最近，重叠子图的概念得到了关注。这些子图能够捕获关于顶点子组的更多信息，允许一个顶点属于多个组或子图，而不是仅限于一个组。&lt;br&gt;&lt;h4&gt;4. 图聚类中的重要问题&lt;/h4&gt;   - 图聚类中的一个重要问题是寻找最稠密的重叠子图（DOS）。&lt;br&gt;&lt;h4&gt;5. 提出的解决方案&lt;/h4&gt;   - 本文提出了一种通过聚合贪婪枚举（DOSAGE）算法解决DOS问题的新方法，旨在增强生成最稠密重叠子图的过程，从而构建更为稳健的超图。&lt;br&gt;&lt;h4&gt;6. 实验结果&lt;/h4&gt;   - 在标准基准测试上的实验表明，DOSAGE算法在节点分类任务中显著优于HGNNs和其他六种方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2021-06-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/LRA.2021.3137503n&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://bitbucket.org/fafz/stein-icp&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Quantification of uncertainty in point cloud matching is critical in many
tasks such as pose estimation, sensor fusion, and grasping. Iterative closest
point (ICP) is a commonly used pose estimation algorithm which provides a point
estimate of the transformation between two point clouds. There are many sources
of uncertainty in this process that may arise due to sensor noise, ambiguous
environment, and occlusion. However, for safety critical problems such as
autonomous driving, a point estimate of the pose transformation is not
sufficient as it does not provide information about the multiple solutions.
Current probabilistic ICP methods usually do not capture all sources of
uncertainty and may provide unreliable transformation estimates which can have
a detrimental effect in state estimation or decision making tasks that use this
information. In this work we propose a new algorithm to align two point clouds
that can precisely estimate the uncertainty of ICP's transformation parameters.
We develop a Stein variational inference framework with gradient based
optimization of ICP's cost function. The method provides a non-parametric
estimate of the transformation, can model complex multi-modal distributions,
and can be effectively parallelized on a GPU. Experiments using 3D kinect data
as well as sparse indoor/outdoor LiDAR data show that our method is capable of
efficiently producing accurate pose uncertainty estimates.</description>
      <guid isPermaLink="false">2106.03287v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Hyperedge Modeling in Hypergraph Neural Networks by using Densest Overlapping Subgraphs</title>
      <link>http://arxiv.org/abs/2409.10340v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 本文介绍了三滤波器到法线+（3F2N+），这是对之前工作三滤波器到法线（3F2N）的扩展。&lt;br&gt;&lt;h4&gt;2. 主要目标&lt;/h4&gt;   - 重点在于将不连续性判别能力纳入表面法线估计器（SNEs）。&lt;br&gt;&lt;h4&gt;3. 方法概述&lt;/h4&gt;   - 3F2N+通过利用一种新颖的不连续性判别模块（DDM）实现该能力。&lt;br&gt;   - DDM结合了深度曲率最小化和相关系数最大化，采用条件随机场（CRFs）。&lt;br&gt;&lt;h4&gt;4. 数据集构建&lt;/h4&gt;   - 为评估SNEs在噪声数据上的鲁棒性，创建了一个大规模合成表面法线（SSN）数据集，包含20个场景（10个室内场景和10个室外场景，且部分场景添加了随机高斯噪声）。&lt;br&gt;&lt;h4&gt;5. 实验结果&lt;/h4&gt;   - 实验表明，3F2N+在所有几何基础的表面法线估计器中表现最佳。&lt;br&gt;   - 在干净的室内、干净的室外、噪声室内和噪声室外数据集上的平均角误差分别为7.85°、8.95°、9.25°和11.98°。&lt;br&gt;&lt;h4&gt;6. 附加实验&lt;/h4&gt;   - 进行了三项额外实验，以展示3F2N+在下游机器人感知任务中的有效性，包括自由空间检测、6D物体姿态估计和点云补全。&lt;br&gt;&lt;h4&gt;7. 资源获取&lt;/h4&gt;   - 源代码和数据集公开可用，网址为 [https://mias.group/3F2Nplus](https://mias.group/3F2Nplus)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hypergraphs tackle the limitations of traditional graphs by introducing {\em
hyperedges}. While graph edges connect only two nodes, hyperedges connect an
arbitrary number of nodes along their edges. Also, the underlying
message-passing mechanisms in Hypergraph Neural Networks (HGNNs) are in the
form of vertex-hyperedge-vertex, which let HGNNs capture and utilize richer and
more complex structural information than traditional Graph Neural Networks
(GNNs). More recently, the idea of overlapping subgraphs has emerged. These
subgraphs can capture more information about subgroups of vertices without
limiting one vertex belonging to just one group, allowing vertices to belong to
multiple groups or subgraphs. In addition, one of the most important problems
in graph clustering is to find densest overlapping subgraphs (DOS). In this
paper, we propose a solution to the DOS problem via Agglomerative Greedy
Enumeration (DOSAGE) algorithm as a novel approach to enhance the process of
generating the densest overlapping subgraphs and, hence, a robust construction
of the hypergraphs. Experiments on standard benchmarks show that the DOSAGE
algorithm significantly outperforms the HGNNs and six other methods on the node
classification task.</description>
      <guid isPermaLink="false">2409.10340v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Three-Filters-to-Normal+: Revisiting Discontinuity Discrimination in Depth-to-Normal Translation</title>
      <link>http://arxiv.org/abs/2312.07964v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 本文介绍了三滤波器到法线+（3F2N+），这是对之前工作三滤波器到法线（3F2N）的扩展。&lt;br&gt;&lt;h4&gt;2. 主要目标&lt;/h4&gt;   - 重点在于将不连续性判别能力纳入表面法线估计器（SNEs）。&lt;br&gt;&lt;h4&gt;3. 方法概述&lt;/h4&gt;   - 3F2N+通过利用一种新颖的不连续性判别模块（DDM）实现该能力。&lt;br&gt;   - DDM结合了深度曲率最小化和相关系数最大化，采用条件随机场（CRFs）。&lt;br&gt;&lt;h4&gt;4. 数据集构建&lt;/h4&gt;   - 为评估SNEs在噪声数据上的鲁棒性，创建了一个大规模合成表面法线（SSN）数据集，包含20个场景（10个室内场景和10个室外场景，且部分场景添加了随机高斯噪声）。&lt;br&gt;&lt;h4&gt;5. 实验结果&lt;/h4&gt;   - 实验表明，3F2N+在所有几何基础的表面法线估计器中表现最佳。&lt;br&gt;   - 在干净的室内、干净的室外、噪声室内和噪声室外数据集上的平均角误差分别为7.85°、8.95°、9.25°和11.98°。&lt;br&gt;&lt;h4&gt;6. 附加实验&lt;/h4&gt;   - 进行了三项额外实验，以展示3F2N+在下游机器人感知任务中的有效性，包括自由空间检测、6D物体姿态估计和点云补全。&lt;br&gt;&lt;h4&gt;7. 资源获取&lt;/h4&gt;   - 源代码和数据集公开可用，网址为 [https://mias.group/3F2Nplus](https://mias.group/3F2Nplus)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2023-12-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This article introduces three-filters-to-normal+ (3F2N+), an extension of our
previous work three-filters-to-normal (3F2N), with a specific focus on
incorporating discontinuity discrimination capability into surface normal
estimators (SNEs). 3F2N+ achieves this capability by utilizing a novel
discontinuity discrimination module (DDM), which combines depth curvature
minimization and correlation coefficient maximization through conditional
random fields (CRFs). To evaluate the robustness of SNEs on noisy data, we
create a large-scale synthetic surface normal (SSN) dataset containing 20
scenarios (ten indoor scenarios and ten outdoor scenarios with and without
random Gaussian noise added to depth images). Extensive experiments demonstrate
that 3F2N+ achieves greater performance than all other geometry-based surface
normal estimators, with average angular errors of 7.85$^\circ$, 8.95$^\circ$,
9.25$^\circ$, and 11.98$^\circ$ on the clean-indoor, clean-outdoor,
noisy-indoor, and noisy-outdoor datasets, respectively. We conduct three
additional experiments to demonstrate the effectiveness of incorporating our
proposed 3F2N+ into downstream robot perception tasks, including freespace
detection, 6D object pose estimation, and point cloud completion. Our source
code and datasets are publicly available at https://mias.group/3F2Nplus.</description>
      <guid isPermaLink="false">2312.07964v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Incremental Multiview Point Cloud Registration with Two-stage Candidate Retrieval</title>
      <link>http://arxiv.org/abs/2407.07525v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 多视点云配准是多种计算机视觉任务的基础。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 传统方法通常采用全局范式，首先构建姿态图，然后进行运动同步以确定绝对姿态。&lt;br&gt;   - 这种分离的方法可能无法充分利用多视角配准的特性，并且在低重叠场景中表现不佳。&lt;br&gt;&lt;h4&gt;3. 提出的新方法&lt;/h4&gt;   - 本文提出了一种增量多视点云配准方法，逐步将所有扫描数据注册到一个不断增长的元形状中。&lt;br&gt;&lt;h4&gt;4. 增量顺序的确定&lt;/h4&gt;   - 采用两阶段的粗到细策略进行点云候选检索：&lt;br&gt;     - **第一阶段**：基于邻域融合增强的全局聚合特征进行粗选择。&lt;br&gt;     - **第二阶段**：通过几何匹配进一步对候选进行重新排序。&lt;br&gt;&lt;h4&gt;5. 误差缓解技术&lt;/h4&gt;   - 应用变换平均技术，以减轻配准过程中积累的误差。&lt;br&gt;&lt;h4&gt;6. 密度变异处理&lt;/h4&gt;   - 利用基于水库抽样的技术，解决密度变异问题，同时减少计算负担。&lt;br&gt;&lt;h4&gt;7. 实验验证&lt;/h4&gt;   - 在各种基准测试中，综合实验结果验证了该方法的有效性和通用性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-07-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multiview point cloud registration serves as a cornerstone of various
computer vision tasks. Previous approaches typically adhere to a global
paradigm, where a pose graph is initially constructed followed by motion
synchronization to determine the absolute pose. However, this separated
approach may not fully leverage the characteristics of multiview registration
and might struggle with low-overlap scenarios. In this paper, we propose an
incremental multiview point cloud registration method that progressively
registers all scans to a growing meta-shape. To determine the incremental
ordering, we employ a two-stage coarse-to-fine strategy for point cloud
candidate retrieval. The first stage involves the coarse selection of scans
based on neighbor fusion-enhanced global aggregation features, while the second
stage further reranks candidates through geometric-based matching.
Additionally, we apply a transformation averaging technique to mitigate
accumulated errors during the registration process. Finally, we utilize a
Reservoir sampling-based technique to address density variance issues while
reducing computational load. Comprehensive experimental results across various
benchmarks validate the effectiveness and generalization of our approach.</description>
      <guid isPermaLink="false">2407.07525v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>BCOT: A Markerless High-Precision 3D Object Tracking Benchmark</title>
      <link>http://arxiv.org/abs/2203.13437v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 基于模板的3D对象跟踪缺乏高精度的真实场景基准，主要因为在不使用标记的情况下难以准确标注真实移动视频对象的3D姿态。&lt;br&gt;&lt;h4&gt;2. 提出的方法&lt;/h4&gt;   - 本文提出了一种多视角方法，用于估计真实移动对象的准确3D姿态，并利用双目数据构建新的单目无纹理3D对象跟踪基准。&lt;br&gt;&lt;h4&gt;3. 方法特点&lt;/h4&gt;   - 该方法不需要标记，且摄像头仅需同步、相对固定并经过标定。&lt;br&gt;&lt;h4&gt;4. 优化过程&lt;/h4&gt;   - 基于物体中心模型，通过最小化所有视角的形状重投影约束来联合优化物体姿态，显著提高了与单视角方法相比的准确性，且比基于深度的方法更为准确。&lt;br&gt;&lt;h4&gt;5. 基准数据集&lt;/h4&gt;   - 新的基准数据集包含20个无纹理物体、22个场景、404个视频序列和126K张在真实场景中捕获的图像。&lt;br&gt;&lt;h4&gt;6. 标注精度&lt;/h4&gt;   - 根据理论分析和验证实验，标注误差保证小于2毫米。&lt;br&gt;&lt;h4&gt;7. 性能评估&lt;/h4&gt;   - 使用新数据集重新评估了现有最先进的3D对象跟踪方法，并报告了它们在真实场景中的性能排名。&lt;br&gt;&lt;h4&gt;8. 资源获取&lt;/h4&gt;   - BCOT基准和代码可在提供的网址访问。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2022-03-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Template-based 3D object tracking still lacks a high-precision benchmark of
real scenes due to the difficulty of annotating the accurate 3D poses of real
moving video objects without using markers. In this paper, we present a
multi-view approach to estimate the accurate 3D poses of real moving objects,
and then use binocular data to construct a new benchmark for monocular
textureless 3D object tracking. The proposed method requires no markers, and
the cameras only need to be synchronous, relatively fixed as cross-view and
calibrated. Based on our object-centered model, we jointly optimize the object
pose by minimizing shape re-projection constraints in all views, which greatly
improves the accuracy compared with the single-view approach, and is even more
accurate than the depth-based method. Our new benchmark dataset contains 20
textureless objects, 22 scenes, 404 video sequences and 126K images captured in
real scenes. The annotation error is guaranteed to be less than 2mm, according
to both theoretical analysis and validation experiments. We re-evaluate the
state-of-the-art 3D object tracking methods with our dataset, reporting their
performance ranking in real scenes. Our BCOT benchmark and code can be found at
https://ar3dv.github.io/BCOT-Benchmark/.</description>
      <guid isPermaLink="false">2203.13437v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Introduction to Machine Learning</title>
      <link>http://arxiv.org/abs/2409.02668v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  textbook&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 书籍概述&lt;/h4&gt;   - 本书介绍了机器学习算法开发和分析的数学基础和技术。&lt;br&gt;&lt;h4&gt;2. 引言章节&lt;/h4&gt;   - 描述了全书使用的符号，并回顾了基本的微积分、线性代数和概率概念。&lt;br&gt;   - 引入了测度论术语，作为阅读指南，帮助理解相关章节。&lt;br&gt;&lt;h4&gt;3. 背景材料&lt;/h4&gt;   - 提供了矩阵分析和优化的背景材料，为书中使用的许多算法提供理论支持，如随机梯度下降和近端方法。&lt;br&gt;&lt;h4&gt;4. 统计预测基础&lt;/h4&gt;   - 讨论了统计预测的基本概念，并介绍了再生核理论和希尔伯特空间技术，这些在后续章节中被广泛使用。&lt;br&gt;&lt;h4&gt;5. 监督统计学习算法&lt;/h4&gt;   - 详细描述了多种监督学习算法，包括线性方法、支持向量机、决策树、提升方法和神经网络。&lt;br&gt;&lt;h4&gt;6. 生成模型&lt;/h4&gt;   - 转向生成方法，首先介绍采样方法和马尔可夫链理论。&lt;br&gt;&lt;h4&gt;7. 图模型理论&lt;/h4&gt;   - 描述图模型的理论，介绍潜变量模型的变分方法和基于深度学习的生成模型。&lt;br&gt;&lt;h4&gt;8. 无监督学习方法&lt;/h4&gt;   - 重点讨论无监督学习方法，包括聚类、因子分析和流形学习。&lt;br&gt;&lt;h4&gt;9. 理论讨论&lt;/h4&gt;   - 最后一章侧重于理论，讨论集中不等式和泛化界限。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This book introduces the mathematical foundations and techniques that lead to
the development and analysis of many of the algorithms that are used in machine
learning. It starts with an introductory chapter that describes notation used
throughout the book and serve at a reminder of basic concepts in calculus,
linear algebra and probability and also introduces some measure theoretic
terminology, which can be used as a reading guide for the sections that use
these tools. The introductory chapters also provide background material on
matrix analysis and optimization. The latter chapter provides theoretical
support to many algorithms that are used in the book, including stochastic
gradient descent, proximal methods, etc. After discussing basic concepts for
statistical prediction, the book includes an introduction to reproducing kernel
theory and Hilbert space techniques, which are used in many places, before
addressing the description of various algorithms for supervised statistical
learning, including linear methods, support vector machines, decision trees,
boosting, or neural networks. The subject then switches to generative methods,
starting with a chapter that presents sampling methods and an introduction to
the theory of Markov chains. The following chapter describe the theory of
graphical models, an introduction to variational methods for models with latent
variables, and to deep-learning based generative models. The next chapters
focus on unsupervised learning methods, for clustering, factor analysis and
manifold learning. The final chapter of the book is theory-oriented and
discusses concentration inequalities and generalization bounds.</description>
      <guid isPermaLink="false">2409.02668v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>GSIFN: A Graph-Structured and Interlaced-Masked Multimodal Transformer-based Fusion Network for Multimodal Sentiment Analysis</title>
      <link>http://arxiv.org/abs/2408.14809v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究主题&lt;/h4&gt;   - 多模态情感分析（MSA）利用多种数据模态来分析人类情感。&lt;br&gt;&lt;h4&gt;2. 现有模型的挑战&lt;/h4&gt;   - 现有的MSA模型通常采用先进的多模态融合和表示学习方法，但面临两个主要挑战：&lt;br&gt;     - **模态组合的解耦与参数冗余**：现有多模态融合方法导致融合性能和效率不足。&lt;br&gt;     - **表示能力与计算开销的权衡**：在单模态特征提取器和编码器中，存在表示能力与计算开销之间的艰难权衡。&lt;br&gt;&lt;h4&gt;3. 提出的解决方案&lt;/h4&gt;   - 本文提出的GSIFN（图结构互交掩蔽多模态Transformer网络）包括两个主要组件：&lt;br&gt;     - **图结构和互交掩蔽的多模态Transformer**：&lt;br&gt;       - 采用互交掩蔽机制构建稳健的多模态图嵌入，实现全模态一体化的Transformer融合，并大幅降低计算开销。&lt;br&gt;     - **自监督学习框架**：&lt;br&gt;       - 具有低计算开销和高性能，利用并行LSTM和矩阵记忆增强非语言模态特征以生成单模态标签。&lt;br&gt;&lt;h4&gt;4. 实验评估&lt;/h4&gt;   - 在MSA数据集CMU-MOSI、CMU-MOSEI和CH-SIMS上进行评估，GSIFN展现出优越的性能，相较于之前的最先进模型显著降低了计算开销。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal Sentiment Analysis (MSA) leverages multiple data modals to analyze
human sentiment. Existing MSA models generally employ cutting-edge multimodal
fusion and representation learning-based methods to promote MSA capability.
However, there are two key challenges: (i) in existing multimodal fusion
methods, the decoupling of modal combinations and tremendous parameter
redundancy, lead to insufficient fusion performance and efficiency; (ii) a
challenging trade-off exists between representation capability and
computational overhead in unimodal feature extractors and encoders. Our
proposed GSIFN incorporates two main components to solve these problems: (i) a
graph-structured and interlaced-masked multimodal Transformer. It adopts the
Interlaced Mask mechanism to construct robust multimodal graph embedding,
achieve all-modal-in-one Transformer-based fusion, and greatly reduce the
computational overhead; (ii) a self-supervised learning framework with low
computational overhead and high performance, which utilizes a parallelized LSTM
with matrix memory to enhance non-verbal modal features for unimodal label
generation. Evaluated on the MSA datasets CMU-MOSI, CMU-MOSEI, and CH-SIMS,
GSIFN demonstrates superior performance with significantly lower computational
overhead compared with previous state-of-the-art models.</description>
      <guid isPermaLink="false">2408.14809v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>SimMAT: Exploring Transferability from Vision Foundation Models to Any Image Modality</title>
      <link>http://arxiv.org/abs/2409.08083v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Github link: https://github.com/mt-cly/SimMAT&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 背景介绍&lt;/h4&gt;   - 基础模型如ChatGPT和Sora在大规模数据训练下产生了革命性的社会影响。&lt;br&gt;&lt;h4&gt;2. 挑战&lt;/h4&gt;   - 在许多不同领域，传感器收集与自然图像相似规模的数据以训练强大的基础模型极具挑战性。&lt;br&gt;&lt;h4&gt;3. 研究目标&lt;/h4&gt;   - 本文提出了一个简单有效的框架SimMAT，研究一个开放问题：从在自然RGB图像上训练的视觉基础模型向其他具有不同物理属性的图像模态（如偏振图像）的可转移性。&lt;br&gt;&lt;h4&gt;4. 框架构成&lt;/h4&gt;   - SimMAT由一个模态无关的转移层（MAT）和一个预训练的基础模型组成。&lt;br&gt;&lt;h4&gt;5. 应用示例&lt;/h4&gt;   - 将SimMAT应用于一个代表性的视觉基础模型“Segment Anything Model”（SAM），以支持评估的新图像模态。&lt;br&gt;&lt;h4&gt;6. 基准构建&lt;/h4&gt;   - 由于缺乏相关基准，构建了一个新的基准来评估迁移学习性能。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 实验确认了迁移视觉基础模型在提高其他传感器性能方面的潜力，具体表现为SimMAT在评估模态上平均提高了分割性能（mIoU）从22.15%提升至53.88%，且持续优于其他基线。&lt;br&gt;&lt;h4&gt;8. 研究意义&lt;/h4&gt;   - 希望SimMAT能够提高对跨模态迁移学习的关注，并为各个领域带来更好的视觉基础模型结果。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/mt-cly/simmat&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models like ChatGPT and Sora that are trained on a huge scale of
data have made a revolutionary social impact. However, it is extremely
challenging for sensors in many different fields to collect similar scales of
natural images to train strong foundation models. To this end, this work
presents a simple and effective framework SimMAT to study an open problem: the
transferability from vision foundation models trained on natural RGB images to
other image modalities of different physical properties (e.g., polarization).
SimMAT consists of a modality-agnostic transfer layer (MAT) and a pretrained
foundation model. We apply SimMAT to a representative vision foundation model
Segment Anything Model (SAM) to support any evaluated new image modality. Given
the absence of relevant benchmarks, we construct a new benchmark to evaluate
the transfer learning performance. Our experiments confirm the intriguing
potential of transferring vision foundation models in enhancing other sensors'
performance. Specifically, SimMAT can improve the segmentation performance
(mIoU) from 22.15% to 53.88% on average for evaluated modalities and
consistently outperforms other baselines. We hope that SimMAT can raise
awareness of cross-modal transfer learning and benefit various fields for
better results with vision foundation models.</description>
      <guid isPermaLink="false">2409.08083v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Domain-Invariant Representation Learning of Bird Sounds</title>
      <link>http://arxiv.org/abs/2409.08589v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 被动声学监测（PAM）对生物声学研究至关重要，能够实现非侵入性的物种追踪和生物多样性监测。&lt;br&gt;&lt;h4&gt;2. 公民科学平台的作用&lt;/h4&gt;   - 像Xeno-Canto这样的公民科学平台提供了大量注释数据集，这些数据集来自于有针对性的录音，特定物种被故意记录。&lt;br&gt;&lt;h4&gt;3. 领域转移问题&lt;/h4&gt;   - PAM需要在被动声景中进行监测，这造成了焦点录音与被动录音之间的领域转移，给基于焦点录音训练的深度学习模型带来了挑战。&lt;br&gt;&lt;h4&gt;4. 解决方案&lt;/h4&gt;   - 本文利用监督对比学习来改善鸟类声音分类中的领域泛化，强化来自不同领域的同类样本之间的领域不变性。&lt;br&gt;&lt;h4&gt;5. ProtoCLR方法&lt;/h4&gt;   - 提出了ProtoCLR（原型对比学习表示），通过将样本与类原型进行比较，而非成对比较，降低了SupCon损失的计算复杂性。&lt;br&gt;&lt;h4&gt;6. 新基准的提出&lt;/h4&gt;   - 提出了一个基于BirdSet的大规模鸟类声音数据集的新少样本分类基准，以评估模型性能。&lt;br&gt;&lt;h4&gt;7. 效果验证&lt;/h4&gt;   - 展示了该方法在实现强转移性能方面的有效性，证明了其在鸟类声音分类任务中的潜力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Passive acoustic monitoring (PAM) is crucial for bioacoustic research,
enabling non-invasive species tracking and biodiversity monitoring. Citizen
science platforms like Xeno-Canto provide large annotated datasets from focal
recordings, where the target species is intentionally recorded. However, PAM
requires monitoring in passive soundscapes, creating a domain shift between
focal and passive recordings, which challenges deep learning models trained on
focal recordings. To address this, we leverage supervised contrastive learning
to improve domain generalization in bird sound classification, enforcing domain
invariance across same-class examples from different domains. We also propose
ProtoCLR (Prototypical Contrastive Learning of Representations), which reduces
the computational complexity of the SupCon loss by comparing examples to class
prototypes instead of pairwise comparisons. Additionally, we present a new
few-shot classification benchmark based on BirdSet, a large-scale bird sound
dataset, and demonstrate the effectiveness of our approach in achieving strong
transfer performance.</description>
      <guid isPermaLink="false">2409.08589v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Complex Emotion Recognition System using basic emotions via Facial Expression, EEG, and ECG Signals: a review</title>
      <link>http://arxiv.org/abs/2409.07493v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  29 pages, 11 figures&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 系统介绍&lt;/h4&gt;   - 复杂情感识别系统（CERS）通过分析基本情感的组合、它们之间的相互关系以及动态变化来解读复杂情感状态。&lt;br&gt;&lt;h4&gt;2. 算法应用&lt;/h4&gt;   - CERS利用先进算法，提供情感动态的深入洞察，促进对情感的细致理解和定制化响应。&lt;br&gt;&lt;h4&gt;3. 机器情感识别的挑战&lt;/h4&gt;   - 达到这种情感识别水平需要知识蒸馏和对类似人类认知的新概念的理解。&lt;br&gt;&lt;h4&gt;4. 数据集获取的难点&lt;/h4&gt;   - 为CERS获取大规模数据集面临挑战，因为捕捉细微情感的复杂性要求采用专业的数据收集和处理方法。&lt;br&gt;&lt;h4&gt;5. 生理信号的作用&lt;/h4&gt;   - 引入生理信号（如心电图（ECG）和脑电图（EEG））可以显著增强CERS，提供用户情感状态的宝贵洞察，提高数据集质量并增强系统可靠性。&lt;br&gt;&lt;h4&gt;6. 文献综述&lt;/h4&gt;   - 本研究进行了全面的文献回顾，评估机器学习、深度学习和元学习方法在基本和复杂情感识别中的有效性，尤其是利用EEG、ECG信号和面部表情数据集。&lt;br&gt;&lt;h4&gt;7. 应用前景&lt;/h4&gt;   - 选定的研究论文探讨了CERS的潜在应用、临床影响和结果，旨在促进其在临床决策中的接受和整合。&lt;br&gt;&lt;h4&gt;8. 研究空白和挑战&lt;/h4&gt;   - 本研究强调了理解CERS的研究空白和挑战，鼓励相关研究和组织进行进一步探索。&lt;br&gt;&lt;h4&gt;9. 元学习的重要性&lt;/h4&gt;   - 强调元学习方法在提高CERS性能和指导未来研究中的重要性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Complex Emotion Recognition System (CERS) deciphers complex emotional
states by examining combinations of basic emotions expressed, their
interconnections, and the dynamic variations. Through the utilization of
advanced algorithms, CERS provides profound insights into emotional dynamics,
facilitating a nuanced understanding and customized responses. The attainment
of such a level of emotional recognition in machines necessitates the knowledge
distillation and the comprehension of novel concepts akin to human cognition.
The development of AI systems for discerning complex emotions poses a
substantial challenge with significant implications for affective computing.
Furthermore, obtaining a sizable dataset for CERS proves to be a daunting task
due to the intricacies involved in capturing subtle emotions, necessitating
specialized methods for data collection and processing. Incorporating
physiological signals such as Electrocardiogram (ECG) and Electroencephalogram
(EEG) can notably enhance CERS by furnishing valuable insights into the user's
emotional state, enhancing the quality of datasets, and fortifying system
dependability. A comprehensive literature review was conducted in this study to
assess the efficacy of machine learning, deep learning, and meta-learning
approaches in both basic and complex emotion recognition utilizing EEG, ECG
signals, and facial expression datasets. The chosen research papers offer
perspectives on potential applications, clinical implications, and results of
CERSs, with the objective of promoting their acceptance and integration into
clinical decision-making processes. This study highlights research gaps and
challenges in understanding CERSs, encouraging further investigation by
relevant studies and organizations. Lastly, the significance of meta-learning
approaches in improving CERS performance and guiding future research endeavors
is underscored.</description>
      <guid isPermaLink="false">2409.07493v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Connecting Context-specific Adaptation in Humans to Meta-learning</title>
      <link>http://arxiv.org/abs/2011.13782v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 认知控制是系统适应任务需求的能力，是认知的重要组成部分。&lt;br&gt;&lt;h4&gt;2. 认知控制的特点&lt;/h4&gt;   - 认知控制具有上下文敏感性，成人和儿童都会从上下文线索中推断任务需求，并利用这些推断从模糊线索中学习。&lt;br&gt;&lt;h4&gt;3. 研究问题&lt;/h4&gt;   - 人们如何利用上下文线索适应新任务的具体方式尚不清楚。&lt;br&gt;&lt;h4&gt;4. 研究连接&lt;/h4&gt;   - 本文将认知控制的上下文敏感性与上下文条件适应的元学习方法联系起来。&lt;br&gt;&lt;h4&gt;5. 人类学习与现有方法的区别&lt;/h4&gt;   - 人类学习与当前元学习方法的一个重要区别是：现有元学习算法不利用特定任务的上下文线索，而是仅依赖在线反馈（如任务特定标签或奖励）。&lt;br&gt;&lt;h4&gt;6. 提出的新框架&lt;/h4&gt;   - 引入了一种利用任务上下文信息来指导任务特定模型初始化的框架，以便在适应在线反馈之前进行调整。&lt;br&gt;&lt;h4&gt;7. 研究成果&lt;/h4&gt;   - 显示上下文条件的元学习能够捕捉认知任务中的人类行为，并可扩展以提高在少样本分类和低样本强化学习等多种设置中的学习速度。&lt;br&gt;&lt;h4&gt;8. 研究意义&lt;/h4&gt;   - 通过任务信息引导的元学习能够捕捉复杂的人类行为，从而加深对认知控制的理解。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2020-11-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cognitive control, the ability of a system to adapt to the demands of a task,
is an integral part of cognition. A widely accepted fact about cognitive
control is that it is context-sensitive: Adults and children alike infer
information about a task's demands from contextual cues and use these
inferences to learn from ambiguous cues. However, the precise way in which
people use contextual cues to guide adaptation to a new task remains poorly
understood. This work connects the context-sensitive nature of cognitive
control to a method for meta-learning with context-conditioned adaptation. We
begin by identifying an essential difference between human learning and current
approaches to meta-learning: In contrast to humans, existing meta-learning
algorithms do not make use of task-specific contextual cues but instead rely
exclusively on online feedback in the form of task-specific labels or rewards.
To remedy this, we introduce a framework for using contextual information about
a task to guide the initialization of task-specific models before adaptation to
online feedback. We show how context-conditioned meta-learning can capture
human behavior in a cognitive task and how it can be scaled to improve the
speed of learning in various settings, including few-shot classification and
low-sample reinforcement learning. Our work demonstrates that guiding
meta-learning with task information can capture complex, human-like behavior,
thereby deepening our understanding of cognitive control.</description>
      <guid isPermaLink="false">2011.13782v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>NEVLP: Noise-Robust Framework for Efficient Vision-Language Pre-training</title>
      <link>http://arxiv.org/abs/2409.09582v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 视觉语言模型（VLMs）在各种视听语言任务中的成功依赖于大规模网络爬取数据集的预训练。&lt;br&gt;&lt;h4&gt;2. 问题识别&lt;/h4&gt;   - 网络数据的噪声和不完整性使得数据集规模对性能至关重要，导致端到端训练变得越来越不可行。&lt;br&gt;&lt;h4&gt;3. 提出的新框架&lt;/h4&gt;   - 本文提出了NEVLP，一个噪声鲁棒的高效视觉语言预训练框架，旨在减少对预训练数据的需求。&lt;br&gt;&lt;h4&gt;4. 模态间桥接&lt;/h4&gt;   - 通过一个变换器，连接冻结的图像编码器和大型语言模型，以弥补模态间的差距。&lt;br&gt;&lt;h4&gt;5. 学习策略&lt;/h4&gt;   - 引入两种创新的学习策略：噪声自适应学习和概念增强学习，以减轻噪声的影响。&lt;br&gt;&lt;h4&gt;6. 噪声自适应学习&lt;/h4&gt;   - 估计每个图像-文本对的噪声概率，基于变换器的记忆效应，并在图像-文本对比学习中采用噪声自适应正则化，条件交叉模态对齐。&lt;br&gt;&lt;h4&gt;7. 概念增强学习&lt;/h4&gt;   - 通过引入视觉概念（图像中的对象）来丰富不完整文本，为图像-文本匹配和图像基础文本生成提供现有对象的先验信息，从而缓解文本不完整的问题。&lt;br&gt;&lt;h4&gt;8. 框架效果&lt;/h4&gt;   - 该框架有效利用噪声数据，且在多个视觉语言任务（如图像-文本检索、图像描述和视觉问答）中以更少的预训练数据实现了最先进的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The success of Vision Language Models (VLMs) on various vision-language tasks
heavily relies on pre-training with large scale web-crawled datasets. However,
the noisy and incomplete nature of web data makes dataset scale crucial for
performance, rendering end-to-end training increasingly prohibitive. In this
paper, we propose NEVLP, a noise-robust framework for efficient vision-language
pre-training that requires less pre-training data. Specifically, we bridge the
modality gap between a frozen image encoder and a large language model with a
transformer and introduce two innovative learning strategies: noise-adaptive
learning and concept-enhanced learning to mitigate the impact of noise. In
noise-adaptive learning, we estimate the noise probability of each image-text
pair based on the transformer's memorization effect and employ noise-adaptive
regularization on image-text contrastive learning to condition cross-modal
alignment. In concept-enhanced learning, we enrich incomplete text by
incorporating visual concepts (objects in the image) to provide prior
information about existing objects for image-text matching and image-grounded
text generation, thereby mitigating text incompletion. Our framework
effectively utilizes noisy web data and achieves state-of-the-art performance
with less pre-training data across a wide range of vision-language tasks,
including image-text retrieval, image captioning, and visual question
answering.</description>
      <guid isPermaLink="false">2409.09582v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Self-Supervised Learning of Depth and Ego-Motion from Video by Alternative Training and Geometric Constraints from 3D to 2D</title>
      <link>http://arxiv.org/abs/2108.01980v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 3 figures&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 自监督学习从未标记的单目视频中提取深度和自我运动（ego-motion）已取得了良好的成果，并引起了广泛关注。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 现有方法通常通过基于运动结构（SFM）的相邻帧的光度一致性共同训练深度和姿态网络，但这种耦合关系严重影响学习性能。&lt;br&gt;&lt;h4&gt;3. 问题识别&lt;/h4&gt;   - 重投影关系对尺度模糊敏感，尤其是在姿态学习中，导致学习效果不佳。&lt;br&gt;&lt;h4&gt;4. 研究目标&lt;/h4&gt;   - 本文旨在提高深度-姿态学习性能，无需辅助任务，并解决上述问题。&lt;br&gt;&lt;h4&gt;5. 新方法&lt;/h4&gt;   - 采用交替训练每个任务的方法，结合极线几何约束到基于迭代最近点（ICP）的点云匹配过程中。&lt;br&gt;&lt;h4&gt;6. 训练策略&lt;/h4&gt;   - 与共同训练深度和姿态网络不同，关键思想是通过交替训练各自的网络并固定另一个网络，来更好地利用这两个任务的互依关系。&lt;br&gt;&lt;h4&gt;7. 损失函数设计&lt;/h4&gt;   - 设计了对数尺度3D结构一致性损失，以在训练时更加关注小的深度值。&lt;br&gt;&lt;h4&gt;8. 优化过程改进&lt;/h4&gt;   - 进一步将极线几何纳入基于ICP的学习过程，以简化姿态学习的优化。&lt;br&gt;&lt;h4&gt;9. 实验结果&lt;/h4&gt;   - 在多个基准数据集上的广泛实验表明，提出的算法在性能上优于最先进的自监督方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2021-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised learning of depth and ego-motion from unlabeled monocular
video has acquired promising results and drawn extensive attention. Most
existing methods jointly train the depth and pose networks by photometric
consistency of adjacent frames based on the principle of structure-from-motion
(SFM). However, the coupling relationship of the depth and pose networks
seriously influences the learning performance, and the re-projection relations
is sensitive to scale ambiguity, especially for pose learning. In this paper,
we aim to improve the depth-pose learning performance without the auxiliary
tasks and address the above issues by alternative training each task and
incorporating the epipolar geometric constraints into the Iterative Closest
Point (ICP) based point clouds match process. Distinct from jointly training
the depth and pose networks, our key idea is to better utilize the mutual
dependency of these two tasks by alternatively training each network with
respective losses while fixing the other. We also design a log-scale 3D
structural consistency loss to put more emphasis on the smaller depth values
during training. To makes the optimization easier, we further incorporate the
epipolar geometry into the ICP based learning process for pose learning.
Extensive experiments on various benchmarks datasets indicate the superiority
of our algorithm over the state-of-the-art self-supervised methods.</description>
      <guid isPermaLink="false">2108.01980v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>ECLAIR: A High-Fidelity Aerial LiDAR Dataset for Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2404.10699v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 7 figures&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究介绍&lt;/h4&gt;   - 本文介绍了ECLAIR（扩展激光雷达分类用于AI识别），这是一个新型的户外大规模航空激光雷达数据集，旨在推动点云语义分割研究。&lt;br&gt;&lt;h4&gt;2. 数据集特点&lt;/h4&gt;   - 该数据集是迄今为止规模最大、种类最多的激光雷达数据集，覆盖面积达到10平方公里，包含近6亿个点，并且涵盖11个不同的物体类别。&lt;br&gt;&lt;h4&gt;3. 数据质量保障&lt;/h4&gt;   - 为了确保数据集的质量和实用性，数据点标签经过内部专家团队的严格审查，确保语义标注的准确性和一致性。&lt;br&gt;&lt;h4&gt;4. 应用领域&lt;/h4&gt;   - 数据集旨在推动3D城市建模、场景理解和公用基础设施管理等领域的发展，提供新的挑战和潜在应用。&lt;br&gt;&lt;h4&gt;5. 基准测试&lt;/h4&gt;   - 作为基准，文中报告了基于Minkowski引擎的体素点云分割方法的定性和定量分析。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-04-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/sharpershape/eclair-dataset&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce ECLAIR (Extended Classification of Lidar for AI Recognition), a
new outdoor large-scale aerial LiDAR dataset designed specifically for
advancing research in point cloud semantic segmentation. As the most extensive
and diverse collection of its kind to date, the dataset covers a total area of
10$km^2$ with close to 600 million points and features eleven distinct object
categories. To guarantee the dataset's quality and utility, we have thoroughly
curated the point labels through an internal team of experts, ensuring accuracy
and consistency in semantic labeling. The dataset is engineered to move forward
the fields of 3D urban modeling, scene understanding, and utility
infrastructure management by presenting new challenges and potential
applications. As a benchmark, we report qualitative and quantitative analysis
of a voxel-based point cloud segmentation approach based on the Minkowski
Engine.</description>
      <guid isPermaLink="false">2404.10699v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>GeoBEV: Learning Geometric BEV Representation for Multi-view 3D Object Detection</title>
      <link>http://arxiv.org/abs/2409.01816v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 鸟瞰视图（BEV）表示已成为多视角3D物体检测的主流范式，展现了良好的感知能力。&lt;br&gt;&lt;h4&gt;2. 现有方法不足&lt;/h4&gt;   - 现有方法忽视了BEV表示的几何质量，使其处于低分辨率状态，无法恢复场景的真实几何信息。&lt;br&gt;&lt;h4&gt;3. 问题识别&lt;/h4&gt;   - 本文识别出导致BEV表示分辨率低的原因，并提出了径向-笛卡尔BEV采样（RC-Sampling）方法。&lt;br&gt;&lt;h4&gt;4. RC-Sampling优势&lt;/h4&gt;   - RC-Sampling能够高效生成高分辨率的密集BEV表示，无需复杂的操作。&lt;br&gt;&lt;h4&gt;5. 新标签设计&lt;/h4&gt;   - 设计了一种新颖的“箱内标签”（In-Box Label），替代传统的来自LiDAR点的深度标签。&lt;br&gt;&lt;h4&gt;6. 标签特性&lt;/h4&gt;   - 箱内标签反映物体的实际几何结构，而不仅仅是其表面，将真实的几何信息注入BEV表示中。&lt;br&gt;&lt;h4&gt;7. 损失函数&lt;/h4&gt;   - 开发了“中心感知内损失”（Centroid-Aware Inner Loss, CAI Loss），用于捕捉物体的细粒度内在几何结构。&lt;br&gt;&lt;h4&gt;8. 综合框架&lt;/h4&gt;   - 将上述模块整合到一个新的多视角3D物体检测框架中，称为GeoBEV。&lt;br&gt;&lt;h4&gt;9. 实验结果&lt;/h4&gt;   - 在nuScenes数据集上的广泛实验表明，GeoBEV达到了最先进的性能，突显其有效性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Bird's-Eye-View (BEV) representation has emerged as a mainstream paradigm for
multi-view 3D object detection, demonstrating impressive perceptual
capabilities. However, existing methods overlook the geometric quality of BEV
representation, leaving it in a low-resolution state and failing to restore the
authentic geometric information of the scene. In this paper, we identify the
reasons why previous approaches are constrained by low BEV representation
resolution and propose Radial-Cartesian BEV Sampling (RC-Sampling), enabling
efficient generation of high-resolution dense BEV representations without the
need for complex operators. Additionally, we design a novel In-Box Label to
substitute the traditional depth label generated from the LiDAR points. This
label reflects the actual geometric structure of objects rather than just their
surfaces, injecting real-world geometric information into the BEV
representation. Furthermore, in conjunction with the In-Box Label, a
Centroid-Aware Inner Loss (CAI Loss) is developed to capture the fine-grained
inner geometric structure of objects. Finally, we integrate the aforementioned
modules into a novel multi-view 3D object detection framework, dubbed GeoBEV.
Extensive experiments on the nuScenes dataset exhibit that GeoBEV achieves
state-of-the-art performance, highlighting its effectiveness.</description>
      <guid isPermaLink="false">2409.01816v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Signed Graph Autoencoder for Explainable and Polarization-Aware Network Embeddings</title>
      <link>http://arxiv.org/abs/2409.10452v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 基于图神经网络（GNN）的自编码器近年来受到关注，因其能提取具有信息性的潜在表示，表征复杂拓扑结构（如图形）。&lt;br&gt;&lt;h4&gt;2. 研究缺口&lt;/h4&gt;   - 尽管图自编码器普遍存在，但针对有向网络的可解释神经生成模型的开发与评估较少。&lt;br&gt;&lt;h4&gt;3. 提出的新框架&lt;/h4&gt;   - 本文提出了签名图原型自编码器（SGAAE）框架，旨在填补这一空白。&lt;br&gt;&lt;h4&gt;4. 节点表示提取&lt;/h4&gt;   - SGAAE提取节点级表示，表征网络中节点在不同极端特征（称为原型）上的归属。&lt;br&gt;&lt;h4&gt;5. 学习多面体&lt;/h4&gt;   - 通过将图投影到一个学习的多面体上，SGAAE控制其极化特性。&lt;br&gt;&lt;h4&gt;6. 分析方法&lt;/h4&gt;   - 框架采用基于Skellam分布的新近提出的似然分析方法，结合关系原型分析和GNN。&lt;br&gt;&lt;h4&gt;7. 实验评估&lt;/h4&gt;   - 实验结果表明，SGAAE能够成功推断不同潜在结构下的节点归属，同时提取通过对立观点参与形成的竞争社群。&lt;br&gt;&lt;h4&gt;8. 网络极化问题&lt;/h4&gt;   - 引入了2级网络极化问题，并展示了SGAAE如何表征这种设置。&lt;br&gt;&lt;h4&gt;9. 模型性能&lt;/h4&gt;   - 在四个真实世界数据集上进行的签名链接预测任务中，SGAAE表现出色，超过多种基线模型。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autoencoders based on Graph Neural Networks (GNNs) have garnered significant
attention in recent years for their ability to extract informative latent
representations, characterizing the structure of complex topologies, such as
graphs. Despite the prevalence of Graph Autoencoders, there has been limited
focus on developing and evaluating explainable neural-based graph generative
models specifically designed for signed networks. To address this gap, we
propose the Signed Graph Archetypal Autoencoder (SGAAE) framework. SGAAE
extracts node-level representations that express node memberships over distinct
extreme profiles, referred to as archetypes, within the network. This is
achieved by projecting the graph onto a learned polytope, which governs its
polarization. The framework employs a recently proposed likelihood for
analyzing signed networks based on the Skellam distribution, combined with
relational archetypal analysis and GNNs. Our experimental evaluation
demonstrates the SGAAEs' capability to successfully infer node memberships over
the different underlying latent structures while extracting competing
communities formed through the participation of the opposing views in the
network. Additionally, we introduce the 2-level network polarization problem
and show how SGAAE is able to characterize such a setting. The proposed model
achieves high performance in different tasks of signed link prediction across
four real-world datasets, outperforming several baseline models.</description>
      <guid isPermaLink="false">2409.10452v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>BiEquiFormer: Bi-Equivariant Representations for Global Point Cloud Registration</title>
      <link>http://arxiv.org/abs/2407.08729v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究目标&lt;/h4&gt;   - 本文旨在解决全局点云配准（PCR）问题，即在不考虑扫描初始姿态的情况下找到点云之间的最佳对齐。&lt;br&gt;&lt;h4&gt;2. 挑战&lt;/h4&gt;   - 传统优化方法在处理此问题时面临计算限制，导致其效果不佳。&lt;br&gt;&lt;h4&gt;3. 深度学习方法的局限性&lt;/h4&gt;   - 现有的深度学习方法在点云随机放置时性能显著下降。&lt;br&gt;&lt;h4&gt;4. 提出的解决方案&lt;/h4&gt;   - 建议采用等变深度学习方法解决这一任务，并对PCR的特定双等变性进行特征描述。&lt;br&gt;&lt;h4&gt;5. 新模型设计&lt;/h4&gt;   - 设计了BiEquiformer，这是一个新颖且可扩展的双等变管道，能够对输入点云的独立变换保持等变性。&lt;br&gt;&lt;h4&gt;6. 信息融合机制&lt;/h4&gt;   - 与简单独立处理点云的方法不同，BiEquiformer设计了具有表现力的双等变层，能够融合两个点云的信息。&lt;br&gt;&lt;h4&gt;7. 效果提升&lt;/h4&gt;   - 该方法能够提取高质量的超级点对应关系，从而实现稳健的点云配准。&lt;br&gt;&lt;h4&gt;8. 实验结果&lt;/h4&gt;   - 与最先进的方法进行广泛比较，结果表明在标准设置下性能相当，在稳健设置下表现优越，适用于3DMatch和具有挑战性的低重叠3DLoMatch数据集。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-07-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The goal of this paper is to address the problem of global point cloud
registration (PCR) i.e., finding the optimal alignment between point clouds
irrespective of the initial poses of the scans. This problem is notoriously
challenging for classical optimization methods due to computational
constraints. First, we show that state-of-the-art deep learning methods suffer
from huge performance degradation when the point clouds are arbitrarily placed
in space. We propose that equivariant deep learning should be utilized for
solving this task and we characterize the specific type of bi-equivariance of
PCR. Then, we design BiEquiformer a novel and scalable bi-equivariant pipeline
i.e. equivariant to the independent transformations of the input point clouds.
While a naive approach would process the point clouds independently we design
expressive bi-equivariant layers that fuse the information from both point
clouds. This allows us to extract high-quality superpoint correspondences and
in turn, robust point-cloud registration. Extensive comparisons against
state-of-the-art methods show that our method achieves comparable performance
in the canonical setting and superior performance in the robust setting in both
the 3DMatch and the challenging low-overlap 3DLoMatch dataset.</description>
      <guid isPermaLink="false">2407.08729v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Camera Multiple 3D Object Tracking on the Move for Autonomous Vehicles</title>
      <link>http://arxiv.org/abs/2204.09151v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at CVPRW 2022&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 自主车辆的发展使得可以使用完整的相机传感器阵列捕捉周围环境，这对物体检测和跟踪提出了新挑战。&lt;br&gt;&lt;h4&gt;2. 挑战&lt;/h4&gt;   - 需要解决在不同相机视角下保持一致的检测和跟踪结果的问题。&lt;br&gt;&lt;h4&gt;3. 提出的新方法&lt;/h4&gt;   - 本文提出了一种新的全球关联图模型（Global Association Graph Model）与链接预测方法，旨在预测现有轨迹的位置，并通过跨注意力运动建模和外观重识别将检测与轨迹连接。&lt;br&gt;&lt;h4&gt;4. 解决目标&lt;/h4&gt;   - 该方法旨在解决由于不一致的3D物体检测引起的问题。&lt;br&gt;&lt;h4&gt;5. 模型优势&lt;/h4&gt;   - 该模型还旨在提高标准3D物体检测器在nuScenes检测挑战中的检测准确性。&lt;br&gt;&lt;h4&gt;6. 实验结果&lt;/h4&gt;   - 在nuScenes数据集上的实验结果表明，所提出的方法在现有基于视觉的跟踪数据集上实现了最先进的（SOTA）性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2022-04-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The development of autonomous vehicles provides an opportunity to have a
complete set of camera sensors capturing the environment around the car. Thus,
it is important for object detection and tracking to address new challenges,
such as achieving consistent results across views of cameras. To address these
challenges, this work presents a new Global Association Graph Model with Link
Prediction approach to predict existing tracklets location and link detections
with tracklets via cross-attention motion modeling and appearance
re-identification. This approach aims at solving issues caused by inconsistent
3D object detection. Moreover, our model exploits to improve the detection
accuracy of a standard 3D object detector in the nuScenes detection challenge.
The experimental results on the nuScenes dataset demonstrate the benefits of
the proposed method to produce SOTA performance on the existing vision-based
tracking dataset.</description>
      <guid isPermaLink="false">2204.09151v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Identification of head impact locations, speeds, and force based on head kinematics</title>
      <link>http://arxiv.org/abs/2409.08177v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究目标&lt;/h4&gt;   - 研究头部撞击信息（如撞击方向、速度和力量）对研究创伤性脑损伤及设计和评估保护装备的重要性。&lt;br&gt;&lt;h4&gt;2. 模型开发&lt;/h4&gt;   - 本研究提出了一种深度学习模型，旨在根据头部运动学数据准确预测头部撞击信息，包括位置、速度、方向和力量。&lt;br&gt;&lt;h4&gt;3. 数据集&lt;/h4&gt;   - 利用包含16,000个模拟头部撞击的数据库，这些撞击使用了Riddell头盔有限元模型。&lt;br&gt;&lt;h4&gt;4. 方法&lt;/h4&gt;   - 实施了长短期记忆（LSTM）网络以处理头部运动学数据，包括三轴线性加速度和角速度。&lt;br&gt;&lt;h4&gt;5. 结果&lt;/h4&gt;   - 模型能够准确预测描述撞击位置、方向、速度和撞击力特征的参数，所有任务的R²值均超过70%。&lt;br&gt;&lt;h4&gt;6. 进一步验证&lt;/h4&gt;   - 通过使用由仪器化护齿和视频记录的实地数据集进行了进一步验证，数据集包含79个明确识别的头部撞击。&lt;br&gt;&lt;h4&gt;7. 模型表现&lt;/h4&gt;   - 深度学习模型显著优于现有方法，在识别撞击位置方面达到79.7%的准确率，而传统方法的最高准确率仅为49.4%。&lt;br&gt;&lt;h4&gt;8. 研究结论&lt;/h4&gt;   - 模型的高精度表明其在提高运动头盔设计和安全性方面的潜力，能够提供更准确的撞击数据。&lt;br&gt;&lt;h4&gt;9. 未来研究方向&lt;/h4&gt;   - 未来的研究应在不同头盔和运动上测试模型，并利用大规模的体内数据集验证模型准确性，同时采用迁移学习等技术以扩大其有效性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/xzhan96-stf/impact_retriever&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Objective: Head impact information including impact directions, speeds and
force are important to study traumatic brain injury, design and evaluate
protective gears. This study presents a deep learning model developed to
accurately predict head impact information, including location, speed,
orientation, and force, based on head kinematics during helmeted impacts.
Methods: Leveraging a dataset of 16,000 simulated helmeted head impacts using
the Riddell helmet finite element model, we implemented a Long Short-Term
Memory (LSTM) network to process the head kinematics: tri-axial linear
accelerations and angular velocities. Results: The models accurately predict
the impact parameters describing impact location, direction, speed, and the
impact force profile with R2 exceeding 70% for all tasks. Further validation
was conducted using an on-field dataset recorded by instrumented mouthguards
and videos, consisting of 79 head impacts in which the impact location can be
clearly identified. The deep learning model significantly outperformed existing
methods, achieving a 79.7% accuracy in identifying impact locations, compared
to lower accuracies with traditional methods (the highest accuracy of existing
methods is 49.4%). Conclusion: The precision underscores the model's potential
in enhancing helmet design and safety in sports by providing more accurate
impact data. Future studies should test the models across various helmets and
sports on large in vivo datasets to validate the accuracy of the models,
employing techniques like transfer learning to broaden its effectiveness.</description>
      <guid isPermaLink="false">2409.08177v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>BonnBeetClouds3D: A Dataset Towards Point Cloud-based Organ-level Phenotyping of Sugar Beet Plants under Field Conditions</title>
      <link>http://arxiv.org/abs/2312.14706v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 农业生产在未来几十年面临气候变化和可持续性需求带来的严峻挑战，需减少对环境的影响。&lt;br&gt;&lt;h4&gt;2. 解决方案&lt;/h4&gt;   - 通过机器人实现非化学除草、无人机（UAV）监测作物，以及培育新型更具抗逆性的作物品种是应对这些挑战的有效途径。&lt;br&gt;&lt;h4&gt;3. 表型分析的重要性&lt;/h4&gt;   - 植物性状分析（表型分析）在植物育种中是一项重要活动，但涉及大量人工劳动。&lt;br&gt;&lt;h4&gt;4. 研究目标&lt;/h4&gt;   - 本文旨在解决精确表型分析所需的自动化细粒度器官级几何分析问题。&lt;br&gt;&lt;h4&gt;5. 数据集的提出&lt;/h4&gt;   - 由于该领域实际数据相对稀缺，本文提出了一种新数据集，该数据集通过无人机获取高分辨率图像，包含48个植物品种，覆盖广泛的形态和外观多样性。&lt;br&gt;&lt;h4&gt;6. 自动化表型分析&lt;/h4&gt;   - 提供的数据集支持开发能在不同品种上良好泛化的自动化表型分析方法。&lt;br&gt;&lt;h4&gt;7. 图像处理方法&lt;/h4&gt;   - 基于多个视角的重叠高分辨率图像，计算光测量密集点云，并为植物、叶片及显著点（如尖端和基部）提供详细准确的逐点标签。&lt;br&gt;&lt;h4&gt;8. 专家测量数据&lt;/h4&gt;   - 包括德国联邦植物品种办公室专家对真实植物进行的表型特征测量，允许对新方法在分割、关键点检测及下游任务上的评估。&lt;br&gt;&lt;h4&gt;9. 数据集的应用&lt;/h4&gt;   - 提供的标注点云支持细粒度植物分析，促进自动化表型分析方法的发展，同时也为表面重建、点云补全及点云的语义解释等领域的进一步研究提供支持。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2023-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Agricultural production is facing severe challenges in the next decades
induced by climate change and the need for sustainability, reducing its impact
on the environment. Advancements in field management through non-chemical
weeding by robots in combination with monitoring of crops by autonomous
unmanned aerial vehicles (UAVs) and breeding of novel and more resilient crop
varieties are helpful to address these challenges. The analysis of plant
traits, called phenotyping, is an essential activity in plant breeding, it
however involves a great amount of manual labor. With this paper, we address
the problem of automatic fine-grained organ-level geometric analysis needed for
precision phenotyping. As the availability of real-world data in this domain is
relatively scarce, we propose a novel dataset that was acquired using UAVs
capturing high-resolution images of a real breeding trial containing 48 plant
varieties and therefore covering great morphological and appearance diversity.
This enables the development of approaches for autonomous phenotyping that
generalize well to different varieties. Based on overlapping high-resolution
images from multiple viewing angles, we compute photogrammetric dense point
clouds and provide detailed and accurate point-wise labels for plants, leaves,
and salient points as the tip and the base. Additionally, we include
measurements of phenotypic traits performed by experts from the German Federal
Plant Variety Office on the real plants, allowing the evaluation of new
approaches not only on segmentation and keypoint detection but also directly on
the downstream tasks. The provided labeled point clouds enable fine-grained
plant analysis and support further progress in the development of automatic
phenotyping approaches, but also enable further research in surface
reconstruction, point cloud completion, and semantic interpretation of point
clouds.</description>
      <guid isPermaLink="false">2312.14706v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Machine learning of phases and structures for model systems in physics</title>
      <link>http://arxiv.org/abs/2409.03023v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 two-column pages and 8 figures, invited review to the JPSJ issue
  of Special Topics "Machine Learning Physics"&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 相变的检测是凝聚态物理中的一个基本挑战，传统上通过分析方法和直接数值模拟来解决。&lt;br&gt;&lt;h4&gt;2. 机器学习的崛起&lt;/h4&gt;   - 近年来，机器学习技术作为强有力的工具，补充了这些标准方法，为相位和结构的确定提供了有价值的见解。&lt;br&gt;&lt;h4&gt;3. 提升传统方法的应用&lt;/h4&gt;   - 机器学习不仅增强了传统方法的应用效果，还改善了相位和结构的识别。&lt;br&gt;&lt;h4&gt;4. 研究重点&lt;/h4&gt;   - 本文回顾了该领域的最新进展，特别关注作者在相位和结构确定方面的贡献。&lt;br&gt;&lt;h4&gt;5. 应用系统&lt;/h4&gt;   - 讨论了在多个系统中使用监督和无监督学习方法的应用，包括：&lt;br&gt;     - (a) 2D点渗透&lt;br&gt;     - (b) 3D安德森局域化模型&lt;br&gt;     - (c) 2D $J_1$-$J_2$ Ising模型&lt;br&gt;     - (d) 大角度收敛束电子衍射模式的预测&lt;br&gt;通过这些研究，展示了机器学习在凝聚态物理研究中的潜力和应用。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The detection of phase transitions is a fundamental challenge in condensed
matter physics, traditionally addressed through analytical methods and direct
numerical simulations. In recent years, machine learning techniques have
emerged as powerful tools to complement these standard approaches, offering
valuable insights into phase and structure determination. Additionally, they
have been shown to enhance the application of traditional methods. In this
work, we review recent advancements in this area, with a focus on our
contributions to phase and structure determination using supervised and
unsupervised learning methods in several systems: (a) 2D site percolation, (b)
the 3D Anderson model of localization, (c) the 2D $J_1$-$J_2$ Ising model, and
(d) the prediction of large-angle convergent beam electron diffraction
patterns.</description>
      <guid isPermaLink="false">2409.03023v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>NeuralOOD: Improving Out-of-Distribution Generalization Performance with Brain-machine Fusion Learning Framework</title>
      <link>http://arxiv.org/abs/2408.14950v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 深度神经网络（DNN）在传统计算机视觉（CV）任务中展现出卓越的识别能力，但在面对分布外（OOD）数据时，准确性通常显著下降。&lt;br&gt;&lt;h4&gt;2. 人类的优势&lt;/h4&gt;   - 与DNN模型不同，人类在面对OOD场景时能保持较低的错误率，这部分归因于人脑中存储的丰富先验认知知识。&lt;br&gt;&lt;h4&gt;3. 现有研究的局限性&lt;/h4&gt;   - 以往的OOD泛化研究主要集中在单一模态上，忽视了多模态学习方法的优势。&lt;br&gt;&lt;h4&gt;4. 新框架的提出&lt;/h4&gt;   - 本文提出了一种新颖的脑机融合学习（BMFL）框架，旨在利用多模态学习方法提高OOD泛化能力。&lt;br&gt;&lt;h4&gt;5. 交叉注意机制&lt;/h4&gt;   - 采用交叉注意机制，将计算机视觉模型的视觉知识与人脑中的先验认知知识融合。&lt;br&gt;&lt;h4&gt;6. 视觉神经编码模型&lt;/h4&gt;   - 使用预训练的视觉神经编码模型从视觉特征预测功能性磁共振成像（fMRI），消除了对fMRI数据收集和预处理的需求，从而有效减少传统BMFL方法的工作量。&lt;br&gt;&lt;h4&gt;7. 脑转换器的构建&lt;/h4&gt;   - 构建了一个脑转换器，以促进从fMRI数据中提取知识。&lt;br&gt;&lt;h4&gt;8. 正则化方法&lt;/h4&gt;   - 在训练过程中引入皮尔逊相关系数最大化正则化方法，以改善融合能力并提供更好的约束。&lt;br&gt;&lt;h4&gt;9. 实验结果&lt;/h4&gt;   - 在ImageNet-1k验证数据集及六个策划的OOD数据集上，我们的模型超越了DINOv2和基线模型，展示了其在多样化场景中的优越性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep Neural Networks (DNNs) have demonstrated exceptional recognition
capabilities in traditional computer vision (CV) tasks. However, existing CV
models often suffer a significant decrease in accuracy when confronted with
out-of-distribution (OOD) data. In contrast to these DNN models, human can
maintain a consistently low error rate when facing OOD scenes, partly
attributed to the rich prior cognitive knowledge stored in the human brain.
Previous OOD generalization researches only focus on the single modal,
overlooking the advantages of multimodal learning method. In this paper, we
utilize the multimodal learning method to improve the OOD generalization and
propose a novel Brain-machine Fusion Learning (BMFL) framework. We adopt the
cross-attention mechanism to fuse the visual knowledge from CV model and prior
cognitive knowledge from the human brain. Specially, we employ a pre-trained
visual neural encoding model to predict the functional Magnetic Resonance
Imaging (fMRI) from visual features which eliminates the need for the fMRI data
collection and pre-processing, effectively reduces the workload associated with
conventional BMFL methods. Furthermore, we construct a brain transformer to
facilitate the extraction of knowledge inside the fMRI data. Moreover, we
introduce the Pearson correlation coefficient maximization regularization
method into the training process, which improves the fusion capability with
better constrains. Our model outperforms the DINOv2 and baseline models on the
ImageNet-1k validation dataset as well as six curated OOD datasets, showcasing
its superior performance in diverse scenarios.</description>
      <guid isPermaLink="false">2408.14950v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Dense Point Clouds Matter: Dust-GS for Scene Reconstruction from Sparse Viewpoints</title>
      <link>http://arxiv.org/abs/2409.08613v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 3D高斯溅射（3DGS）在场景合成和新视角合成任务中表现出色。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 3DGS的初始化通常依赖于来自运动重建（SfM）方法的点云。在稀疏视点下进行场景重建时，初始点云的质量和输入图像的数量会显著限制3DGS的有效性。&lt;br&gt;&lt;h4&gt;3. 提出新框架&lt;/h4&gt;   - 本研究提出了Dust-GS，一个旨在克服3DGS在稀疏视点条件下限制的新框架。&lt;br&gt;&lt;h4&gt;4. 创新的初始化技术&lt;/h4&gt;   - Dust-GS不再仅依赖SfM，而是引入了一种创新的点云初始化技术，即使在稀疏输入数据的情况下也能有效工作。&lt;br&gt;&lt;h4&gt;5. 混合策略&lt;/h4&gt;   - 该方法采用混合策略，整合了自适应深度掩膜技术，以提高重建场景的准确性和细节。&lt;br&gt;&lt;h4&gt;6. 实验结果&lt;/h4&gt;   - 在多个基准数据集上进行的广泛实验表明，Dust-GS在稀疏视点场景中超越了传统3DGS方法，能够以更少的输入图像实现更优质的场景重建。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Gaussian Splatting (3DGS) has demonstrated remarkable performance in scene
synthesis and novel view synthesis tasks. Typically, the initialization of 3D
Gaussian primitives relies on point clouds derived from Structure-from-Motion
(SfM) methods. However, in scenarios requiring scene reconstruction from sparse
viewpoints, the effectiveness of 3DGS is significantly constrained by the
quality of these initial point clouds and the limited number of input images.
In this study, we present Dust-GS, a novel framework specifically designed to
overcome the limitations of 3DGS in sparse viewpoint conditions. Instead of
relying solely on SfM, Dust-GS introduces an innovative point cloud
initialization technique that remains effective even with sparse input data.
Our approach leverages a hybrid strategy that integrates an adaptive
depth-based masking technique, thereby enhancing the accuracy and detail of
reconstructed scenes. Extensive experiments conducted on several benchmark
datasets demonstrate that Dust-GS surpasses traditional 3DGS methods in
scenarios with sparse viewpoints, achieving superior scene reconstruction
quality with a reduced number of input images.</description>
      <guid isPermaLink="false">2409.08613v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Boosting CLIP Adaptation for Image Quality Assessment via Meta-Prompt Learning and Gradient Regularization</title>
      <link>http://arxiv.org/abs/2409.05381v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 图像质量评估（IQA）在计算机视觉领域仍然是一个未解决的挑战，主要由于复杂的失真条件、多样的图像内容和数据可用性有限。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 现有的盲图像质量评估（BIQA）方法严重依赖大量人工标注来训练模型，这种方法既费力又昂贵，因创建IQA数据集的要求非常高。&lt;br&gt;&lt;h4&gt;3. 引入新框架&lt;/h4&gt;   - 本文提出了一种新颖的梯度调节元提示IQA框架（GRMP-IQA），旨在减少对标记样本的依赖。&lt;br&gt;&lt;h4&gt;4. 框架目标&lt;/h4&gt;   - GRMP-IQA旨在快速适应强大的视觉-语言预训练模型CLIP，以提高数据有限情况下的准确性。&lt;br&gt;&lt;h4&gt;5. 关键模块&lt;/h4&gt;   - GRMP-IQA包括两个关键模块：&lt;br&gt;     - **元提示预训练模块**：利用元学习范式预训练软提示，分享不同失真的元知识，实现对各种IQA任务的快速适应。&lt;br&gt;     - **质量感知梯度正则化**：在微调过程中调整更新梯度，聚焦于与质量相关的特征，防止模型过拟合于语义信息。&lt;br&gt;&lt;h4&gt;6. 实验结果&lt;/h4&gt;   - 在五个标准BIQA数据集上的广泛实验表明，GRMP-IQA在数据有限的情况下优于现有最先进的BIQA方法，例如在LIVEC上获得0.836的SRCC值（对比0.760）和在KonIQ上获得0.853的SRCC值（对比0.812）。&lt;br&gt;&lt;h4&gt;7. 数据利用效率&lt;/h4&gt;   - 值得注意的是，仅使用20%的训练数据，GRMP-IQA的表现超越了大多数现有的完全监督BIQA方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Image Quality Assessment (IQA) remains an unresolved challenge in the field
of computer vision, due to complex distortion conditions, diverse image
content, and limited data availability. The existing Blind IQA (BIQA) methods
heavily rely on extensive human annotations to train models, which is both
labor-intensive and costly due to the demanding nature of creating IQA
datasets. To mitigate the dependence on labeled samples, this paper introduces
a novel Gradient-Regulated Meta-Prompt IQA Framework (GRMP-IQA). This framework
aims to fast adapt the powerful visual-language pre-trained model, CLIP, to
downstream IQA tasks, significantly improving accuracy in scenarios with
limited data. Specifically, the GRMP-IQA comprises two key modules: Meta-Prompt
Pre-training Module and Quality-Aware Gradient Regularization. The Meta Prompt
Pre-training Module leverages a meta-learning paradigm to pre-train soft
prompts with shared meta-knowledge across different distortions, enabling rapid
adaptation to various IQA tasks. On the other hand, the Quality-Aware Gradient
Regularization is designed to adjust the update gradients during fine-tuning,
focusing the model's attention on quality-relevant features and preventing
overfitting to semantic information. Extensive experiments on five standard
BIQA datasets demonstrate the superior performance to the state-of-the-art BIQA
methods under limited data setting, i.e., achieving SRCC values of 0.836 (vs.
0.760 on LIVEC) and 0.853 (vs. 0.812 on KonIQ). Notably, utilizing just 20\% of
the training data, our GRMP-IQA outperforms most existing fully supervised BIQA
methods.</description>
      <guid isPermaLink="false">2409.05381v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Joint image reconstruction and segmentation of real-time cardiac MRI in free-breathing using a model based on disentangled representation learning</title>
      <link>http://arxiv.org/abs/2409.08619v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to the Journal of Cardiovascular Magnetic Resonance&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究目的&lt;/h4&gt;   - 提出了一个联合图像重建和分割的方法，基于解耦表示学习，旨在实现心脏动态磁共振成像（MR）实时和自由呼吸下的应用。&lt;br&gt;&lt;h4&gt;2. 可行性研究&lt;/h4&gt;   - 在八名健康参与者和五名有间歇性心房颤动患者中进行了探索性可行性研究，测试了该方法在基于自家开发的螺旋bSSFP脉冲序列的欠采样实时采集中的表现。&lt;br&gt;&lt;h4&gt;3. 图像与分割比较&lt;/h4&gt;   - 将生成的图像和预测的左心室（LV）分割与参考标准（心电图门控的分割Cartesian cine）进行比较，使用重复的呼吸保持和手动分割作为标准。&lt;br&gt;&lt;h4&gt;4. 图像质量评估&lt;/h4&gt;   - 在健康参与者中，实时呼吸保持（RT-BH）和Cartesian cine的图像质量相当（RT-BH: 1.99 ± 0.98, Cartesian: 1.94 ± 0.86, p=0.052），但在自由呼吸（RT-FB）中质量稍差（RT-FB: 2.40 ± 0.98, p&lt;0.001）。&lt;br&gt;&lt;h4&gt;5. 对心律失常患者的评估&lt;/h4&gt;   - 在心律失常患者中，实时方法的图像质量良好（RT-BH: 2.10 ± 1.28, p&lt;0.001; RT-FB: 2.40 ± 1.13, p&lt;0.001; Cartesian: 2.68 ± 1.13）。&lt;br&gt;&lt;h4&gt;6. 观察者可靠性&lt;/h4&gt;   - 内部观察者可靠性良好（ICC=0.77, 95%可信区间[0.75, 0.79], p&lt;0.001）。&lt;br&gt;&lt;h4&gt;7. 功能分析结果&lt;/h4&gt;   - 从该模型导出的射血分数（EF）相较于临床参考标准显示出正偏差（RT-BH均值EF: 58.5 ± 5.6%, 偏差: +3.47%, 95%可信区间[-0.86, 7.79%]; RT-FB均值: 57.9 ± 10.6%, 偏差: +1.45%, [-3.02, 5.91%]; Cartesian均值: 54.9 ± 6.7%）。&lt;br&gt;&lt;h4&gt;8. 临床实践的替代方案&lt;/h4&gt;   - 该实时MR成像技术能够在1-2分钟内获取高质量的心脏动态数据，无需心电图门控和呼吸保持，因此提供了比当前的分段采集方法更具前景的替代方案，具有更短的扫描时间、更高的患者舒适度以及对心律失常和患者不配合的更强鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A joint image reconstruction and segmentation approach based on disentangled
representation learning was trained to enable cardiac cine MR imaging in
real-time and under free-breathing. An exploratory feasibility study tested the
proposed method in undersampled real-time acquisitions based on an in-house
developed spiral bSSFP pulse sequence in eight healthy participants and five
patients with intermittent atrial fibrillation. Images and predicted LV
segmentations were compared to the reference standard of ECG-gated segmented
Cartesian cine in repeated breath-holds and corresponding manual segmentation.
On a 5-point Likert scale, image quality of the real-time breath-hold approach
and Cartesian cine was comparable in healthy participants (RT-BH: 1.99 $\pm$
.98, Cartesian: 1.94 $\pm$ .86, p=.052), but slightly inferior in
free-breathing (RT-FB: 2.40 $\pm$ .98, p&lt;.001). In patients with arrhythmia,
image quality from both real-time approaches was favourable (RT-BH: 2.10 $\pm$
1.28, p&lt;.001, RT-FB: 2.40 $\pm$ 1.13, p&lt;.001, Cartesian: 2.68 $\pm$ 1.13).
Intra-observer reliability was good (ICC=.77, 95%-confidence interval [.75,
.79], p&lt;.001). In functional analysis, a positive bias was observed for
ejection fractions derived from the proposed model compared to the clinical
reference standard (RT-BH mean EF: 58.5 $\pm$ 5.6%, bias: +3.47%,
95%-confidence interval [-.86, 7.79%], RT-FB mean: 57.9 $\pm$ 10.6%, bias:
+1.45%, [-3.02, 5.91%], Cartesian mean: 54.9 $\pm$ 6.7%). The introduced
real-time MR imaging technique is capable of acquiring high-quality cardiac
cine data in 1-2 minutes without the need for ECG gating and breath-holds. It
thus offers a promising alternative to the current clinical practice of
segmented acquisition, with shorter scan times, higher patient comfort and
increased robustness to arrhythmia and patient incompliance.</description>
      <guid isPermaLink="false">2409.08619v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Few-Shot Segmentation Without Meta-Learning: A Good Transductive Inference Is All You Need?</title>
      <link>http://arxiv.org/abs/2012.06166v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CVPR 2021. Code available at
  https://github.com/mboudiaf/RePRI-for-Few-Shot-Segmentation&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 推理方式的重要性&lt;/h4&gt;   - 论文强调，在少样本分割任务中，推理方式对性能有显著影响。这一方面在文献中常被忽视，研究者更多关注元学习范式。&lt;br&gt;&lt;h4&gt;2. 引入传导推理&lt;/h4&gt;   - 提出了针对给定查询图像的传导推理方法，利用未标记像素的统计信息。&lt;br&gt;&lt;h4&gt;3. 新损失函数&lt;/h4&gt;   - 优化了一个包含三个互补项的新损失函数：&lt;br&gt;     - i) 标记支持像素的交叉熵；&lt;br&gt;     - ii) 查询图像未标记像素后验的香农熵；&lt;br&gt;     - iii) 基于预测前景比例的全局KL散度正则化器。&lt;br&gt;&lt;h4&gt;4. 计算负担&lt;/h4&gt;   - 由于推理使用简单的线性分类器，计算负担与归纳推理相当，可以在任何基础训练上使用。&lt;br&gt;&lt;h4&gt;5. 训练方式&lt;/h4&gt;   - 该方法放弃了情景训练，仅使用基础类的标准交叉熵训练，仍然在1-shot场景中取得了竞争力的性能。&lt;br&gt;&lt;h4&gt;6. 性能提升&lt;/h4&gt;   - 随着可用样本数量的增加，性能差距进一步扩大。在PASCAL-5i数据集上，5-shot和10-shot场景下分别提高了约5%和6%。&lt;br&gt;&lt;h4&gt;7. 领域迁移设置&lt;/h4&gt;   - 引入了一个新的设置，考虑领域迁移，即基础类和新类来自不同的数据集。该方法在这种更现实的设置中取得了最佳表现。&lt;br&gt;&lt;h4&gt;8. 代码可用性&lt;/h4&gt;   - 研究者提供了可在线获取的代码链接：[GitHub - RePRI-for-Few-Shot-Segmentation](https://github.com/mboudiaf/RePRI-for-Few-Shot-Segmentation)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2020-12-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/mboudiaf/RePRI-for-Few-Shot-Segmentation&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We show that the way inference is performed in few-shot segmentation tasks
has a substantial effect on performances -- an aspect often overlooked in the
literature in favor of the meta-learning paradigm. We introduce a transductive
inference for a given query image, leveraging the statistics of its unlabeled
pixels, by optimizing a new loss containing three complementary terms: i) the
cross-entropy on the labeled support pixels; ii) the Shannon entropy of the
posteriors on the unlabeled query-image pixels; and iii) a global KL-divergence
regularizer based on the proportion of the predicted foreground. As our
inference uses a simple linear classifier of the extracted features, its
computational load is comparable to inductive inference and can be used on top
of any base training. Foregoing episodic training and using only standard
cross-entropy training on the base classes, our inference yields competitive
performances on standard benchmarks in the 1-shot scenarios. As the number of
available shots increases, the gap in performances widens: on PASCAL-5i, our
method brings about 5% and 6% improvements over the state-of-the-art, in the 5-
and 10-shot scenarios, respectively. Furthermore, we introduce a new setting
that includes domain shifts, where the base and novel classes are drawn from
different datasets. Our method achieves the best performances in this more
realistic setting. Our code is freely available online:
https://github.com/mboudiaf/RePRI-for-Few-Shot-Segmentation.</description>
      <guid isPermaLink="false">2012.06166v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Open-World Test-Time Training: Self-Training with Contrast Learning</title>
      <link>http://arxiv.org/abs/2409.09591v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10page&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 传统的测试时训练（TTT）方法在应对领域转移时，通常假设类别集是固定的，这限制了其在现实场景中的适用性。&lt;br&gt;&lt;h4&gt;2. 研究目标&lt;/h4&gt;   - 开放世界测试时训练（OWTTT）旨在解决将深度学习模型推广到未知目标领域分布的挑战，特别是在存在强烈的分布外（OOD）数据时。&lt;br&gt;&lt;h4&gt;3. 现有问题&lt;/h4&gt;   - 现有TTT方法在面对强OOD数据时，通常难以维持性能。&lt;br&gt;   - OWTTT主要集中于区分强弱OOD数据，但在TTT的早期阶段，初始特征提取受到强OOD和噪声的干扰，导致对某些类别的对比减弱和错误分类。&lt;br&gt;&lt;h4&gt;4. 解决方案&lt;/h4&gt;   - 提出了开放世界动态对比学习（OWDCL），这种创新方法利用对比学习来增强正样本对的对比度。&lt;br&gt;   - 该策略不仅在早期阶段增强了对比度，还显著提高了模型在后续阶段的鲁棒性。&lt;br&gt;&lt;h4&gt;5. 实验结果&lt;/h4&gt;   - 在比较数据集上，OWDCL模型实现了最先进的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traditional test-time training (TTT) methods, while addressing domain shifts,
often assume a consistent class set, limiting their applicability in real-world
scenarios characterized by infinite variety. Open-World Test-Time Training
(OWTTT) addresses the challenge of generalizing deep learning models to unknown
target domain distributions, especially in the presence of strong
Out-of-Distribution (OOD) data. Existing TTT methods often struggle to maintain
performance when confronted with strong OOD data. In OWTTT, the focus has
predominantly been on distinguishing between overall strong and weak OOD data.
However, during the early stages of TTT, initial feature extraction is hampered
by interference from strong OOD and corruptions, resulting in diminished
contrast and premature classification of certain classes as strong OOD. To
address this, we introduce Open World Dynamic Contrastive Learning (OWDCL), an
innovative approach that utilizes contrastive learning to augment positive
sample pairs. This strategy not only bolsters contrast in the early stages but
also significantly enhances model robustness in subsequent stages. In
comparison datasets, our OWDCL model has produced the most advanced
performance.</description>
      <guid isPermaLink="false">2409.09591v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Lepard: Learning partial point cloud matching in rigid and deformable scenes</title>
      <link>http://arxiv.org/abs/2111.12591v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to CVPR'2022. Code and data:
  https://github.com/rabbityl/lepard&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 本文介绍了Lepard，一种基于学习的部分点云匹配方法，适用于刚性和可变形场景。&lt;br&gt;&lt;h4&gt;2. 关键技术&lt;/h4&gt;   - **点云表示解耦**：将点云表示分解为特征空间和3D位置空间。&lt;br&gt;   - **位置编码方法**：通过向量的点积显式揭示3D相对距离信息。&lt;br&gt;   - **位置重定位技术**：修改跨点云的相对位置。&lt;br&gt;&lt;h4&gt;3. 实验验证&lt;/h4&gt;   - 进行的消融研究证明了上述技术的有效性。&lt;br&gt;&lt;h4&gt;4. 刚性场景表现&lt;/h4&gt;   - Lepard与RANSAC和ICP结合，在3DMatch和3DLoMatch数据集上实现了93.9%和71.3%的最先进注册召回率。&lt;br&gt;&lt;h4&gt;5. 可变形场景表现&lt;/h4&gt;   - 在可变形场景中，Lepard在新构建的4DMatch和4DLoMatch基准上，分别比现有方法提高了27.1%和34.8%的非刚性特征匹配召回率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2021-11-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/rabbityl/lepard&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Lepard, a Learning based approach for partial point cloud matching
in rigid and deformable scenes. The key characteristics are the following
techniques that exploit 3D positional knowledge for point cloud matching: 1) An
architecture that disentangles point cloud representation into feature space
and 3D position space. 2) A position encoding method that explicitly reveals 3D
relative distance information through the dot product of vectors. 3) A
repositioning technique that modifies the crosspoint-cloud relative positions.
Ablation studies demonstrate the effectiveness of the above techniques. In
rigid cases, Lepard combined with RANSAC and ICP demonstrates state-of-the-art
registration recall of 93.9% / 71.3% on the 3DMatch / 3DLoMatch. In deformable
cases, Lepard achieves +27.1% / +34.8% higher non-rigid feature matching recall
than the prior art on our newly constructed 4DMatch / 4DLoMatch benchmark.</description>
      <guid isPermaLink="false">2111.12591v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Filling Missing Values Matters for Range Image-Based Point Cloud Segmentation</title>
      <link>http://arxiv.org/abs/2405.10175v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  No Comments&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 点云分割（PCS）在机器人感知和导航任务中至关重要。&lt;br&gt;   - 为了有效理解大规模户外点云，通常采用其范围图像表示，这种表示方式紧凑且结构化。&lt;br&gt;&lt;h4&gt;2. 现有问题&lt;/h4&gt;   - 范围图像中的不良缺失值损害了物体的形状和模式，使得模型难以学习完整的几何信息，从而导致PCS模型性能较差。&lt;br&gt;&lt;h4&gt;3. 问题原因&lt;/h4&gt;   - 不合理的投影方法和扫描去倾斜处理主要导致范围图像中的缺失值。&lt;br&gt;   - 此外，几乎所有先前的工作未考虑在PCS任务中填补意外缺失值。&lt;br&gt;&lt;h4&gt;4. 解决方案&lt;/h4&gt;   - 提出了一种新投影方法：**扫描展开++（SU++）**，旨在避免生成的范围图像中出现大量缺失值。&lt;br&gt;   - 引入了一种简单有效的方法：**范围依赖的$K$-最近邻插值（$K$NNI）**，进一步填补缺失值。&lt;br&gt;&lt;h4&gt;5. 新模型&lt;/h4&gt;   - 提出了**填补缺失值网络（FMVNet）**和**快速FMVNet**。&lt;br&gt;&lt;h4&gt;6. 实验结果&lt;/h4&gt;   - 在SemanticKITTI、SemanticPOSS和nuScenes数据集上的广泛实验结果表明，采用SU++和$K$NNI后，现有的范围图像基PCS模型的性能均优于基线模型。&lt;br&gt;   - FMVNet和Fast FMVNet在速度与准确性的权衡方面实现了最先进的性能。&lt;br&gt;&lt;h4&gt;7. 应用前景&lt;/h4&gt;   - 提出的解决方案可应用于其他基于范围图像的任务和实际应用中。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud segmentation (PCS) plays an essential role in robot perception
and navigation tasks. To efficiently understand large-scale outdoor point
clouds, their range image representation is commonly adopted. This image-like
representation is compact and structured, making range image-based PCS models
practical. However, undesirable missing values in the range images damage the
shapes and patterns of objects. This problem creates difficulty for the models
in learning coherent and complete geometric information from the objects.
Consequently, the PCS models only achieve inferior performance. Delving deeply
into this issue, we find that the use of unreasonable projection approaches and
deskewing scans mainly leads to unwanted missing values in the range images.
Besides, almost all previous works fail to consider filling in the unexpected
missing values in the PCS task. To alleviate this problem, we first propose a
new projection method, namely scan unfolding++ (SU++), to avoid massive missing
values in the generated range images. Then, we introduce a simple yet effective
approach, namely range-dependent $K$-nearest neighbor interpolation ($K$NNI),
to further fill in missing values. Finally, we introduce the Filling Missing
Values Network (FMVNet) and Fast FMVNet. Extensive experimental results on
SemanticKITTI, SemanticPOSS, and nuScenes datasets demonstrate that by
employing the proposed SU++ and $K$NNI, existing range image-based PCS models
consistently achieve better performance than the baseline models. Besides, both
FMVNet and Fast FMVNet achieve state-of-the-art performance in terms of the
speed-accuracy trade-off. The proposed methods can be applied to other range
image-based tasks and practical applications.</description>
      <guid isPermaLink="false">2405.10175v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>M-BEST-RQ: A Multi-Channel Speech Foundation Model for Smart Glasses</title>
      <link>http://arxiv.org/abs/2409.11494v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  In submission to IEEE ICASSP 2025&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 多通道可穿戴设备（如智能眼镜）的普及催生了许多应用，特别是在定向语音识别和增强听力方面。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 当前解决这些任务的方法使用独立训练的模型，未能充分利用大量未标记数据。&lt;br&gt;&lt;h4&gt;3. 研究目标&lt;/h4&gt;   - 提出M-BEST-RQ，这是首个针对智能眼镜的多通道语音基础模型，旨在通过大规模自监督学习（SSL）实现阵列几何无关的方法。&lt;br&gt;&lt;h4&gt;4. 方法创新&lt;/h4&gt;   - 与以往仅在模拟环境中评估多通道语音SSL的工作不同，本研究策划了一系列真实的下游任务来评估模型。&lt;br&gt;&lt;h4&gt;5. 下游任务&lt;/h4&gt;   - 任务包括：&lt;br&gt;     - 对话自动语音识别（ASR）&lt;br&gt;     - 球面主动源定位&lt;br&gt;     - 佩戴者语音活动检测&lt;br&gt;   - 数据来源于MMCSG和EasyCom数据集。&lt;br&gt;&lt;h4&gt;6. 实验结果&lt;/h4&gt;   - 通用的M-BEST-RQ编码器在所有任务中能够匹敌或超越监督模型。&lt;br&gt;   - 特别是在对话ASR任务中，使用仅8小时的标记语音，模型的表现超越了在2000小时标记数据上训练的监督ASR基线，证明了该方法的有效性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The growing popularity of multi-channel wearable devices, such as smart
glasses, has led to a surge of applications such as targeted speech recognition
and enhanced hearing. However, current approaches to solve these tasks use
independently trained models, which may not benefit from large amounts of
unlabeled data. In this paper, we propose M-BEST-RQ, the first multi-channel
speech foundation model for smart glasses, which is designed to leverage
large-scale self-supervised learning (SSL) in an array-geometry agnostic
approach. While prior work on multi-channel speech SSL only evaluated on
simulated settings, we curate a suite of real downstream tasks to evaluate our
model, namely (i) conversational automatic speech recognition (ASR), (ii)
spherical active source localization, and (iii) glasses wearer voice activity
detection, which are sourced from the MMCSG and EasyCom datasets. We show that
a general-purpose M-BEST-RQ encoder is able to match or surpass supervised
models across all tasks. For the conversational ASR task in particular, using
only 8 hours of labeled speech, our model outperforms a supervised ASR baseline
that is trained on 2000 hours of labeled data, which demonstrates the
effectiveness of our approach.</description>
      <guid isPermaLink="false">2409.11494v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Recurrent Graph Transformer Network for Multiple Fault Localization in Naval Shipboard Systems</title>
      <link>http://arxiv.org/abs/2409.10792v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 现代12kV海军舰艇系统中集成了电力电子构件，增强了能量管理和功能，但也带来了复杂的故障检测和控制挑战。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 传统的故障诊断方法难以在多个位置检测和管理故障，同时保持系统的稳定性和性能。&lt;br&gt;&lt;h4&gt;3. 研究目标&lt;/h4&gt;   - 本文提出了一种时间递归图变换网络，用于海军MVDC 12kV舰载系统的故障诊断。&lt;br&gt;&lt;h4&gt;4. 方法创新&lt;/h4&gt;   - 该深度图神经网络采用门控递归单元（GRU）捕捉时间特征，并使用多头注意力机制提取空间特征，从而提升诊断准确性。&lt;br&gt;&lt;h4&gt;5. 故障识别能力&lt;/h4&gt;   - 该方法能够有效识别和评估连续的多重故障，具备高精度。&lt;br&gt;&lt;h4&gt;6. 实验验证&lt;/h4&gt;   - 在由ESDRC团队设计的MVDC 12kV舰载系统中实施并验证该方法，涵盖所有关键组件。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 结果显示故障定位准确性显著提高，性能指标比其他机器学习方法提升了1-4%。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The integration of power electronics building blocks in modern MVDC 12kV
Naval ship systems enhances energy management and functionality but also
introduces complex fault detection and control challenges. These challenges
strain traditional fault diagnostic methods, making it difficult to detect and
manage faults across multiple locations while maintaining system stability and
performance. This paper proposes a temporal recurrent graph transformer network
for fault diagnosis in naval MVDC 12kV shipboard systems. The deep graph neural
network uses gated recurrent units to capture temporal features and a
multi-head attention mechanism to extract spatial features, enhancing
diagnostic accuracy. The approach effectively identifies and evaluates
successive multiple faults with high precision. The method is implemented and
validated on the MVDC 12kV shipboard system designed by the ESDRC team,
incorporating all key components. Results show significant improvements in
fault localization accuracy, with a 1-4% increase in performance metrics
compared to other machine learning methods.</description>
      <guid isPermaLink="false">2409.10792v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>SpOT: Spatiotemporal Modeling for 3D Object Tracking</title>
      <link>http://arxiv.org/abs/2207.05856v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 机器视觉系统在工业生产和日常生活中越来越受欢迎，能高效管理大量视觉感知任务。&lt;br&gt;&lt;h4&gt;2. 技术挑战&lt;/h4&gt;   - 使用单一传感器同时获取准确的深度和纹理信息存在困难，因此通常采用相机和LiDAR的多模态数据来提升性能。&lt;br&gt;&lt;h4&gt;3. 云边协作&lt;/h4&gt;   - 云边协作作为一种新兴计算方法，旨在改善用户体验并确保数据安全。&lt;br&gt;&lt;h4&gt;4. 研究目标&lt;/h4&gt;   - 提出解决多模态3D物体检测中特征压缩问题的创新方案。&lt;br&gt;&lt;h4&gt;5. 方法介绍&lt;/h4&gt;   - 设计了两种模式以满足不同应用需求：&lt;br&gt;     - **传输友好的特征压缩（T-FFC）**：仅传输网络主干最后一层的输出，从边缘设备发送到云设备。云设备通过通道扩展模块和两个空间上采样模块处理接收的特征，以生成多尺度特征。&lt;br&gt;     - **准确性友好的特征压缩（A-FFC）**：在T-FFC基础上，传输额外的两种特征，帮助云设备生成更准确的多尺度特征。&lt;br&gt;&lt;h4&gt;6. 实验结果&lt;/h4&gt;   - 在KITTI数据集上，使用VirConv-L检测网络的实验结果显示：&lt;br&gt;     - T-FFC模式能够将特征压缩至6061倍，同时检测性能减少不到3%。&lt;br&gt;     - A-FFC模式则将特征压缩至约901倍，几乎没有检测性能的下降。&lt;br&gt;&lt;h4&gt;7. 附加模块&lt;/h4&gt;   - 设计了可选的残差提取和3D物体重建模块，以促进检测物体的重建，重建的物体有效反映了原始物体的细节。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2022-07-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D multi-object tracking aims to uniquely and consistently identify all
mobile entities through time. Despite the rich spatiotemporal information
available in this setting, current 3D tracking methods primarily rely on
abstracted information and limited history, e.g. single-frame object bounding
boxes. In this work, we develop a holistic representation of traffic scenes
that leverages both spatial and temporal information of the actors in the
scene. Specifically, we reformulate tracking as a spatiotemporal problem by
representing tracked objects as sequences of time-stamped points and bounding
boxes over a long temporal history. At each timestamp, we improve the location
and motion estimates of our tracked objects through learned refinement over the
full sequence of object history. By considering time and space jointly, our
representation naturally encodes fundamental physical priors such as object
permanence and consistency across time. Our spatiotemporal tracking framework
achieves state-of-the-art performance on the Waymo and nuScenes benchmarks.</description>
      <guid isPermaLink="false">2207.05856v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Feature Compression for Cloud-Edge Multimodal 3D Object Detection</title>
      <link>http://arxiv.org/abs/2409.04123v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 机器视觉系统在工业生产和日常生活中越来越受欢迎，能高效管理大量视觉感知任务。&lt;br&gt;&lt;h4&gt;2. 技术挑战&lt;/h4&gt;   - 使用单一传感器同时获取准确的深度和纹理信息存在困难，因此通常采用相机和LiDAR的多模态数据来提升性能。&lt;br&gt;&lt;h4&gt;3. 云边协作&lt;/h4&gt;   - 云边协作作为一种新兴计算方法，旨在改善用户体验并确保数据安全。&lt;br&gt;&lt;h4&gt;4. 研究目标&lt;/h4&gt;   - 提出解决多模态3D物体检测中特征压缩问题的创新方案。&lt;br&gt;&lt;h4&gt;5. 方法介绍&lt;/h4&gt;   - 设计了两种模式以满足不同应用需求：&lt;br&gt;     - **传输友好的特征压缩（T-FFC）**：仅传输网络主干最后一层的输出，从边缘设备发送到云设备。云设备通过通道扩展模块和两个空间上采样模块处理接收的特征，以生成多尺度特征。&lt;br&gt;     - **准确性友好的特征压缩（A-FFC）**：在T-FFC基础上，传输额外的两种特征，帮助云设备生成更准确的多尺度特征。&lt;br&gt;&lt;h4&gt;6. 实验结果&lt;/h4&gt;   - 在KITTI数据集上，使用VirConv-L检测网络的实验结果显示：&lt;br&gt;     - T-FFC模式能够将特征压缩至6061倍，同时检测性能减少不到3%。&lt;br&gt;     - A-FFC模式则将特征压缩至约901倍，几乎没有检测性能的下降。&lt;br&gt;&lt;h4&gt;7. 附加模块&lt;/h4&gt;   - 设计了可选的残差提取和3D物体重建模块，以促进检测物体的重建，重建的物体有效反映了原始物体的细节。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine vision systems, which can efficiently manage extensive visual
perception tasks, are becoming increasingly popular in industrial production
and daily life. Due to the challenge of simultaneously obtaining accurate depth
and texture information with a single sensor, multimodal data captured by
cameras and LiDAR is commonly used to enhance performance. Additionally,
cloud-edge cooperation has emerged as a novel computing approach to improve
user experience and ensure data security in machine vision systems. This paper
proposes a pioneering solution to address the feature compression problem in
multimodal 3D object detection. Given a sparse tensor-based object detection
network at the edge device, we introduce two modes to accommodate different
application requirements: Transmission-Friendly Feature Compression (T-FFC) and
Accuracy-Friendly Feature Compression (A-FFC). In T-FFC mode, only the output
of the last layer of the network's backbone is transmitted from the edge
device. The received feature is processed at the cloud device through a channel
expansion module and two spatial upsampling modules to generate multi-scale
features. In A-FFC mode, we expand upon the T-FFC mode by transmitting two
additional types of features. These added features enable the cloud device to
generate more accurate multi-scale features. Experimental results on the KITTI
dataset using the VirConv-L detection network showed that T-FFC was able to
compress the features by a factor of 6061 with less than a 3% reduction in
detection performance. On the other hand, A-FFC compressed the features by a
factor of about 901 with almost no degradation in detection performance. We
also designed optional residual extraction and 3D object reconstruction modules
to facilitate the reconstruction of detected objects. The reconstructed objects
effectively reflected details of the original objects.</description>
      <guid isPermaLink="false">2409.04123v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>ML-SemReg: Boosting Point Cloud Registration with Multi-level Semantic Consistency</title>
      <link>http://arxiv.org/abs/2407.09862v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ECCV2024&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 近期点云配准的进展主要依赖于几何信息，但这些方法在低重叠情况下仍然面临挑战，限制了它们的实际应用。&lt;br&gt;&lt;h4&gt;2. 研究目标&lt;/h4&gt;   - 提出ML-SemReg，一个即插即用的点云配准框架，充分利用语义信息以提高配准效果。&lt;br&gt;&lt;h4&gt;3. 主要观点&lt;/h4&gt;   - 通过渲染语义线索，识别出不匹配的两种类型：类间不匹配和类内不匹配，这些可以通过多层次语义一致性有效解决。&lt;br&gt;&lt;h4&gt;4. 方法创新&lt;/h4&gt;   - **Group Matching模块**：旨在处理类间不匹配，输出多个匹配组，确保局部语义一致性。&lt;br&gt;   - **Mask Matching模块**：基于场景语义一致性，抑制类内不匹配。&lt;br&gt;&lt;h4&gt;5. 性能提升&lt;/h4&gt;   - 通过这两个模块，ML-SemReg生成高内点比率的对应关系。&lt;br&gt;&lt;h4&gt;6. 实验结果&lt;/h4&gt;   - 大量实验表明，ML-SemReg在性能和鲁棒性方面表现出色。例如，在KITTI数据集中，当使用ML-SemReg时，MAC的配准召回率提高了近34个百分点。&lt;br&gt;&lt;h4&gt;7. 代码获取&lt;/h4&gt;   - 相关代码可在[GitHub - ML-SemReg](https://github.com/Laka-3DV/ML-SemReg)获取。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-07-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/laka-3dv/ml-semreg&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in point cloud registration mostly leverage geometric
information. Although these methods have yielded promising results, they still
struggle with problems of low overlap, thus limiting their practical usage. In
this paper, we propose ML-SemReg, a plug-and-play point cloud registration
framework that fully exploits semantic information. Our key insight is that
mismatches can be categorized into two types, i.e., inter- and intra-class,
after rendering semantic clues, and can be well addressed by utilizing
multi-level semantic consistency. We first propose a Group Matching module to
address inter-class mismatching, outputting multiple matching groups that
inherently satisfy Local Semantic Consistency. For each group, a Mask Matching
module based on Scene Semantic Consistency is then introduced to suppress
intra-class mismatching. Benefit from those two modules, ML-SemReg generates
correspondences with a high inlier ratio. Extensive experiments demonstrate
excellent performance and robustness of ML-SemReg, e.g., in hard-cases of the
KITTI dataset, the Registration Recall of MAC increases by almost 34 percentage
points when our ML-SemReg is equipped. Code is available at
\url{https://github.com/Laka-3DV/ML-SemReg}</description>
      <guid isPermaLink="false">2407.09862v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Exploring the Impact of Data Quantity on ASR in Extremely Low-resource Languages</title>
      <link>http://arxiv.org/abs/2409.08872v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 本研究探讨数据增强技术在低资源自动语音识别（ASR）中的有效性，重点关注两种濒危的南岛语言：Amis和Seediq。&lt;br&gt;&lt;h4&gt;2. 研究目标&lt;/h4&gt;   - 鉴于自监督学习（SSL）在低资源环境中的潜力，研究数据量对SSL模型持续预训练的影响。&lt;br&gt;&lt;h4&gt;3. 方法创新&lt;/h4&gt;   - 提出了一个新颖的数据选择方案，利用多语言语料库来增强有限的目标语言数据。&lt;br&gt;   - 该方案使用语言分类器提取语音嵌入，并利用单类分类器识别与目标语言在音位和音韵上相近的语句。&lt;br&gt;&lt;h4&gt;4. 数据选择过程&lt;/h4&gt;   - 语句根据决策分数进行排名和选择，确保在SSL-ASR流程中包含高度相关的数据。&lt;br&gt;&lt;h4&gt;5. 实验结果&lt;/h4&gt;   - 实验结果表明，该方法对Amis和Seediq的ASR性能有显著改善。&lt;br&gt;&lt;h4&gt;6. 研究意义&lt;/h4&gt;   - 这些发现强调了通过跨语言迁移学习进行数据增强的可行性和潜力，尤其是针对低资源语言的ASR。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study investigates the efficacy of data augmentation techniques for
low-resource automatic speech recognition (ASR), focusing on two endangered
Austronesian languages, Amis and Seediq. Recognizing the potential of
self-supervised learning (SSL) in low-resource settings, we explore the impact
of data volume on the continued pre-training of SSL models. We propose a novel
data-selection scheme leveraging a multilingual corpus to augment the limited
target language data. This scheme utilizes a language classifier to extract
utterance embeddings and employs one-class classifiers to identify utterances
phonetically and phonologically proximate to the target languages. Utterances
are ranked and selected based on their decision scores, ensuring the inclusion
of highly relevant data in the SSL-ASR pipeline. Our experimental results
demonstrate the effectiveness of this approach, yielding substantial
improvements in ASR performance for both Amis and Seediq. These findings
underscore the feasibility and promise of data augmentation through
cross-lingual transfer learning for low-resource language ASR.</description>
      <guid isPermaLink="false">2409.08872v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Developing, Analyzing, and Evaluating Self-Drive Algorithms Using Drive-by-Wire Electric Vehicles</title>
      <link>http://arxiv.org/abs/2409.03114v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Supported by the National Science Foundation under Grants No. 2150292
  and 2150096&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 可靠的车道跟随算法对于安全和有效的自动驾驶至关重要。&lt;br&gt;&lt;h4&gt;2. 研究目标&lt;/h4&gt;   - 本项目的主要目标是开发和评估不同的车道跟随程序，以寻找最可靠的算法用于车辆与一切（V2X）项目。&lt;br&gt;&lt;h4&gt;3. 实验方法&lt;/h4&gt;   - 算法首先在模拟器上进行测试，随后在配备驱动控制系统的真实车辆上使用ROS（机器人操作系统）进行测试。&lt;br&gt;&lt;h4&gt;4. 性能评估&lt;/h4&gt;   - 通过可靠性、舒适度、速度和适应性等指标评估算法性能。&lt;br&gt;&lt;h4&gt;5. 主要发现&lt;/h4&gt;   - 结果显示，最可靠的两种方法能够同时检测车道线，并使用无监督学习将其分离。&lt;br&gt;   - 这些方法在各种驾驶场景中表现出强大鲁棒性，适合整合到V2X项目中。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/benatfroemming/REU-2024-Lane-Following&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reliable lane-following algorithms are essential for safe and effective
autonomous driving. This project was primarily focused on developing and
evaluating different lane-following programs to find the most reliable
algorithm for a Vehicle to Everything (V2X) project. The algorithms were first
tested on a simulator and then with real vehicles equipped with a drive-by-wire
system using ROS (Robot Operating System). Their performance was assessed
through reliability, comfort, speed, and adaptability metrics. The results show
that the two most reliable approaches detect both lane lines and use
unsupervised learning to separate them. These approaches proved to be robust in
various driving scenarios, making them suitable candidates for integration into
the V2X project.</description>
      <guid isPermaLink="false">2409.03114v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>P2M2-Net: Part-Aware Prompt-Guided Multimodal Point Cloud Completion</title>
      <link>http://arxiv.org/abs/2312.17611v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Best Poster Award of CAD/Graphics 2023&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 从严重遮挡的点云中推断缺失区域是一个高度挑战性的任务，尤其是在具有丰富几何和结构细节的3D形状中，存在固有的未知部分模糊性。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 现有方法通常采用监督学习的方式学习一对一映射，或训练生成模型来合成缺失点，以完成3D点云形状。&lt;br&gt;   - 这些方法缺乏对完成过程的可控性，结果要么是确定性的，要么表现出不受控制的多样性。&lt;br&gt;&lt;h4&gt;3. 研究创新&lt;/h4&gt;   - 本文提出了一种新颖的基于提示的点云补全框架，称为P2M2-Net，以实现更可控且多样化的形状补全。&lt;br&gt;&lt;h4&gt;4. 方法概述&lt;/h4&gt;   - 给定一个输入的部分点云和描述缺失区域的文本提示（如语义和结构信息），我们的基于Transformer的补全网络能够高效融合多模态特征，并生成遵循提示指导的多样化结果。&lt;br&gt;&lt;h4&gt;5. 数据集与实验&lt;/h4&gt;   - 在新的大规模PartNet-Prompt数据集上训练P2M2-Net，并在两个具有挑战性的形状补全基准上进行广泛实验。&lt;br&gt;&lt;h4&gt;6. 实验结果&lt;/h4&gt;   - 定量和定性结果表明，结合提示可以实现更可控的基于部分的点云补全和生成。&lt;br&gt;&lt;h4&gt;7. 代码与数据获取&lt;/h4&gt;   - 代码和数据可在[GitHub - P2M2-Net](https://github.com/JLU-ICL/P2M2-Net)获得。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2023-12-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Inferring missing regions from severely occluded point clouds is highly
challenging. Especially for 3D shapes with rich geometry and structure details,
inherent ambiguities of the unknown parts are existing. Existing approaches
either learn a one-to-one mapping in a supervised manner or train a generative
model to synthesize the missing points for the completion of 3D point cloud
shapes. These methods, however, lack the controllability for the completion
process and the results are either deterministic or exhibiting uncontrolled
diversity. Inspired by the prompt-driven data generation and editing, we
propose a novel prompt-guided point cloud completion framework, coined
P2M2-Net, to enable more controllable and more diverse shape completion. Given
an input partial point cloud and a text prompt describing the part-aware
information such as semantics and structure of the missing region, our
Transformer-based completion network can efficiently fuse the multimodal
features and generate diverse results following the prompt guidance. We train
the P2M2-Net on a new large-scale PartNet-Prompt dataset and conduct extensive
experiments on two challenging shape completion benchmarks. Quantitative and
qualitative results show the efficacy of incorporating prompts for more
controllable part-aware point cloud completion and generation. Code and data
are available at https://github.com/JLU-ICL/P2M2-Net.</description>
      <guid isPermaLink="false">2312.17611v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>The Benefits of Balance: From Information Projections to Variance Reduction</title>
      <link>http://arxiv.org/abs/2408.15065v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 数据平衡在多个模态/来源中以多种形式出现，出现在多个基础模型（如CLIP和DINO）中，以实现通用表示学习。&lt;br&gt;&lt;h4&gt;2. 研究目标&lt;/h4&gt;   - 本文展示了一种迭代算法的意外好处，该算法通常用于避免表示崩溃，且能够减少这些来源的经验分布函数的估计方差。&lt;br&gt;&lt;h4&gt;3. 理论贡献&lt;/h4&gt;   - 提供了非渐进界限，量化这种方差减少效应，并将其与适当定义的马尔可夫算子的特征衰减相关联。&lt;br&gt;&lt;h4&gt;4. 应用实例&lt;/h4&gt;   - 解释了对比多模态学习和自监督聚类中的各种数据平衡形式如何被视为这种方差减少方案的实例。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Data balancing across multiple modalities/sources appears in various forms in
several foundation models (e.g., CLIP and DINO) achieving universal
representation learning. We show that this iterative algorithm, usually used to
avoid representation collapse, enjoys an unsuspected benefit: reducing the
variance of estimators that are functionals of the empirical distribution over
these sources. We provide non-asymptotic bounds quantifying this variance
reduction effect and relate them to the eigendecays of appropriately defined
Markov operators. We explain how various forms of data balancing in contrastive
multimodal learning and self-supervised clustering can be interpreted as
instances of this variance reduction scheme.</description>
      <guid isPermaLink="false">2408.15065v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>SLIM: Scalable and Lightweight LiDAR Mapping in Urban Environments</title>
      <link>http://arxiv.org/abs/2409.08681v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 16 figures&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - LiDAR点云地图因其高一致性被广泛用于道路机器人导航，但密集点云在长期操作中面临高内存消耗和维护性降低的挑战。&lt;br&gt;&lt;h4&gt;2. 研究目标&lt;/h4&gt;   - 本文提出了一种可扩展且轻量化的映射系统SLIM，用于城市环境中的长期LiDAR映射。&lt;br&gt;&lt;h4&gt;3. 系统设计&lt;/h4&gt;   - SLIM系统首先将结构化点云参数化为线和面。这种轻量化的结构化表示满足地图合并、姿态图优化和束调整的要求，确保增量管理和局部一致性。&lt;br&gt;&lt;h4&gt;4. 长期操作方法&lt;/h4&gt;   - 设计了一种以地图为中心的非线性因子恢复方法，旨在稀疏化姿态，同时保持映射精度。&lt;br&gt;&lt;h4&gt;5. 实验验证&lt;/h4&gt;   - 使用来自经典LiDAR映射数据集（如KITTI、NCLT和HeLiPR）的多会话真实世界LiDAR数据验证SLIM系统。&lt;br&gt;   - 实验结果显示其在映射精度、轻量性和可扩展性方面的能力。&lt;br&gt;&lt;h4&gt;6. 地图重用&lt;/h4&gt;   - 通过基于地图的机器人定位验证了地图的重用效果。&lt;br&gt;&lt;h4&gt;7. 最终成果&lt;/h4&gt;   - 在多会话LiDAR数据下，SLIM系统提供了低内存消耗（130 KB/km）的全球一致地图。&lt;br&gt;&lt;h4&gt;8. 代码开放&lt;/h4&gt;   - 研究团队已将代码开源，以造福社区。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LiDAR point cloud maps are extensively utilized on roads for robot navigation
due to their high consistency. However, dense point clouds face challenges of
high memory consumption and reduced maintainability for long-term operations.
In this study, we introduce SLIM, a scalable and lightweight mapping system for
long-term LiDAR mapping in urban environments. The system begins by
parameterizing structural point clouds into lines and planes. These lightweight
and structural representations meet the requirements of map merging, pose graph
optimization, and bundle adjustment, ensuring incremental management and local
consistency. For long-term operations, a map-centric nonlinear factor recovery
method is designed to sparsify poses while preserving mapping accuracy. We
validate the SLIM system with multi-session real-world LiDAR data from
classical LiDAR mapping datasets, including KITTI, NCLT, and HeLiPR. The
experiments demonstrate its capabilities in mapping accuracy, lightweightness,
and scalability. Map re-use is also verified through map-based robot
localization. Ultimately, with multi-session LiDAR data, the SLIM system
provides a globally consistent map with low memory consumption (130 KB/km). We
have made our code open-source to benefit the community.</description>
      <guid isPermaLink="false">2409.08681v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive Sampling for Continuous Group Equivariant Neural Networks</title>
      <link>http://arxiv.org/abs/2409.08741v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, published in the Geometry-grounded Representation Learning
  and Generative Modeling (GRaM) Workshop at ICML 2024&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 可调节网络（steerable networks）用于处理具有内在对称性的数据，通常采用基于傅里叶的非线性函数。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 这些方法需要对整个群体进行采样，导致在连续群体中需要离散化。&lt;br&gt;   - 随着样本数量的增加，模型的性能和等变性（equivariance）提高，但同时计算成本也增加。&lt;br&gt;&lt;h4&gt;3. 研究目标&lt;/h4&gt;   - 本文提出一种自适应采样方法，动态调整采样过程以适应数据中的对称性，从而减少所需的群体样本数量，并降低计算需求。&lt;br&gt;&lt;h4&gt;4. 方法探索&lt;/h4&gt;   - 探索了不同实现方式及其对模型性能、等变性和计算效率的影响。&lt;br&gt;&lt;h4&gt;5. 实验结果&lt;/h4&gt;   - 研究结果表明，改进了模型性能，同时内存效率也有所提高。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Steerable networks, which process data with intrinsic symmetries, often use
Fourier-based nonlinearities that require sampling from the entire group,
leading to a need for discretization in continuous groups. As the number of
samples increases, both performance and equivariance improve, yet this also
leads to higher computational costs. To address this, we introduce an adaptive
sampling approach that dynamically adjusts the sampling process to the
symmetries in the data, reducing the number of required group samples and
lowering the computational demands. We explore various implementations and
their effects on model performance, equivariance, and computational efficiency.
Our findings demonstrate improved model performance, and a marginal increase in
memory efficiency.</description>
      <guid isPermaLink="false">2409.08741v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Towards Resilient 6G O-RAN: An Energy-Efficient URLLC Resource Allocation Framework</title>
      <link>http://arxiv.org/abs/2409.05553v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This manuscript is being submitted for peer review and potential
  publication in the IEEE Open Journal of the Communications Society&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 超可靠低延迟通信（URLLC）在“下一代”（NextG）蜂窝网络中的需求促使了高效资源利用的新方法。&lt;br&gt;&lt;h4&gt;2. 现有文献的局限性&lt;/h4&gt;   - 当前关于6G O-RAN的文献主要关注于改善移动宽带（eMBB）性能或单独优化URLLC延迟，往往忽视了在实际约束下同时优化两者的复杂平衡。&lt;br&gt;&lt;h4&gt;3. 研究目标&lt;/h4&gt;   - 本文提出了一种基于深度强化学习（DRL）的资源分配框架，结合元学习，旨在自适应管理eMBB和URLLC服务。&lt;br&gt;&lt;h4&gt;4. 模型创新&lt;/h4&gt;   - 该方法有效分配异构网络资源，旨在在不同环境条件下最大化能源效率（EE）并最小化URLLC延迟。&lt;br&gt;&lt;h4&gt;5. 关键因素&lt;/h4&gt;   - 强调了在多连接（MC）场景中准确估计流量分布的重要性，因为其不确定性可能显著降低能源效率。&lt;br&gt;&lt;h4&gt;6. 实验结果&lt;/h4&gt;   - 提出的框架在不同路径损失模型下展示了优越的适应性，超越了传统方法，为更具韧性和高效的6G网络奠定了基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The demands of ultra-reliable low-latency communication (URLLC) in ``NextG"
cellular networks necessitate innovative approaches for efficient resource
utilisation. The current literature on 6G O-RAN primarily addresses improved
mobile broadband (eMBB) performance or URLLC latency optimisation individually,
often neglecting the intricate balance required to optimise both simultaneously
under practical constraints. This paper addresses this gap by proposing a
DRL-based resource allocation framework integrated with meta-learning to manage
eMBB and URLLC services adaptively. Our approach efficiently allocates
heterogeneous network resources, aiming to maximise energy efficiency (EE)
while minimising URLLC latency, even under varying environmental conditions. We
highlight the critical importance of accurately estimating the traffic
distribution flow in the multi-connectivity (MC) scenario, as its uncertainty
can significantly degrade EE. The proposed framework demonstrates superior
adaptability across different path loss models, outperforming traditional
methods and paving the way for more resilient and efficient 6G networks.</description>
      <guid isPermaLink="false">2409.05553v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Multi-view Hypergraph-based Contrastive Learning Model for Cold-Start Micro-video Recommendation</title>
      <link>http://arxiv.org/abs/2409.09638v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 随着移动设备的普及和 TikTok、Kwai 等微视频平台的迅速发展，对个性化微视频推荐系统的需求显著增加。&lt;br&gt;&lt;h4&gt;2. 微视频特点&lt;/h4&gt;   - 微视频通常包含多样的信息，如文本元数据、视觉线索（如封面图像）和动态视频内容，这些都显著影响用户的互动和参与模式。&lt;br&gt;&lt;h4&gt;3. 现有方法的局限性&lt;/h4&gt;   - 现有方法常常面临过度平滑的问题，限制了有效捕捉全面互动信息的能力。&lt;br&gt;   - 冷启动场景仍然是一个挑战，由于互动数据稀疏和可用互动信号的未充分利用。&lt;br&gt;&lt;h4&gt;4. 研究目标&lt;/h4&gt;   - 为了解决上述问题，提出了一种基于多视角超图的对比学习模型（MHCR），用于冷启动微视频推荐。&lt;br&gt;&lt;h4&gt;5. 模型创新&lt;/h4&gt;   - MHCR 引入了多视角多模态特征提取层，以从各种角度捕捉互动信号。&lt;br&gt;   - 整合了多视角自监督学习任务，以提供额外的监督信号。&lt;br&gt;&lt;h4&gt;6. 实验结果&lt;/h4&gt;   - 在两个真实世界数据集上进行的广泛实验表明，MHCR 显著优于现有的视频推荐模型，能够有效缓解冷启动问题。&lt;br&gt;&lt;h4&gt;7. 代码获取&lt;/h4&gt;   - 提供了代码的访问链接：[MHCR代码](https://anonymous.4open.science/r/MHCR-02EF)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the widespread use of mobile devices and the rapid growth of micro-video
platforms such as TikTok and Kwai, the demand for personalized micro-video
recommendation systems has significantly increased. Micro-videos typically
contain diverse information, such as textual metadata, visual cues (e.g., cover
images), and dynamic video content, significantly affecting user interaction
and engagement patterns. However, most existing approaches often suffer from
the problem of over-smoothing, which limits their ability to capture
comprehensive interaction information effectively. Additionally, cold-start
scenarios present ongoing challenges due to sparse interaction data and the
underutilization of available interaction signals.
  To address these issues, we propose a Multi-view Hypergraph-based Contrastive
learning model for cold-start micro-video Recommendation (MHCR). MHCR
introduces a multi-view multimodal feature extraction layer to capture
interaction signals from various perspectives and incorporates multi-view
self-supervised learning tasks to provide additional supervisory signals.
Through extensive experiments on two real-world datasets, we show that MHCR
significantly outperforms existing video recommendation models and effectively
mitigates cold-start challenges. Our code is available at
https://anonymous.4open.science/r/MHCR-02EF.</description>
      <guid isPermaLink="false">2409.09638v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Exploring the Adjugate Matrix Approach to Quaternion Pose Extraction</title>
      <link>http://arxiv.org/abs/2205.09116v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  67 pages, 5 appendices, 9 figures&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 四元数在计算机图形学、机器视觉和机器人等领域的旋转相关问题中非常重要。&lt;br&gt;&lt;h4&gt;2. 研究目标&lt;/h4&gt;   - 本文研究四元数与旋转矩阵之间复杂几何关系，通过利用相关特征方程的伴随矩阵，获得四元数特征向量的流形。&lt;br&gt;&lt;h4&gt;3. 重要论点&lt;/h4&gt;   - 参数化的四元数与其对应的旋转矩阵不能被表示为单值函数，特别是在机器学习任务中。&lt;br&gt;   - 四元数解必须被视为流形，每个由伴随矩阵表示的单值区间都有不同的代数解。&lt;br&gt;&lt;h4&gt;4. 新构建的应用&lt;/h4&gt;   - 利用四元数伴随变量重新审视多个经典姿态估计应用，包括：&lt;br&gt;     - 2D点云匹配&lt;br&gt;     - 2D点云到投影匹配&lt;br&gt;     - 3D点云匹配&lt;br&gt;     - 3D正交点云到投影匹配&lt;br&gt;     - 3D透视点云到投影匹配&lt;br&gt;&lt;h4&gt;5. 主要成果&lt;/h4&gt;   - 对3D正交最小二乘姿态提取问题找到精确解，并成功应用于透视姿态提取问题，取得了优于现有方法的结果。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2022-05-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Quaternions are important for a wide variety of rotation-related problems in
computer graphics, machine vision, and robotics. We study the nontrivial
geometry of the relationship between quaternions and rotation matrices by
exploiting the adjugate matrix of the characteristic equation of a related
eigenvalue problem to obtain the manifold of the space of a quaternion
eigenvector. We argue that quaternions parameterized by their corresponding
rotation matrices cannot be expressed, for example, in machine learning tasks,
as single-valued functions: the quaternion solution must instead be treated as
a manifold, with different algebraic solutions for each of several
single-valued sectors represented by the adjugate matrix. We conclude with
novel constructions exploiting the quaternion adjugate variables to revisit
several classic pose estimation applications: 2D point-cloud matching, 2D
point-cloud-to-projection matching, 3D point-cloud matching, 3D orthographic
point-cloud-to-projection matching, and 3D perspective
point-cloud-to-projection matching. We find an exact solution to the 3D
orthographic least squares pose extraction problem, and apply it successfully
also to the perspective pose extraction problem with results that improve on
existing methods.</description>
      <guid isPermaLink="false">2205.09116v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>3D Unsupervised Learning by Distilling 2D Open-Vocabulary Segmentation Models for Autonomous Driving</title>
      <link>http://arxiv.org/abs/2405.15286v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  25 pages, 6 figures, codes are available at
  https://github.com/sbysbysbys/UOV&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 点云数据标注在自动驾驶中被认为是耗时且昂贵的任务，而无监督学习可以通过从未标注数据中学习点云表示来避免这一问题。&lt;br&gt;&lt;h4&gt;2. 研究目标&lt;/h4&gt;   - 本文提出UOV，一种新颖的3D无监督框架，辅以2D开放词汇分割模型。&lt;br&gt;&lt;h4&gt;3. 框架结构&lt;/h4&gt;   - **第一阶段**：&lt;br&gt;     - 创新性地整合2D开放词汇模型的高质量文本和图像特征，提出三模态对比预训练（Tri-Modal contrastive Pre-training, TMP）。&lt;br&gt;   - **第二阶段**：&lt;br&gt;     - 利用点云与图像之间的空间映射生成伪标签，实现跨模态知识蒸馏。&lt;br&gt;&lt;h4&gt;4. 噪声处理&lt;/h4&gt;   - 引入近似平面交互（Approximate Flat Interaction, AFI）来解决对齐过程中的噪声和标签混淆问题。&lt;br&gt;&lt;h4&gt;5. 实验验证&lt;/h4&gt;   - 在多个相关数据集上进行了广泛实验，以验证UOV的优越性。&lt;br&gt;&lt;h4&gt;6. 性能结果&lt;/h4&gt;   - 在nuScenes的无标注点云分割任务中，达到了创纪录的47.73% mIoU，超过了之前最佳模型10.70% mIoU。&lt;br&gt;   - 在nuScenes和SemanticKITTI上，用1%的数据进行微调时，表现分别为51.75% mIoU和48.14% mIoU，超越了所有先前的预训练模型。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-05-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/sbysbysbys/uov&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud data labeling is considered a time-consuming and expensive task
in autonomous driving, whereas unsupervised learning can avoid it by learning
point cloud representations from unannotated data. In this paper, we propose
UOV, a novel 3D Unsupervised framework assisted by 2D Open-Vocabulary
segmentation models. It consists of two stages: In the first stage, we
innovatively integrate high-quality textual and image features of 2D
open-vocabulary models and propose the Tri-Modal contrastive Pre-training
(TMP). In the second stage, spatial mapping between point clouds and images is
utilized to generate pseudo-labels, enabling cross-modal knowledge
distillation. Besides, we introduce the Approximate Flat Interaction (AFI) to
address the noise during alignment and label confusion. To validate the
superiority of UOV, extensive experiments are conducted on multiple related
datasets. We achieved a record-breaking 47.73% mIoU on the annotation-free
point cloud segmentation task in nuScenes, surpassing the previous best model
by 10.70% mIoU. Meanwhile, the performance of fine-tuning with 1% data on
nuScenes and SemanticKITTI reached a remarkable 51.75% mIoU and 48.14% mIoU,
outperforming all previous pre-trained models.</description>
      <guid isPermaLink="false">2405.15286v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Domain Learning by Meta-Learning: Taking Optimal Steps in Multi-Domain Loss Landscapes by Inner-Loop Learning</title>
      <link>http://arxiv.org/abs/2102.13147v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  IEEE International Symposium on Biomedical Imaging 2021&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究主题&lt;/h4&gt;   - 本文关注于多领域学习（MDL）在多模态应用中的模型无关解决方案。&lt;br&gt;&lt;h4&gt;2. 现有技术的局限&lt;/h4&gt;   - 许多现有的MDL技术依赖于特定模型，通常需要对架构进行复杂的修改以构建领域特定模块。&lt;br&gt;   - 将这些技术应用于新问题时（如使用U-Net进行语义分割），往往需要进行多项低级实现工作。&lt;br&gt;&lt;h4&gt;3. 研究目标&lt;/h4&gt;   - 针对新兴的多模态数据（如多种结构的神经影像模态），旨在纯算法实现MDL，使广泛使用的神经网络能够无缝地实现MDL。&lt;br&gt;&lt;h4&gt;4. 技术方法&lt;/h4&gt;   - 采用加权损失函数，并通过元学习（learning-to-learn）中的技术扩展为有效的过程。&lt;br&gt;   - 具体而言，采用内循环梯度步骤动态估计损失函数超参数的后验分布。&lt;br&gt;&lt;h4&gt;5. 模型无关性&lt;/h4&gt;   - 本方法为模型无关，不需要额外的模型参数或网络架构更改，而只需少量高效的算法修改即可提高MDL性能。&lt;br&gt;&lt;h4&gt;6. 应用示例&lt;/h4&gt;   - 在医学影像中的应用，特别是自动分割白质高信号（WMH）问题。&lt;br&gt;&lt;h4&gt;7. 数据来源&lt;/h4&gt;   - 考虑两种具有互补信息的神经影像模态（T1-MR和FLAIR）来解决该问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2021-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We consider a model-agnostic solution to the problem of Multi-Domain Learning
(MDL) for multi-modal applications. Many existing MDL techniques are
model-dependent solutions which explicitly require nontrivial architectural
changes to construct domain-specific modules. Thus, properly applying these MDL
techniques for new problems with well-established models, e.g. U-Net for
semantic segmentation, may demand various low-level implementation efforts. In
this paper, given emerging multi-modal data (e.g., various structural
neuroimaging modalities), we aim to enable MDL purely algorithmically so that
widely used neural networks can trivially achieve MDL in a model-independent
manner. To this end, we consider a weighted loss function and extend it to an
effective procedure by employing techniques from the recently active area of
learning-to-learn (meta-learning). Specifically, we take inner-loop gradient
steps to dynamically estimate posterior distributions over the hyperparameters
of our loss function. Thus, our method is model-agnostic, requiring no
additional model parameters and no network architecture changes; instead, only
a few efficient algorithmic modifications are needed to improve performance in
MDL. We demonstrate our solution to a fitting problem in medical imaging,
specifically, in the automatic segmentation of white matter hyperintensity
(WMH). We look at two neuroimaging modalities (T1-MR and FLAIR) with
complementary information fitting for our problem.</description>
      <guid isPermaLink="false">2102.13147v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Spatio-Temporal-Network Point Processes for Modeling Crime Events with Landmarks</title>
      <link>http://arxiv.org/abs/2409.10882v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 自激点过程广泛用于建模犯罪事件在连续地理空间中的传播效应，考虑事件的发生时间和位置。&lt;br&gt;&lt;h4&gt;2. 城市环境的特性&lt;/h4&gt;   - 在城市环境中，犯罪事件通常受到城市街道网络结构的限制，犯罪的传播效应受此网络地理特征的影响。&lt;br&gt;&lt;h4&gt;3. 基础设施的影响&lt;/h4&gt;   - 城市基础设施的复杂分布在塑造空间犯罪模式方面也起着重要作用。&lt;br&gt;&lt;h4&gt;4. 新框架的提出&lt;/h4&gt;   - 本文介绍了一种新颖的时空网络点过程框架，用于犯罪建模，整合了这些城市环境特征，结合了自注意力图神经网络。&lt;br&gt;&lt;h4&gt;5. 街道网络结构&lt;/h4&gt;   - 框架将街道网络结构作为事件空间的基础，犯罪事件可以在网络边缘的随机位置发生。&lt;br&gt;&lt;h4&gt;6. 测量犯罪移动模式&lt;/h4&gt;   - 事件之间的距离使用街道网络距离进行测量，以真实捕捉犯罪移动模式。&lt;br&gt;&lt;h4&gt;7. 事件标记的定义&lt;/h4&gt;   - 提出了通过将事件的犯罪类别与附近地标的类型连接来定义犯罪事件的新标记，旨在捕捉城市设计如何影响各种犯罪类型的混合结构。&lt;br&gt;&lt;h4&gt;8. 图注意力网络&lt;/h4&gt;   - 采用图注意力网络架构学习标记间的交互关系。&lt;br&gt;&lt;h4&gt;9. 实验结果&lt;/h4&gt;   - 在西班牙瓦伦西亚的犯罪数据上进行的大规模实验显示，框架在理解犯罪格局和预测区域犯罪风险方面的有效性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-exciting point processes are widely used to model the contagious effects
of crime events living within continuous geographic space, using their
occurrence time and locations. However, in urban environments, most events are
naturally constrained within the city's street network structure, and the
contagious effects of crime are governed by such a network geography.
Meanwhile, the complex distribution of urban infrastructures also plays an
important role in shaping crime patterns across space. We introduce a novel
spatio-temporal-network point process framework for crime modeling that
integrates these urban environmental characteristics by incorporating
self-attention graph neural networks. Our framework incorporates the street
network structure as the underlying event space, where crime events can occur
at random locations on the network edges. To realistically capture criminal
movement patterns, distances between events are measured using street network
distances. We then propose a new mark for a crime event by concatenating the
event's crime category with the type of its nearby landmark, aiming to capture
how the urban design influences the mixing structures of various crime types. A
graph attention network architecture is adopted to learn the existence of
mark-to-mark interactions. Extensive experiments on crime data from Valencia,
Spain, demonstrate the effectiveness of our framework in understanding the
crime landscape and forecasting crime risks across regions.</description>
      <guid isPermaLink="false">2409.10882v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>NCT-CRC-HE: Not All Histopathological Datasets Are Equally Useful</title>
      <link>http://arxiv.org/abs/2409.11546v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 近年来，针对组织病理图像分析提出了众多基于深度学习的解决方案，这些方案通常表现出极高的准确性。&lt;br&gt;&lt;h4&gt;2. 研究问题&lt;/h4&gt;   - 关键问题在于，这些高准确率是否受到与组织病理无关的低层次图像特性（如显微图像处理和预处理）影响。&lt;br&gt;&lt;h4&gt;3. 数据集分析&lt;/h4&gt;   - 本文分析了广泛使用的NCT-CRC-HE-100K结直肠癌数据集，揭示了数据集及其结果可能受到特定数据偏差的影响。&lt;br&gt;&lt;h4&gt;4. 数据集问题&lt;/h4&gt;   - 主要问题包括：&lt;br&gt;     - 不当的颜色归一化&lt;br&gt;     - 不同类别之间存在严重的JPEG伪影&lt;br&gt;     - 由于图像动态范围处理不当导致的完全损坏的组织样本&lt;br&gt;&lt;h4&gt;5. 模型表现&lt;/h4&gt;   - 即使是最简单的模型，仅使用每幅图像的红、绿、蓝三种颜色强度，也能在这个9类数据集上实现超过50%的准确率。&lt;br&gt;   - 使用未明确捕捉细胞形态特征的颜色直方图，准确率超过82%。&lt;br&gt;&lt;h4&gt;6. 高效模型结果&lt;/h4&gt;   - 基于EfficientNet-B0的ImageNet预训练模型在该数据集上实现了超过97.7%的准确率，超越了所有之前提出的解决方案，包括专用的基础组织病理模型和大型细胞形态意识神经网络。&lt;br&gt;&lt;h4&gt;7. 数据集可用性&lt;/h4&gt;   - NCT-CRC-HE数据集公开可用，任何人都可以自由使用以复制所展示的结果。&lt;br&gt;&lt;h4&gt;8. 资源获取&lt;/h4&gt;   - 本文中使用的代码和预训练模型可在 [GitHub](https://github.com/gmalivenko/NCT-CRC-HE-experiments) 上获取。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Numerous deep learning-based solutions have been proposed for
histopathological image analysis over the past years. While they usually
demonstrate exceptionally high accuracy, one key question is whether their
precision might be affected by low-level image properties not related to
histopathology but caused by microscopy image handling and pre-processing. In
this paper, we analyze a popular NCT-CRC-HE-100K colorectal cancer dataset used
in numerous prior works and show that both this dataset and the obtained
results may be affected by data-specific biases. The most prominent revealed
dataset issues are inappropriate color normalization, severe JPEG artifacts
inconsistent between different classes, and completely corrupted tissue samples
resulting from incorrect image dynamic range handling. We show that even the
simplest model using only 3 features per image (red, green and blue color
intensities) can demonstrate over 50% accuracy on this 9-class dataset, while
using color histogram not explicitly capturing cell morphology features yields
over 82% accuracy. Moreover, we show that a basic EfficientNet-B0 ImageNet
pretrained model can achieve over 97.7% accuracy on this dataset, outperforming
all previously proposed solutions developed for this task, including dedicated
foundation histopathological models and large cell morphology-aware neural
networks. The NCT-CRC-HE dataset is publicly available and can be freely used
to replicate the presented results. The codes and pre-trained models used in
this paper are available at
https://github.com/gmalivenko/NCT-CRC-HE-experiments</description>
      <guid isPermaLink="false">2409.11546v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>UniDet3D: Multi-dataset Indoor 3D Object Detection</title>
      <link>http://arxiv.org/abs/2409.04234v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 背景与需求&lt;/h4&gt;   - 随着客户对智能解决方案（如机器人和增强现实）的需求增长，3D物体检测从点云中受到了广泛关注。&lt;br&gt;&lt;h4&gt;2. 现有数据集的局限性&lt;/h4&gt;   - 现有的单个室内数据集规模小、缺乏多样性，无法训练出强大且通用的3D物体检测模型。&lt;br&gt;&lt;h4&gt;3. 基础模型的挑战&lt;/h4&gt;   - 尽管更通用的方法利用基础模型，但在特定任务的监督训练下仍然表现不佳。&lt;br&gt;&lt;h4&gt;4. 提出的模型&lt;/h4&gt;   - 本文提出了\ours{}，一种简单而有效的3D物体检测模型，能够在多种室内环境中工作。&lt;br&gt;&lt;h4&gt;5. 数据集统一&lt;/h4&gt;   - 通过统一不同的标签空间，\ours{}能够通过监督联合训练方案在多个数据集上学习强表示。&lt;br&gt;&lt;h4&gt;6. 网络架构&lt;/h4&gt;   - 该网络架构基于基础的变压器编码器，便于运行、定制和扩展预测流程。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 大规模实验表明，\ours{}在6个室内基准测试中显著优于现有的3D物体检测方法，具体提升如下：&lt;br&gt;     - ScanNet (+1.1 mAP50)&lt;br&gt;     - ARKitScenes (+19.4 mAP25)&lt;br&gt;     - S3DIS (+9.1 mAP50)&lt;br&gt;     - MultiScan (+9.3 mAP50)&lt;br&gt;     - 3RScan (+3.2 mAP50)&lt;br&gt;     - ScanNet++ (+2.7 mAP50)&lt;br&gt;&lt;h4&gt;8. 代码可用性&lt;/h4&gt;   - 相关代码可在 [GitHub](https://github.com/filapro/unidet3d) 上获取。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/filapro/unidet3d&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Growing customer demand for smart solutions in robotics and augmented reality
has attracted considerable attention to 3D object detection from point clouds.
Yet, existing indoor datasets taken individually are too small and
insufficiently diverse to train a powerful and general 3D object detection
model. In the meantime, more general approaches utilizing foundation models are
still inferior in quality to those based on supervised training for a specific
task. In this work, we propose \ours{}, a simple yet effective 3D object
detection model, which is trained on a mixture of indoor datasets and is
capable of working in various indoor environments. By unifying different label
spaces, \ours{} enables learning a strong representation across multiple
datasets through a supervised joint training scheme. The proposed network
architecture is built upon a vanilla transformer encoder, making it easy to
run, customize and extend the prediction pipeline for practical use. Extensive
experiments demonstrate that \ours{} obtains significant gains over existing 3D
object detection methods in 6 indoor benchmarks: ScanNet (+1.1 mAP50),
ARKitScenes (+19.4 mAP25), S3DIS (+9.1 mAP50), MultiScan (+9.3 mAP50), 3RScan
(+3.2 mAP50), and ScanNet++ (+2.7 mAP50). Code is available at
https://github.com/filapro/unidet3d .</description>
      <guid isPermaLink="false">2409.04234v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Large-displacement 3D Object Tracking with Hybrid Non-local Optimization</title>
      <link>http://arxiv.org/abs/2207.12620v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 优化方法的优缺点&lt;/h4&gt;   - 基于优化的3D物体跟踪方法精确且快速，但对大幅度帧间位移敏感。&lt;br&gt;&lt;h4&gt;2. 研究目标&lt;/h4&gt;   - 提出一种快速有效的非局部3D跟踪方法。&lt;br&gt;&lt;h4&gt;3. 关键观察&lt;/h4&gt;   - 错误的局部极小值主要是由于平面外旋转引起的。&lt;br&gt;&lt;h4&gt;4. 混合方法&lt;/h4&gt;   - 采用混合方法，结合非局部和局部优化，以处理不同参数，实现在6D姿态空间中的高效非局部搜索。&lt;br&gt;&lt;h4&gt;5. 预计算方法&lt;/h4&gt;   - 提出了基于预计算的稳健轮廓跟踪方法用于姿态优化。通过使用长搜索线和多个候选对应关系，可以适应不同的帧位移，无需粗到细的搜索。&lt;br&gt;&lt;h4&gt;6. 快速更新&lt;/h4&gt;   - 预计算后，姿态更新可以非常快速地进行，使得非局部优化能够实时运行。&lt;br&gt;&lt;h4&gt;7. 性能优势&lt;/h4&gt;   - 方法在小幅和大幅位移情况下均优于以往所有方法。在大幅位移情况下，准确率显著提高（81.7% 对比 19.4%）。&lt;br&gt;&lt;h4&gt;8. 实时速度&lt;/h4&gt;   - 实现了超过50帧每秒的实时速度，仅使用CPU。&lt;br&gt;&lt;h4&gt;9. 代码可用性&lt;/h4&gt;   - 源代码可在 [GitHub](https://github.com/cvbubbles/nonlocal-3dtracking) 上获取。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2022-07-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/cvbubbles/nonlocal-3dtracking&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Optimization-based 3D object tracking is known to be precise and fast, but
sensitive to large inter-frame displacements. In this paper we propose a fast
and effective non-local 3D tracking method. Based on the observation that
erroneous local minimum are mostly due to the out-of-plane rotation, we propose
a hybrid approach combining non-local and local optimizations for different
parameters, resulting in efficient non-local search in the 6D pose space. In
addition, a precomputed robust contour-based tracking method is proposed for
the pose optimization. By using long search lines with multiple candidate
correspondences, it can adapt to different frame displacements without the need
of coarse-to-fine search. After the pre-computation, pose updates can be
conducted very fast, enabling the non-local optimization to run in real time.
Our method outperforms all previous methods for both small and large
displacements. For large displacements, the accuracy is greatly improved
($81.7\% \;\text{v.s.}\; 19.4\%$). At the same time, real-time speed ($&gt;$50fps)
can be achieved with only CPU. The source code is available at
\url{https://github.com/cvbubbles/nonlocal-3dtracking}.</description>
      <guid isPermaLink="false">2207.12620v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>PARE-Net: Position-Aware Rotation-Equivariant Networks for Robust Point Cloud Registration</title>
      <link>http://arxiv.org/abs/2407.10142v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 基本需求&lt;/h4&gt;   - 学习旋转不变的特征是点云配准的基本要求。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限&lt;/h4&gt;   - 现有方法通常使用对旋转敏感的网络提取特征，并采用旋转增强技术粗略地学习近似不变映射，这导致网络对旋转易碎、过重，并阻碍特征的独特性。&lt;br&gt;&lt;h4&gt;3. 新方法提出&lt;/h4&gt;   - 提出了一个新颖的基于位置感知的旋转等变网络，旨在实现高效、轻量和鲁棒的配准。&lt;br&gt;&lt;h4&gt;4. 模型偏差&lt;/h4&gt;   - 该网络能提供强的模型归纳偏差，学习旋转等变/不变特征，从而解决上述局限。&lt;br&gt;&lt;h4&gt;5. 特征改进&lt;/h4&gt;   - 引入了位置感知卷积，以更好地学习局部结构的空间信息，从而提高描述符的独特性。&lt;br&gt;&lt;h4&gt;6. 假设生成&lt;/h4&gt;   - 提出了基于特征的假设生成器，利用编码细粒度结构方向的旋转等变特征生成可靠的模型假设。每个对应关系均可生成假设，比传统的需要多个可靠对应关系的估计器更高效。&lt;br&gt;&lt;h4&gt;7. 损失函数&lt;/h4&gt;   - 提出了对比旋转损失，以增强旋转等变特征对数据退化的鲁棒性。&lt;br&gt;&lt;h4&gt;8. 实验验证&lt;/h4&gt;   - 在室内和室外数据集上的大量实验表明，所提方法在配准召回率上显著优于现有最先进方法，同时保持轻量和快速。&lt;br&gt;&lt;h4&gt;9. 鲁棒性测试&lt;/h4&gt;   - 在旋转数据集上的实验展示了该方法对旋转变换的鲁棒性。&lt;br&gt;&lt;h4&gt;10. 代码可用性&lt;/h4&gt;    - 代码已公开，可在 [PARENet GitHub](https://github.com/yaorz97/PARENet) 上获取。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-07-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/yaorz97/parenet&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning rotation-invariant distinctive features is a fundamental requirement
for point cloud registration. Existing methods often use rotation-sensitive
networks to extract features, while employing rotation augmentation to learn an
approximate invariant mapping rudely. This makes networks fragile to rotations,
overweight, and hinders the distinctiveness of features. To tackle these
problems, we propose a novel position-aware rotation-equivariant network, for
efficient, light-weighted, and robust registration. The network can provide a
strong model inductive bias to learn rotation-equivariant/invariant features,
thus addressing the aforementioned limitations. To further improve the
distinctiveness of descriptors, we propose a position-aware convolution, which
can better learn spatial information of local structures. Moreover, we also
propose a feature-based hypothesis proposer. It leverages rotation-equivariant
features that encode fine-grained structure orientations to generate reliable
model hypotheses. Each correspondence can generate a hypothesis, thus it is
more efficient than classic estimators that require multiple reliable
correspondences. Accordingly, a contrastive rotation loss is presented to
enhance the robustness of rotation-equivariant features against data
degradation. Extensive experiments on indoor and outdoor datasets demonstrate
that our method significantly outperforms the SOTA methods in terms of
registration recall while being lightweight and keeping a fast speed. Moreover,
experiments on rotated datasets demonstrate its robustness against rotation
variations. Code is available at https://github.com/yaorz97/PARENet.</description>
      <guid isPermaLink="false">2407.10142v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Data Efficient Child-Adult Speaker Diarization with Simulated Conversations</title>
      <link>http://arxiv.org/abs/2409.08881v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under review&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 背景与重要性&lt;/h4&gt;   - 自动化儿童语音分析对诸如神经认知评估等应用至关重要。&lt;br&gt;&lt;h4&gt;2. 核心任务&lt;/h4&gt;   - 说话者分离（speaker diarization）是自动分析的重要组成部分，旨在识别“谁在何时说话”。&lt;br&gt;&lt;h4&gt;3. 现状问题&lt;/h4&gt;   - 由于隐私问题和缺乏注释数据集，公开可用的儿童-成人说话者分离解决方案非常稀缺。&lt;br&gt;&lt;h4&gt;4. 数据注释挑战&lt;/h4&gt;   - 对每种场景进行手动数据注释既耗时又昂贵。&lt;br&gt;&lt;h4&gt;5. 提出解决方案&lt;/h4&gt;   - 为克服这些挑战，提出了一种数据高效的解决方案，通过使用 AudioSet 创建模拟的儿童-成人对话。&lt;br&gt;&lt;h4&gt;6. 模型训练&lt;/h4&gt;   - 训练了基于 Whisper Encoder 的模型，在儿童-成人说话者分离任务中，在真实数据集上实现了强大的零样本性能。&lt;br&gt;&lt;h4&gt;7. 性能提升&lt;/h4&gt;   - 仅用 30 分钟的真实训练数据进行微调时，模型性能显著提升，使用 LoRA 技术进一步改善了迁移学习的表现。&lt;br&gt;&lt;h4&gt;8. 代码与模型可用性&lt;/h4&gt;   - 源代码和在模拟对话上训练的儿童-成人说话者分离模型已公开可用。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/usc-sail/child-adult-diarization&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automating child speech analysis is crucial for applications such as
neurocognitive assessments. Speaker diarization, which identifies ``who spoke
when'', is an essential component of the automated analysis. However, publicly
available child-adult speaker diarization solutions are scarce due to privacy
concerns and a lack of annotated datasets, while manually annotating data for
each scenario is both time-consuming and costly. To overcome these challenges,
we propose a data-efficient solution by creating simulated child-adult
conversations using AudioSet. We then train a Whisper Encoder-based model,
achieving strong zero-shot performance on child-adult speaker diarization using
real datasets. The model performance improves substantially when fine-tuned
with only 30 minutes of real train data, with LoRA further improving the
transfer learning performance. The source code and the child-adult speaker
diarization model trained on simulated conversations are publicly available.</description>
      <guid isPermaLink="false">2409.08881v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>CRA-PCN: Point Cloud Completion with Intra- and Inter-level Cross-Resolution Transformers</title>
      <link>http://arxiv.org/abs/2401.01552v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to AAAI 2024&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 背景与重要性&lt;/h4&gt;   - 点云补全是恢复完整点云的重要任务，通常由于遮挡、传感器分辨率有限等原因导致不完整。&lt;br&gt;&lt;h4&gt;2. 主流方法&lt;/h4&gt;   - 最近，粗到细的生成架构在点云补全中取得了显著成功，逐渐成为主流方法。&lt;br&gt;&lt;h4&gt;3. 关键技术&lt;/h4&gt;   - 本文揭示了这些方法背后的关键因素：精心设计的特征提取操作，具有明确的跨分辨率聚合。&lt;br&gt;&lt;h4&gt;4. 提出的新模型&lt;/h4&gt;   - 引入了“跨分辨率变换器”（Cross-Resolution Transformer），该模型通过局部注意力机制高效地执行跨分辨率聚合。&lt;br&gt;&lt;h4&gt;5. 特征捕捉能力&lt;/h4&gt;   - 通过递归设计，该操作能够捕捉比常见聚合操作更多的特征尺度，有助于捕捉细微的几何特征。&lt;br&gt;&lt;h4&gt;6. 方法比较&lt;/h4&gt;   - 尽管之前的方法探讨了多种形式的跨层级交叉分辨率聚合，但对内部层级聚合及其组合的有效性尚未进行分析。&lt;br&gt;&lt;h4&gt;7. 统一设计&lt;/h4&gt;   - 跨分辨率变换器可以通过切换输入，统一执行内部或跨层级的交叉分辨率聚合。&lt;br&gt;&lt;h4&gt;8. 模型整合&lt;/h4&gt;   - 将两种形式的跨分辨率变换器整合到一个上采样块中，用于点生成，并采用粗到细的方法逐步预测完整形状。&lt;br&gt;&lt;h4&gt;9. 实验结果&lt;/h4&gt;   - 大量实验表明，该方法在多个广泛使用的基准测试中显著超越了当前最先进的方法。&lt;br&gt;&lt;h4&gt;10. 代码可用性&lt;/h4&gt;    - 相关代码已公开，可在 GitHub 上获取。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-01-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1609/aaai.v38i5.28268&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/easyry/cra-pcn&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud completion is an indispensable task for recovering complete point
clouds due to incompleteness caused by occlusion, limited sensor resolution,
etc. The family of coarse-to-fine generation architectures has recently
exhibited great success in point cloud completion and gradually became
mainstream. In this work, we unveil one of the key ingredients behind these
methods: meticulously devised feature extraction operations with explicit
cross-resolution aggregation. We present Cross-Resolution Transformer that
efficiently performs cross-resolution aggregation with local attention
mechanisms. With the help of our recursive designs, the proposed operation can
capture more scales of features than common aggregation operations, which is
beneficial for capturing fine geometric characteristics. While prior
methodologies have ventured into various manifestations of inter-level
cross-resolution aggregation, the effectiveness of intra-level one and their
combination has not been analyzed. With unified designs, Cross-Resolution
Transformer can perform intra- or inter-level cross-resolution aggregation by
switching inputs. We integrate two forms of Cross-Resolution Transformers into
one up-sampling block for point generation, and following the coarse-to-fine
manner, we construct CRA-PCN to incrementally predict complete shapes with
stacked up-sampling blocks. Extensive experiments demonstrate that our method
outperforms state-of-the-art methods by a large margin on several widely used
benchmarks. Codes are available at https://github.com/EasyRy/CRA-PCN.</description>
      <guid isPermaLink="false">2401.01552v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Semi-Supervised Sparse Gaussian Classification: Provable Benefits of Unlabeled Data</title>
      <link>http://arxiv.org/abs/2409.03335v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 半监督学习的基本 premise&lt;/h4&gt;   - 半监督学习（SSL）的前提是结合标注和未标注数据可以产生更准确的模型。&lt;br&gt;&lt;h4&gt;2. 理论理解的不足&lt;/h4&gt;   - 尽管在实践中取得了成功，但对SSL的理论理解仍不完全。&lt;br&gt;&lt;h4&gt;3. 研究重点&lt;/h4&gt;   - 本文研究高维稀疏高斯分类中的SSL，重点在于构建准确的分类器。&lt;br&gt;&lt;h4&gt;4. 特征选择的关键任务&lt;/h4&gt;   - 特征选择是构建准确分类器的关键任务，旨在识别能区分两类样本的少数变量。&lt;br&gt;&lt;h4&gt;5. 信息论和计算下界分析&lt;/h4&gt;   - 分析了准确特征选择的信息论下界和计算下界，假设低度似然硬度猜想。&lt;br&gt;&lt;h4&gt;6. 确定有利的参数范围&lt;/h4&gt;   - 识别出在问题参数（维度、稀疏性、标注和未标注样本数量）下SSL有利于分类的特定范围。&lt;br&gt;&lt;h4&gt;7. 多项式时间构建分类器&lt;/h4&gt;   - 在特定范围内，可以在多项式时间内构建准确的SSL分类器。&lt;br&gt;&lt;h4&gt;8. 传统学习方法的局限性&lt;/h4&gt;   - 任何仅使用标注或未标注数据的计算有效的监督或无监督学习方案都将失败。&lt;br&gt;&lt;h4&gt;9. 结合数据的优势&lt;/h4&gt;   - 本文强调了在高维度中结合标注和未标注数据在分类和特征选择方面的可证明优势。&lt;br&gt;&lt;h4&gt;10. 模拟实验&lt;/h4&gt;    - 提供了补充理论分析的模拟实验结果。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The premise of semi-supervised learning (SSL) is that combining labeled and
unlabeled data yields significantly more accurate models. Despite empirical
successes, the theoretical understanding of SSL is still far from complete. In
this work, we study SSL for high dimensional sparse Gaussian classification. To
construct an accurate classifier a key task is feature selection, detecting the
few variables that separate the two classes. % For this SSL setting, we analyze
information theoretic lower bounds for accurate feature selection as well as
computational lower bounds, assuming the low-degree likelihood hardness
conjecture. % Our key contribution is the identification of a regime in the
problem parameters (dimension, sparsity, number of labeled and unlabeled
samples) where SSL is guaranteed to be advantageous for classification.
Specifically, there is a regime where it is possible to construct in polynomial
time an accurate SSL classifier. However, % any computationally efficient
supervised or unsupervised learning schemes, that separately use only the
labeled or unlabeled data would fail. Our work highlights the provable benefits
of combining labeled and unlabeled data for {classification and} feature
selection in high dimensions. We present simulations that complement our
theoretical analysis.</description>
      <guid isPermaLink="false">2409.03335v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Meta-Learn Unimodal Signals with Weak Supervision for Multimodal Sentiment Analysis</title>
      <link>http://arxiv.org/abs/2408.16029v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 多模态情感分析的目标&lt;/h4&gt;   - 多模态情感分析旨在有效整合来自不同来源的信息，以推断情感。&lt;br&gt;&lt;h4&gt;2. 标注缺失的问题&lt;/h4&gt;   - 在许多情况下，单一模态的标签没有注释，因此大多数工作依赖于多模态标签进行训练。&lt;br&gt;&lt;h4&gt;3. 噪声标签问题&lt;/h4&gt;   - 多模态注释并不总是理想的单模态标签替代品，这导致单模态信号的学习面临噪声标签问题，未能实现对各个模态的细致优化。&lt;br&gt;&lt;h4&gt;4. 探索弱监督学习&lt;/h4&gt;   - 本文探索在多模态标签的弱监督下学习单模态标签。&lt;br&gt;&lt;h4&gt;5. 提出的框架&lt;/h4&gt;   - 提出了一个新的元单标签生成（MUG）框架，以解决上述问题，利用可用的多模态标签学习相应的单模态标签。&lt;br&gt;&lt;h4&gt;6. 对比投影模块&lt;/h4&gt;   - 首先设计了一个基于对比的投影模块，以弥合单模态和多模态表示之间的差距，从而使用多模态注释指导MUCN的学习。&lt;br&gt;&lt;h4&gt;7. 去噪任务设计&lt;/h4&gt;   - 提出了单模态和多模态去噪任务，通过双层优化策略对MUCN进行显式监督训练。&lt;br&gt;&lt;h4&gt;8. 联合训练任务&lt;/h4&gt;   - 共同训练单模态和多模态学习任务，以提取用于多模态推断的区分性单模态特征。&lt;br&gt;&lt;h4&gt;9. 实验结果&lt;/h4&gt;   - 实验结果表明，MUG框架优于竞争基线，能够学习准确的单模态标签。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal sentiment analysis aims to effectively integrate information from
various sources to infer sentiment, where in many cases there are no
annotations for unimodal labels. Therefore, most works rely on multimodal
labels for training. However, there exists the noisy label problem for the
learning of unimodal signals as multimodal annotations are not always the ideal
substitutes for the unimodal ones, failing to achieve finer optimization for
individual modalities. In this paper, we explore the learning of unimodal
labels under the weak supervision from the annotated multimodal labels.
Specifically, we propose a novel meta uni-label generation (MUG) framework to
address the above problem, which leverages the available multimodal labels to
learn the corresponding unimodal labels by the meta uni-label correction
network (MUCN). We first design a contrastive-based projection module to bridge
the gap between unimodal and multimodal representations, so as to use
multimodal annotations to guide the learning of MUCN. Afterwards, we propose
unimodal and multimodal denoising tasks to train MUCN with explicit supervision
via a bi-level optimization strategy. We then jointly train unimodal and
multimodal learning tasks to extract discriminative unimodal features for
multimodal inference. Experimental results suggest that MUG outperforms
competitive baselines and can learn accurate unimodal labels.</description>
      <guid isPermaLink="false">2408.16029v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>DexSim2Real$^{2}$: Building Explicit World Model for Precise Articulated Object Dexterous Manipulation</title>
      <link>http://arxiv.org/abs/2409.08750v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Webpage: https://jiangtaoran.github.io/dexsim2real2_website/.
  arXiv admin note: text overlap with arXiv:2302.10693&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 日常生活中的重要性&lt;/h4&gt;   - 关节物体操控在日常生活中非常普遍。&lt;br&gt;&lt;h4&gt;2. 提出的框架&lt;/h4&gt;   - 本文介绍了DexSim2Real²，一个新的机器人学习框架，旨在实现目标导向的关节物体操控，适用于两指夹持器和多指灵巧手。&lt;br&gt;&lt;h4&gt;3. 显式世界模型的构建&lt;/h4&gt;   - 框架的关键是通过主动的一步交互构建未见关节物体的显式世界模型。&lt;br&gt;&lt;h4&gt;4. 采样基础的模型预测控制&lt;/h4&gt;   - 该显式世界模型使得基于采样的模型预测控制成为可能，能够在无需人类示范或强化学习的情况下规划实现不同操控目标的轨迹。&lt;br&gt;&lt;h4&gt;5. 交互运动预测&lt;/h4&gt;   - 首先，使用在自监督交互数据或互联网人类操控视频上训练的可供性估计网络来预测交互动作。&lt;br&gt;&lt;h4&gt;6. 数字双胞胎的构建&lt;/h4&gt;   - 在真实机器人上执行交互后，框架基于交互前后的两个点云构建关节物体的数字双胞胎。&lt;br&gt;&lt;h4&gt;7. 高维动作空间的处理&lt;/h4&gt;   - 对于灵巧的多指操控，提出利用特征抓握（eigengrasp）来减少高维动作空间，从而提高轨迹搜索的效率。&lt;br&gt;&lt;h4&gt;8. 实验验证&lt;/h4&gt;   - 大量实验验证了该框架在使用两指夹持器和16自由度灵巧手进行精确关节物体操控的有效性。&lt;br&gt;&lt;h4&gt;9. 显式世界模型的泛化能力&lt;/h4&gt;   - 显式世界模型的强鲁棒性还支持更高级的操控策略，例如使用不同工具进行操控。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Articulated object manipulation is ubiquitous in daily life. In this paper,
we present DexSim2Real$^{2}$, a novel robot learning framework for
goal-conditioned articulated object manipulation using both two-finger grippers
and multi-finger dexterous hands. The key of our framework is constructing an
explicit world model of unseen articulated objects through active one-step
interactions. This explicit world model enables sampling-based model predictive
control to plan trajectories achieving different manipulation goals without
needing human demonstrations or reinforcement learning. It first predicts an
interaction motion using an affordance estimation network trained on
self-supervised interaction data or videos of human manipulation from the
internet. After executing this interaction on the real robot, the framework
constructs a digital twin of the articulated object in simulation based on the
two point clouds before and after the interaction. For dexterous multi-finger
manipulation, we propose to utilize eigengrasp to reduce the high-dimensional
action space, enabling more efficient trajectory searching. Extensive
experiments validate the framework's effectiveness for precise articulated
object manipulation in both simulation and the real world using a two-finger
gripper and a 16-DoF dexterous hand. The robust generalizability of the
explicit world model also enables advanced manipulation strategies, such as
manipulating with different tools.</description>
      <guid isPermaLink="false">2409.08750v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Spoofing-Aware Speaker Verification Robust Against Domain and Channel Mismatches</title>
      <link>http://arxiv.org/abs/2409.06327v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To appear in 2024 IEEE Spoken Language Technology Workshop, Dec
  02-05, 2024, Macao, China&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 现实应用中的挑战&lt;/h4&gt;   - 建立一个能够抵御常见威胁（如欺骗攻击、信道不匹配和领域不匹配）的说话人验证系统是一个挑战。&lt;br&gt;&lt;h4&gt;2. 传统ASV系统的局限性&lt;/h4&gt;   - 传统的自动说话人验证（ASV）系统通常分别处理这些问题，导致在面临多重挑战时表现不佳。&lt;br&gt;&lt;h4&gt;3. 提出的综合框架&lt;/h4&gt;   - 本文提出一个综合框架，将成对学习和欺骗攻击模拟纳入元学习范式，以增强对多方面威胁的鲁棒性。&lt;br&gt;&lt;h4&gt;4. 模型设计&lt;/h4&gt;   - 采用不对称双路径模型和多任务学习策略，同时处理ASV、反欺骗和欺骗感知ASV任务。&lt;br&gt;&lt;h4&gt;5. 新测试数据集的引入&lt;/h4&gt;   - 引入新的测试数据集CNComplex，以评估系统在这些组合威胁下的表现。&lt;br&gt;&lt;h4&gt;6. 实验结果&lt;/h4&gt;   - 实验结果表明，所提综合模型在各种场景下显著改善了传统ASV系统的性能，展示了其在现实应用中的潜力。&lt;br&gt;&lt;h4&gt;7. 框架的泛化能力&lt;/h4&gt;   - 提出的框架在不同条件下的泛化能力突显了其鲁棒性和可靠性，成为实际ASV应用的有希望方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In real-world applications, it is challenging to build a speaker verification
system that is simultaneously robust against common threats, including spoofing
attacks, channel mismatch, and domain mismatch. Traditional automatic speaker
verification (ASV) systems often tackle these issues separately, leading to
suboptimal performance when faced with simultaneous challenges. In this paper,
we propose an integrated framework that incorporates pair-wise learning and
spoofing attack simulation into the meta-learning paradigm to enhance
robustness against these multifaceted threats. This novel approach employs an
asymmetric dual-path model and a multi-task learning strategy to handle ASV,
anti-spoofing, and spoofing-aware ASV tasks concurrently. A new testing
dataset, CNComplex, is introduced to evaluate system performance under these
combined threats. Experimental results demonstrate that our integrated model
significantly improves performance over traditional ASV systems across various
scenarios, showcasing its potential for real-world deployment. Additionally,
the proposed framework's ability to generalize across different conditions
highlights its robustness and reliability, making it a promising solution for
practical ASV applications.</description>
      <guid isPermaLink="false">2409.06327v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Self-supervised Learning for Acoustic Few-Shot Classification</title>
      <link>http://arxiv.org/abs/2409.09647v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 标注数据的限制&lt;/h4&gt;   - 标注数据有限，自监督学习是减少标注需求的重要方法。&lt;br&gt;&lt;h4&gt;2. 领域关注差异&lt;/h4&gt;   - 自监督学习在图像领域已被广泛探索，但在声学领域关注较少。&lt;br&gt;&lt;h4&gt;3. 声学应用的需求&lt;/h4&gt;   - 在许多声学应用中，减少标注是关键要求，尤其在生物声学中，几乎没有足够的标签可用于完全监督学习。&lt;br&gt;&lt;h4&gt;4. 现有方法的局限性&lt;/h4&gt;   - 目前普遍使用的声学识别器通常在无关数据上进行预训练，然后用于生物声学任务。&lt;br&gt;&lt;h4&gt;5. 提出的新方法&lt;/h4&gt;   - 本文提出了一种新的方法，即在实际任务数据上训练，并结合自监督预训练与少量样本分类，以提高准确性。&lt;br&gt;&lt;h4&gt;6. 新架构的介绍&lt;/h4&gt;   - 介绍并评估一种结合基于CNN的预处理和基于状态空间模型（SSMs）特征提取的新架构。&lt;br&gt;&lt;h4&gt;7. 架构选择的动机&lt;/h4&gt;   - 仅使用CNN的网络在有效捕捉时间信息方面存在困难，而时间信息对于声学信号分类至关重要。&lt;br&gt;&lt;h4&gt;8. SSMs的优势&lt;/h4&gt;   - 状态空间模型（如S4和Mamba）在捕捉序列数据中的长程依赖性方面表现优异。&lt;br&gt;&lt;h4&gt;9. 预训练与微调&lt;/h4&gt;   - 使用对比学习在实际任务数据上进行预训练，然后用极少量标注数据进行微调。&lt;br&gt;&lt;h4&gt;10. 性能评估&lt;/h4&gt;    - 在标准基准和真实世界数据上评估该架构在（$n$-shot，$n$-class）分类上的表现。&lt;br&gt;&lt;h4&gt;11. 结果表现&lt;/h4&gt;    - 评估结果表明，该方法在少样本分类问题上优于现有的最先进架构。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Labelled data are limited and self-supervised learning is one of the most
important approaches for reducing labelling requirements. While it has been
extensively explored in the image domain, it has so far not received the same
amount of attention in the acoustic domain. Yet, reducing labelling is a key
requirement for many acoustic applications. Specifically in bioacoustic, there
are rarely sufficient labels for fully supervised learning available. This has
led to the widespread use of acoustic recognisers that have been pre-trained on
unrelated data for bioacoustic tasks. We posit that training on the actual task
data and combining self-supervised pre-training with few-shot classification is
a superior approach that has the ability to deliver high accuracy even when
only a few labels are available. To this end, we introduce and evaluate a new
architecture that combines CNN-based preprocessing with feature extraction
based on state space models (SSMs). This combination is motivated by the fact
that CNN-based networks alone struggle to capture temporal information
effectively, which is crucial for classifying acoustic signals. SSMs,
specifically S4 and Mamba, on the other hand, have been shown to have an
excellent ability to capture long-range dependencies in sequence data. We
pre-train this architecture using contrastive learning on the actual task data
and subsequent fine-tuning with an extremely small amount of labelled data. We
evaluate the performance of this proposed architecture for ($n$-shot,
$n$-class) classification on standard benchmarks as well as real-world data.
Our evaluation shows that it outperforms state-of-the-art architectures on the
few-shot classification problem.</description>
      <guid isPermaLink="false">2409.09647v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>SGFormer: Single-Layer Graph Transformers with Approximation-Free Linear Complexity</title>
      <link>http://arxiv.org/abs/2409.09007v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Extended version of NeurIPS2023 contribution arXiv:2306.10759&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 大图学习的挑战&lt;/h4&gt;   - 在大型图上学习表示是一项长期存在的挑战，主要由于图的相互依赖性。&lt;br&gt;&lt;h4&gt;2. Transformer的优势&lt;/h4&gt;   - Transformer在小型图上表现良好，因其全球注意力机制能够捕捉所有对之间的交互，超越观察到的结构。&lt;br&gt;&lt;h4&gt;3. 现有方法的复杂性&lt;/h4&gt;   - 许多现有方法借鉴了Transformer在语言和视觉任务中的精神，通过堆叠深层注意力传播层来构建复杂架构。&lt;br&gt;&lt;h4&gt;4. 研究的目的&lt;/h4&gt;   - 本文旨在评估在图上采用多层注意力的必要性，指出这限制了效率。&lt;br&gt;&lt;h4&gt;5. 混合传播层的分析&lt;/h4&gt;   - 分析一种通用的混合传播层，包括全对注意力和基于图的传播，表明多层传播可以简化为单层传播，并具备相同的表示学习能力。&lt;br&gt;&lt;h4&gt;6. 技术路径的建议&lt;/h4&gt;   - 提出了一种新技术路径，通过简化模型架构而不牺牲表达能力，构建强大且高效的图上Transformer。&lt;br&gt;&lt;h4&gt;7. 提出的新模型&lt;/h4&gt;   - 提出简化单层图Transformer (SGFormer)，其主要组件是单层全球注意力，能够线性扩展至图的大小，无需任何近似以适应全对交互。&lt;br&gt;&lt;h4&gt;8. 实验结果&lt;/h4&gt;   - SGFormer在web规模图ogbn-papers100M上成功扩展，相较于同类Transformer在中型图上实现了数量级的推理加速，并在有限标记数据上表现出竞争力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/qitianwu/sgformer&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning representations on large graphs is a long-standing challenge due to
the inter-dependence nature. Transformers recently have shown promising
performance on small graphs thanks to its global attention for capturing
all-pair interactions beyond observed structures. Existing approaches tend to
inherit the spirit of Transformers in language and vision tasks, and embrace
complicated architectures by stacking deep attention-based propagation layers.
In this paper, we attempt to evaluate the necessity of adopting multi-layer
attentions in Transformers on graphs, which considerably restricts the
efficiency. Specifically, we analyze a generic hybrid propagation layer,
comprised of all-pair attention and graph-based propagation, and show that
multi-layer propagation can be reduced to one-layer propagation, with the same
capability for representation learning. It suggests a new technical path for
building powerful and efficient Transformers on graphs, particularly through
simplifying model architectures without sacrificing expressiveness. As
exemplified by this work, we propose a Simplified Single-layer Graph
Transformers (SGFormer), whose main component is a single-layer global
attention that scales linearly w.r.t. graph sizes and requires none of any
approximation for accommodating all-pair interactions. Empirically, SGFormer
successfully scales to the web-scale graph ogbn-papers100M, yielding
orders-of-magnitude inference acceleration over peer Transformers on
medium-sized graphs, and demonstrates competitiveness with limited labeled
data.</description>
      <guid isPermaLink="false">2409.09007v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Learning Dense Features for Point Cloud Registration Using a Graph Attention Network</title>
      <link>http://arxiv.org/abs/2206.06731v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 3 figures&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 点云配准的基础任务&lt;/h4&gt;   - 点云配准在定位、地图制作、跟踪和重建等多个应用中是基本任务。&lt;br&gt;&lt;h4&gt;2. 成功配准的关键&lt;/h4&gt;   - 成功的配准依赖于提取稳健且具有区分性的几何特征。&lt;br&gt;&lt;h4&gt;3. 现有方法的计算需求&lt;/h4&gt;   - 尽管现有基于学习的方法需要高计算能力来同时处理大量原始点，但由于GPU的强大并行计算能力，计算限制不再是问题。&lt;br&gt;&lt;h4&gt;4. 提出的框架&lt;/h4&gt;   - 本文介绍了一种高效、经济的框架，使用图注意力网络（Graph Attention Network）进行点云匹配和配准，称为DFGAT。&lt;br&gt;&lt;h4&gt;5. 检测器功能&lt;/h4&gt;   - DFGAT的检测器负责在大型原始数据集中找到高度可靠的关键点。&lt;br&gt;&lt;h4&gt;6. 描述符功能&lt;/h4&gt;   - DFGAT的描述符结合这些关键点及其邻域，提取不变密度特征，为匹配做准备。&lt;br&gt;&lt;h4&gt;7. 图注意力网络的优势&lt;/h4&gt;   - 图注意力网络利用注意力机制增强点云之间的关系。&lt;br&gt;&lt;h4&gt;8. 最优传输问题的考虑&lt;/h4&gt;   - 最后，将此视为最优传输问题，并使用Sinkhorn算法找到正负匹配。&lt;br&gt;&lt;h4&gt;9. 实验验证&lt;/h4&gt;   - 在KITTI数据集上进行了全面测试，评估该方法的有效性。&lt;br&gt;&lt;h4&gt;10. 结果表现&lt;/h4&gt;    - 实验结果表明，该方法通过高效紧凑的关键点选择和描述，能够实现最佳的匹配指标，并在与其他最先进方法比较中，达到99.88%的最高配准成功率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2022-06-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.3390/app12147023&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud registration is a fundamental task in many applications such as
localization, mapping, tracking, and reconstruction. Successful registration
relies on extracting robust and discriminative geometric features. Though
existing learning based methods require high computing capacity for processing
a large number of raw points at the same time, computational capacity
limitation is not an issue thanks to the powerful parallel computing process
using GPU. In this paper, we introduce a framework that efficiently and
economically extracts dense features using a graph attention network for point
cloud matching and registration (DFGAT). The detector of the DFGAT is
responsible for finding highly reliable key points in large raw data sets. The
descriptor of the DFGAT takes these keypoints combined with their neighbors to
extract invariant density features in preparation for the matching. The graph
attention network (GAT) uses the attention mechanism that enriches the
relationships between point clouds. Finally, we consider this as an optimal
transport problem and use the Sinkhorn algorithm to find positive and negative
matches. We perform thorough tests on the KITTI dataset and evaluate the
effectiveness of this approach. The results show that this method with the
efficiently compact keypoint selection and description can achieve the best
performance matching metrics and reach the highest success ratio of 99.88%
registration in comparison with other state of the art approaches.</description>
      <guid isPermaLink="false">2206.06731v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Twin Deformable Point Convolutions for Point Cloud Semantic Segmentation in Remote Sensing Scenes</title>
      <link>http://arxiv.org/abs/2405.19735v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 深度学习在点云处理中的应用&lt;/h4&gt;   - 深度学习技术在遥感领域的点云处理中的应用，导致点云分割成为近年来的研究热点。&lt;br&gt;&lt;h4&gt;2. 应用领域&lt;/h4&gt;   - 点云分割可应用于现实世界的3D建模、智能城市等多个领域。&lt;br&gt;&lt;h4&gt;3. 现有解决方案的局限性&lt;/h4&gt;   - 尽管现有方法取得了显著进展，但忽略了遥感领域中点云按照经纬度和高度严格排列的固有特性。&lt;br&gt;&lt;h4&gt;4. 提出的新方法&lt;/h4&gt;   - 为考虑这一特性，提出了称为双形变点卷积（Twin Deformable point Convolutions，TDConvs）的新型卷积算子。&lt;br&gt;&lt;h4&gt;5. 自适应特征学习&lt;/h4&gt;   - TDConvs旨在通过分别在经纬度平面和高度方向上学习可变采样点，实现自适应特征学习。&lt;br&gt;&lt;h4&gt;6. 经纬度平面的建模&lt;/h4&gt;   - 提出了圆柱形可变点卷积（Cylinder-wise Deformable point Convolution，CyDConv）算子，通过在经纬度方向构建圆柱状网格生成二维圆柱图。&lt;br&gt;&lt;h4&gt;7. 特征融合&lt;/h4&gt;   - 进行了多尺度融合，将提取的经纬度特征与空间几何特征结合，通过不同尺度的相邻点特征聚合实现。&lt;br&gt;&lt;h4&gt;8. 高度方向的建模&lt;/h4&gt;   - 引入球形可变点卷积（Sphere-wise Deformable point Convolution，SpDConv）算子，通过构建球形网格结构自适应偏移三维空间中的采样点，以建模高度方向的特性。&lt;br&gt;&lt;h4&gt;9. 实验结果&lt;/h4&gt;   - 在现有流行基准测试上的实验表明，TDConvs在分割性能上超过了现有的最先进方法，达到了最佳效果。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-05-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Thanks to the application of deep learning technology in point cloud
processing of the remote sensing field, point cloud segmentation has become a
research hotspot in recent years, which can be applied to real-world 3D, smart
cities, and other fields. Although existing solutions have made unprecedented
progress, they ignore the inherent characteristics of point clouds in remote
sensing fields that are strictly arranged according to latitude, longitude, and
altitude, which brings great convenience to the segmentation of point clouds in
remote sensing fields. To consider this property cleverly, we propose novel
convolution operators, termed Twin Deformable point Convolutions (TDConvs),
which aim to achieve adaptive feature learning by learning deformable sampling
points in the latitude-longitude plane and altitude direction, respectively.
First, to model the characteristics of the latitude-longitude plane, we propose
a Cylinder-wise Deformable point Convolution (CyDConv) operator, which
generates a two-dimensional cylinder map by constructing a cylinder-like grid
in the latitude-longitude direction. Furthermore, to better integrate the
features of the latitude-longitude plane and the spatial geometric features, we
perform a multi-scale fusion of the extracted latitude-longitude features and
spatial geometric features, and realize it through the aggregation of adjacent
point features of different scales. In addition, a Sphere-wise Deformable point
Convolution (SpDConv) operator is introduced to adaptively offset the sampling
points in three-dimensional space by constructing a sphere grid structure,
aiming at modeling the characteristics in the altitude direction. Experiments
on existing popular benchmarks conclude that our TDConvs achieve the best
segmentation performance, surpassing the existing state-of-the-art methods.</description>
      <guid isPermaLink="false">2405.19735v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>A Review on Semi-Supervised Relation Extraction</title>
      <link>http://arxiv.org/abs/2103.07575v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 关系抽取的重要性&lt;/h4&gt;   - 关系抽取（RE）在从非结构化文本中提取知识方面发挥着重要作用，但需要大量标注语料。&lt;br&gt;&lt;h4&gt;2. 半监督学习的目标&lt;/h4&gt;   - 为减少昂贵的标注工作，半监督学习旨在利用标注和未标注的数据。&lt;br&gt;&lt;h4&gt;3. 三种典型的半监督RE方法&lt;/h4&gt;   - **自我集成（Self-Ensembling）**：&lt;br&gt;     - 通过强制在扰动下保持一致性，但可能面临监督不足的问题。&lt;br&gt;   &lt;br&gt;   - **自我训练（Self-Training）**：&lt;br&gt;     - 迭代生成伪标签，并使用扩展后的标注集进行自我重训练。&lt;br&gt;   - **双重学习（Dual Learning）**：&lt;br&gt;     - 利用主任务和副任务之间的相互反馈。&lt;br&gt;&lt;h4&gt;4. 具体方法的代表性&lt;/h4&gt;   - **Mean-Teacher（Tarvainen and Valpola, 2017）**：&lt;br&gt;     - 代表自我集成方法，旨在缓解其弱点。&lt;br&gt;   - **LST（Li et al., 2019）**：&lt;br&gt;     - 代表自我训练方法，旨在改善其不足之处。&lt;br&gt;   - **DualRE（Lin et al., 2019）**：&lt;br&gt;     - 代表双重学习方法，旨在克服其局限性。&lt;br&gt;&lt;h4&gt;5. 方法比较与探讨&lt;/h4&gt;   - 本文回顾并比较这三种方法，分析它们在半监督关系抽取中的应用与效果。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2021-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Relation extraction (RE) plays an important role in extracting knowledge from
unstructured text but requires a large amount of labeled corpus. To reduce the
expensive annotation efforts, semisupervised learning aims to leverage both
labeled and unlabeled data. In this paper, we review and compare three typical
methods in semi-supervised RE with deep learning or meta-learning:
self-ensembling, which forces consistent under perturbations but may confront
insufficient supervision; self-training, which iteratively generates pseudo
labels and retrain itself with the enlarged labeled set; dual learning, which
leverages a primal task and a dual task to give mutual feedback. Mean-teacher
(Tarvainen and Valpola, 2017), LST (Li et al., 2019), and DualRE (Lin et al.,
2019) are elaborated as the representatives to alleviate the weakness of these
three methods, respectively.</description>
      <guid isPermaLink="false">2103.07575v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Contrasformer: A Brain Network Contrastive Transformer for Neurodegenerative Condition Identification</title>
      <link>http://arxiv.org/abs/2409.10944v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 神经疾病理解的重要性&lt;/h4&gt;   - 理解神经疾病是神经科学中的基本问题，通常需要分析来自功能性磁共振成像（fMRI）数据的脑网络。&lt;br&gt;&lt;h4&gt;2. 现有技术的挑战&lt;/h4&gt;   - 尽管图神经网络（GNNs）和图变换器在多个领域得到了广泛应用，但在脑网络中的应用面临挑战，主要是由于：&lt;br&gt;     - 数据集受到来自子群体的分布变化引起的噪声影响。&lt;br&gt;     - 忽视节点身份的影响，阻碍了疾病特定模式的识别。&lt;br&gt;&lt;h4&gt;3. 提出的解决方案&lt;/h4&gt;   - 提出了Contrasformer，一种新颖的对比脑网络变换器，旨在解决上述挑战。&lt;br&gt;   - 通过双流注意机制生成增强先验知识的对比图，以应对子群体之间的分布变化。&lt;br&gt;&lt;h4&gt;4. 节点身份的强调&lt;/h4&gt;   - 采用带身份嵌入的交叉注意机制，突出节点的身份信息。&lt;br&gt;&lt;h4&gt;5. 确保组一致性的辅助损失&lt;/h4&gt;   - 引入三种辅助损失以确保组的一致性，增强模型的鲁棒性。&lt;br&gt;&lt;h4&gt;6. 性能评估&lt;/h4&gt;   - 在4个功能性脑网络数据集上评估，涵盖4种不同疾病，Contrasformer在准确性上超越了现有的最先进方法，提升幅度可达10.8%。&lt;br&gt;&lt;h4&gt;7. 解释性案例研究&lt;/h4&gt;   - 案例研究展示了其解释性，尤其在神经科学的背景下，提供了对结果的深入理解。&lt;br&gt;&lt;h4&gt;8. 研究贡献&lt;/h4&gt;   - 本文为分析脑网络提供了解决方案，为理解神经疾病提供了有价值的见解。&lt;br&gt;&lt;h4&gt;9. 代码可用性&lt;/h4&gt;   - 相关代码可在GitHub上获取：[Contrasformer](https://github.com/AngusMonroe/Contrasformer)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/angusmonroe/contrasformer&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding neurological disorder is a fundamental problem in neuroscience,
which often requires the analysis of brain networks derived from functional
magnetic resonance imaging (fMRI) data. Despite the prevalence of Graph Neural
Networks (GNNs) and Graph Transformers in various domains, applying them to
brain networks faces challenges. Specifically, the datasets are severely
impacted by the noises caused by distribution shifts across sub-populations and
the neglect of node identities, both obstruct the identification of
disease-specific patterns. To tackle these challenges, we propose
Contrasformer, a novel contrastive brain network Transformer. It generates a
prior-knowledge-enhanced contrast graph to address the distribution shifts
across sub-populations by a two-stream attention mechanism. A cross attention
with identity embedding highlights the identity of nodes, and three auxiliary
losses ensure group consistency. Evaluated on 4 functional brain network
datasets over 4 different diseases, Contrasformer outperforms the
state-of-the-art methods for brain networks by achieving up to 10.8\%
improvement in accuracy, which demonstrates its efficacy in neurological
disorder identification. Case studies illustrate its interpretability,
especially in the context of neuroscience. This paper provides a solution for
analyzing brain networks, offering valuable insights into neurological
disorders. Our code is available at
\url{https://github.com/AngusMonroe/Contrasformer}.</description>
      <guid isPermaLink="false">2409.10944v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Open-Set Semantic Uncertainty Aware Metric-Semantic Graph Matching</title>
      <link>http://arxiv.org/abs/2409.11555v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 水下物体级映射的需求&lt;/h4&gt;   - 水下物体级映射需要结合视觉基础模型，以处理海洋场景中遇到的不常见和往往是之前未见的物体类别。&lt;br&gt;&lt;h4&gt;2. 语义不确定性的度量&lt;/h4&gt;   - 本研究计算了由视觉基础模型生成的开放集物体检测的语义不确定性，并将其纳入物体级不确定性跟踪框架中。&lt;br&gt;&lt;h4&gt;3. 物体级不确定性与几何关系&lt;/h4&gt;   - 利用物体级的不确定性和物体之间的几何关系，实现对未知物体类别的稳健物体级环闭检测。&lt;br&gt;&lt;h4&gt;4. 环闭检测问题的建模&lt;/h4&gt;   - 将上述环闭检测问题建模为图匹配问题。&lt;br&gt;&lt;h4&gt;5. 图匹配的复杂性&lt;/h4&gt;   - 虽然图匹配一般是NP完全问题，但提出的图匹配问题的等效形式作为图编辑问题进行了求解。&lt;br&gt;&lt;h4&gt;6. 求解器的测试&lt;/h4&gt;   - 在多个具有挑战性的水下场景中测试了该求解器以及其他三种求解器，结果表明所提出的方法在海洋环境中实时使用是可行的。&lt;br&gt;&lt;h4&gt;7. 实验结果的有效性&lt;/h4&gt;   - 实验结果显示，该方法在稳健的开放集、多物体和语义不确定性感知的环闭检测中表现良好。&lt;br&gt;&lt;h4&gt;8. 方法的通用性&lt;/h4&gt;   - 在KITTI数据集上的进一步实验结果表明，该方法能够推广到大规模的地面场景。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Underwater object-level mapping requires incorporating visual foundation
models to handle the uncommon and often previously unseen object classes
encountered in marine scenarios. In this work, a metric of semantic uncertainty
for open-set object detections produced by visual foundation models is
calculated and then incorporated into an object-level uncertainty tracking
framework. Object-level uncertainties and geometric relationships between
objects are used to enable robust object-level loop closure detection for
unknown object classes. The above loop closure detection problem is formulated
as a graph-matching problem. While graph matching, in general, is NP-Complete,
a solver for an equivalent formulation of the proposed graph matching problem
as a graph editing problem is tested on multiple challenging underwater scenes.
Results for this solver as well as three other solvers demonstrate that the
proposed methods are feasible for real-time use in marine environments for the
robust, open-set, multi-object, semantic-uncertainty-aware loop closure
detection. Further experimental results on the KITTI dataset demonstrate that
the method generalizes to large-scale terrestrial scenes.</description>
      <guid isPermaLink="false">2409.11555v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Exploring Point-BEV Fusion for 3D Point Cloud Object Tracking with Transformer</title>
      <link>http://arxiv.org/abs/2208.05216v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  arXiv admin note: substantial text overlap with arXiv:2112.02857&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. LiDAR传感器的普及&lt;/h4&gt;   - 随着LiDAR传感器在自动驾驶中的广泛应用，3D物体跟踪受到了越来越多的关注。&lt;br&gt;&lt;h4&gt;2. 3D物体跟踪的目标&lt;/h4&gt;   - 在点云序列中，3D物体跟踪的目标是根据给定的物体模板预测物体在连续帧中的位置和方向。&lt;br&gt;&lt;h4&gt;3. 引入Transformers的动机&lt;/h4&gt;   - 受到Transformers成功的启发，提出了Point Tracking TRansformer (PTTR)，旨在以粗到细的方式高效预测高质量的3D跟踪结果。&lt;br&gt;&lt;h4&gt;4. PTTR的三项新设计&lt;/h4&gt;   - **关系感知采样**：设计了一种关系感知采样方法，以在下采样过程中保留与给定模板相关的点，而非随机采样。&lt;br&gt;   - **点关系变换器**：提出了点关系变换器，用于在模板和搜索区域之间有效聚合和匹配特征。&lt;br&gt;   - **预测精炼模块**：基于粗略跟踪结果，采用一种新颖的预测精炼模块，通过局部特征池化获得最终的精炼预测。&lt;br&gt;&lt;h4&gt;5. Bird's-Eye View (BEV)的优势&lt;/h4&gt;   - 鉴于鸟瞰图在捕捉物体运动方面的有利特性，进一步设计了一个更先进的框架PTTR++，结合点云的点视图和BEV表示，以利用它们在生成高质量跟踪结果中的互补效果。&lt;br&gt;&lt;h4&gt;6. PTTR++的性能提升&lt;/h4&gt;   - PTTR++在PTTR的基础上显著提升了跟踪性能，同时保持较低的计算开销。&lt;br&gt;&lt;h4&gt;7. 实验验证&lt;/h4&gt;   - 在多个数据集上进行了广泛实验，结果表明所提出的方法在3D跟踪精度和效率上表现优越。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2022-08-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/jasonkks/pttr&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the prevalence of LiDAR sensors in autonomous driving, 3D object
tracking has received increasing attention. In a point cloud sequence, 3D
object tracking aims to predict the location and orientation of an object in
consecutive frames given an object template. Motivated by the success of
transformers, we propose Point Tracking TRansformer (PTTR), which efficiently
predicts high-quality 3D tracking results in a coarse-to-fine manner with the
help of transformer operations. PTTR consists of three novel designs. 1)
Instead of random sampling, we design Relation-Aware Sampling to preserve
relevant points to the given template during subsampling. 2) We propose a Point
Relation Transformer for effective feature aggregation and feature matching
between the template and search region. 3) Based on the coarse tracking
results, we employ a novel Prediction Refinement Module to obtain the final
refined prediction through local feature pooling. In addition, motivated by the
favorable properties of the Bird's-Eye View (BEV) of point clouds in capturing
object motion, we further design a more advanced framework named PTTR++, which
incorporates both the point-wise view and BEV representation to exploit their
complementary effect in generating high-quality tracking results. PTTR++
substantially boosts the tracking performance on top of PTTR with low
computational overhead. Extensive experiments over multiple datasets show that
our proposed approaches achieve superior 3D tracking accuracy and efficiency.</description>
      <guid isPermaLink="false">2208.05216v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Future Does Matter: Boosting 3D Object Detection with Temporal Motion Estimation in Point Cloud Sequences</title>
      <link>http://arxiv.org/abs/2409.04390v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. LiDAR 3D物体检测的重要性&lt;/h4&gt;   - 精确和稳健的LiDAR 3D物体检测对于自动驾驶中的全面场景理解至关重要。&lt;br&gt;&lt;h4&gt;2. 检测性能的限制&lt;/h4&gt;   - LiDAR检测性能受到点云数据固有限制的影响，尤其是在远距离和遮挡条件下。&lt;br&gt;&lt;h4&gt;3. 时间聚合的优势&lt;/h4&gt;   - 最近的研究表明，时间聚合通过融合多帧视角信息，显著提高了检测精度，丰富了物体的空间表示。&lt;br&gt;&lt;h4&gt;4. 新框架的提出&lt;/h4&gt;   - 本研究介绍了一种新的LiDAR 3D物体检测框架——LiSTM，旨在促进空间-时间特征学习，并结合跨帧运动预测信息。&lt;br&gt;&lt;h4&gt;5. 动态先验的引入&lt;/h4&gt;   - 通过引入由非学习型运动估计模型生成的动态先验，提升LiDAR检测器的空间-时间解释能力。&lt;br&gt;&lt;h4&gt;6. 运动引导特征聚合（MGFA）&lt;/h4&gt;   - 提出了MGFA方法，利用来自过去和未来运动状态的物体轨迹，将空间-时间相关性建模为驾驶序列上的高斯热图。&lt;br&gt;&lt;h4&gt;7. 基于运动的热图&lt;/h4&gt;   - 该运动基础的热图引导时间特征融合，丰富所提取的物体特征。&lt;br&gt;&lt;h4&gt;8. 双重相关性加权模块（DCWM）&lt;/h4&gt;   - 设计了DCWM，有效促进过去和未来帧之间的交互，通过场景和通道的特征抽象。&lt;br&gt;&lt;h4&gt;9. 解码器的使用&lt;/h4&gt;   - 采用基于级联交叉注意力的解码器来细化3D预测结果。&lt;br&gt;&lt;h4&gt;10. 实验验证&lt;/h4&gt;    - 在Waymo和nuScenes数据集上进行了实验，证明所提出框架在有效空间-时间特征学习方面实现了卓越的3D检测性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate and robust LiDAR 3D object detection is essential for comprehensive
scene understanding in autonomous driving. Despite its importance, LiDAR
detection performance is limited by inherent constraints of point cloud data,
particularly under conditions of extended distances and occlusions. Recently,
temporal aggregation has been proven to significantly enhance detection
accuracy by fusing multi-frame viewpoint information and enriching the spatial
representation of objects. In this work, we introduce a novel LiDAR 3D object
detection framework, namely LiSTM, to facilitate spatial-temporal feature
learning with cross-frame motion forecasting information. We aim to improve the
spatial-temporal interpretation capabilities of the LiDAR detector by
incorporating a dynamic prior, generated from a non-learnable motion estimation
model. Specifically, Motion-Guided Feature Aggregation (MGFA) is proposed to
utilize the object trajectory from previous and future motion states to model
spatial-temporal correlations into gaussian heatmap over a driving sequence.
This motion-based heatmap then guides the temporal feature fusion, enriching
the proposed object features. Moreover, we design a Dual Correlation Weighting
Module (DCWM) that effectively facilitates the interaction between past and
prospective frames through scene- and channel-wise feature abstraction. In the
end, a cascade cross-attention-based decoder is employed to refine the 3D
prediction. We have conducted experiments on the Waymo and nuScenes datasets to
demonstrate that the proposed framework achieves superior 3D detection
performance with effective spatial-temporal feature learning.</description>
      <guid isPermaLink="false">2409.04390v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Snail-Radar: A large-scale diverse dataset for the evaluation of 4D-radar-based SLAM systems</title>
      <link>http://arxiv.org/abs/2407.11705v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 4 figures, 5 tables&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 4D雷达的优势&lt;/h4&gt;   - 4D雷达因其在恶劣天气和动态环境中的鲁棒性，越来越受到自动驾驶系统的青睐，适用于里程计和地图构建。&lt;br&gt;&lt;h4&gt;2. 现有数据集的局限性&lt;/h4&gt;   - 现有数据集通常覆盖区域有限，且通常只使用单一平台进行采集。&lt;br&gt;&lt;h4&gt;3. 新数据集的提出&lt;/h4&gt;   - 本文介绍了一个专门为4D雷达定位和地图构建设计的大规模多样化数据集。&lt;br&gt;&lt;h4&gt;4. 数据采集平台&lt;/h4&gt;   - 数据集使用了三种不同的平台进行采集：手持设备、电动自行车和SUV。&lt;br&gt;&lt;h4&gt;5. 环境条件的多样性&lt;/h4&gt;   - 数据采集涵盖了多种环境条件，包括晴天、夜间和大雨。&lt;br&gt;&lt;h4&gt;6. 采集时间与地点&lt;/h4&gt;   - 数据收集时间为2023年9月至2024年2月，地点包括植被校园的道路和高速公路的隧道。&lt;br&gt;&lt;h4&gt;7. 多次路线遍历&lt;/h4&gt;   - 每条路线被多次遍历，以促进地点识别的评估。&lt;br&gt;&lt;h4&gt;8. 传感器配置&lt;/h4&gt;   - 传感器组合包括3D激光雷达、4D雷达、立体相机、消费级惯性测量单元（IMU）和GNSS/INS系统。&lt;br&gt;&lt;h4&gt;9. 数据同步方法&lt;/h4&gt;   - 传感器数据包通过两步过程与GNSS时间进行同步：首先使用凸包算法平滑主机时间抖动，然后通过里程计和相关算法修正恒定时间偏移。&lt;br&gt;&lt;h4&gt;10. 外部校准&lt;/h4&gt;    - 传感器之间的外部校准通过手动测量和后续的非线性优化实现。&lt;br&gt;&lt;h4&gt;11. 参考运动生成&lt;/h4&gt;    - 通过将激光雷达扫描注册到地面激光扫描仪（TLS）点云图上，使用激光雷达惯性里程计（LIO）方法在定位模式下生成参考运动。&lt;br&gt;&lt;h4&gt;12. 数据回退技术&lt;/h4&gt;    - 引入了一种数据回退技术，以实现向后LIO处理。&lt;br&gt;&lt;h4&gt;13. 研究潜力&lt;/h4&gt;    - 该数据集预计将推动雷达基础的点云注册、里程计、地图构建和地点识别的研究。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-07-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 4D radars are increasingly favored for odometry and mapping of autonomous
systems due to their robustness in harsh weather and dynamic environments.
Existing datasets, however, often cover limited areas and are typically
captured using a single platform. To address this gap, we present a diverse
large-scale dataset specifically designed for 4D radar-based localization and
mapping. This dataset was gathered using three different platforms: a handheld
device, an e-bike, and an SUV, under a variety of environmental conditions,
including clear days, nighttime, and heavy rain. The data collection occurred
from September 2023 to February 2024, encompassing diverse settings such as
roads in a vegetated campus and tunnels on highways. Each route was traversed
multiple times to facilitate place recognition evaluations. The sensor suite
included a 3D lidar, 4D radars, stereo cameras, consumer-grade IMUs, and a
GNSS/INS system. Sensor data packets were synchronized to GNSS time using a
two-step process: a convex hull algorithm was applied to smooth host time
jitter, and then odometry and correlation algorithms were used to correct
constant time offsets. Extrinsic calibration between sensors was achieved
through manual measurements and subsequent nonlinear optimization. The
reference motion for the platforms was generated by registering lidar scans to
a terrestrial laser scanner (TLS) point cloud map using a lidar inertial
odometry (LIO) method in localization mode. Additionally, a data reversion
technique was introduced to enable backward LIO processing. We believe this
dataset will boost research in radar-based point cloud registration, odometry,
mapping, and place recognition.</description>
      <guid isPermaLink="false">2407.11705v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Comparative Analysis of Pretrained Audio Representations in Music Recommender Systems</title>
      <link>http://arxiv.org/abs/2409.08987v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 音乐信息检索的进展&lt;/h4&gt;   - 多年来，音乐信息检索（MIR）提出了多种在大量音乐数据上进行预训练的模型。&lt;br&gt;&lt;h4&gt;2. 迁移学习的有效性&lt;/h4&gt;   - 迁移学习展示了预训练后端模型在各种下游任务中的有效性，包括自动标记和流派分类。&lt;br&gt;&lt;h4&gt;3. MIR与音乐推荐系统的研究缺口&lt;/h4&gt;   - MIR论文通常未探讨预训练模型在音乐推荐系统（MRS）中的效率。&lt;br&gt;   - 推荐系统领域倾向于使用传统的端到端神经网络学习，而非这些预训练模型。&lt;br&gt;&lt;h4&gt;4. 研究目的&lt;/h4&gt;   - 本研究旨在填补这一研究空白，评估六种预训练后端模型在MRS中的适用性。&lt;br&gt;&lt;h4&gt;5. 评估模型&lt;/h4&gt;   - 研究评估了六个模型：MusicFM、Music2Vec、MERT、EncodecMAE、Jukebox和MusiCNN。&lt;br&gt;&lt;h4&gt;6. 推荐模型的比较&lt;/h4&gt;   - 使用三种推荐模型进行性能评估：K近邻（KNN）、浅层神经网络和BERT4Rec。&lt;br&gt;&lt;h4&gt;7. 性能发现&lt;/h4&gt;   - 结果显示，预训练的音频表示在传统MIR任务和MRS之间表现出显著的性能变异，表明后端模型捕捉的音乐信息在不同任务中可能存在差异。&lt;br&gt;&lt;h4&gt;8. 研究意义&lt;/h4&gt;   - 本研究为进一步探索预训练音频表示以增强音乐推荐系统奠定了基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3640457.3688172&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/Darel13712/pretrained-audio-representations&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Over the years, Music Information Retrieval (MIR) has proposed various models
pretrained on large amounts of music data. Transfer learning showcases the
proven effectiveness of pretrained backend models with a broad spectrum of
downstream tasks, including auto-tagging and genre classification. However, MIR
papers generally do not explore the efficiency of pretrained models for Music
Recommender Systems (MRS). In addition, the Recommender Systems community tends
to favour traditional end-to-end neural network learning over these models. Our
research addresses this gap and evaluates the applicability of six pretrained
backend models (MusicFM, Music2Vec, MERT, EncodecMAE, Jukebox, and MusiCNN) in
the context of MRS. We assess their performance using three recommendation
models: K-nearest neighbours (KNN), shallow neural network, and BERT4Rec. Our
findings suggest that pretrained audio representations exhibit significant
performance variability between traditional MIR tasks and MRS, indicating that
valuable aspects of musical information captured by backend models may differ
depending on the task. This study establishes a foundation for further
exploration of pretrained audio representations to enhance music recommendation
systems.</description>
      <guid isPermaLink="false">2409.08987v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>3D Object Detection and High-Resolution Traffic Parameters Extraction Using Low-Resolution LiDAR Data</title>
      <link>http://arxiv.org/abs/2401.06946v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages, 11 figures. This paper has been submitted for consideration
  for presentation at the 103rd Annual Meeting of the Transportation Research
  Board, January 2024&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 交通数据收集的重要性&lt;/h4&gt;   - 交通量数据收集对交通工程和城市规划至关重要，因为它提供了交通模式、拥堵和基础设施效率的重要见解。&lt;br&gt;&lt;h4&gt;2. 传统方法的局限性&lt;/h4&gt;   - 传统的手动交通数据收集方法既耗时又昂贵。&lt;br&gt;&lt;h4&gt;3. LiDAR技术的革新&lt;/h4&gt;   - 现代技术的出现，特别是激光雷达（LiDAR），使得高效准确的数据收集成为可能。&lt;br&gt;&lt;h4&gt;4. LiDAR的主要局限性&lt;/h4&gt;   - 以往研究发现，LiDAR的广泛应用受到两个主要限制：&lt;br&gt;     - 需要多个LiDAR系统以获取完整的点云信息。&lt;br&gt;     - 3D边界框注释的劳动密集性过程。&lt;br&gt;&lt;h4&gt;5. 提出的创新框架&lt;/h4&gt;   - 本研究提出了一种创新框架，旨在减少对多个LiDAR系统的需求，并简化繁琐的3D注释过程。&lt;br&gt;&lt;h4&gt;6. 单一LiDAR系统的使用&lt;/h4&gt;   - 采用单一LiDAR系统，降低数据采集成本，并通过点云补全（Point Cloud Completion, PCC）框架解决缺失点云信息的问题。&lt;br&gt;&lt;h4&gt;7. 点云补全技术&lt;/h4&gt;   - 开发了PCC框架，通过点密度填补缺失的点云信息。&lt;br&gt;&lt;h4&gt;8. 零样本学习技术&lt;/h4&gt;   - 使用零样本学习技术来检测车辆和行人。&lt;br&gt;&lt;h4&gt;9. 特征提取框架&lt;/h4&gt;   - 提出了一个独特的框架，用于从目标对象中提取从低到高的特征，如高度、加速度和速度。&lt;br&gt;&lt;h4&gt;10. 自动生成3D边界框&lt;/h4&gt;    - 利用2D边界框检测和提取的高度信息，研究能够自动生成3D边界框，无需人工干预。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-01-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traffic volume data collection is a crucial aspect of transportation
engineering and urban planning, as it provides vital insights into traffic
patterns, congestion, and infrastructure efficiency. Traditional manual methods
of traffic data collection are both time-consuming and costly. However, the
emergence of modern technologies, particularly Light Detection and Ranging
(LiDAR), has revolutionized the process by enabling efficient and accurate data
collection. Despite the benefits of using LiDAR for traffic data collection,
previous studies have identified two major limitations that have impeded its
widespread adoption. These are the need for multiple LiDAR systems to obtain
complete point cloud information of objects of interest, as well as the
labor-intensive process of annotating 3D bounding boxes for object detection
tasks. In response to these challenges, the current study proposes an
innovative framework that alleviates the need for multiple LiDAR systems and
simplifies the laborious 3D annotation process. To achieve this goal, the study
employed a single LiDAR system, that aims at reducing the data acquisition cost
and addressed its accompanying limitation of missing point cloud information by
developing a Point Cloud Completion (PCC) framework to fill in missing point
cloud information using point density. Furthermore, we also used zero-shot
learning techniques to detect vehicles and pedestrians, as well as proposed a
unique framework for extracting low to high features from the object of
interest, such as height, acceleration, and speed. Using the 2D bounding box
detection and extracted height information, this study is able to generate 3D
bounding boxes automatically without human intervention.</description>
      <guid isPermaLink="false">2401.06946v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Fast ($\sim N$) Diffusion Map Algorithm</title>
      <link>http://arxiv.org/abs/2409.05901v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究主题&lt;/h4&gt;   - 探讨简约流形学习技术，特别是扩散映射（Diffusion-maps）。&lt;br&gt;&lt;h4&gt;2. 算法与实现&lt;/h4&gt;   - 展示了一种算法及其实现，具有约 $O(N)$ 的计算复杂度，其中 $N$ 代表样本数量。&lt;br&gt;&lt;h4&gt;3. 重要性&lt;/h4&gt;   - 这些技术对于大规模无监督学习任务至关重要，因其不依赖于任何先验假设。&lt;br&gt;&lt;h4&gt;4. 采样定理的限制&lt;/h4&gt;   - 提到采样定理的限制，强调在无监督学习中处理大规模数据时的挑战。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this work we explore parsimonious manifold learning techniques,
specifically for Diffusion-maps. We demonstrate an algorithm and it's
implementation with computational complexity (in both time and memory) of $\sim
N$, with $N$ representing the number-of-samples. These techniques are essential
for large-scale unsupervised learning tasks without any prior assumptions, due
to sampling theorem limitations.</description>
      <guid isPermaLink="false">2409.05901v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>TagOOD: A Novel Approach to Out-of-Distribution Detection via Vision-Language Representations and Class Center Learning</title>
      <link>http://arxiv.org/abs/2408.15566v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ACMMM2024&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 多模态融合的重要性&lt;/h4&gt;   - 多模态融合（如视觉和语言数据）正在迅速受到关注，这种丰富的数据表示提升了各种任务的性能。&lt;br&gt;&lt;h4&gt;2. OOD检测的挑战&lt;/h4&gt;   - 现有的OOD（分布外）检测方法主要依赖整体图像特征，这些特征可能包含无关信息，从而阻碍OOD样本的检测，限制了整体性能。&lt;br&gt;&lt;h4&gt;3. 提出的新方法&lt;/h4&gt;   - 本文提出了TagOOD，一种新颖的OOD检测方法，利用视觉-语言表示实现标签无关的对象特征解耦。&lt;br&gt;&lt;h4&gt;4. 对象特征解耦&lt;/h4&gt;   - 这种解耦使得对对象语义的分析更加聚焦，从而提升OOD检测性能。&lt;br&gt;&lt;h4&gt;5. 轻量级网络训练&lt;/h4&gt;   - TagOOD在提取的对象特征上训练一个轻量级网络，以学习代表性类别中心，这些中心捕捉了IND（分布内）对象类别的中心趋势。&lt;br&gt;&lt;h4&gt;6. 减少无关特征影响&lt;/h4&gt;   - 通过学习类别中心，最小化无关图像特征对OOD检测的影响。&lt;br&gt;&lt;h4&gt;7. OOD样本检测方法&lt;/h4&gt;   - 该方法通过计算学习中心与测试样本之间的距离度量作为OOD评分，有效地检测OOD样本。&lt;br&gt;&lt;h4&gt;8. 实验评估&lt;/h4&gt;   - 在多个基准数据集上进行广泛实验，证明TagOOD优于现有的OOD检测方法。&lt;br&gt;&lt;h4&gt;9. 研究的潜在应用&lt;/h4&gt;   - 该工作为进一步探索多模态信息在OOD检测中的利用提供了新视角，具有广泛的应用潜力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/Jarvisgivemeasuit/tagood&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal fusion, leveraging data like vision and language, is rapidly
gaining traction. This enriched data representation improves performance across
various tasks. Existing methods for out-of-distribution (OOD) detection, a
critical area where AI models encounter unseen data in real-world scenarios,
rely heavily on whole-image features. These image-level features can include
irrelevant information that hinders the detection of OOD samples, ultimately
limiting overall performance. In this paper, we propose \textbf{TagOOD}, a
novel approach for OOD detection that leverages vision-language representations
to achieve label-free object feature decoupling from whole images. This
decomposition enables a more focused analysis of object semantics, enhancing
OOD detection performance. Subsequently, TagOOD trains a lightweight network on
the extracted object features to learn representative class centers. These
centers capture the central tendencies of IND object classes, minimizing the
influence of irrelevant image features during OOD detection. Finally, our
approach efficiently detects OOD samples by calculating distance-based metrics
as OOD scores between learned centers and test samples. We conduct extensive
experiments to evaluate TagOOD on several benchmark datasets and demonstrate
its superior performance compared to existing OOD detection methods. This work
presents a novel perspective for further exploration of multimodal information
utilization in OOD detection, with potential applications across various tasks.</description>
      <guid isPermaLink="false">2408.15566v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Pathfinder for Low-altitude Aircraft with Binary Neural Network</title>
      <link>http://arxiv.org/abs/2409.08824v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 全球拓扑地图的重要性&lt;/h4&gt;   - 先前的全球拓扑地图（如OpenStreetMap, OSM）可以提升地面移动机器人进行自主映射的性能。&lt;br&gt;&lt;h4&gt;2. 地图的不完整性&lt;/h4&gt;   - 现有的地图通常由于缺乏对部分路径的标注而不完整。&lt;br&gt;&lt;h4&gt;3. 提出的解决方案&lt;/h4&gt;   - 本文提出了一种使用低空飞行器携带的空中传感器来制作OSM的方案。&lt;br&gt;&lt;h4&gt;4. 核心方法&lt;/h4&gt;   - 该OSM制作者的核心是一个基于LiDAR和相机数据的新型高效路径寻址方法，即二元双流路段分割模型。&lt;br&gt;&lt;h4&gt;5. 多尺度特征提取&lt;/h4&gt;   - 实现了基于UNet架构的多尺度特征提取，用于处理图像和点云数据。&lt;br&gt;&lt;h4&gt;6. 稀疏点云的处理&lt;/h4&gt;   - 设计了一种基于注意力引导的门控块，以整合图像和点云特征，减少稀疏点云带来的影响。&lt;br&gt;&lt;h4&gt;7. 模型效率提升&lt;/h4&gt;   - 提出了每个模型组件的二元化流程，包括图像分支的视觉变换器（ViT）变体作为编码器，以及新的焦点损失和感知损失以优化模型训练。&lt;br&gt;&lt;h4&gt;8. 实验结果&lt;/h4&gt;   - 在两个数据集上的实验结果表明，提出的路径寻址方法在从低级空中传感器寻找路径时达到了最先进的准确性和高效率。&lt;br&gt;&lt;h4&gt;9. OSM地图的生成&lt;/h4&gt;   - 基于分割的道路骨架，可以创建完整的OSM先前地图。&lt;br&gt;&lt;h4&gt;10. 代码和数据获取&lt;/h4&gt;    - 相关代码和数据可通过链接获取：[GitHub - Pathfinder](https://github.com/IMRL/Pathfinder)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/imrl/pathfinder&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A prior global topological map (e.g., the OpenStreetMap, OSM) can boost the
performance of autonomous mapping by a ground mobile robot. However, the prior
map is usually incomplete due to lacking labeling in partial paths. To solve
this problem, this paper proposes an OSM maker using airborne sensors carried
by low-altitude aircraft, where the core of the OSM maker is a novel efficient
pathfinder approach based on LiDAR and camera data, i.e., a binary dual-stream
road segmentation model. Specifically, a multi-scale feature extraction based
on the UNet architecture is implemented for images and point clouds. To reduce
the effect caused by the sparsity of point cloud, an attention-guided gated
block is designed to integrate image and point-cloud features. For enhancing
the efficiency of the model, we propose a binarization streamline to each model
component, including a variant of vision transformer (ViT) architecture as the
encoder of the image branch, and new focal and perception losses to optimize
the model training. The experimental results on two datasets demonstrate that
our pathfinder method achieves SOTA accuracy with high efficiency in finding
paths from the low-level airborne sensors, and we can create complete OSM prior
maps based on the segmented road skeletons. Code and data are available
at:https://github.com/IMRL/Pathfinder}{https://github.com/IMRL/Pathfinder.</description>
      <guid isPermaLink="false">2409.08824v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Pre-Training for 3D Hand Pose Estimation with Contrastive Learning on Large-Scale Hand Images in the Wild</title>
      <link>http://arxiv.org/abs/2409.09714v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  HANDS@ECCV24 (Extended Abstracts)&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究框架介绍&lt;/h4&gt;   - 提出了一个基于自然环境手部图像的对比学习框架，旨在为3D手姿态估计进行预训练，称为HandCLR。&lt;br&gt;&lt;h4&gt;2. 预训练的重要性&lt;/h4&gt;   - 在大规模图像上进行预训练在各种任务中取得了良好结果，但以往的3D手姿态预训练方法未充分利用来自自然视频的多样化手图像。&lt;br&gt;&lt;h4&gt;3. 数据准备&lt;/h4&gt;   - 为了便于可扩展的预训练，首先从自然视频中准备了大量手部图像，并设计了基于对比学习的方法。&lt;br&gt;&lt;h4&gt;4. 数据来源&lt;/h4&gt;   - 收集了超过200万幅手部图像，数据来源包括100DOH和Ego4D等人类中心的视频。&lt;br&gt;&lt;h4&gt;5. 信息提取方法&lt;/h4&gt;   - 关注手部相似性的对比，从不同样本中提取相似手势的配对，提出了一种新的对比学习方法，使得相似手势在潜在空间中更靠近。&lt;br&gt;&lt;h4&gt;6. 实验结果&lt;/h4&gt;   - 实验表明，该方法优于传统的对比学习方法，后者主要依赖于单幅图像的数据增强来生成正样本对。&lt;br&gt;&lt;h4&gt;7. 性能提升&lt;/h4&gt;   - 在多个数据集上，该方法显著提高了性能：在FreiHand上提升15%，在DexYCB上提升10%，在AssemblyHands上提升4%。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a contrastive learning framework based on in-the-wild hand images
tailored for pre-training 3D hand pose estimators, dubbed HandCLR. Pre-training
on large-scale images achieves promising results in various tasks, but prior 3D
hand pose pre-training methods have not fully utilized the potential of diverse
hand images accessible from in-the-wild videos. To facilitate scalable
pre-training, we first prepare an extensive pool of hand images from
in-the-wild videos and design our method with contrastive learning.
Specifically, we collected over 2.0M hand images from recent human-centric
videos, such as 100DOH and Ego4D. To extract discriminative information from
these images, we focus on the similarity of hands; pairs of similar hand poses
originating from different samples, and propose a novel contrastive learning
method that embeds similar hand pairs closer in the latent space. Our
experiments demonstrate that our method outperforms conventional contrastive
learning approaches that produce positive pairs sorely from a single image with
data augmentation. We achieve significant improvements over the
state-of-the-art method in various datasets, with gains of 15% on FreiHand, 10%
on DexYCB, and 4% on AssemblyHands.</description>
      <guid isPermaLink="false">2409.09714v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Modified Meta-Thompson Sampling for Linear Bandits and Its Bayes Regret Analysis</title>
      <link>http://arxiv.org/abs/2409.06329v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 元学习的定义&lt;/h4&gt;   - 元学习的特点在于其学习如何学习，使得学习策略能够适应不同任务。&lt;br&gt;&lt;h4&gt;2. Meta-Thompson Sampling的介绍&lt;/h4&gt;   - 最近的研究提出了Meta-Thompson Sampling（Meta-TS），该方法通过与从未知先验分布中抽取的赌博实例交互来进行元学习。&lt;br&gt;&lt;h4&gt;3. 现有分析的局限性&lt;/h4&gt;   - 现有分析仅限于高斯赌博问题。&lt;br&gt;&lt;h4&gt;4. 上下文多臂赌博框架&lt;/h4&gt;   - 上下文多臂赌博框架是高斯赌博的扩展，挑战代理利用上下文向量预测最有价值的臂，优化平衡探索与利用，以最小化后悔。&lt;br&gt;&lt;h4&gt;5. Meta-TSLB算法的提出&lt;/h4&gt;   - 本文介绍了Meta-TSLB算法，这是针对线性上下文赌博的修改版Meta-TS。&lt;br&gt;&lt;h4&gt;6. 理论分析&lt;/h4&gt;   - 我们对Meta-TSLB进行了理论分析，推导出其贝叶斯后悔的界限为 $O((m+\log(m))\sqrt{n\log(n)})$，其中 $m$ 代表赌博实例的数量，$n$ 代表汤普森采样的轮次。&lt;br&gt;&lt;h4&gt;7. 对Meta-TS分析的补充&lt;/h4&gt;   - 本研究补充了Meta-TS在线性上下文赌博上的分析。&lt;br&gt;&lt;h4&gt;8. 实验评估&lt;/h4&gt;   - 在不同设置下对Meta-TSLB的性能进行了实验评估，并分析了其泛化能力，展示了其适应未见实例的潜力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Meta-learning is characterized by its ability to learn how to learn, enabling
the adaptation of learning strategies across different tasks. Recent research
introduced the Meta-Thompson Sampling (Meta-TS), which meta-learns an unknown
prior distribution sampled from a meta-prior by interacting with bandit
instances drawn from it. However, its analysis was limited to Gaussian bandit.
The contextual multi-armed bandit framework is an extension of the Gaussian
Bandit, which challenges agent to utilize context vectors to predict the most
valuable arms, optimally balancing exploration and exploitation to minimize
regret over time. This paper introduces Meta-TSLB algorithm, a modified Meta-TS
for linear contextual bandits. We theoretically analyze Meta-TSLB and derive an
$ O((m+\log(m))\sqrt{n\log(n)})$ bound on its Bayes regret, in which $m$
represents the number of bandit instances, and $n$ the number of rounds of
Thompson Sampling. Additionally, our work complements the analysis of Meta-TS
for linear contextual bandits. The performance of Meta-TSLB is evaluated
experimentally under different settings, and we experimente and analyze the
generalization capability of Meta-TSLB, showcasing its potential to adapt to
unseen instances.</description>
      <guid isPermaLink="false">2409.06329v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Turbo your multi-modal classification with contrastive learning</title>
      <link>http://arxiv.org/abs/2409.09282v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 对比学习的兴起&lt;/h4&gt;   - 对比学习已成为多模态表示学习中一种引人注目的方法。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 之前的多模态研究主要集中在跨模态理解上，而忽视了模态内的对比学习，这限制了每种模态的表示能力。&lt;br&gt;&lt;h4&gt;3. 提出的新策略&lt;/h4&gt;   - 本文提出了一种新颖的对比学习策略，称为“Turbo”，旨在通过联合模态内和跨模态对比学习来促进多模态理解。&lt;br&gt;&lt;h4&gt;4. 数据处理方法&lt;/h4&gt;   - 多模态数据对通过两次前向传播，使用不同的隐藏丢弃掩码，以获取每个模态的两种不同表示。&lt;br&gt;&lt;h4&gt;5. 对比目标的生成&lt;/h4&gt;   - 利用这些表示，生成多个模态内和跨模态的对比目标用于训练。&lt;br&gt;&lt;h4&gt;6. 自监督与监督结合&lt;/h4&gt;   - 最后，将自监督的Turbo与监督的多模态分类相结合。&lt;br&gt;&lt;h4&gt;7. 实验验证&lt;/h4&gt;   - 在两个音频-文本分类任务上验证了该方法的有效性，尤其是在语音情感识别基准数据集上达到了最先进的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Contrastive learning has become one of the most impressive approaches for
multi-modal representation learning. However, previous multi-modal works mainly
focused on cross-modal understanding, ignoring in-modal contrastive learning,
which limits the representation of each modality. In this paper, we propose a
novel contrastive learning strategy, called $Turbo$, to promote multi-modal
understanding by joint in-modal and cross-modal contrastive learning.
Specifically, multi-modal data pairs are sent through the forward pass twice
with different hidden dropout masks to get two different representations for
each modality. With these representations, we obtain multiple in-modal and
cross-modal contrastive objectives for training. Finally, we combine the
self-supervised Turbo with the supervised multi-modal classification and
demonstrate its effectiveness on two audio-text classification tasks, where the
state-of-the-art performance is achieved on a speech emotion recognition
benchmark dataset.</description>
      <guid isPermaLink="false">2409.09282v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Temporal and Spatial Online Integrated Calibration for Camera and LiDAR</title>
      <link>http://arxiv.org/abs/2207.10454v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 传感器的广泛应用&lt;/h4&gt;   - 相机和LiDAR在大多数辅助和自动驾驶系统中被广泛使用。&lt;br&gt;&lt;h4&gt;2. 时间同步与外部校准的挑战&lt;/h4&gt;   - 目前关于相机和LiDAR的时间同步和外部校准的相关研究较少，且大多集中于在线传感器数据融合。&lt;br&gt;&lt;h4&gt;3. 面临的技术挑战&lt;/h4&gt;   - 时间和空间校准技术面临缺乏相关性和实时性的挑战。&lt;br&gt;&lt;h4&gt;4. 提出的模型&lt;/h4&gt;   - 本文引入姿态估计模型和环境鲁棒线特征提取，以提高数据融合的相关性和实时纠正能力。&lt;br&gt;&lt;h4&gt;5. 动态目标消除&lt;/h4&gt;   - 动态目标消除旨在考虑相邻时刻点云匹配的对应关系，寻找最佳策略。&lt;br&gt;&lt;h4&gt;6. 优化搜索过程&lt;/h4&gt;   - 搜索优化过程旨在提供计算精度和效率兼具的准确参数。&lt;br&gt;&lt;h4&gt;7. 方法评估&lt;/h4&gt;   - 在KITTI基准测试中评估该方法，使用真实值进行验证。&lt;br&gt;&lt;h4&gt;8. 在线实验结果&lt;/h4&gt;   - 在线实验表明，该方法在时间校准方面比软同步方法提高了38.5%的准确性。&lt;br&gt;&lt;h4&gt;9. 空间校准的自动纠正&lt;/h4&gt;   - 在空间校准中，该方法能在0.4秒内自动纠正干扰误差，且精度达到0.3度。&lt;br&gt;&lt;h4&gt;10. 研究和应用的促进&lt;/h4&gt;    - 本研究能够促进传感器融合的研究和应用。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2022-07-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While camera and LiDAR are widely used in most of the assisted and autonomous
driving systems, only a few works have been proposed to associate the temporal
synchronization and extrinsic calibration for camera and LiDAR which are
dedicated to online sensors data fusion. The temporal and spatial calibration
technologies are facing the challenges of lack of relevance and real-time. In
this paper, we introduce the pose estimation model and environmental robust
line features extraction to improve the relevance of data fusion and instant
online ability of correction. Dynamic targets eliminating aims to seek optimal
policy considering the correspondence of point cloud matching between adjacent
moments. The searching optimization process aims to provide accurate parameters
with both computation accuracy and efficiency. To demonstrate the benefits of
this method, we evaluate it on the KITTI benchmark with ground truth value. In
online experiments, our approach improves the accuracy by 38.5\% than the soft
synchronization method in temporal calibration. While in spatial calibration,
our approach automatically corrects disturbance errors within 0.4 second and
achieves an accuracy of 0.3-degree. This work can promote the research and
application of sensor fusion.</description>
      <guid isPermaLink="false">2207.10454v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Time-Series Forecasting, Knowledge Distillation, and Refinement within a Multimodal PDE Foundation Model</title>
      <link>http://arxiv.org/abs/2409.11609v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 符号编码在多操作学习中的应用&lt;/h4&gt;   - 符号编码被用于多操作学习，以嵌入额外的信息，适用于不同的时间序列数据。&lt;br&gt;&lt;h4&gt;2. 时空系统的描述&lt;/h4&gt;   - 对于由时间相关的偏微分方程描述的时空系统，这些方程本身提供了额外的模态来识别系统。&lt;br&gt;&lt;h4&gt;3. 多模态预测神经网络的开发&lt;/h4&gt;   - 利用符号表达式与时间序列样本的结合，能够开发多模态预测神经网络。&lt;br&gt;&lt;h4&gt;4. 现有方法的挑战&lt;/h4&gt;   - 当前方法的一个关键挑战是，符号信息（即方程）需手动预处理（简化、重排等），以匹配现有的符号库，这增加了成本并降低了灵活性，尤其是在处理新的偏微分方程时。&lt;br&gt;&lt;h4&gt;5. 新的符号库提案&lt;/h4&gt;   - 提出了一个基于SymPy的新符号库，用于将偏微分方程编码为时间序列模型的额外模态。&lt;br&gt;&lt;h4&gt;6. 自动化和成本效益&lt;/h4&gt;   - 所提出的方法成本极低，且是自动化的，能够保持高预测准确性，适用于预测任务。&lt;br&gt;&lt;h4&gt;7. 贝叶斯滤波模块&lt;/h4&gt;   - 包括一个贝叶斯滤波模块，将不同模态连接起来，以细化已学习的方程。&lt;br&gt;&lt;h4&gt;8. 改进预测精度&lt;/h4&gt;   - 该模块提高了学习到的符号表示和预测时间序列的准确性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/jingminsun/prose_v1&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Symbolic encoding has been used in multi-operator learning as a way to embed
additional information for distinct time-series data. For spatiotemporal
systems described by time-dependent partial differential equations, the
equation itself provides an additional modality to identify the system. The
utilization of symbolic expressions along side time-series samples allows for
the development of multimodal predictive neural networks. A key challenge with
current approaches is that the symbolic information, i.e. the equations, must
be manually preprocessed (simplified, rearranged, etc.) to match and relate to
the existing token library, which increases costs and reduces flexibility,
especially when dealing with new differential equations. We propose a new token
library based on SymPy to encode differential equations as an additional
modality for time-series models. The proposed approach incurs minimal cost, is
automated, and maintains high prediction accuracy for forecasting tasks.
Additionally, we include a Bayesian filtering module that connects the
different modalities to refine the learned equation. This improves the accuracy
of the learned symbolic representation and the predicted time-series.</description>
      <guid isPermaLink="false">2409.11609v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Mamba24/8D: Enhancing Global Interaction in Point Clouds via State Space Model</title>
      <link>http://arxiv.org/abs/2406.17442v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. Transformers在3D点云语义分割中的表现&lt;/h4&gt;   - Transformers在3D点云语义分割任务中取得了显著的效果。&lt;br&gt;&lt;h4&gt;2. 计算复杂度问题&lt;/h4&gt;   - 然而，Transformer的二次复杂度导致计算成本高，限制了可以同时处理的点的数量，并妨碍了长距离依赖的建模。&lt;br&gt;&lt;h4&gt;3. 受状态空间模型（SSM）启发&lt;/h4&gt;   - 受到近期状态空间模型（SSM）在长序列建模中潜力的启发，本文引入了Mamba，一个基于SSM的架构，应用于点云领域。&lt;br&gt;&lt;h4&gt;4. Mamba24/8D架构&lt;/h4&gt;   - 提出Mamba24/8D，具备强大的全局建模能力，同时保持线性复杂度。&lt;br&gt;&lt;h4&gt;5. 多路径序列化策略&lt;/h4&gt;   - 为了使点云的无序性与Mamba的因果性质相适应，提出了一种适用于点云的多路径序列化策略。&lt;br&gt;&lt;h4&gt;6. ConvMamba模块&lt;/h4&gt;   - 提出ConvMamba模块，以弥补Mamba在局部几何建模和单向建模方面的不足。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - Mamba24/8D在多个3D点云分割任务上取得了最先进的结果，包括ScanNet v2、ScanNet200和nuScenes。&lt;br&gt;&lt;h4&gt;8. 有效性验证&lt;/h4&gt;   - 通过大量实验，验证了该方法的有效性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-06-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transformers have demonstrated impressive results for 3D point cloud semantic
segmentation. However, the quadratic complexity of transformer makes
computation cost high, limiting the number of points that can be processed
simultaneously and impeding the modeling of long-range dependencies. Drawing
inspiration from the great potential of recent state space models (SSM) for
long sequence modeling, we introduce Mamba, a SSM-based architecture, to the
point cloud domain and propose Mamba24/8D, which has strong global modeling
capability under linear complexity. Specifically, to make disorderness of point
clouds fit in with the causal nature of Mamba, we propose a multi-path
serialization strategy applicable to point clouds. Besides, we propose the
ConvMamba block to compensate for the shortcomings of Mamba in modeling local
geometries and in unidirectional modeling. Mamba24/8D obtains state of the art
results on several 3D point cloud segmentation tasks, including ScanNet v2,
ScanNet200 and nuScenes, while its effectiveness is validated by extensive
experiments.</description>
      <guid isPermaLink="false">2406.17442v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>GINTRIP: Interpretable Temporal Graph Regression using Information bottleneck and Prototype-based method</title>
      <link>http://arxiv.org/abs/2409.10996v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 深度神经网络的应用&lt;/h4&gt;   - 深度神经网络（DNNs）在多个领域表现出色，但在时间图回归任务中的应用面临显著的可解释性挑战。&lt;br&gt;&lt;h4&gt;2. 可解释性问题的根源&lt;/h4&gt;   - 这一问题源于DNN的复杂性以及图中潜在的时空模式复杂性，亟需创新解决方案。&lt;br&gt;&lt;h4&gt;3. 图神经网络的可解释性&lt;/h4&gt;   - 尽管图神经网络（GNNs）的可解释性问题与DNNs相似，但目前尚无显著工作结合信息瓶颈（IB）原则和原型方法来解决时间GNN的可解释性。&lt;br&gt;&lt;h4&gt;4. 研究创新&lt;/h4&gt;   - 本研究提出了一种新颖的方法，独特地结合IB原则和原型基础的方法，以增强时间图回归模型的可解释性。&lt;br&gt;&lt;h4&gt;5. 主要贡献&lt;/h4&gt;   - **GINTRIP框架**：引入了时间回归任务中的图可解释性（GINTRIP），首次将IB和原型方法结合应用于可解释的时间图任务。&lt;br&gt;   - **互信息的新理论界限**：推导了互信息（MI）的新理论界限，扩展了IB原则在图回归任务中的适用性。&lt;br&gt;   - **多任务学习**：整合了无监督辅助分类头，促进多任务学习和多样化概念表示，从而增强模型瓶颈的可解释性。&lt;br&gt;&lt;h4&gt;6. 模型评估&lt;/h4&gt;   - 在真实世界的交通数据集上进行评估，结果显示该模型在预测准确性和可解释性相关指标上均优于现有方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep neural networks (DNNs) have demonstrated remarkable performance across
various domains, yet their application to temporal graph regression tasks faces
significant challenges regarding interpretability. This critical issue, rooted
in the inherent complexity of both DNNs and underlying spatio-temporal patterns
in the graph, calls for innovative solutions. While interpretability concerns
in Graph Neural Networks (GNNs) mirror those of DNNs, to the best of our
knowledge, no notable work has addressed the interpretability of temporal GNNs
using a combination of Information Bottleneck (IB) principles and
prototype-based methods. Our research introduces a novel approach that uniquely
integrates these techniques to enhance the interpretability of temporal graph
regression models. The key contributions of our work are threefold: We
introduce the \underline{G}raph \underline{IN}terpretability in
\underline{T}emporal \underline{R}egression task using \underline{I}nformation
bottleneck and \underline{P}rototype (GINTRIP) framework, the first combined
application of IB and prototype-based methods for interpretable temporal graph
tasks. We derive a novel theoretical bound on mutual information (MI),
extending the applicability of IB principles to graph regression tasks. We
incorporate an unsupervised auxiliary classification head, fostering multi-task
learning and diverse concept representation, which enhances the model
bottleneck's interpretability. Our model is evaluated on real-world traffic
datasets, outperforming existing methods in both forecasting accuracy and
interpretability-related metrics.</description>
      <guid isPermaLink="false">2409.10996v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Bridging Multi-Task Learning and Meta-Learning: Towards Efficient Training and Effective Adaptation</title>
      <link>http://arxiv.org/abs/2106.09017v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICML 2021 camera-ready version. Code is released at
  https://github.com/AI-secure/multi-task-learning&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 多任务学习（MTL）的目标&lt;/h4&gt;   - MTL旨在通过联合学习多个相关任务来提高泛化能力。&lt;br&gt;&lt;h4&gt;2. 元学习的比较&lt;/h4&gt;   - 除了联合训练方案，现代元学习允许在测试阶段使用有限标签的未见任务，以便快速适应。&lt;br&gt;&lt;h4&gt;3. MTL与元学习的相似性&lt;/h4&gt;   - 尽管在问题表述上存在微妙差异，MTL和元学习都认为现有训练任务间的共享结构能够提升泛化和适应性。&lt;br&gt;&lt;h4&gt;4. 研究目标&lt;/h4&gt;   - 本文进一步探讨这两种学习范式之间的紧密联系，结合理论分析和实证研究。&lt;br&gt;&lt;h4&gt;5. 理论分析&lt;/h4&gt;   - 首先证明MTL与一类基于梯度的元学习（GBML）算法具有相同的优化形式。&lt;br&gt;   - 证明在充分深度的过参数化神经网络中，MTL和GBML的预测函数是接近的。&lt;br&gt;&lt;h4&gt;6. 预测相似性&lt;/h4&gt;   - 这一结果表明，这两种模型在相同未见任务上的预测是相似的。&lt;br&gt;&lt;h4&gt;7. 实证验证&lt;/h4&gt;   - 通过适当的实现，实证支持理论发现，MTL在少样本图像分类基准上与最先进的GBML算法具有竞争力。&lt;br&gt;&lt;h4&gt;8. 计算效率&lt;/h4&gt;   - 现有的GBML算法通常涉及成本高昂的二阶双层优化，而本文的一阶MTL方法在大型数据集（如mini-ImageNet）上速度快一个数量级。&lt;br&gt;&lt;h4&gt;9. 研究贡献&lt;/h4&gt;   - 本文有助于弥合这两种学习范式之间的差距，并提供一种计算上高效的GBML替代方案，支持快速任务适应。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2021-06-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/AI-secure/multi-task-learning&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-task learning (MTL) aims to improve the generalization of several
related tasks by learning them jointly. As a comparison, in addition to the
joint training scheme, modern meta-learning allows unseen tasks with limited
labels during the test phase, in the hope of fast adaptation over them. Despite
the subtle difference between MTL and meta-learning in the problem formulation,
both learning paradigms share the same insight that the shared structure
between existing training tasks could lead to better generalization and
adaptation. In this paper, we take one important step further to understand the
close connection between these two learning paradigms, through both theoretical
analysis and empirical investigation. Theoretically, we first demonstrate that
MTL shares the same optimization formulation with a class of gradient-based
meta-learning (GBML) algorithms. We then prove that for over-parameterized
neural networks with sufficient depth, the learned predictive functions of MTL
and GBML are close. In particular, this result implies that the predictions
given by these two models are similar over the same unseen task. Empirically,
we corroborate our theoretical findings by showing that, with proper
implementation, MTL is competitive against state-of-the-art GBML algorithms on
a set of few-shot image classification benchmarks. Since existing GBML
algorithms often involve costly second-order bi-level optimization, our
first-order MTL method is an order of magnitude faster on large-scale datasets
such as mini-ImageNet. We believe this work could help bridge the gap between
these two learning paradigms, and provide a computationally efficient
alternative to GBML that also supports fast task adaptation.</description>
      <guid isPermaLink="false">2106.09017v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>MOTSLAM: MOT-assisted monocular dynamic SLAM using single-view depth estimation</title>
      <link>http://arxiv.org/abs/2210.02038v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 视觉SLAM的背景&lt;/h4&gt;   - 针对静态场景的视觉SLAM系统已经开发出具有满意的准确性和鲁棒性。&lt;br&gt;&lt;h4&gt;2. 动态物体跟踪的重要性&lt;/h4&gt;   - 动态3D物体跟踪成为视觉SLAM中的重要能力，尤其在自动驾驶、增强现实和虚拟现实等场景中。&lt;br&gt;&lt;h4&gt;3. 单目图像的挑战&lt;/h4&gt;   - 仅使用单目图像进行动态SLAM仍然具有挑战性，因为动态特征的关联和位置估计困难。&lt;br&gt;&lt;h4&gt;4. MOTSLAM系统的提出&lt;/h4&gt;   - 本文提出了MOTSLAM，一个使用单目配置的动态视觉SLAM系统，能够跟踪动态物体的姿态和边界框。&lt;br&gt;&lt;h4&gt;5. 多目标跟踪（MOT）&lt;/h4&gt;   - MOTSLAM首先进行多目标跟踪，同时检测2D和3D边界框，以创建初步的3D对象。&lt;br&gt;&lt;h4&gt;6. 深度估计的应用&lt;/h4&gt;   - 采用基于神经网络的单目深度估计，获取动态特征的深度信息。&lt;br&gt;&lt;h4&gt;7. 联合优化过程&lt;/h4&gt;   - 使用新颖的束调整方法，联合优化相机姿态、物体姿态以及静态和动态地图点。&lt;br&gt;&lt;h4&gt;8. 实验验证&lt;/h4&gt;   - 在KITTI数据集上的实验表明，该系统在单目动态SLAM中，达到了相机自运动和物体跟踪的最佳性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2022-10-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual SLAM systems targeting static scenes have been developed with
satisfactory accuracy and robustness. Dynamic 3D object tracking has then
become a significant capability in visual SLAM with the requirement of
understanding dynamic surroundings in various scenarios including autonomous
driving, augmented and virtual reality. However, performing dynamic SLAM solely
with monocular images remains a challenging problem due to the difficulty of
associating dynamic features and estimating their positions. In this paper, we
present MOTSLAM, a dynamic visual SLAM system with the monocular configuration
that tracks both poses and bounding boxes of dynamic objects. MOTSLAM first
performs multiple object tracking (MOT) with associated both 2D and 3D bounding
box detection to create initial 3D objects. Then, neural-network-based
monocular depth estimation is applied to fetch the depth of dynamic features.
Finally, camera poses, object poses, and both static, as well as dynamic map
points, are jointly optimized using a novel bundle adjustment. Our experiments
on the KITTI dataset demonstrate that our system has reached best performance
on both camera ego-motion and object tracking on monocular dynamic SLAM.</description>
      <guid isPermaLink="false">2210.02038v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>GlobalPointer: Large-Scale Plane Adjustment with Bi-Convex Relaxation</title>
      <link>http://arxiv.org/abs/2407.13537v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ECCV 2024. The first two authors contributed equally to
  this work. Code: https://github.com/wu-cvgl/GlobalPointer&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 平面调整的重要性&lt;/h4&gt;   - 平面调整（PA）在许多3D应用中至关重要，涉及同时进行姿态估计和平面恢复。&lt;br&gt;&lt;h4&gt;2. 挑战性问题&lt;/h4&gt;   - 尽管已有进展，但在多视角点云注册领域，平面调整问题依然具有挑战性。&lt;br&gt;&lt;h4&gt;3. 现有方法的局限性&lt;/h4&gt;   - 目前的最先进方法仅在良好初始化的情况下才能实现全局最优收敛。&lt;br&gt;   - 高时间复杂度使得这些方法在大规模问题上不够实用。&lt;br&gt;&lt;h4&gt;4. 新优化策略的提出&lt;/h4&gt;   - 本文提出了一种新颖的优化策略，称为**双凸松弛（Bi-Convex Relaxation）**，将原始问题解耦为两个更简单的子问题。&lt;br&gt;&lt;h4&gt;5. 凸松弛技术的应用&lt;/h4&gt;   - 通过凸松弛技术重新构造每个子问题，并交替解决这两个子问题，直至原始问题收敛。&lt;br&gt;&lt;h4&gt;6. 算法变体的提出&lt;/h4&gt;   - 基于此策略，提出了两种算法变体解决平面调整问题：&lt;br&gt;     - **GlobalPointer**：基于点到平面的误差。&lt;br&gt;     - **GlobalPointer++**：基于平面到平面的误差。&lt;br&gt;&lt;h4&gt;7. 实验验证&lt;/h4&gt;   - 在合成和真实数据集上的广泛实验表明，所提方法能够实现大规模平面调整，具有线性时间复杂度、较大的收敛范围和对差初始化的鲁棒性。&lt;br&gt;&lt;h4&gt;8. 准确性&lt;/h4&gt;   - 与之前的方法相比，所提方法在准确性上达到了相似的水平。&lt;br&gt;&lt;h4&gt;9. 代码可用性&lt;/h4&gt;   - 相关代码可在GitHub上获取：[GlobalPointer](https://github.com/wu-cvgl/GlobalPointer)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-07-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/wu-cvgl/GlobalPointer&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Plane adjustment (PA) is crucial for many 3D applications, involving
simultaneous pose estimation and plane recovery. Despite recent advancements,
it remains a challenging problem in the realm of multi-view point cloud
registration. Current state-of-the-art methods can achieve globally optimal
convergence only with good initialization. Furthermore, their high time
complexity renders them impractical for large-scale problems. To address these
challenges, we first exploit a novel optimization strategy termed
\textit{Bi-Convex Relaxation}, which decouples the original problem into two
simpler sub-problems, reformulates each sub-problem using a convex relaxation
technique, and alternately solves each one until the original problem
converges. Building on this strategy, we propose two algorithmic variants for
solving the plane adjustment problem, namely \textit{GlobalPointer} and
\textit{GlobalPointer++}, based on point-to-plane and plane-to-plane errors,
respectively. Extensive experiments on both synthetic and real datasets
demonstrate that our method can perform large-scale plane adjustment with
linear time complexity, larger convergence region, and robustness to poor
initialization, while achieving similar accuracy as prior methods. The code is
available at https://github.com/wu-cvgl/GlobalPointer.</description>
      <guid isPermaLink="false">2407.13537v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Train-On-Request: An On-Device Continual Learning Workflow for Adaptive Real-World Brain Machine Interfaces</title>
      <link>http://arxiv.org/abs/2409.09161v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 6 figures, to be published in 2024 IEEE Biomedical Circuits
  and Systems Conference (BioCAS)&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 脑机接口的扩展&lt;/h4&gt;   - 脑机接口（BMI）由于硬件和算法的进步，正在超越临床环境。&lt;br&gt;&lt;h4&gt;2. 面临的挑战&lt;/h4&gt;   - 用户友好性和信号变异性仍然是BMIs面临的主要挑战。&lt;br&gt;&lt;h4&gt;3. 分类模型的适应需求&lt;/h4&gt;   - 为了在实际应用中保持高性能，分类模型需要定期适应新条件，因此制定最佳的再训练策略至关重要。&lt;br&gt;&lt;h4&gt;4. 提出的TOR方法&lt;/h4&gt;   - 提出了TOR（按需训练工作流程），使得用户可以针对新条件进行模型的特定适应，以应对信号随时间的变异。&lt;br&gt;&lt;h4&gt;5. 持续学习的应用&lt;/h4&gt;   - TOR利用持续学习保留跨会话的知识，减少会话间的变异性。&lt;br&gt;&lt;h4&gt;6. 按需学习的能力&lt;/h4&gt;   - 用户可以通过设备上的学习（ODL）按需精炼模型，以提高准确性，从而适应变化的条件。&lt;br&gt;&lt;h4&gt;7. 评估结果&lt;/h4&gt;   - 在使用非污名化的可穿戴BMI头带记录的运动数据集上进行评估，达到了92%的准确率，并且再校准时间低至1.6分钟，比传统转移学习流程减少46%。&lt;br&gt;&lt;h4&gt;8. 在极端边缘环境中的适应性&lt;/h4&gt;   - TOR适用于极端边缘环境，通过在RISC-V超低功耗系统（GAP9）上部署训练过程，实现了21.6毫秒的延迟和每个训练步骤1毫焦耳的能量消耗。&lt;br&gt;&lt;h4&gt;9. 创新性贡献&lt;/h4&gt;   - 这是首次展示BMI模型在实时环境中对EEG信号固有变异性进行在线、节能、高效动态适应的工作。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Brain-machine interfaces (BMIs) are expanding beyond clinical settings thanks
to advances in hardware and algorithms. However, they still face challenges in
user-friendliness and signal variability. Classification models need periodic
adaptation for real-life use, making an optimal re-training strategy essential
to maximize user acceptance and maintain high performance. We propose TOR, a
train-on-request workflow that enables user-specific model adaptation to novel
conditions, addressing signal variability over time. Using continual learning,
TOR preserves knowledge across sessions and mitigates inter-session
variability. With TOR, users can refine, on demand, the model through on-device
learning (ODL) to enhance accuracy adapting to changing conditions. We evaluate
the proposed methodology on a motor-movement dataset recorded with a
non-stigmatizing wearable BMI headband, achieving up to 92% accuracy and a
re-calibration time as low as 1.6 minutes, a 46% reduction compared to a naive
transfer learning workflow. We additionally demonstrate that TOR is suitable
for ODL in extreme edge settings by deploying the training procedure on a
RISC-V ultra-low-power SoC (GAP9), resulting in 21.6 ms of latency and 1 mJ of
energy consumption per training step. To the best of our knowledge, this work
is the first demonstration of an online, energy-efficient, dynamic adaptation
of a BMI model to the intrinsic variability of EEG signals in real-time
settings.</description>
      <guid isPermaLink="false">2409.09161v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Learning to Solve Combinatorial Optimization under Positive Linear Constraints via Non-Autoregressive Neural Networks</title>
      <link>http://arxiv.org/abs/2409.04495v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  English version of the same paper published on Scientia Sinica
  Informationis&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 组合优化的重要性&lt;/h4&gt;   - 组合优化（CO）是计算机科学与应用数学交叉的基本问题。&lt;br&gt;&lt;h4&gt;2. 解决组合优化的挑战&lt;/h4&gt;   - 由于组合优化问题的固有复杂性，精确解决这些问题具有挑战性。&lt;br&gt;&lt;h4&gt;3. 深度神经网络的研究前沿&lt;/h4&gt;   - 基于深度神经网络的求解器成为当前研究的前沿领域。&lt;br&gt;&lt;h4&gt;4. 非自回归神经网络的设计&lt;/h4&gt;   - 本文设计了一系列非自回归神经网络，用于处理带有正线性约束的组合优化问题。&lt;br&gt;&lt;h4&gt;5. 正线性约束的广泛适用性&lt;/h4&gt;   - 正线性约束涵盖多种组合优化问题，突破了现有非自回归网络的通用性瓶颈。&lt;br&gt;&lt;h4&gt;6. 相较于自回归网络的优势&lt;/h4&gt;   - 非自回归网络在效率更高且保持排列不变性方面优于现有的自回归神经网络求解器。&lt;br&gt;&lt;h4&gt;7. 离线无监督学习的优势&lt;/h4&gt;   - 该方法对高质量标签的需求较低，避免了监督学习中对最优标签的依赖。&lt;br&gt;&lt;h4&gt;8. 在线可微搜索方法&lt;/h4&gt;   - 在线可微搜索方法显著提升了神经网络求解器对未见问题的泛化能力。&lt;br&gt;&lt;h4&gt;9. 验证框架的有效性&lt;/h4&gt;   - 在解决代表性的组合优化问题（如设施选址、最大集合覆盖、旅行商问题）时验证了该框架的有效性。&lt;br&gt;&lt;h4&gt;10. 与现有求解器的竞争性&lt;/h4&gt;    - 本文的非自回归神经求解器在效率和效能方面与最先进的求解器（如SCIP和Gurobi）相竞争，甚至在某些情况下表现更优。&lt;br&gt;&lt;h4&gt;11. 代码可用性&lt;/h4&gt;    - 相关代码可在GitHub上获取：[NAR-CO-Solver](https://github.com/Thinklab-SJTU/NAR-CO-Solver)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/thinklab-sjtu/nar-co-solver&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Combinatorial optimization (CO) is the fundamental problem at the
intersection of computer science, applied mathematics, etc. The inherent
hardness in CO problems brings up challenge for solving CO exactly, making
deep-neural-network-based solvers a research frontier. In this paper, we design
a family of non-autoregressive neural networks to solve CO problems under
positive linear constraints with the following merits. First, the positive
linear constraint covers a wide range of CO problems, indicating that our
approach breaks the generality bottleneck of existing non-autoregressive
networks. Second, compared to existing autoregressive neural network solvers,
our non-autoregressive networks have the advantages of higher efficiency and
preserving permutation invariance. Third, our offline unsupervised learning has
lower demand on high-quality labels, getting rid of the demand of optimal
labels in supervised learning. Fourth, our online differentiable search method
significantly improves the generalizability of our neural network solver to
unseen problems. We validate the effectiveness of this framework in solving
representative CO problems including facility location, max-set covering, and
traveling salesman problem. Our non-autoregressive neural solvers are
competitive to and can be even superior to state-of-the-art solvers such as
SCIP and Gurobi, especially when both efficiency and efficacy are considered.
Code is available at https://github.com/Thinklab-SJTU/NAR-CO-Solver</description>
      <guid isPermaLink="false">2409.04495v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>3D Branch Point Cloud Completion for Robotic Pruning in Apple Orchards</title>
      <link>http://arxiv.org/abs/2404.05953v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to IROS2024&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 机器人修剪是一个快速发展的研究领域，旨在应对农业劳动力短缺的问题。&lt;br&gt;&lt;h4&gt;2. 基本需求&lt;/h4&gt;   - 机器人修剪的基本需求是对树枝的详细几何和拓扑结构进行感知。&lt;br&gt;&lt;h4&gt;3. 点云的挑战&lt;/h4&gt;   - 在农业环境中获得的点云常常不完整，受到多种限制，影响后续机器人的修剪精度。&lt;br&gt;&lt;h4&gt;4. 解决方案&lt;/h4&gt;   - 本文通过基于仿真的深度神经网络解决点云质量问题，利用Real-to-Simulation（Real2Sim）数据生成管道。&lt;br&gt;&lt;h4&gt;5. Real2Sim的优点&lt;/h4&gt;   - Real2Sim消除了手动参数化的需求，并保证了仿真数据的现实性。&lt;br&gt;&lt;h4&gt;6. 应用于点云处理&lt;/h4&gt;   - 该仿真基础的神经网络被应用于对真实世界部分树枝进行点云完成和骨架化，无需额外的真实世界训练。&lt;br&gt;&lt;h4&gt;7. 仿真与真实的结果&lt;/h4&gt;   - Sim2Real的定性完成和骨架化结果展示了模型在几何重建和拓扑预测方面的卓越能力。&lt;br&gt;&lt;h4&gt;8. 定量评估&lt;/h4&gt;   - 通过比较原始不完整数据和完整数据的树枝级特征表征误差，定量评估Sim2Real性能。&lt;br&gt;&lt;h4&gt;9. 误差减少&lt;/h4&gt;   - 使用最佳完整数据，树枝直径和角度估计的平均绝对误差（MAE）分别降低了75%和8%，表明Real2Sim数据在零-shot泛化设置中的有效性。&lt;br&gt;&lt;h4&gt;10. 对修剪精度的贡献&lt;/h4&gt;    - 特征表征的改善提高了机器人修剪的精度和效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-04-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robotic branch pruning is a significantly growing research area to cope with
the shortage of labor force in the context of agriculture. One fundamental
requirement in robotic pruning is the perception of detailed geometry and
topology of branches. However, the point clouds obtained in agricultural
settings often exhibit incompleteness due to several constraints, thereby
restricting the accuracy of downstream robotic pruning. In this work, we
addressed the issue of point cloud quality through a simulation-based deep
neural network, leveraging a Real-to-Simulation (Real2Sim) data generation
pipeline that not only eliminates the need for manual parameterization but also
guarantees the realism of simulated data. The simulation-based neural network
was applied to jointly perform point cloud completion and skeletonization on
real-world partial branches, without additional real-world training. The
Sim2Real qualitative completion and skeletonization results showed the model's
remarkable capability for geometry reconstruction and topology prediction.
Additionally, we quantitatively evaluated the Sim2Real performance by comparing
branch-level trait characterization errors using raw incomplete data and
complete data. The Mean Absolute Error (MAE) reduced by 75% and 8% for branch
diameter and branch angle estimation, respectively, using the best complete
data, which indicates the effectiveness of the Real2Sim data in a zero-shot
generalization setting. The characterization improvements contributed to the
precision and efficacy of robotic branch pruning.</description>
      <guid isPermaLink="false">2404.05953v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>MambaPlace:Text-to-Point-Cloud Cross-Modal Place Recognition with Attention Mamba Mechanisms</title>
      <link>http://arxiv.org/abs/2408.15740v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 视觉语言位置识别（VLVPR）通过结合图像中的自然语言描述来增强机器人定位性能。&lt;br&gt;&lt;h4&gt;2. 方法论&lt;/h4&gt;   - VLVPR利用语言信息指导机器人进行位置匹配，克服仅依赖视觉的局限性。&lt;br&gt;&lt;h4&gt;3. 多模态融合的核心&lt;/h4&gt;   - 多模态融合的本质在于挖掘不同模态之间的互补信息。&lt;br&gt;&lt;h4&gt;4. 现有方法的不足&lt;/h4&gt;   - 一般的融合方法依赖传统神经架构，无法有效捕捉跨模态交互的动态，尤其是在复杂的内部和外部模态相关性存在时。&lt;br&gt;&lt;h4&gt;5. 提出的新框架&lt;/h4&gt;   - 本文提出了一种新颖的从粗到细、端到端连接的跨模态位置识别框架，称为MambaPlace。&lt;br&gt;&lt;h4&gt;6. 粗定位阶段&lt;/h4&gt;   - 在粗定位阶段，文本描述和3D点云分别通过预训练的T5和实例编码器进行编码。&lt;br&gt;   - 然后，使用文本注意力Mamba（TAM）和点云Mamba（PCM）进行数据增强和对齐。&lt;br&gt;&lt;h4&gt;7. 细定位阶段&lt;/h4&gt;   - 在随后的细定位阶段，文本描述和3D点云的特征进行跨模态融合，并通过级联跨注意力Mamba（CCAM）进一步增强。&lt;br&gt;&lt;h4&gt;8. 定位偏移预测&lt;/h4&gt;   - 最终，从融合的文本点云特征中预测位置信息，实现最精准的定位。&lt;br&gt;&lt;h4&gt;9. 实验结果&lt;/h4&gt;   - 大量实验表明，MambaPlace在KITTI360Pose数据集上相比现有最先进的方法实现了更高的定位准确性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/CV4RA/MambaPlace&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision Language Place Recognition (VLVPR) enhances robot localization
performance by incorporating natural language descriptions from images. By
utilizing language information, VLVPR directs robot place matching, overcoming
the constraint of solely depending on vision. The essence of multimodal fusion
lies in mining the complementary information between different modalities.
However, general fusion methods rely on traditional neural architectures and
are not well equipped to capture the dynamics of cross modal interactions,
especially in the presence of complex intra modal and inter modal correlations.
To this end, this paper proposes a novel coarse to fine and end to end
connected cross modal place recognition framework, called MambaPlace. In the
coarse localization stage, the text description and 3D point cloud are encoded
by the pretrained T5 and instance encoder, respectively. They are then
processed using Text Attention Mamba (TAM) and Point Clouds Mamba (PCM) for
data enhancement and alignment. In the subsequent fine localization stage, the
features of the text description and 3D point cloud are cross modally fused and
further enhanced through cascaded Cross Attention Mamba (CCAM). Finally, we
predict the positional offset from the fused text point cloud features,
achieving the most accurate localization. Extensive experiments show that
MambaPlace achieves improved localization accuracy on the KITTI360Pose dataset
compared to the state of the art methods.</description>
      <guid isPermaLink="false">2408.15740v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>ClearDepth: Enhanced Stereo Perception of Transparent Objects for Robotic Manipulation</title>
      <link>http://arxiv.org/abs/2409.08926v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 7 figures&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 透明物体深度感知的挑战&lt;/h4&gt;   - 透明物体的深度感知在日常生活和物流中面临挑战，主要是由于标准3D传感器无法准确捕捉透明或反射表面的深度。&lt;br&gt;&lt;h4&gt;2. 影响应用&lt;/h4&gt;   - 这种限制显著影响依赖深度图和点云的应用，尤其是在机器人操作中。&lt;br&gt;&lt;h4&gt;3. 新算法的开发&lt;/h4&gt;   - 开发了一种基于视觉变换器的算法，用于透明物体的立体深度恢复。&lt;br&gt;&lt;h4&gt;4. 特征后融合模块&lt;/h4&gt;   - 该方法配备了创新的特征后融合模块，通过图像中的结构特征提高深度恢复的准确性。&lt;br&gt;&lt;h4&gt;5. 数据集收集成本&lt;/h4&gt;   - 为解决与透明物体的立体摄像头感知相关的数据集收集高成本问题，采用了参数对齐、领域自适应和物理真实的Sim2Real模拟进行高效数据生成。&lt;br&gt;&lt;h4&gt;6. AI算法加速&lt;/h4&gt;   - 数据生成过程通过AI算法加速。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 实验结果显示，该模型在现实场景中具有出色的Sim2Real泛化能力，能够精确映射透明物体的深度。&lt;br&gt;&lt;h4&gt;8. 应用于机器人操作&lt;/h4&gt;   - 该技术有助于机器人操作中的透明物体的深度感知。&lt;br&gt;&lt;h4&gt;9. 项目详细信息&lt;/h4&gt;   - 项目详细信息可在 [ClearDepth网站](https://sites.google.com/view/cleardepth/) 查阅。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transparent object depth perception poses a challenge in everyday life and
logistics, primarily due to the inability of standard 3D sensors to accurately
capture depth on transparent or reflective surfaces. This limitation
significantly affects depth map and point cloud-reliant applications,
especially in robotic manipulation. We developed a vision transformer-based
algorithm for stereo depth recovery of transparent objects. This approach is
complemented by an innovative feature post-fusion module, which enhances the
accuracy of depth recovery by structural features in images. To address the
high costs associated with dataset collection for stereo camera-based
perception of transparent objects, our method incorporates a parameter-aligned,
domain-adaptive, and physically realistic Sim2Real simulation for efficient
data generation, accelerated by AI algorithm. Our experimental results
demonstrate the model's exceptional Sim2Real generalizability in real-world
scenarios, enabling precise depth mapping of transparent objects to assist in
robotic manipulation. Project details are available at
https://sites.google.com/view/cleardepth/ .</description>
      <guid isPermaLink="false">2409.08926v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive Meta-Domain Transfer Learning (AMDTL): A Novel Approach for Knowledge Transfer in AI</title>
      <link>http://arxiv.org/abs/2409.06800v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 介绍新方法&lt;/h4&gt;   - 本文提出了一种新方法，称为自适应元领域迁移学习（AMDTL），结合了元学习的原理和领域特定的适应策略。&lt;br&gt;&lt;h4&gt;2. 目标&lt;/h4&gt;   - AMDTL旨在提高人工智能模型在多样化和未知领域中的迁移能力。&lt;br&gt;&lt;h4&gt;3. 解决的挑战&lt;/h4&gt;   - 该方法解决了迁移学习中的主要挑战，包括领域不对齐、负迁移和灾难性遗忘。&lt;br&gt;&lt;h4&gt;4. 混合框架&lt;/h4&gt;   - AMDTL采用混合框架，强调泛化能力和上下文特化。&lt;br&gt;&lt;h4&gt;5. 框架组成&lt;/h4&gt;   - 集成了一个在多样化任务分布上训练的元学习器，使用对抗训练技术对齐领域特征分布，以及基于上下文领域嵌入的动态特征调节机制。&lt;br&gt;&lt;h4&gt;6. 实验结果&lt;/h4&gt;   - 在基准数据集上的实验结果表明，AMDTL在准确性、适应效率和鲁棒性方面优于现有的迁移学习方法。&lt;br&gt;&lt;h4&gt;7. 理论与实践基础&lt;/h4&gt;   - 该研究为AMDTL在多个领域的应用提供了坚实的理论和实践基础。&lt;br&gt;&lt;h4&gt;8. 未来前景&lt;/h4&gt;   - 开辟了开发更具适应性和包容性的人工智能系统的新视角。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/mlaurelli/amdtl&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents Adaptive Meta-Domain Transfer Learning (AMDTL), a novel
methodology that combines principles of meta-learning with domain-specific
adaptations to enhance the transferability of artificial intelligence models
across diverse and unknown domains. AMDTL aims to address the main challenges
of transfer learning, such as domain misalignment, negative transfer, and
catastrophic forgetting, through a hybrid framework that emphasizes both
generalization and contextual specialization. The framework integrates a
meta-learner trained on a diverse distribution of tasks, adversarial training
techniques for aligning domain feature distributions, and dynamic feature
regulation mechanisms based on contextual domain embeddings. Experimental
results on benchmark datasets demonstrate that AMDTL outperforms existing
transfer learning methodologies in terms of accuracy, adaptation efficiency,
and robustness. This research provides a solid theoretical and practical
foundation for the application of AMDTL in various fields, opening new
perspectives for the development of more adaptable and inclusive AI systems.</description>
      <guid isPermaLink="false">2409.06800v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>PointRegGPT: Boosting 3D Point Cloud Registration using Generative Point-Cloud Pairs for Training</title>
      <link>http://arxiv.org/abs/2407.14054v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To appear at the European Conference on Computer Vision (ECCV) 2024&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 数据的重要性&lt;/h4&gt;   - 数据在基于学习的方法中，对于3D点云注册至关重要。&lt;br&gt;&lt;h4&gt;2. 现有数据集的挑战&lt;/h4&gt;   - 真实世界的数据集构建成本高，而基于渲染的合成数据存在领域差距问题。&lt;br&gt;&lt;h4&gt;3. 提出的方法&lt;/h4&gt;   - 本文介绍了PointRegGPT，通过生成点云对来提升3D点云注册的效果。&lt;br&gt;&lt;h4&gt;4. 训练对的生成&lt;/h4&gt;   - 通过对单一深度图应用随机相机运动，将其重新投影为目标深度图，从而生成训练对。&lt;br&gt;&lt;h4&gt;5. 增强数据真实感&lt;/h4&gt;   - 采用深度修复扩散生成模型，利用重新投影的源深度图对目标深度图进行处理，以提高数据的真实感。&lt;br&gt;&lt;h4&gt;6. 深度修正模块&lt;/h4&gt;   - 设计了深度修正模块，以减轻在重新投影过程中因点穿透造成的伪影。&lt;br&gt;&lt;h4&gt;7. 创新性&lt;/h4&gt;   - 这是首次采用生成方法探索室内点云注册的真实数据生成。&lt;br&gt;&lt;h4&gt;8. 性能提升&lt;/h4&gt;   - 采用该方法后，多个近期算法的性能显著提升，并在两个常见基准上持续实现最先进成果。&lt;br&gt;&lt;h4&gt;9. 资源发布&lt;/h4&gt;   - 代码和数据集将会发布在 [GitHub](https://github.com/Chen-Suyi/PointRegGPT)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-07-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/chen-suyi/pointreggpt&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Data plays a crucial role in training learning-based methods for 3D point
cloud registration. However, the real-world dataset is expensive to build,
while rendering-based synthetic data suffers from domain gaps. In this work, we
present PointRegGPT, boosting 3D point cloud registration using generative
point-cloud pairs for training. Given a single depth map, we first apply a
random camera motion to re-project it into a target depth map. Converting them
to point clouds gives a training pair. To enhance the data realism, we
formulate a generative model as a depth inpainting diffusion to process the
target depth map with the re-projected source depth map as the condition. Also,
we design a depth correction module to alleviate artifacts caused by point
penetration during the re-projection. To our knowledge, this is the first
generative approach that explores realistic data generation for indoor point
cloud registration. When equipped with our approach, several recent algorithms
can improve their performance significantly and achieve SOTA consistently on
two common benchmarks. The code and dataset will be released on
https://github.com/Chen-Suyi/PointRegGPT.</description>
      <guid isPermaLink="false">2407.14054v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>PADLoC: LiDAR-Based Deep Loop Closure Detection and Registration Using Panoptic Attention</title>
      <link>http://arxiv.org/abs/2209.09699v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 图基SLAM系统的关键组成部分是检测轨迹中的环闭合，以减少由于里程计累积的漂移。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限&lt;/h4&gt;   - 大多数基于LiDAR的方法仅使用几何信息，忽略了场景的语义信息。&lt;br&gt;&lt;h4&gt;3. 研究贡献&lt;/h4&gt;   - 本文介绍了PADLoC，旨在实现LiDAR基础上的环闭合检测和注册。&lt;br&gt;&lt;h4&gt;4. 创新方法&lt;/h4&gt;   - 提出了一个新型的基于变换器的头部，用于点云匹配和注册。&lt;br&gt;   - 在训练过程中利用全景信息，提出了一种新型损失函数，将匹配问题重构为语义标签的分类任务和实例标签的图连接分配。&lt;br&gt;&lt;h4&gt;5. 推理过程&lt;/h4&gt;   - 在推理时，PADLoC不需要全景注释，使其比其他方法更具通用性。&lt;br&gt;&lt;h4&gt;6. 性能提升&lt;/h4&gt;   - 通过使用两个共享的匹配和注册头，并交换其源输入和目标输入，增强了前后一致性，从而提高了整体性能。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 在多个真实世界数据集上的广泛评估表明，PADLoC实现了最先进的结果。&lt;br&gt;&lt;h4&gt;8. 附加资源&lt;/h4&gt;   - 本研究的代码可在 [PADLoC网站](http://padloc.cs.uni-freiburg.de) 上公开获取。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2022-09-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/LRA.2023.3239312&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/robot-learning-freiburg/PADLoC&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A key component of graph-based SLAM systems is the ability to detect loop
closures in a trajectory to reduce the drift accumulated over time from the
odometry. Most LiDAR-based methods achieve this goal by using only the
geometric information, disregarding the semantics of the scene. In this work,
we introduce PADLoC for joint loop closure detection and registration in
LiDAR-based SLAM frameworks. We propose a novel transformer-based head for
point cloud matching and registration, and to leverage panoptic information
during training time. In particular, we propose a novel loss function that
reframes the matching problem as a classification task for the semantic labels
and as a graph connectivity assignment for the instance labels. During
inference, PADLoC does not require panoptic annotations, making it more
versatile than other methods. Additionally, we show that using two shared
matching and registration heads with their source and target inputs swapped
increases the overall performance by enforcing forward-backward consistency. We
perform extensive evaluations of PADLoC on multiple real-world datasets
demonstrating that it achieves state-of-the-art results. The code of our work
is publicly available at http://padloc.cs.uni-freiburg.de.</description>
      <guid isPermaLink="false">2209.09699v3</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Can Graph Reordering Speed Up Graph Neural Network Training? An Experimental Study</title>
      <link>http://arxiv.org/abs/2409.11129v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To be published in proceedings of the 51st International Conference
  on Very Large Data Bases (VLDB), September 1-5, 2025&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 图神经网络（GNNs）是一种能够在图结构数据上学习的神经网络。&lt;br&gt;&lt;h4&gt;2. 挑战&lt;/h4&gt;   - 在大规模图上训练GNNs面临挑战，主要由于需要迭代聚合来自邻近顶点的高维特征，并进行神经网络操作。&lt;br&gt;   - 图的稀疏性导致了亚优化的内存访问模式和较长的训练时间。&lt;br&gt;&lt;h4&gt;3. 图重排序&lt;/h4&gt;   - 图重排序是一种优化策略，旨在改善图数据布局，已被证明能加速图分析工作负载。&lt;br&gt;   - 然而，图重排序对GNN训练性能的影响尚未被研究。&lt;br&gt;&lt;h4&gt;4. 复杂性&lt;/h4&gt;   - 重排序对GNN性能的影响复杂，需考虑多个方面，包括：&lt;br&gt;     - GNN超参数（如层数、隐藏维度和特征大小）。&lt;br&gt;     - 神经网络操作。&lt;br&gt;     - 大量中间顶点状态。&lt;br&gt;     - GPU加速。&lt;br&gt;&lt;h4&gt;5. 研究贡献&lt;/h4&gt;   - 本研究通过对12种重排序策略在两个先进的GNN系统（PyTorch Geometric和Deep Graph Library）中的实证评估，填补了这一研究空白。&lt;br&gt;&lt;h4&gt;6. 实验结果&lt;/h4&gt;   - 结果表明，图重排序在CPU和GPU训练中均有效减少训练时间。&lt;br&gt;   - GNN超参数对重排序的有效性有影响。&lt;br&gt;   - 重排序指标在选择重排序策略中起重要作用。&lt;br&gt;   - 轻量级重排序在基于GPU的训练中表现优于基于CPU的训练。&lt;br&gt;   - 投入的重排序时间在许多情况下可以被摊销。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/nikolaimerkel/reordering&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) are a type of neural network capable of learning
on graph-structured data. However, training GNNs on large-scale graphs is
challenging due to iterative aggregations of high-dimensional features from
neighboring vertices within sparse graph structures combined with neural
network operations. The sparsity of graphs frequently results in suboptimal
memory access patterns and longer training time. Graph reordering is an
optimization strategy aiming to improve the graph data layout. It has shown to
be effective to speed up graph analytics workloads, but its effect on the
performance of GNN training has not been investigated yet. The generalization
of reordering to GNN performance is nontrivial, as multiple aspects must be
considered: GNN hyper-parameters such as the number of layers, the number of
hidden dimensions, and the feature size used in the GNN model, neural network
operations, large intermediate vertex states, and GPU acceleration.
  In our work, we close this gap by performing an empirical evaluation of 12
reordering strategies in two state-of-the-art GNN systems, PyTorch Geometric
and Deep Graph Library. Our results show that graph reordering is effective in
reducing training time for CPU- and GPU-based training, respectively. Further,
we find that GNN hyper-parameters influence the effectiveness of reordering,
that reordering metrics play an important role in selecting a reordering
strategy, that lightweight reordering performs better for GPU-based than for
CPU-based training, and that invested reordering time can in many cases be
amortized.</description>
      <guid isPermaLink="false">2409.11129v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>From Words to Wheels: Automated Style-Customized Policy Generation for Autonomous Driving</title>
      <link>http://arxiv.org/abs/2409.11694v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 7 figures&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 技术背景&lt;/h4&gt;   - 自主驾驶技术迅速发展，基础模型提升了交互性和用户体验。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限&lt;/h4&gt;   - 当前的自主车辆在基于命令的驾驶风格实现上存在显著限制。&lt;br&gt;   - 大多数现有方法依赖于需要专家输入的预定义驾驶风格，或使用数据驱动技术（如逆强化学习）从驾驶数据中提取风格。&lt;br&gt;&lt;h4&gt;3. 主要挑战&lt;/h4&gt;   - 难以获取特定驾驶数据以进行风格匹配（例如，Robotaxis）。&lt;br&gt;   - 难以将驾驶风格指标与用户偏好对齐。&lt;br&gt;   - 受限于预先存在的风格，限制了定制化和对新命令的泛化能力。&lt;br&gt;&lt;h4&gt;4. 研究贡献&lt;/h4&gt;   - 本文提出了Words2Wheels框架，能够自动生成基于自然语言用户命令的定制驾驶策略。&lt;br&gt;&lt;h4&gt;5. 核心方法&lt;/h4&gt;   - 使用风格定制奖励函数生成不依赖于先前驾驶数据的定制驾驶策略。&lt;br&gt;   - 结合大型语言模型和驾驶风格数据库，高效检索、适应和泛化驾驶风格。&lt;br&gt;&lt;h4&gt;6. 评估机制&lt;/h4&gt;   - 统计评估模块确保生成的驾驶风格与用户偏好一致。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 实验表明，Words2Wheels在准确性、泛化能力和适应性方面超越了现有方法，为定制化的自主驾驶行为提供了新解决方案。&lt;br&gt;&lt;h4&gt;8. 附加资源&lt;/h4&gt;   - 代码和演示可在[Words2Wheels网站](https://yokhon.github.io/Words2Wheels/)获取。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous driving technology has witnessed rapid advancements, with
foundation models improving interactivity and user experiences. However,
current autonomous vehicles (AVs) face significant limitations in delivering
command-based driving styles. Most existing methods either rely on predefined
driving styles that require expert input or use data-driven techniques like
Inverse Reinforcement Learning to extract styles from driving data. These
approaches, though effective in some cases, face challenges: difficulty
obtaining specific driving data for style matching (e.g., in Robotaxis),
inability to align driving style metrics with user preferences, and limitations
to pre-existing styles, restricting customization and generalization to new
commands. This paper introduces Words2Wheels, a framework that automatically
generates customized driving policies based on natural language user commands.
Words2Wheels employs a Style-Customized Reward Function to generate a
Style-Customized Driving Policy without relying on prior driving data. By
leveraging large language models and a Driving Style Database, the framework
efficiently retrieves, adapts, and generalizes driving styles. A Statistical
Evaluation module ensures alignment with user preferences. Experimental results
demonstrate that Words2Wheels outperforms existing methods in accuracy,
generalization, and adaptability, offering a novel solution for customized AV
driving behavior. Code and demo available at
https://yokhon.github.io/Words2Wheels/.</description>
      <guid isPermaLink="false">2409.11694v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>One-Class Meta-Learning: Towards Generalizable Few-Shot Open-Set Classification</title>
      <link>http://arxiv.org/abs/2109.06859v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages, submitted to BMVC 2021&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 现实世界中的分类任务常常需要在开放集环境中进行，尤其是少样本学习问题，由于每个已知类别的样本量小，现有的开放集方法效果有限。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 大多数多类少样本方法仅限于闭集场景，无法有效处理开放集问题。&lt;br&gt;&lt;h4&gt;3. 研究目标&lt;/h4&gt;   - 本文旨在解决少样本开放集分类问题，首先提出少样本单类分类的方法，然后扩展到少样本多类开放集分类。&lt;br&gt;&lt;h4&gt;4. 方法概述&lt;/h4&gt;   - 提出了两种独立的少样本单类分类方法：&lt;br&gt;     - **Meta Binary Cross-Entropy (Meta-BCE)**：为单类分类学习单独的特征表示。&lt;br&gt;     - **One-Class Meta-Learning (OCML)**：根据标准多类特征表示生成单类分类器。&lt;br&gt;&lt;h4&gt;5. 方法优点&lt;/h4&gt;   - 这两种方法可以增强任何现有的少样本学习方法，并且无需重新训练即可在少样本多类开放集环境中工作，同时保持闭集性能不下降。&lt;br&gt;&lt;h4&gt;6. 效果评估&lt;/h4&gt;   - 本文展示了两种方法在不同问题设置下的优缺点，并在三个标准基准数据集（miniImageNet、tieredImageNet和Caltech-UCSD-Birds-200-2011）上进行了评估。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 结果表明，两种方法在少样本多类开放集和少样本单类任务上超过了当前最先进的方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2021-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-world classification tasks are frequently required to work in an
open-set setting. This is especially challenging for few-shot learning problems
due to the small sample size for each known category, which prevents existing
open-set methods from working effectively; however, most multiclass few-shot
methods are limited to closed-set scenarios. In this work, we address the
problem of few-shot open-set classification by first proposing methods for
few-shot one-class classification and then extending them to few-shot
multiclass open-set classification. We introduce two independent few-shot
one-class classification methods: Meta Binary Cross-Entropy (Meta-BCE), which
learns a separate feature representation for one-class classification, and
One-Class Meta-Learning (OCML), which learns to generate one-class classifiers
given standard multiclass feature representation. Both methods can augment any
existing few-shot learning method without requiring retraining to work in a
few-shot multiclass open-set setting without degrading its closed-set
performance. We demonstrate the benefits and drawbacks of both methods in
different problem settings and evaluate them on three standard benchmark
datasets, miniImageNet, tieredImageNet, and Caltech-UCSD-Birds-200-2011, where
they surpass the state-of-the-art methods in the few-shot multiclass open-set
and few-shot one-class tasks.</description>
      <guid isPermaLink="false">2109.06859v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>TripletTrack: 3D Object Tracking using Triplet Embeddings and LSTM</title>
      <link>http://arxiv.org/abs/2210.16204v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to CVPR 2022 Workshop on Autonomous Driving&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 3D物体跟踪是自动驾驶系统中的关键任务，对于系统感知周围环境至关重要。&lt;br&gt;&lt;h4&gt;2. 传感器选择&lt;/h4&gt;   - 目前对仅依赖低成本传感器（如摄像头）的自动驾驶算法的兴趣日益增加。&lt;br&gt;&lt;h4&gt;3. 研究目标&lt;/h4&gt;   - 本文探讨结合三元嵌入和运动表示进行3D物体跟踪的有效性。&lt;br&gt;&lt;h4&gt;4. 方法概述&lt;/h4&gt;   - 从现成的3D物体检测器出发，应用一种跟踪机制，通过局部物体特征嵌入和运动描述符计算亲和力分数来匹配物体。&lt;br&gt;&lt;h4&gt;5. 特征嵌入&lt;/h4&gt;   - 特征嵌入经过训练，包含视觉外观和单目3D物体特征的信息。&lt;br&gt;&lt;h4&gt;6. 运动描述符&lt;/h4&gt;   - 运动描述符提供强大的物体轨迹表示。&lt;br&gt;&lt;h4&gt;7. 性能表现&lt;/h4&gt;   - 本方法有效地重新识别物体，并在遮挡、漏检情况下表现可靠且准确，能够在不同视场中检测物体的重新出现。&lt;br&gt;&lt;h4&gt;8. 实验评估&lt;/h4&gt;   - 实验结果显示，本方法在nuScenes数据集上大幅超越当前最先进的方法，并在KITTI数据集上也取得了竞争力的结果。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2022-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/CVPRW56347.2022.00496&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D object tracking is a critical task in autonomous driving systems. It plays
an essential role for the system's awareness about the surrounding environment.
At the same time there is an increasing interest in algorithms for autonomous
cars that solely rely on inexpensive sensors, such as cameras. In this paper we
investigate the use of triplet embeddings in combination with motion
representations for 3D object tracking. We start from an off-the-shelf 3D
object detector, and apply a tracking mechanism where objects are matched by an
affinity score computed on local object feature embeddings and motion
descriptors. The feature embeddings are trained to include information about
the visual appearance and monocular 3D object characteristics, while motion
descriptors provide a strong representation of object trajectories. We will
show that our approach effectively re-identifies objects, and also behaves
reliably and accurately in case of occlusions, missed detections and can detect
re-appearance across different field of views. Experimental evaluation shows
that our approach outperforms state-of-the-art on nuScenes by a large margin.
We also obtain competitive results on KITTI.</description>
      <guid isPermaLink="false">2210.16204v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Finetuning CLIP to Reason about Pairwise Differences</title>
      <link>http://arxiv.org/abs/2409.09721v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 视觉-语言模型（VLMs），如CLIP，通过对比学习训练文本和图像对，生成对齐的图像和文本嵌入，这些嵌入在许多下游任务中非常有用。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - CLIP的嵌入空间缺乏纯文本替代方法的一些结构特性。例如，文本嵌入在嵌入空间中能够通过向量运算满足类比关系，而CLIP缺乏这种特性。&lt;br&gt;&lt;h4&gt;3. 研究目标&lt;/h4&gt;   - 提出一种新方法，原生地以对比方式训练CLIP，以便更好地推理嵌入空间中的差异。&lt;br&gt;&lt;h4&gt;4. 方法概述&lt;/h4&gt;   - 对CLIP进行微调，使得图像嵌入空间中的差异与图像差异的文本描述相对应，这些描述通过大型语言模型在图像-标题配对数据集上合成生成。&lt;br&gt;&lt;h4&gt;5. 实验结果&lt;/h4&gt;   - 该方法显著提高了根据特定属性（如“象比猫大”）对图像进行排名的能力，这对检索或构建基于属性的分类器非常有用。&lt;br&gt;   - 在许多下游图像分类任务中，零-shot分类性能也得到了改善。&lt;br&gt;&lt;h4&gt;6. 新机制&lt;/h4&gt;   - 引入一种新的推理机制，称为比较提示（comparative prompting），利用对感兴趣类别之间差异的文本描述的先验知识，进一步提高分类性能。&lt;br&gt;&lt;h4&gt;7. 几何特性&lt;/h4&gt;   - 最终，所得到的嵌入在嵌入空间中遵循更高程度的几何特性，这在文本到图像生成任务中尤为明显。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language models (VLMs) such as CLIP are trained via contrastive
learning between text and image pairs, resulting in aligned image and text
embeddings that are useful for many downstream tasks. A notable drawback of
CLIP, however, is that the resulting embedding space seems to lack some of the
structure of their purely text-based alternatives. For instance, while text
embeddings have been long noted to satisfy \emph{analogies} in embedding space
using vector arithmetic, CLIP has no such property. In this paper, we propose
an approach to natively train CLIP in a contrastive manner to reason about
differences in embedding space. We finetune CLIP so that the differences in
image embedding space correspond to \emph{text descriptions of the image
differences}, which we synthetically generate with large language models on
image-caption paired datasets. We first demonstrate that our approach yields
significantly improved capabilities in ranking images by a certain attribute
(e.g., elephants are larger than cats), which is useful in retrieval or
constructing attribute-based classifiers, and improved zeroshot classification
performance on many downstream image classification tasks. In addition, our
approach enables a new mechanism for inference that we refer to as comparative
prompting, where we leverage prior knowledge of text descriptions of
differences between classes of interest, achieving even larger performance
gains in classification. Finally, we illustrate that the resulting embeddings
obey a larger degree of geometric properties in embedding space, such as in
text-to-image generation.</description>
      <guid isPermaLink="false">2409.09721v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>fVDB: A Deep-Learning Framework for Sparse, Large-Scale, and High-Performance Spatial Intelligence</title>
      <link>http://arxiv.org/abs/2407.01781v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究主题&lt;/h4&gt;   - 提出了fVDB，一个新颖的GPU优化框架，专门用于大规模3D数据的深度学习。&lt;br&gt;&lt;h4&gt;2. 功能概述&lt;/h4&gt;   - fVDB提供了一整套可微分原语，用于构建深度学习架构，支持3D学习中的常见任务，如卷积、池化、注意力机制、光线追踪和网格化等。&lt;br&gt;&lt;h4&gt;3. 特性比较&lt;/h4&gt;   - fVDB的特性集（原语和操作符）远超现有框架，但效率没有损失，其操作符的性能与其他范围较窄的框架相匹配或更优。&lt;br&gt;&lt;h4&gt;4. 数据处理能力&lt;/h4&gt;   - fVDB能够处理比以往工作更大尺寸和空间分辨率的数据集，同时在小输入上保持竞争力的内存占用。&lt;br&gt;&lt;h4&gt;5. 技术创新&lt;/h4&gt;   - 采用单一新型的VDB索引网格加速结构，结合多项关键创新，包括：&lt;br&gt;     - GPU加速的稀疏网格构建。&lt;br&gt;     - 使用张量核心的卷积操作。&lt;br&gt;     - 采用层次数字微分分析器（HDDA）算法的快速光线追踪内核。&lt;br&gt;     - 锯齿张量的使用。&lt;br&gt;&lt;h4&gt;6. 与PyTorch的集成&lt;/h4&gt;   - fVDB与PyTorch完全集成，支持与现有工作流程的互操作性。&lt;br&gt;&lt;h4&gt;7. 应用示例&lt;/h4&gt;   - 在多个代表性任务上展示了fVDB的有效性，包括大规模点云分割、高分辨率3D生成建模、无限尺度神经辐射场和大规模点云重建。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-07-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3658226&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present fVDB, a novel GPU-optimized framework for deep learning on
large-scale 3D data. fVDB provides a complete set of differentiable primitives
to build deep learning architectures for common tasks in 3D learning such as
convolution, pooling, attention, ray-tracing, meshing, etc.
  fVDB simultaneously provides a much larger feature set (primitives and
operators) than established frameworks with no loss in efficiency: our
operators match or exceed the performance of other frameworks with narrower
scope. Furthermore, fVDB can process datasets with much larger footprint and
spatial resolution than prior works, while providing a competitive memory
footprint on small inputs. To achieve this combination of versatility and
performance, fVDB relies on a single novel VDB index grid acceleration
structure paired with several key innovations including GPU accelerated sparse
grid construction, convolution using tensorcores, fast ray tracing kernels
using a Hierarchical Digital Differential Analyzer algorithm (HDDA), and jagged
tensors.
  Our framework is fully integrated with PyTorch enabling interoperability with
existing pipelines, and we demonstrate its effectiveness on a number of
representative tasks such as large-scale point-cloud segmentation, high
resolution 3D generative modeling, unbounded scale Neural Radiance Fields, and
large-scale point cloud reconstruction.</description>
      <guid isPermaLink="false">2407.01781v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Weakly-Supervised Object Detection on Static Images through (Hallucinated) Motion</title>
      <link>http://arxiv.org/abs/2409.09616v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 动作在多种任务中引起了关注，但其作为弱监督目标检测（WSOD）静态图像中的一种模态的潜力尚未被探索。&lt;br&gt;&lt;h4&gt;2. 研究目标&lt;/h4&gt;   - 本研究旨在通过整合运动信息来增强WSOD方法。&lt;br&gt;&lt;h4&gt;3. 方法概述&lt;/h4&gt;   - 利用静态图像中的“幻觉运动”来提升图像数据集上的WSOD性能。&lt;br&gt;   - 采用Siamese网络增强运动相关的表示学习。&lt;br&gt;   - 通过运动归一化处理相机运动。&lt;br&gt;&lt;h4&gt;4. 选择性训练&lt;/h4&gt;   - 基于物体运动选择性训练图像，以提高检测效果。&lt;br&gt;&lt;h4&gt;5. 实验验证&lt;/h4&gt;   - 在COCO和YouTube-BB数据集上的实验验证表明，所提方法在性能上超越了当前最先进的方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While motion has garnered attention in various tasks, its potential as a
modality for weakly-supervised object detection (WSOD) in static images remains
unexplored. Our study introduces an approach to enhance WSOD methods by
integrating motion information. This method involves leveraging hallucinated
motion from static images to improve WSOD on image datasets, utilizing a
Siamese network for enhanced representation learning with motion, addressing
camera motion through motion normalization, and selectively training images
based on object motion. Experimental validation on the COCO and YouTube-BB
datasets demonstrates improvements over a state-of-the-art method.</description>
      <guid isPermaLink="false">2409.09616v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Dual-Level Cross-Modal Contrastive Clustering</title>
      <link>http://arxiv.org/abs/2409.04561v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages,4 figures&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 图像聚类是无监督学习的关键任务，涉及将图像分组到不同的聚类中，无需标签。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 以往的深度聚类方法虽然取得了显著成果，但主要关注图像本身的内在信息，忽视了外部监督知识对图像语义理解的提升。&lt;br&gt;&lt;h4&gt;3. 新兴技术&lt;/h4&gt;   - 最近，基于大规模数据集的视觉-语言预训练模型在多种下游任务中表现出色，但视觉表示学习与文本语义学习之间仍存在差距。&lt;br&gt;&lt;h4&gt;4. 研究目标&lt;/h4&gt;   - 本文提出一种新颖的图像聚类框架，称为双层跨模态对比聚类（DXMC），旨在解决上述挑战。&lt;br&gt;&lt;h4&gt;5. 方法步骤&lt;/h4&gt;   - **引入外部文本信息**：构建语义空间，并生成图像-文本对。&lt;br&gt;   - **特征提取**：将图像-文本对分别送入预训练的图像编码器和文本编码器，获得图像和文本嵌入。&lt;br&gt;   - **模型训练**：将嵌入送入四个精心设计的网络进行处理。&lt;br&gt;&lt;h4&gt;6. 学习策略&lt;/h4&gt;   - 进行双层跨模态对比学习，比较不同模态和不同层次的判别表示。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 在五个基准数据集上的广泛实验结果表明，所提方法优于现有的聚类方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/Regan-Zhang/DXMC&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Image clustering, which involves grouping images into different clusters
without labels, is a key task in unsupervised learning. Although previous deep
clustering methods have achieved remarkable results, they only explore the
intrinsic information of the image itself but overlook external supervision
knowledge to improve the semantic understanding of images. Recently,
visual-language pre-trained model on large-scale datasets have been used in
various downstream tasks and have achieved great results. However, there is a
gap between visual representation learning and textual semantic learning, and
how to properly utilize the representation of two different modalities for
clustering is still a big challenge. To tackle the challenges, we propose a
novel image clustering framwork, named Dual-level Cross-Modal Contrastive
Clustering (DXMC). Firstly, external textual information is introduced for
constructing a semantic space which is adopted to generate image-text pairs.
Secondly, the image-text pairs are respectively sent to pre-trained image and
text encoder to obtain image and text embeddings which subsquently are fed into
four well-designed networks. Thirdly, dual-level cross-modal contrastive
learning is conducted between discriminative representations of different
modalities and distinct level. Extensive experimental results on five benchmark
datasets demonstrate the superiority of our proposed method.</description>
      <guid isPermaLink="false">2409.04561v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Visual Prompt Engineering for Medical Vision Language Models in Radiology</title>
      <link>http://arxiv.org/abs/2408.15802v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ECCV 2024 Workshop on Emergent Visual Abilities and
  Limits of Foundation Models&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 医学图像分类在放射学领域面临显著挑战，特别是在对未见病理的泛化能力方面。&lt;br&gt;&lt;h4&gt;2. 现有方法&lt;/h4&gt;   - CLIP通过利用多模态学习，提供了一种有前景的解决方案，以改善零-shot分类性能。&lt;br&gt;&lt;h4&gt;3. 问题识别&lt;/h4&gt;   - 在医学领域，病变可能较小，且在嵌入空间中可能表现不佳。&lt;br&gt;&lt;h4&gt;4. 研究目标&lt;/h4&gt;   - 本文探索视觉提示工程的潜力，以增强视觉语言模型（VLMs）在放射学中的能力。&lt;br&gt;&lt;h4&gt;5. 方法介绍&lt;/h4&gt;   - 利用BiomedCLIP，该模型在大量生物医学图像-文本对上训练，研究在放射图像中直接嵌入视觉标记的影响，以引导模型关注关键区域。&lt;br&gt;&lt;h4&gt;6. 实验评估&lt;/h4&gt;   - 在JSRT数据集上进行评估，重点关注肺结节恶性肿瘤分类。&lt;br&gt;&lt;h4&gt;7. 结果分析&lt;/h4&gt;   - 结果表明，加入视觉提示（如箭头、圆圈和轮廓）显著提高了分类指标，包括AUROC、AUPRC、F1分数和准确率。&lt;br&gt;&lt;h4&gt;8. 模型可解释性&lt;/h4&gt;   - 研究提供了注意力图，展示了模型在临床相关区域的增强可解释性和关注点。&lt;br&gt;&lt;h4&gt;9. 研究结论&lt;/h4&gt;   - 这些发现强调了视觉提示工程作为一种简单而有效的方法，以提升医学图像分析中VLM的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Medical image classification in radiology faces significant challenges,
particularly in generalizing to unseen pathologies. In contrast, CLIP offers a
promising solution by leveraging multimodal learning to improve zero-shot
classification performance. However, in the medical domain, lesions can be
small and might not be well represented in the embedding space. Therefore, in
this paper, we explore the potential of visual prompt engineering to enhance
the capabilities of Vision Language Models (VLMs) in radiology. Leveraging
BiomedCLIP, trained on extensive biomedical image-text pairs, we investigate
the impact of embedding visual markers directly within radiological images to
guide the model's attention to critical regions. Our evaluation on the JSRT
dataset, focusing on lung nodule malignancy classification, demonstrates that
incorporating visual prompts $\unicode{x2013}$ such as arrows, circles, and
contours $\unicode{x2013}$ significantly improves classification metrics
including AUROC, AUPRC, F1 score, and accuracy. Moreover, the study provides
attention maps, showcasing enhanced model interpretability and focus on
clinically relevant areas. These findings underscore the efficacy of visual
prompt engineering as a straightforward yet powerful approach to advance VLM
performance in medical image analysis.</description>
      <guid isPermaLink="false">2408.15802v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Zero-shot Point Cloud Completion Via 2D Priors</title>
      <link>http://arxiv.org/abs/2404.06814v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究目的&lt;/h4&gt;   - 3D点云补全旨在从部分观察到的点云中恢复完整形状。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 传统的补全方法通常依赖于大量点云数据进行训练，其有效性往往局限于与训练期间看到的对象类别相似的情况。&lt;br&gt;&lt;h4&gt;3. 提出的新框架&lt;/h4&gt;   - 本文提出一种零-shot框架，旨在完成任何未见类别的部分观察点云。&lt;br&gt;&lt;h4&gt;4. 方法论&lt;/h4&gt;   - 利用高斯点渲染（Gaussian Splatting），开发了点云着色（Point Cloud Colorization）和零-shot分形补全（Zero-shot Fractal Completion）技术，这些技术利用预训练扩散模型的2D先验信息来推断缺失区域。&lt;br&gt;&lt;h4&gt;5. 实验结果&lt;/h4&gt;   - 在合成和真实扫描点云的实验结果表明，所提方法在补全各种对象上优于现有方法，且无需特定训练数据。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-04-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D point cloud completion is designed to recover complete shapes from
partially observed point clouds. Conventional completion methods typically
depend on extensive point cloud data for training %, with their effectiveness
often constrained to object categories similar to those seen during training.
In contrast, we propose a zero-shot framework aimed at completing partially
observed point clouds across any unseen categories. Leveraging point rendering
via Gaussian Splatting, we develop techniques of Point Cloud Colorization and
Zero-shot Fractal Completion that utilize 2D priors from pre-trained diffusion
models to infer missing regions. Experimental results on both synthetic and
real-world scanned point clouds demonstrate that our approach outperforms
existing methods in completing a variety of objects without any requirement for
specific training data.</description>
      <guid isPermaLink="false">2404.06814v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Learning Feature Descriptors for Pre- and Intra-operative Point Cloud Matching for Laparoscopic Liver Registration</title>
      <link>http://arxiv.org/abs/2211.03688v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究目的&lt;/h4&gt;   - 在腹腔镜肝脏手术（LLS）中，通过将3D术前模型与从腹腔镜视频重建的术中部分表面进行配准，可以将术前信息叠加到术中场景上。&lt;br&gt;&lt;h4&gt;2. 研究背景&lt;/h4&gt;   - 本研究探索了基于学习的特征描述符的使用，这是在腹腔镜肝脏配准中首次探索的领域。同时，缺乏用于训练和评估学习型描述符的数据集。&lt;br&gt;&lt;h4&gt;3. 数据集介绍&lt;/h4&gt;   - 提出了LiverMatch数据集，包括16个术前模型及其模拟的术中3D表面。&lt;br&gt;&lt;h4&gt;4. 方法概述&lt;/h4&gt;   - 提出了LiverMatch网络，旨在为每个点输出特征描述符、可见性评分和匹配点。&lt;br&gt;&lt;h4&gt;5. 实验比较&lt;/h4&gt;   - 将LiverMatch网络与最接近的网络和基于直方图的3D描述符进行了比较，测试数据集包含两个未见的术前模型和1400个术中表面。&lt;br&gt;&lt;h4&gt;6. 结果分析&lt;/h4&gt;   - 结果表明，LiverMatch网络能够预测比其他两种方法更准确且密集的匹配，并能无缝集成到基于RANSAC-ICP的配准算法中，实现准确的初始对齐。&lt;br&gt;&lt;h4&gt;7. 研究结论&lt;/h4&gt;   - 在LLR中使用基于学习的特征描述符是有前景的，因为它可以帮助实现准确的初始刚性对齐，为后续的非刚性配准提供初始化。&lt;br&gt;&lt;h4&gt;8. 后续计划&lt;/h4&gt;   - 研究团队将在论文接受后发布数据集和代码。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2022-11-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Purpose: In laparoscopic liver surgery (LLS), pre-operative information can
be overlaid onto the intra-operative scene by registering a 3D pre-operative
model to the intra-operative partial surface reconstructed from the
laparoscopic video. To assist with this task, we explore the use of
learning-based feature descriptors, which, to our best knowledge, have not been
explored for use in laparoscopic liver registration. Furthermore, a dataset to
train and evaluate the use of learning-based descriptors does not exist.
  Methods: We present the LiverMatch dataset consisting of 16 preoperative
models and their simulated intra-operative 3D surfaces. We also propose the
LiverMatch network designed for this task, which outputs per-point feature
descriptors, visibility scores, and matched points.
  Results: We compare the proposed LiverMatch network with anetwork closest to
LiverMatch, and a histogram-based 3D descriptor on the testing split of the
LiverMatch dataset, which includes two unseen pre-operative models and 1400
intra-operative surfaces. Results suggest that our LiverMatch network can
predict more accurate and dense matches than the other two methods and can be
seamlessly integrated with a RANSAC-ICP-based registration algorithm to achieve
an accurate initial alignment.
  Conclusion: The use of learning-based feature descriptors in LLR is
promising, as it can help achieve an accurate initial rigid alignment, which,
in turn, serves as an initialization for subsequent non-rigid registration. We
will release the dataset and code upon acceptance.</description>
      <guid isPermaLink="false">2211.03688v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Uplifting Range-View-based 3D Semantic Segmentation in Real-Time with Multi-Sensor Fusion</title>
      <link>http://arxiv.org/abs/2407.09697v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 基于范围视图（RV）的3D点云分割因其紧凑的数据形式而被广泛采用。&lt;br&gt;&lt;h4&gt;2. 现有问题&lt;/h4&gt;   - RV方法在处理遮挡点时表现不够鲁棒，且由于3D点云的稀疏性，投影的RGB图像存在失真问题。&lt;br&gt;&lt;h4&gt;3. 提出的方法&lt;/h4&gt;   - 本文提出了一种新的LiDAR和相机范围视图基础的3D点云语义分割方法（LaCRange）。&lt;br&gt;&lt;h4&gt;4. 关键策略&lt;/h4&gt;   - 设计了一种失真补偿知识蒸馏（DCKD）策略，以缓解RGB图像RV投影的负面影响。&lt;br&gt;&lt;h4&gt;5. 特征融合模块&lt;/h4&gt;   - 引入了基于上下文的特征融合模块，以实现鲁棒且保留信息的传感器融合。&lt;br&gt;&lt;h4&gt;6. 点云精细化方案&lt;/h4&gt;   - 为解决RV的分辨率限制及其3D拓扑不足，提出了一种新的点精细化方案，以适当地聚合2D特征并增强3D点特征。&lt;br&gt;&lt;h4&gt;7. 实验评估&lt;/h4&gt;   - 在大规模自动驾驶数据集（如SemanticKITTI和nuScenes）上评估了所提方法。&lt;br&gt;&lt;h4&gt;8. 性能表现&lt;/h4&gt;   - 除了实现实时处理外，所提方法在nuScenes基准测试中达到了最先进的结果。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-07-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Range-View(RV)-based 3D point cloud segmentation is widely adopted due to its
compact data form. However, RV-based methods fall short in providing robust
segmentation for the occluded points and suffer from distortion of projected
RGB images due to the sparse nature of 3D point clouds. To alleviate these
problems, we propose a new LiDAR and Camera Range-view-based 3D point cloud
semantic segmentation method (LaCRange). Specifically, a
distortion-compensating knowledge distillation (DCKD) strategy is designed to
remedy the adverse effect of RV projection of RGB images. Moreover, a
context-based feature fusion module is introduced for robust and preservative
sensor fusion. Finally, in order to address the limited resolution of RV and
its insufficiency of 3D topology, a new point refinement scheme is devised for
proper aggregation of features in 2D and augmentation of point features in 3D.
We evaluated the proposed method on large-scale autonomous driving datasets \ie
SemanticKITTI and nuScenes. In addition to being real-time, the proposed method
achieves state-of-the-art results on nuScenes benchmark</description>
      <guid isPermaLink="false">2407.09697v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Compositional Audio Representation Learning</title>
      <link>http://arxiv.org/abs/2409.09619v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to ICASSP 2025&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 人类听觉感知本质上是组合性的，我们能够从包含多个声音事件的听觉场景中识别音频流。&lt;br&gt;&lt;h4&gt;2. 现有问题&lt;/h4&gt;   - 当前的听觉场景通常使用片段级别的表示，这种表示无法分离其中的声源。&lt;br&gt;&lt;h4&gt;3. 研究目标&lt;/h4&gt;   - 本文旨在学习以声源为中心的音频表示，每个声源通过独特的、解耦的源嵌入进行表示。&lt;br&gt;&lt;h4&gt;4. 提出的方法&lt;/h4&gt;   - 提出了两种新颖的方法来学习以声源为中心的音频表示：&lt;br&gt;     - 一种是基于分类的监督模型。&lt;br&gt;     - 另一种是基于特征重建的无监督模型。&lt;br&gt;   - 两种方法均优于基准模型。&lt;br&gt;&lt;h4&gt;5. 方法评估&lt;/h4&gt;   - 通过音频分类任务全面评估了两种方法的设计选择。&lt;br&gt;&lt;h4&gt;6. 监督学习的好处&lt;/h4&gt;   - 研究发现，监督学习对于学习以声源为中心的表示是有益的。&lt;br&gt;&lt;h4&gt;7. 特征重建的有效性&lt;/h4&gt;   - 重建音频特征比重建声谱图更有助于学习无监督的以声源为中心的表示。&lt;br&gt;&lt;h4&gt;8. 潜在应用&lt;/h4&gt;   - 利用以声源为中心的模型可以提升机器听觉的可解释性和更灵活的解码能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human auditory perception is compositional in nature -- we identify auditory
streams from auditory scenes with multiple sound events. However, such auditory
scenes are typically represented using clip-level representations that do not
disentangle the constituent sound sources. In this work, we learn
source-centric audio representations where each sound source is represented
using a distinct, disentangled source embedding in the audio representation. We
propose two novel approaches to learning source-centric audio representations:
a supervised model guided by classification and an unsupervised model guided by
feature reconstruction, both of which outperform the baselines. We thoroughly
evaluate the design choices of both approaches using an audio classification
task. We find that supervision is beneficial to learn source-centric
representations, and that reconstructing audio features is more useful than
reconstructing spectrograms to learn unsupervised source-centric
representations. Leveraging source-centric models can help unlock the potential
of greater interpretability and more flexible decoding in machine listening.</description>
      <guid isPermaLink="false">2409.09619v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Towards Multi-view Graph Anomaly Detection with Similarity-Guided Contrastive Clustering</title>
      <link>http://arxiv.org/abs/2409.09770v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 图上的异常检测在许多现实应用中非常重要。这些数据通常由多种类型组成（如用户信息和金融数据的交易记录），因此表现出视图异质性。&lt;br&gt;&lt;h4&gt;2. 挑战&lt;/h4&gt;   - 利用多视图信息和学习图的上下文信息以识别稀有异常是具有挑战性的。&lt;br&gt;&lt;h4&gt;3. 现有方法&lt;/h4&gt;   - 许多基于深度学习的方法使用对比学习损失作为正则化项来学习良好的表示。&lt;br&gt;&lt;h4&gt;4. 现有方法的局限性&lt;/h4&gt;   - 许多现有的对比学习方法未能考虑语义信息（如类别成员信息），并且基于聚类的对比学习容易导致次优解。&lt;br&gt;&lt;h4&gt;5. 提出的新方法&lt;/h4&gt;   - 本文提出一种基于自编码器的聚类框架，通过相似性引导的对比损失来检测异常节点。&lt;br&gt;&lt;h4&gt;6. 相似性图的构建&lt;/h4&gt;   - 构建相似性图以帮助模型学习鲁棒的表示，而不强加正负配对之间的硬边界约束。&lt;br&gt;&lt;h4&gt;7. 理论贡献&lt;/h4&gt;   - 理论上证明了提出的相似性引导损失是对比学习损失的一种变体，并展示了其如何缓解与伪标签不可靠性相关的问题，连接到图谱聚类。&lt;br&gt;&lt;h4&gt;8. 实验验证&lt;/h4&gt;   - 在多个数据集上的实验结果验证了所提框架的有效性和效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Anomaly detection on graphs plays an important role in many real-world
applications. Usually, these data are composed of multiple types (e.g., user
information and transaction records for financial data), thus exhibiting view
heterogeneity. Therefore, it can be challenging to leverage such multi-view
information and learn the graph's contextual information to identify rare
anomalies. To tackle this problem, many deep learning-based methods utilize
contrastive learning loss as a regularization term to learn good
representations. However, many existing contrastive-based methods show that
traditional contrastive learning losses fail to consider the semantic
information (e.g., class membership information). In addition, we theoretically
show that clustering-based contrastive learning also easily leads to a
sub-optimal solution. To address these issues, in this paper, we proposed an
autoencoder-based clustering framework regularized by a similarity-guided
contrastive loss to detect anomalous nodes. Specifically, we build a similarity
map to help the model learn robust representations without imposing a hard
margin constraint between the positive and negative pairs. Theoretically, we
show that the proposed similarity-guided loss is a variant of contrastive
learning loss, and how it alleviates the issue of unreliable pseudo-labels with
the connection to graph spectral clustering. Experimental results on several
datasets demonstrate the effectiveness and efficiency of our proposed
framework.</description>
      <guid isPermaLink="false">2409.09770v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>CXTrack: Improving 3D Point Cloud Tracking with Contextual Information</title>
      <link>http://arxiv.org/abs/2211.08542v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 3D单对象跟踪在自动驾驶等多个应用中至关重要，但由于外观变化大和因遮挡及传感器能力有限导致的点稀疏性，这一问题依然具有挑战性。&lt;br&gt;&lt;h4&gt;2. 上下文信息的重要性&lt;/h4&gt;   - 连续帧间的上下文信息对于有效的对象跟踪至关重要，但现有方法往往忽视了这些有用信息，导致重要上下文知识的利用不足。&lt;br&gt;&lt;h4&gt;3. 提出的方法&lt;/h4&gt;   - 本文提出CXTrack，这是一个基于变换器的3D对象跟踪网络，利用上下文信息来改善跟踪结果。&lt;br&gt;&lt;h4&gt;4. 网络设计&lt;/h4&gt;   - 设计了一个以目标为中心的变换器网络，直接输入来自两个连续帧的点特征和上一个边界框，以探索上下文信息并隐式传播目标线索。&lt;br&gt;&lt;h4&gt;5. 精确定位&lt;/h4&gt;   - 为了实现对各种大小对象的准确定位，提出了一个基于变换器的定位头，配备了新颖的中心嵌入模块，以区分目标和干扰项。&lt;br&gt;&lt;h4&gt;6. 实验验证&lt;/h4&gt;   - 在KITTI、nuScenes和Waymo Open Dataset三个大规模数据集上进行了广泛实验，结果表明CXTrack在跟踪性能上达到最先进水平，同时运行速度为34 FPS。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2022-11-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D single object tracking plays an essential role in many applications, such
as autonomous driving. It remains a challenging problem due to the large
appearance variation and the sparsity of points caused by occlusion and limited
sensor capabilities. Therefore, contextual information across two consecutive
frames is crucial for effective object tracking. However, points containing
such useful information are often overlooked and cropped out in existing
methods, leading to insufficient use of important contextual knowledge. To
address this issue, we propose CXTrack, a novel transformer-based network for
3D object tracking, which exploits ConteXtual information to improve the
tracking results. Specifically, we design a target-centric transformer network
that directly takes point features from two consecutive frames and the previous
bounding box as input to explore contextual information and implicitly
propagate target cues. To achieve accurate localization for objects of all
sizes, we propose a transformer-based localization head with a novel center
embedding module to distinguish the target from distractors. Extensive
experiments on three large-scale datasets, KITTI, nuScenes and Waymo Open
Dataset, show that CXTrack achieves state-of-the-art tracking performance while
running at 34 FPS.</description>
      <guid isPermaLink="false">2211.08542v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Multimodality in Meta-Learning: A Comprehensive Survey</title>
      <link>http://arxiv.org/abs/2109.13576v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by Knowledge-Based Systems; 21 pages&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 元学习（meta-learning）作为一种训练框架，因其数据效率高于传统机器学习方法而受到广泛关注。&lt;br&gt;&lt;h4&gt;2. 研究问题&lt;/h4&gt;   - 在复杂任务分布（如多模态任务）中，元学习的泛化能力尚未得到充分研究。&lt;br&gt;&lt;h4&gt;3. 新兴研究&lt;/h4&gt;   - 最近关于基于多模态的元学习的研究逐渐增多。&lt;br&gt;&lt;h4&gt;4. 综述内容&lt;/h4&gt;   - 本文提供了关于多模态元学习的全面概述，涵盖方法论和应用。&lt;br&gt;&lt;h4&gt;5. 定义与挑战&lt;/h4&gt;   - 首先正式定义多模态中的元学习，并指出该领域面临的研究挑战，如如何在多模态场景下丰富少量样本学习（FSL）或零样本学习（ZSL）的输入，以及如何将模型推广到新任务。&lt;br&gt;&lt;h4&gt;6. 新分类法&lt;/h4&gt;   - 提出了一个新的分类法，以系统性地讨论多模态任务中的典型元学习算法。&lt;br&gt;&lt;h4&gt;7. 文献贡献&lt;/h4&gt;   - 调查相关论文的贡献，并通过该分类法进行总结。&lt;br&gt;&lt;h4&gt;8. 未来研究方向&lt;/h4&gt;   - 最后，提出了该有前景领域的潜在研究方向。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2021-09-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Meta-learning has gained wide popularity as a training framework that is more
data-efficient than traditional machine learning methods. However, its
generalization ability in complex task distributions, such as multimodal tasks,
has not been thoroughly studied. Recently, some studies on multimodality-based
meta-learning have emerged. This survey provides a comprehensive overview of
the multimodality-based meta-learning landscape in terms of the methodologies
and applications. We first formalize the definition of meta-learning in
multimodality, along with the research challenges in this growing field, such
as how to enrich the input in few-shot learning (FSL) or zero-shot learning
(ZSL) in multimodal scenarios and how to generalize the models to new tasks. We
then propose a new taxonomy to discuss typical meta-learning algorithms in
multimodal tasks systematically. We investigate the contributions of related
papers and summarize them by our taxonomy. Finally, we propose potential
research directions for this promising field.</description>
      <guid isPermaLink="false">2109.13576v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>MI-HGNN: Morphology-Informed Heterogeneous Graph Neural Network for Legged Robot Contact Perception</title>
      <link>http://arxiv.org/abs/2409.11146v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 5 figures; This work has been submitted to the IEEE for
  possible publication. Copyright may be transferred without notice, after
  which this version may no longer be accessible&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究目标&lt;/h4&gt;   - 提出了形态信息驱动的异构图神经网络（MI-HGNN），用于基于学习的接触感知。&lt;br&gt;&lt;h4&gt;2. 网络架构&lt;/h4&gt;   - MI-HGNN的架构和连通性基于机器人形态构建，节点表示机器人关节，边表示连接。&lt;br&gt;&lt;h4&gt;3. 方法创新&lt;/h4&gt;   - 通过将形态信息约束纳入神经网络，结合模型驱动的知识，提升了基于学习的方法。&lt;br&gt;&lt;h4&gt;4. 应用场景&lt;/h4&gt;   - 将MI-HGNN应用于两个接触感知问题，并在两个四足机器人上进行广泛实验，包括真实世界和模拟数据。&lt;br&gt;&lt;h4&gt;5. 实验结果&lt;/h4&gt;   - 实验表明，所提方法在有效性、泛化能力、模型效率和样本效率方面表现优越。&lt;br&gt;&lt;h4&gt;6. 性能提升&lt;/h4&gt;   - MI-HGNN在参数仅占0.21%的情况下，提升了基于机器人形态对称性的最先进模型性能8.4%。&lt;br&gt;&lt;h4&gt;7. 应用潜力&lt;/h4&gt;   - 虽然MI-HGNN在本研究中应用于腿式机器人的接触感知，但可无缝应用于其他多体动态系统，潜在地改善其他机器人学习框架。&lt;br&gt;&lt;h4&gt;8. 代码公开&lt;/h4&gt;   - 相关代码已公开，可在GitHub上访问：[MI-HGNN Repository](https://github.com/lunarlab-gatech/Morphology-Informed-HGNN)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a Morphology-Informed Heterogeneous Graph Neural Network (MI-HGNN)
for learning-based contact perception. The architecture and connectivity of the
MI-HGNN are constructed from the robot morphology, in which nodes and edges are
robot joints and links, respectively. By incorporating the morphology-informed
constraints into a neural network, we improve a learning-based approach using
model-based knowledge. We apply the proposed MI-HGNN to two contact perception
problems, and conduct extensive experiments using both real-world and simulated
data collected using two quadruped robots. Our experiments demonstrate the
superiority of our method in terms of effectiveness, generalization ability,
model efficiency, and sample efficiency. Our MI-HGNN improved the performance
of a state-of-the-art model that leverages robot morphological symmetry by 8.4%
with only 0.21% of its parameters. Although MI-HGNN is applied to contact
perception problems for legged robots in this work, it can be seamlessly
applied to other types of multi-body dynamical systems and has the potential to
improve other robot learning frameworks. Our code is made publicly available at
https://github.com/lunarlab-gatech/Morphology-Informed-HGNN.</description>
      <guid isPermaLink="false">2409.11146v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Fast Deep Predictive Coding Networks for Videos Feature Extraction without Labels</title>
      <link>http://arxiv.org/abs/2409.04945v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 脑启发的深度预测编码网络（DPCN）能够通过双向信息流有效建模和捕捉视频特征，即使在没有标签的情况下。&lt;br&gt;&lt;h4&gt;2. 方法基础&lt;/h4&gt;   - DPCN基于视频场景的过完备描述，但缺乏有效的稀疏化技术来寻找有区分力和鲁棒性的字典。&lt;br&gt;&lt;h4&gt;3. 现有技术&lt;/h4&gt;   - FISTA被认为是最佳替代方案，但仍存在改进空间。&lt;br&gt;&lt;h4&gt;4. 方法提出&lt;/h4&gt;   - 本文提出一种DPCN，能够快速推理内部模型变量（状态和原因），实现高稀疏性和特征聚类的准确性。&lt;br&gt;&lt;h4&gt;5. 学习过程&lt;/h4&gt;   - 提出的无监督学习过程受到自适应动态规划的启发，采用了主次最小化框架，并对其收敛性进行了严格分析。&lt;br&gt;&lt;h4&gt;6. 实验验证&lt;/h4&gt;   - 在CIFAR-10、Super Mario Bros视频游戏和Coil-100数据集上进行实验，验证所提方法在学习速率、稀疏比和特征聚类准确性上优于之前版本的DPCN。&lt;br&gt;&lt;h4&gt;7. 应用潜力&lt;/h4&gt;   - 由于DPCN的坚实基础和可解释性，该进展为无标签视频中的对象识别开辟了广泛的应用前景。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Brain-inspired deep predictive coding networks (DPCNs) effectively model and
capture video features through a bi-directional information flow, even without
labels. They are based on an overcomplete description of video scenes, and one
of the bottlenecks has been the lack of effective sparsification techniques to
find discriminative and robust dictionaries. FISTA has been the best
alternative. This paper proposes a DPCN with a fast inference of internal model
variables (states and causes) that achieves high sparsity and accuracy of
feature clustering. The proposed unsupervised learning procedure, inspired by
adaptive dynamic programming with a majorization-minimization framework, and
its convergence are rigorously analyzed. Experiments in the data sets CIFAR-10,
Super Mario Bros video game, and Coil-100 validate the approach, which
outperforms previous versions of DPCNs on learning rate, sparsity ratio, and
feature clustering accuracy. Because of DCPN's solid foundation and
explainability, this advance opens the door for general applications in object
recognition in video without labels.</description>
      <guid isPermaLink="false">2409.04945v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>SE3ET: SE(3)-Equivariant Transformer for Low-Overlap Point Cloud Registration</title>
      <link>http://arxiv.org/abs/2407.16823v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 部分点云配准在机器人技术中是一个具有挑战性的问题，尤其是在机器人经历较大变换时，会导致显著的初始姿态误差和测量间的低重叠率。&lt;br&gt;&lt;h4&gt;2. 方法提出&lt;/h4&gt;   - 本研究提出利用等变学习（equivariant learning）来提高配准的鲁棒性。&lt;br&gt;&lt;h4&gt;3. 框架介绍&lt;/h4&gt;   - 提出了SE3ET，一个SE(3)-等变配准框架，采用等变点卷积和等变变换器设计，以学习富有表现力且鲁棒的几何特征。&lt;br&gt;&lt;h4&gt;4. 实验设置&lt;/h4&gt;   - 在室内和室外基准测试上测试所提出的配准方法，这些测试中点云经历了任意变换和低重叠率。&lt;br&gt;&lt;h4&gt;5. 附加实验&lt;/h4&gt;   - 提供了泛化测试和运行时性能评估，以验证方法的有效性和效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-07-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/UMich-CURLY/SE3ET&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Partial point cloud registration is a challenging problem in robotics,
especially when the robot undergoes a large transformation, causing a
significant initial pose error and a low overlap between measurements. This
work proposes exploiting equivariant learning from 3D point clouds to improve
registration robustness. We propose SE3ET, an SE(3)-equivariant registration
framework that employs equivariant point convolution and equivariant
transformer designs to learn expressive and robust geometric features. We
tested the proposed registration method on indoor and outdoor benchmarks where
the point clouds are under arbitrary transformations and low overlapping
ratios. We also provide generalization tests and run-time performance.</description>
      <guid isPermaLink="false">2407.16823v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Multi-scale Feature Fusion with Point Pyramid for 3D Object Detection</title>
      <link>http://arxiv.org/abs/2409.04601v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 有效的点云处理对于基于LiDAR的自动驾驶系统至关重要。多尺度特征理解是智能车辆对象检测所需的能力，因为道路使用者可能以不同的大小出现。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限&lt;/h4&gt;   - 近期方法集中于特征聚合操作的设计，从编码器骨干网络收集不同尺度的特征并分配给感兴趣的点。然而，如何融合这些多尺度特征的重要性被忽视，导致尺度间特征交流不足。&lt;br&gt;&lt;h4&gt;3. 方法提出&lt;/h4&gt;   - 提出了Point Pyramid RCNN（POP-RCNN），一种基于特征金字塔的3D对象检测框架。&lt;br&gt;&lt;h4&gt;4. 模块设计&lt;/h4&gt;   - POP-RCNN包含一个点金字塔特征增强（PPFE）模块，旨在建立不同空间尺度和语义深度之间的连接，以实现信息交换。&lt;br&gt;&lt;h4&gt;5. 特征融合&lt;/h4&gt;   - PPFE模块有效融合多尺度特征，提供丰富的信息，同时避免在特征聚合中增加复杂性。&lt;br&gt;&lt;h4&gt;6. 点密度处理&lt;/h4&gt;   - 为了缓解点密度不一致的影响，集成了一个点密度置信模块。&lt;br&gt;&lt;h4&gt;7. 设计集成&lt;/h4&gt;   - 该设计集成使得可以使用轻量级特征聚合器，强调浅层和深层语义，形成一个用于3D对象检测的检测框架。&lt;br&gt;&lt;h4&gt;8. 适用性&lt;/h4&gt;   - 提出的方案具有较强的适应性，可应用于多种现有框架以增强特征丰富性，特别是在远距离检测方面。&lt;br&gt;&lt;h4&gt;9. 实验结果&lt;/h4&gt;   - 在KITTI和Waymo Open Dataset上的实验结果表明，尽管计算资源有限，所提方法实现了显著的性能提升。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Effective point cloud processing is crucial to LiDARbased autonomous driving
systems. The capability to understand features at multiple scales is required
for object detection of intelligent vehicles, where road users may appear in
different sizes. Recent methods focus on the design of the feature aggregation
operators, which collect features at different scales from the encoder backbone
and assign them to the points of interest. While efforts are made into the
aggregation modules, the importance of how to fuse these multi-scale features
has been overlooked. This leads to insufficient feature communication across
scales. To address this issue, this paper proposes the Point Pyramid RCNN
(POP-RCNN), a feature pyramid-based framework for 3D object detection on point
clouds. POP-RCNN consists of a Point Pyramid Feature Enhancement (PPFE) module
to establish connections across spatial scales and semantic depths for
information exchange. The PPFE module effectively fuses multi-scale features
for rich information without the increased complexity in feature aggregation.
To remedy the impact of inconsistent point densities, a point density
confidence module is deployed. This design integration enables the use of a
lightweight feature aggregator, and the emphasis on both shallow and deep
semantics, realising a detection framework for 3D object detection. With great
adaptability, the proposed method can be applied to a variety of existing
frameworks to increase feature richness, especially for long-distance
detection. By adopting the PPFE in the voxel-based and point-voxel-based
baselines, experimental results on KITTI and Waymo Open Dataset show that the
proposed method achieves remarkable performance even with limited computational
headroom.</description>
      <guid isPermaLink="false">2409.04601v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>ModalityMirror: Improving Audio Classification in Modality Heterogeneity Federated Learning with Multimodal Distillation</title>
      <link>http://arxiv.org/abs/2408.15803v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 多模态联邦学习常面临客户端模态异质性的问题，这导致在多模态学习中次要模态的性能不佳。&lt;br&gt;&lt;h4&gt;2. 问题描述&lt;/h4&gt;   - 在视听学习中，音频通常被视为识别任务中较弱的模态，这种情况尤为普遍。&lt;br&gt;&lt;h4&gt;3. 方法提出&lt;/h4&gt;   - 为了解决这一挑战，提出ModalityMirror，通过利用来自视听联邦学习模型的知识蒸馏来提升音频模型的性能。&lt;br&gt;&lt;h4&gt;4. 方法结构&lt;/h4&gt;   - ModalityMirror包含两个阶段：&lt;br&gt;     - **模态级联邦学习阶段**：聚合单模态编码器。&lt;br&gt;     - **联邦知识蒸馏阶段**：在多模态客户端上训练单模态学生模型。&lt;br&gt;&lt;h4&gt;5. 实验结果&lt;/h4&gt;   - 实验结果表明，与当前最先进的联邦学习方法（如Harmony）相比，ModalityMirror显著提高了音频分类性能，尤其是在视频缺失的视听联邦学习情况下。&lt;br&gt;&lt;h4&gt;6. 研究贡献&lt;/h4&gt;   - 本方法揭示了在多模态联邦学习中利用多样模态谱的潜力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal Federated Learning frequently encounters challenges of client
modality heterogeneity, leading to undesired performances for secondary
modality in multimodal learning. It is particularly prevalent in audiovisual
learning, with audio is often assumed to be the weaker modality in recognition
tasks. To address this challenge, we introduce ModalityMirror to improve audio
model performance by leveraging knowledge distillation from an audiovisual
federated learning model. ModalityMirror involves two phases: a modality-wise
FL stage to aggregate uni-modal encoders; and a federated knowledge
distillation stage on multi-modality clients to train an unimodal student
model. Our results demonstrate that ModalityMirror significantly improves the
audio classification compared to the state-of-the-art FL methods such as
Harmony, particularly in audiovisual FL facing video missing. Our approach
unlocks the potential for exploiting the diverse modality spectrum inherent in
multi-modal FL.</description>
      <guid isPermaLink="false">2408.15803v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>3DMambaComplete: Exploring Structured State Space Model for Point Cloud Completion</title>
      <link>http://arxiv.org/abs/2404.07106v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 8 figures, 7 tables&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 点云补全旨在从初始的、不完整且低质量的输入中生成完整且高保真的点云。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限&lt;/h4&gt;   - 现有的流行策略是利用基于Transformer的模型编码全局特征来促进重建过程，但这种方法常常导致局部细节的丢失。&lt;br&gt;&lt;h4&gt;3. 计算复杂性问题&lt;/h4&gt;   - Transformer中的注意力机制引入了额外的计算复杂性，使得有效处理长序列变得困难。&lt;br&gt;&lt;h4&gt;4. 方法提出&lt;/h4&gt;   - 提出3DMambaComplete，一个基于新颖Mamba框架的点云补全网络。&lt;br&gt;&lt;h4&gt;5. 网络结构&lt;/h4&gt;   - 该网络由三个模块组成：&lt;br&gt;     - **HyperPoint Generation**：使用Mamba的选择机制编码点云特征，并预测一组Hyperpoints，同时估计特定的偏移量，使得下采样的点成为HyperPoints。&lt;br&gt;     - **HyperPoint Spread**：将这些HyperPoints分散到不同的空间位置，以避免集中现象。&lt;br&gt;     - **变形方法**：将HyperPoints的2D网格表示转换为细粒度的3D结构，以进行点云重建。&lt;br&gt;&lt;h4&gt;6. 实验验证&lt;/h4&gt;   - 在多个已建立的基准数据集上进行广泛实验，结果表明3DMambaComplete超越了现有最先进的点云补全方法，得到了定性和定量分析的确认。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-04-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud completion aims to generate a complete and high-fidelity point
cloud from an initially incomplete and low-quality input. A prevalent strategy
involves leveraging Transformer-based models to encode global features and
facilitate the reconstruction process. However, the adoption of pooling
operations to obtain global feature representations often results in the loss
of local details within the point cloud. Moreover, the attention mechanism
inherent in Transformers introduces additional computational complexity,
rendering it challenging to handle long sequences effectively. To address these
issues, we propose 3DMambaComplete, a point cloud completion network built on
the novel Mamba framework. It comprises three modules: HyperPoint Generation
encodes point cloud features using Mamba's selection mechanism and predicts a
set of Hyperpoints. A specific offset is estimated, and the down-sampled points
become HyperPoints. The HyperPoint Spread module disperses these HyperPoints
across different spatial locations to avoid concentration. Finally, a
deformation method transforms the 2D mesh representation of HyperPoints into a
fine-grained 3D structure for point cloud reconstruction. Extensive experiments
conducted on various established benchmarks demonstrate that 3DMambaComplete
surpasses state-of-the-art point cloud completion methods, as confirmed by
qualitative and quantitative analyses.</description>
      <guid isPermaLink="false">2404.07106v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>KAN-HyperpointNet for Point Cloud Sequence-Based 3D Human Action Recognition</title>
      <link>http://arxiv.org/abs/2409.09444v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 基于点云序列的3D动作识别已取得显著的性能和效率。&lt;br&gt;&lt;h4&gt;2. 问题描述&lt;/h4&gt;   - 现有的点云序列建模方法无法有效平衡肢体微动作的精确性与姿态宏观结构的完整性，导致在动作推断中丢失关键的信息线索。&lt;br&gt;&lt;h4&gt;3. 方法提出&lt;/h4&gt;   - 引入D-Hyperpoint，一种通过D-Hyperpoint嵌入模块生成的新颖数据类型。&lt;br&gt;&lt;h4&gt;4. D-Hyperpoint特性&lt;/h4&gt;   - D-Hyperpoint同时 encapsulates 区域性瞬时运动和全局静态姿态，有效总结每个时刻的单元人类动作。&lt;br&gt;&lt;h4&gt;5. KANsMixer模块&lt;/h4&gt;   - 提出D-Hyperpoint KANsMixer模块，递归应用于嵌套的D-Hyperpoint分组，以学习动作区分信息，并创新性地整合Kolmogorov-Arnold Networks (KAN)，增强D-Hyperpoint内的时空交互。&lt;br&gt;&lt;h4&gt;6. 网络架构&lt;/h4&gt;   - 提出KAN-HyperpointNet，一种用于3D动作识别的时空解耦网络架构。&lt;br&gt;&lt;h4&gt;7. 实验验证&lt;/h4&gt;   - 在两个公共数据集（MSR Action3D和NTU-RGB+D 60）上进行广泛实验，证明了该方法的最先进性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud sequence-based 3D action recognition has achieved impressive
performance and efficiency. However, existing point cloud sequence modeling
methods cannot adequately balance the precision of limb micro-movements with
the integrity of posture macro-structure, leading to the loss of crucial
information cues in action inference. To overcome this limitation, we introduce
D-Hyperpoint, a novel data type generated through a D-Hyperpoint Embedding
module. D-Hyperpoint encapsulates both regional-momentary motion and
global-static posture, effectively summarizing the unit human action at each
moment. In addition, we present a D-Hyperpoint KANsMixer module, which is
recursively applied to nested groupings of D-Hyperpoints to learn the action
discrimination information and creatively integrates Kolmogorov-Arnold Networks
(KAN) to enhance spatio-temporal interaction within D-Hyperpoints. Finally, we
propose KAN-HyperpointNet, a spatio-temporal decoupled network architecture for
3D action recognition. Extensive experiments on two public datasets: MSR
Action3D and NTU-RGB+D 60, demonstrate the state-of-the-art performance of our
method.</description>
      <guid isPermaLink="false">2409.09444v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Free-VSC: Free Semantics from Visual Foundation Models for Unsupervised Video Semantic Compression</title>
      <link>http://arxiv.org/abs/2409.11718v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ECCV2024&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 无监督视频语义压缩（UVSC）旨在压缩视频以更好地支持各种分析任务，近年来受到关注。&lt;br&gt;&lt;h4&gt;2. 问题描述&lt;/h4&gt;   - 以往方法的语义丰富性有限，原因包括单一的语义学习目标和有限的训练数据等。&lt;br&gt;&lt;h4&gt;3. 方法提出&lt;/h4&gt;   - 为了提升UVSC任务，提出利用现成视频功能模型（VFM）中的丰富语义。&lt;br&gt;&lt;h4&gt;4. 语义对齐层&lt;/h4&gt;   - 引入VFM共享语义对齐层，并辅以VFM特定的提示，以灵活对齐压缩视频与不同VFM之间的语义。&lt;br&gt;&lt;h4&gt;5. 协作构建语义空间&lt;/h4&gt;   - 允许不同VFM协作构建互相增强的语义空间，从而指导压缩模型的学习。&lt;br&gt;&lt;h4&gt;6. 动态轨迹压缩方案&lt;/h4&gt;   - 提出基于动态轨迹的帧间压缩方案，首先根据历史内容估计语义轨迹，然后沿轨迹预测未来语义作为编码上下文。&lt;br&gt;&lt;h4&gt;7. 系统优势&lt;/h4&gt;   - 该方法减少了系统的总体比特成本，进一步提高了压缩效率。&lt;br&gt;&lt;h4&gt;8. 实验结果&lt;/h4&gt;   - 在三项主流任务和六个数据集上，所提方法的性能超过了以往的编码方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised video semantic compression (UVSC), i.e., compressing videos to
better support various analysis tasks, has recently garnered attention.
However, the semantic richness of previous methods remains limited, due to the
single semantic learning objective, limited training data, etc. To address
this, we propose to boost the UVSC task by absorbing the off-the-shelf rich
semantics from VFMs. Specifically, we introduce a VFMs-shared semantic
alignment layer, complemented by VFM-specific prompts, to flexibly align
semantics between the compressed video and various VFMs. This allows different
VFMs to collaboratively build a mutually-enhanced semantic space, guiding the
learning of the compression model. Moreover, we introduce a dynamic
trajectory-based inter-frame compression scheme, which first estimates the
semantic trajectory based on the historical content, and then traverses along
the trajectory to predict the future semantics as the coding context. This
reduces the overall bitcost of the system, further improving the compression
efficiency. Our approach outperforms previous coding methods on three
mainstream tasks and six datasets.</description>
      <guid isPermaLink="false">2409.11718v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Serialized Point Mamba: A Serialized Point Cloud Mamba Segmentation Model</title>
      <link>http://arxiv.org/abs/2407.12319v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 点云分割对机器人视觉感知和环境理解至关重要，支持机器人导航和3D重建等应用。&lt;br&gt;&lt;h4&gt;2. 挑战描述&lt;/h4&gt;   - 点云数据的稀疏性和无序性给高效和准确的分割带来了挑战。&lt;br&gt;&lt;h4&gt;3. 方法提出&lt;/h4&gt;   - 受到Mamba模型在自然语言处理领域成功的启发，提出序列化点云Mamba分割模型（Serialized Point Mamba）。&lt;br&gt;&lt;h4&gt;4. 技术创新&lt;/h4&gt;   - 该模型利用状态空间模型动态压缩序列，减少内存使用，提高计算效率。&lt;br&gt;&lt;h4&gt;5. 建模能力&lt;/h4&gt;   - Serialized Point Mamba结合了局部与全局建模能力，并保持线性复杂度，在室内和室外数据集上实现了最先进的性能。&lt;br&gt;&lt;h4&gt;6. 新技术应用&lt;/h4&gt;   - 引入了新技术，如分阶段点云序列学习、网格池化和条件位置编码，提升了多样点云任务的有效分割能力。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 在Scannet上取得了76.8的平均交并比（mIoU），在S3DIS上为70.3，在Scannetv2实例分割中记录了40.0的平均精度（mAP）。&lt;br&gt;&lt;h4&gt;8. 性能优势&lt;/h4&gt;   - 该方法具有最低的延迟和合理的内存使用，成为基于Mamba的点语义分割模型中最先进的（SOTA）。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-07-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud segmentation is crucial for robotic visual perception and
environmental understanding, enabling applications such as robotic navigation
and 3D reconstruction. However, handling the sparse and unordered nature of
point cloud data presents challenges for efficient and accurate segmentation.
Inspired by the Mamba model's success in natural language processing, we
propose the Serialized Point Cloud Mamba Segmentation Model (Serialized Point
Mamba), which leverages a state-space model to dynamically compress sequences,
reduce memory usage, and enhance computational efficiency. Serialized Point
Mamba integrates local-global modeling capabilities with linear complexity,
achieving state-of-the-art performance on both indoor and outdoor datasets.
This approach includes novel techniques such as staged point cloud sequence
learning, grid pooling, and Conditional Positional Encoding, facilitating
effective segmentation across diverse point cloud tasks. Our method achieved
76.8 mIoU on Scannet and 70.3 mIoU on S3DIS. In Scannetv2 instance
segmentation, it recorded 40.0 mAP. It also had the lowest latency and
reasonable memory use, making it the SOTA among point semantic segmentation
models based on mamba.</description>
      <guid isPermaLink="false">2407.12319v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Neural Intrinsic Embedding for Non-rigid Point Cloud Matching</title>
      <link>http://arxiv.org/abs/2303.01038v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To appear at CVPR 2023&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 点云作为一种原始的3D数据表示，在3D感知中得到了广泛应用，但缺乏潜在对象的内在结构信息。&lt;br&gt;&lt;h4&gt;2. 问题描述&lt;/h4&gt;   - 缺乏内在结构的信息使得直接建立来自可变形形状的点云之间的对应关系面临很大挑战。&lt;br&gt;&lt;h4&gt;3. 方法提出&lt;/h4&gt;   - 提出神经内在嵌入（Neural Intrinsic Embedding, NIE），将每个顶点嵌入到高维空间中，以尊重其内在结构。&lt;br&gt;&lt;h4&gt;4. 学习框架&lt;/h4&gt;   - 基于NIE，进一步提出了一种弱监督学习框架，用于非刚性点云配准。&lt;br&gt;&lt;h4&gt;5. 方法创新&lt;/h4&gt;   - 与以往研究不同，本方法无需昂贵且敏感的离线基构建（例如，拉普拉斯特征分解），也不需要真实的对应标签作为监督。&lt;br&gt;&lt;h4&gt;6. 实验结果&lt;/h4&gt;   - 实证结果表明，所提框架的表现与现有最先进的基线相当，甚至在某些情况下更优，而后者通常需要更多的监督和/或更多的结构几何输入。&lt;br&gt;&lt;h4&gt;7. 研究贡献&lt;/h4&gt;   - 该研究为非刚性点云配准提供了一种新的有效方法，减少了对高成本监督的依赖。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2023-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As a primitive 3D data representation, point clouds are prevailing in 3D
sensing, yet short of intrinsic structural information of the underlying
objects. Such discrepancy poses great challenges on directly establishing
correspondences between point clouds sampled from deformable shapes. In light
of this, we propose Neural Intrinsic Embedding (NIE) to embed each vertex into
a high-dimensional space in a way that respects the intrinsic structure. Based
upon NIE, we further present a weakly-supervised learning framework for
non-rigid point cloud registration. Unlike the prior works, we do not require
expansive and sensitive off-line basis construction (e.g., eigen-decomposition
of Laplacians), nor do we require ground-truth correspondence labels for
supervision. We empirically show that our framework performs on par with or
even better than the state-of-the-art baselines, which generally require more
supervision and/or more structural geometric input.</description>
      <guid isPermaLink="false">2303.01038v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Speaker Contrastive Learning for Source Speaker Tracing</title>
      <link>http://arxiv.org/abs/2409.10072v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 2 figures, accepted by SLT&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 作为生物特征认证技术的一种，讲者验证系统（SV系统）的安全性至关重要。&lt;br&gt;&lt;h4&gt;2. 安全隐患&lt;/h4&gt;   - SV系统容易受到各种攻击，影响其准确性和可靠性。其中，语音转换攻击通过改变语音特征使一个人的声音听起来像另一个人，从而构成重大威胁。&lt;br&gt;&lt;h4&gt;3. 挑战目标&lt;/h4&gt;   - IEEE SLT2024中的源说话人追踪挑战（SSTC）旨在识别被操纵语音信号中的源说话人信息，专注于语音转换情况下的源说话人验证。&lt;br&gt;&lt;h4&gt;4. 研究方法&lt;/h4&gt;   - 提出一种基于说话人对比学习的源说话人追踪方法，旨在学习转换语音中的潜在源说话人信息。&lt;br&gt;&lt;h4&gt;5. 对比损失机制&lt;/h4&gt;   - 在嵌入提取器的训练中采用说话人对比损失，以学习与源说话人相关的表示，帮助识别在多个干扰者说话人嵌入中真实的源说话人嵌入。&lt;br&gt;&lt;h4&gt;6. 实验结果&lt;/h4&gt;   - 实验表明，所提的说话人对比学习系统在挑战测试集上实现了最低的等错误率（EER）为16.788%，并获得挑战第一名。&lt;br&gt;&lt;h4&gt;7. 研究贡献&lt;/h4&gt;   - 该研究为提升SV系统在面对语音转换攻击时的鲁棒性提供了有效的方法和新的见解。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As a form of biometric authentication technology, the security of speaker
verification systems is of utmost importance. However, SV systems are
inherently vulnerable to various types of attacks that can compromise their
accuracy and reliability. One such attack is voice conversion, which modifies a
persons speech to sound like another person by altering various vocal
characteristics. This poses a significant threat to SV systems. To address this
challenge, the Source Speaker Tracing Challenge in IEEE SLT2024 aims to
identify the source speaker information in manipulated speech signals.
Specifically, SSTC focuses on source speaker verification against voice
conversion to determine whether two converted speech samples originate from the
same source speaker. In this study, we propose a speaker contrastive
learning-based approach for source speaker tracing to learn the latent source
speaker information in converted speech. To learn a more source-speaker-related
representation, we employ speaker contrastive loss during the training of the
embedding extractor. This speaker contrastive loss helps identify the true
source speaker embedding among several distractor speaker embeddings, enabling
the embedding extractor to learn the potentially possessing source speaker
information present in the converted speech. Experiments demonstrate that our
proposed speaker contrastive learning system achieves the lowest EER of 16.788%
on the challenge test set, securing first place in the challenge.</description>
      <guid isPermaLink="false">2409.10072v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Towards understanding evolution of science through language model series</title>
      <link>http://arxiv.org/abs/2409.09636v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究目标&lt;/h4&gt;   - 介绍AnnualBERT，一系列专门设计用于捕捉科学文本时间演变的语言模型。&lt;br&gt;&lt;h4&gt;2. 方法创新&lt;/h4&gt;   - 不同于传统的子词分词和“一体适用”的模型，AnnualBERT采用完整单词作为标记。&lt;br&gt;&lt;h4&gt;3. 模型构成&lt;/h4&gt;   - 基于从零开始预训练的RoBERTa模型，使用了170万篇截至2008年的arXiv论文的全文。&lt;br&gt;   - 逐年训练的模型集合，针对arXiv论文的时间演变进行学习。&lt;br&gt;&lt;h4&gt;4. 性能展示&lt;/h4&gt;   - AnnualBERT模型在标准任务上表现相当，同时在领域特定的自然语言处理（NLP）任务及arXiv引用网络的链接预测任务中达到最先进的性能。&lt;br&gt;&lt;h4&gt;5. 行为量化&lt;/h4&gt;   - 通过探测任务量化模型在表示学习和遗忘方面的行为，随着时间的推移进行分析。&lt;br&gt;&lt;h4&gt;6. 研究意义&lt;/h4&gt;   - 该方法不仅提升了科学文本处理任务的性能，还为科学话语的发展提供了时间上的见解。&lt;br&gt;&lt;h4&gt;7. 模型可用性&lt;/h4&gt;   - 该系列模型可在以下链接获取：[Hugging Face - AnnualBERTs](https://huggingface.co/jd445/AnnualBERTs)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce AnnualBERT, a series of language models designed specifically to
capture the temporal evolution of scientific text. Deviating from the
prevailing paradigms of subword tokenizations and "one model to rule them all",
AnnualBERT adopts whole words as tokens and is composed of a base RoBERTa model
pretrained from scratch on the full-text of 1.7 million arXiv papers published
until 2008 and a collection of progressively trained models on arXiv papers at
an annual basis. We demonstrate the effectiveness of AnnualBERT models by
showing that they not only have comparable performances in standard tasks but
also achieve state-of-the-art performances on domain-specific NLP tasks as well
as link prediction tasks in the arXiv citation network. We then utilize probing
tasks to quantify the models' behavior in terms of representation learning and
forgetting as time progresses. Our approach enables the pretrained models to
not only improve performances on scientific text processing tasks but also to
provide insights into the development of scientific discourse over time. The
series of the models is available at https://huggingface.co/jd445/AnnualBERTs.</description>
      <guid isPermaLink="false">2409.09636v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>CC-3DT: Panoramic 3D Object Tracking via Cross-Camera Fusion</title>
      <link>http://arxiv.org/abs/2212.01247v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://www.vis.xyz/pub/cc-3dt/&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 现代自动驾驶车辆配备多个摄像头，以追踪周围交通参与者的3D位置和轨迹。&lt;br&gt;&lt;h4&gt;2. 问题描述&lt;/h4&gt;   - 现有基于摄像头的3D对象跟踪方法主要优化单摄像头设置，而在多摄像头设置中采用后处理融合。&lt;br&gt;&lt;h4&gt;3. 研究目标&lt;/h4&gt;   - 提出一种全景3D对象跟踪方法，称为CC-3DT，旨在同时在时间和视角上关联和建模对象轨迹，提升整体跟踪一致性。&lt;br&gt;&lt;h4&gt;4. 方法创新&lt;/h4&gt;   - 在关联之前融合来自多个摄像头的3D检测结果，显著减少身份切换并改善运动建模。&lt;br&gt;&lt;h4&gt;5. 实验验证&lt;/h4&gt;   - 在大规模驾驶数据集上进行实验，结果表明，关联前的融合相比后处理融合有显著的改进。&lt;br&gt;&lt;h4&gt;6. 成果体现&lt;/h4&gt;   - 在竞争激烈的NuScenes 3D跟踪基准测试中，设定了新的状态-of-the-art，平均多目标跟踪准确率（AMOTA）提高了12.6%，超越了相同3D检测器的先前方法6.5%。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2022-12-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; To track the 3D locations and trajectories of the other traffic participants
at any given time, modern autonomous vehicles are equipped with multiple
cameras that cover the vehicle's full surroundings. Yet, camera-based 3D object
tracking methods prioritize optimizing the single-camera setup and resort to
post-hoc fusion in a multi-camera setup. In this paper, we propose a method for
panoramic 3D object tracking, called CC-3DT, that associates and models object
trajectories both temporally and across views, and improves the overall
tracking consistency. In particular, our method fuses 3D detections from
multiple cameras before association, reducing identity switches significantly
and improving motion modeling. Our experiments on large-scale driving datasets
show that fusion before association leads to a large margin of improvement over
post-hoc fusion. We set a new state-of-the-art with 12.6% improvement in
average multi-object tracking accuracy (AMOTA) among all camera-based methods
on the competitive NuScenes 3D tracking benchmark, outperforming previously
published methods by 6.5% in AMOTA with the same 3D detector.</description>
      <guid isPermaLink="false">2212.01247v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Generalization Bounds For Meta-Learning: An Information-Theoretic Analysis</title>
      <link>http://arxiv.org/abs/2109.14595v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究目标&lt;/h4&gt;   - 提出一种新颖的信息论分析，探讨元学习算法的泛化特性。&lt;br&gt;&lt;h4&gt;2. 分析框架&lt;/h4&gt;   - 提供对传统学习-学习框架和现代模型无关元学习（MAML）算法的通用理解。&lt;br&gt;&lt;h4&gt;3. 泛化界限&lt;/h4&gt;   - 为MAML的随机变体提供数据依赖的泛化界限，特别适用于深度少样本学习。&lt;br&gt;&lt;h4&gt;4. 比较优势&lt;/h4&gt;   - 相比于之前依赖于梯度平方范数的界限，本文的界限在大多数情况下更为紧致。&lt;br&gt;&lt;h4&gt;5. 实验验证&lt;/h4&gt;   - 在模拟数据和知名少样本基准上进行实证验证，证明了所提出界限的有效性和优越性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2021-09-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/livreq/meta-sgld&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We derive a novel information-theoretic analysis of the generalization
property of meta-learning algorithms. Concretely, our analysis proposes a
generic understanding of both the conventional learning-to-learn framework and
the modern model-agnostic meta-learning (MAML) algorithms. Moreover, we provide
a data-dependent generalization bound for a stochastic variant of MAML, which
is non-vacuous for deep few-shot learning. As compared to previous bounds that
depend on the square norm of gradients, empirical validations on both simulated
data and a well-known few-shot benchmark show that our bound is orders of
magnitude tighter in most situations.</description>
      <guid isPermaLink="false">2109.14595v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>High-Order Evolving Graphs for Enhanced Representation of Traffic Dynamics</title>
      <link>http://arxiv.org/abs/2409.11206v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted manuscript - 2nd Workshop on Vision-Centric Autonomous
  Driving (VCAD) as part of European Conference on Computer Vision (ECCV) 2024&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究目标&lt;/h4&gt;   - 提出一个创新框架，用于交通动态分析，利用高阶演变图（High-Order Evolving Graphs），旨在改善自动驾驶环境中的时空表示。&lt;br&gt;&lt;h4&gt;2. 方法概述&lt;/h4&gt;   - 构建时间双向双部图，有效建模实时交通场景中的复杂互动。&lt;br&gt;&lt;h4&gt;3. 技术集成&lt;/h4&gt;   - 将图神经网络（GNNs）与高阶多聚合策略相结合，显著增强交通场景动态建模，提供更准确和详细的互动分析。&lt;br&gt;&lt;h4&gt;4. 学习技术&lt;/h4&gt;   - 采用受GraphSAGE框架启发的归纳学习技术，使模型能够适应新的、未见过的交通场景，无需重新训练，从而确保鲁棒性和泛化能力。&lt;br&gt;&lt;h4&gt;5. 实验验证&lt;/h4&gt;   - 在ROAD和ROAD Waymo数据集上进行广泛实验，建立全面基准，为后续研究发展提供支持。&lt;br&gt;&lt;h4&gt;6. 结果强调&lt;/h4&gt;   - 强调高阶统计矩和特征门控注意机制在改善交通行为分析中的价值，为推进自动驾驶技术奠定基础。&lt;br&gt;&lt;h4&gt;7. 代码发布&lt;/h4&gt;   - 提供源代码链接，供其他研究者使用和参考：[GitHub - High_Order_Graphs](https://github.com/Addy-1998/High_Order_Graphs)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present an innovative framework for traffic dynamics analysis using
High-Order Evolving Graphs, designed to improve spatio-temporal representations
in autonomous driving contexts. Our approach constructs temporal bidirectional
bipartite graphs that effectively model the complex interactions within traffic
scenes in real-time. By integrating Graph Neural Networks (GNNs) with
high-order multi-aggregation strategies, we significantly enhance the modeling
of traffic scene dynamics, providing a more accurate and detailed analysis of
these interactions. Additionally, we incorporate inductive learning techniques
inspired by the GraphSAGE framework, enabling our model to adapt to new and
unseen traffic scenarios without the need for retraining, thus ensuring robust
generalization. Through extensive experiments on the ROAD and ROAD Waymo
datasets, we establish a comprehensive baseline for further developments,
demonstrating the potential of our method in accurately capturing traffic
behavior. Our results emphasize the value of high-order statistical moments and
feature-gated attention mechanisms in improving traffic behavior analysis,
laying the groundwork for advancing autonomous driving technologies. Our source
code is available at: https://github.com/Addy-1998/High_Order_Graphs</description>
      <guid isPermaLink="false">2409.11206v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Robust Point Cloud Registration in Robotic Inspection with Locally Consistent Gaussian Mixture Model</title>
      <link>http://arxiv.org/abs/2407.17183v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 14 figures&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 在航空零件的机器人检查中，实现扫描数据与模型数据之间的准确配对点云注册至关重要。&lt;br&gt;   - 机器人扫描数据中的噪声和离群点会影响注册精度。&lt;br&gt;&lt;h4&gt;2. 研究目标&lt;/h4&gt;   - 本文提出一种基于概率的注册方法，利用高斯混合模型（GMM）结合局部一致性约束来解决这一挑战。&lt;br&gt;&lt;h4&gt;3. 方法概述&lt;/h4&gt;   - 将注册问题转化为模型拟合问题，约束邻近点之间后验分布的相似性，以增强对应关系的鲁棒性。&lt;br&gt;&lt;h4&gt;4. 算法实现&lt;/h4&gt;   - 采用期望最大化（EM）算法迭代寻找最优的旋转矩阵和位移向量，同时获取GMM参数。&lt;br&gt;   - E步和M步都具有封闭形式解，便于计算。&lt;br&gt;&lt;h4&gt;5. 实验验证&lt;/h4&gt;   - 通过仿真和实际实验验证方法的有效性，在存在噪声和离群点的情况下，均方根误差降低了20%。&lt;br&gt;&lt;h4&gt;6. 方法优势&lt;/h4&gt;   - 提出的注册方法在鲁棒性和准确性上优于现有方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-07-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In robotic inspection of aviation parts, achieving accurate pairwise point
cloud registration between scanned and model data is essential. However, noise
and outliers generated in robotic scanned data can compromise registration
accuracy. To mitigate this challenge, this article proposes a probability-based
registration method utilizing Gaussian Mixture Model (GMM) with local
consistency constraint. This method converts the registration problem into a
model fitting one, constraining the similarity of posterior distributions
between neighboring points to enhance correspondence robustness. We employ the
Expectation Maximization algorithm iteratively to find optimal rotation matrix
and translation vector while obtaining GMM parameters. Both E-step and M-step
have closed-form solutions. Simulation and actual experiments confirm the
method's effectiveness, reducing root mean square error by 20% despite the
presence of noise and outliers. The proposed method excels in robustness and
accuracy compared to existing methods.</description>
      <guid isPermaLink="false">2407.17183v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Data-efficient and Interpretable Inverse Materials Design using a Disentangled Variational Autoencoder</title>
      <link>http://arxiv.org/abs/2409.06740v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 逆向材料设计在加速新材料发现方面取得了成功。&lt;br&gt;   - 许多逆向材料设计方法使用无监督学习，通过学习潜在空间提供材料表示的紧凑描述。&lt;br&gt;&lt;h4&gt;2. 问题描述&lt;/h4&gt;   - 这种学习的潜在空间可能因目标属性和其他材料属性而纠缠，导致逆向设计过程模糊不清。&lt;br&gt;&lt;h4&gt;3. 研究目标&lt;/h4&gt;   - 本文提出一种基于解缠变分自编码器的半监督学习方法，以学习特征、潜在变量和目标属性之间的概率关系。&lt;br&gt;&lt;h4&gt;4. 方法优势&lt;/h4&gt;   - 数据效率高：该方法将所有标记和未标记数据以一致的方式结合。&lt;br&gt;   - 提供专家知识的先验分布，以提高模型的鲁棒性，即使在标记数据有限的情况下。&lt;br&gt;&lt;h4&gt;5. 可解释性&lt;/h4&gt;   - 学习的目标属性与材料的其他属性解缠，从而本质上具有可解释性。&lt;br&gt;   - 通过对模型分类头的后置分析，可以提供额外的可解释性。&lt;br&gt;&lt;h4&gt;6. 实验验证&lt;/h4&gt;   - 在高熵合金实验数据集上验证该新方法，输入为化学成分，单相形成作为单一目标属性。&lt;br&gt;&lt;h4&gt;7. 扩展性&lt;/h4&gt;   - 尽管本研究使用单一属性，但解缠模型可以扩展以定制用于具有多个目标属性的材料逆向设计。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Inverse materials design has proven successful in accelerating novel material
discovery. Many inverse materials design methods use unsupervised learning
where a latent space is learned to offer a compact description of materials
representations. A latent space learned this way is likely to be entangled, in
terms of the target property and other properties of the materials. This makes
the inverse design process ambiguous. Here, we present a semi-supervised
learning approach based on a disentangled variational autoencoder to learn a
probabilistic relationship between features, latent variables and target
properties. This approach is data efficient because it combines all labelled
and unlabelled data in a coherent manner, and it uses expert-informed prior
distributions to improve model robustness even with limited labelled data. It
is in essence interpretable, as the learnable target property is disentangled
out of the other properties of the materials, and an extra layer of
interpretability can be provided by a post-hoc analysis of the classification
head of the model. We demonstrate this new approach on an experimental
high-entropy alloy dataset with chemical compositions as input and single-phase
formation as the single target property. While single property is used in this
work, the disentangled model can be extended to customize for inverse design of
materials with multiple target properties.</description>
      <guid isPermaLink="false">2409.06740v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>RCBEVDet++: Toward High-accuracy Radar-Camera Fusion 3D Perception Network</title>
      <link>http://arxiv.org/abs/2409.04979v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The extended work of RCBEVDet (CVPR2024)&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 感知周围环境是自动驾驶的基本任务。&lt;br&gt;   - 现代自动驾驶系统通常使用多模态传感器以获取全面的环境数据。&lt;br&gt;&lt;h4&gt;2. 传感器选择&lt;/h4&gt;   - 雷达-相机多模态感知系统因其卓越的感知能力和性价比而受到青睐。&lt;br&gt;   - 然而，雷达和相机传感器之间的显著模态差异在信息融合时带来挑战。&lt;br&gt;&lt;h4&gt;3. 研究目标&lt;/h4&gt;   - 本文提出了RCBEVDet，一个雷达-相机融合的3D目标检测框架。&lt;br&gt;&lt;h4&gt;4. 框架构建&lt;/h4&gt;   - RCBEVDet基于现有的相机3D目标检测器，并补充了专门设计的雷达特征提取器RadarBEVNet，以及交叉注意力多层融合模块（CAMF）。&lt;br&gt;&lt;h4&gt;5. RadarBEVNet&lt;/h4&gt;   - RadarBEVNet将稀疏雷达点编码为密集的鸟瞰视图（BEV）特征，使用双流雷达主干和雷达横截面感知BEV编码器。&lt;br&gt;&lt;h4&gt;6. CAMF模块&lt;/h4&gt;   - CAMF模块利用可变形注意力机制对齐雷达和相机的BEV特征，并采用通道和空间融合层进行融合。&lt;br&gt;&lt;h4&gt;7. 模型增强&lt;/h4&gt;   - 引入RCBEVDet++，进一步提升CAMF，通过稀疏融合，支持基于查询的多视角相机感知模型，适应更广泛的感知任务。&lt;br&gt;&lt;h4&gt;8. 实验结果&lt;/h4&gt;   - 在nuScenes数据集上的大量实验显示，该方法与现有的相机3D感知模型无缝集成，提升了其在各类感知任务中的性能。&lt;br&gt;   - 本方法在3D目标检测、BEV语义分割和3D多目标跟踪任务中实现了先进的雷达-相机融合结果。&lt;br&gt;&lt;h4&gt;9. 性能指标&lt;/h4&gt;   - 使用ViT-L作为图像主干，RCBEVDet++在3D目标检测中达到了72.73 NDS和67.34 mAP，无需测试时间增强或模型集成。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Perceiving the surrounding environment is a fundamental task in autonomous
driving. To obtain highly accurate perception results, modern autonomous
driving systems typically employ multi-modal sensors to collect comprehensive
environmental data. Among these, the radar-camera multi-modal perception system
is especially favored for its excellent sensing capabilities and
cost-effectiveness. However, the substantial modality differences between radar
and camera sensors pose challenges in fusing information. To address this
problem, this paper presents RCBEVDet, a radar-camera fusion 3D object
detection framework. Specifically, RCBEVDet is developed from an existing
camera-based 3D object detector, supplemented by a specially designed radar
feature extractor, RadarBEVNet, and a Cross-Attention Multi-layer Fusion (CAMF)
module. Firstly, RadarBEVNet encodes sparse radar points into a dense
bird's-eye-view (BEV) feature using a dual-stream radar backbone and a Radar
Cross Section aware BEV encoder. Secondly, the CAMF module utilizes a
deformable attention mechanism to align radar and camera BEV features and
adopts channel and spatial fusion layers to fuse them. To further enhance
RCBEVDet's capabilities, we introduce RCBEVDet++, which advances the CAMF
through sparse fusion, supports query-based multi-view camera perception
models, and adapts to a broader range of perception tasks. Extensive
experiments on the nuScenes show that our method integrates seamlessly with
existing camera-based 3D perception models and improves their performance
across various perception tasks. Furthermore, our method achieves
state-of-the-art radar-camera fusion results in 3D object detection, BEV
semantic segmentation, and 3D multi-object tracking tasks. Notably, with ViT-L
as the image backbone, RCBEVDet++ achieves 72.73 NDS and 67.34 mAP in 3D object
detection without test-time augmentation or model ensembling.</description>
      <guid isPermaLink="false">2409.04979v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Toward Robust Early Detection of Alzheimer's Disease via an Integrated Multimodal Learning Approach</title>
      <link>http://arxiv.org/abs/2408.16343v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 2 figures&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 阿尔茨海默病（AD）是一种复杂的神经退行性疾病，特征包括记忆丧失、执行功能障碍和人格变化。&lt;br&gt;   - 早期诊断具有挑战性，症状微妙且表现多样，传统的单一诊断方法常导致误诊。&lt;br&gt;&lt;h4&gt;2. 研究目标&lt;/h4&gt;   - 本研究提出了一种先进的多模态分类模型，集成临床、认知、神经影像和脑电图（EEG）数据，以提高诊断准确性。&lt;br&gt;&lt;h4&gt;3. 模型结构&lt;/h4&gt;   - 模型采用特征标记器和表格数据编码架构。&lt;br&gt;   - 使用TimesBlock模块捕捉EEG数据中的复杂时间模式。&lt;br&gt;&lt;h4&gt;4. 交叉模态融合&lt;/h4&gt;   - 通过交叉模态注意力聚合模块，有效融合MRI的空间信息与EEG的时间数据。&lt;br&gt;   - 显著改善了阿尔茨海默病、轻度认知障碍和正常认知之间的区分。&lt;br&gt;&lt;h4&gt;5. 数据集构建&lt;/h4&gt;   - 构建了首个包含EEG、MRI和表格数据的AD分类数据集。&lt;br&gt;&lt;h4&gt;6. 研究意义&lt;/h4&gt;   - 该创新方法旨在促进早期诊断和干预，可能减缓阿尔茨海默病的进展。&lt;br&gt;&lt;h4&gt;7. 资源链接&lt;/h4&gt;   - 源代码和私有ADMC数据集可在GitHub上获取：[MSTNet](https://github.com/JustlfC03/MSTNet)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/justlfc03/mstnet&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Alzheimer's Disease (AD) is a complex neurodegenerative disorder marked by
memory loss, executive dysfunction, and personality changes. Early diagnosis is
challenging due to subtle symptoms and varied presentations, often leading to
misdiagnosis with traditional unimodal diagnostic methods due to their limited
scope. This study introduces an advanced multimodal classification model that
integrates clinical, cognitive, neuroimaging, and EEG data to enhance
diagnostic accuracy. The model incorporates a feature tagger with a tabular
data coding architecture and utilizes the TimesBlock module to capture
intricate temporal patterns in Electroencephalograms (EEG) data. By employing
Cross-modal Attention Aggregation module, the model effectively fuses Magnetic
Resonance Imaging (MRI) spatial information with EEG temporal data,
significantly improving the distinction between AD, Mild Cognitive Impairment,
and Normal Cognition. Simultaneously, we have constructed the first AD
classification dataset that includes three modalities: EEG, MRI, and tabular
data. Our innovative approach aims to facilitate early diagnosis and
intervention, potentially slowing the progression of AD. The source code and
our private ADMC dataset are available at https://github.com/JustlfC03/MSTNet.</description>
      <guid isPermaLink="false">2408.16343v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>A Comprehensive Methodological Survey of Human Activity Recognition Across Divers Data Modalities</title>
      <link>http://arxiv.org/abs/2409.09678v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 人类活动识别（HAR）系统旨在理解人类行为，并为每个动作分配标签。&lt;br&gt;   - HAR在计算机视觉领域受到广泛关注，因其应用范围广泛。&lt;br&gt;&lt;h4&gt;2. 数据模态&lt;/h4&gt;   - HAR可以利用多种数据模态，包括：&lt;br&gt;     - RGB图像和视频&lt;br&gt;     - 骨架数据&lt;br&gt;     - 深度图&lt;br&gt;     - 红外线&lt;br&gt;     - 点云&lt;br&gt;     - 事件流&lt;br&gt;     - 音频&lt;br&gt;     - 加速度和雷达信号&lt;br&gt;   - 每种模态提供独特且互补的信息，适用于不同的应用场景。&lt;br&gt;&lt;h4&gt;3. 研究目标&lt;/h4&gt;   - 本文提供了2014年至2024年间HAR领域最新进展的全面综述。&lt;br&gt;   - 重点关注机器学习（ML）和深度学习（DL）方法，按输入数据模态进行分类。&lt;br&gt;&lt;h4&gt;4. 技术分类&lt;/h4&gt;   - 评审了单一模态和多模态技术，突出融合基础和共同学习框架。&lt;br&gt;&lt;h4&gt;5. 具体内容&lt;/h4&gt;   - 涵盖手工构建的动作特征、识别人类与物体交互的方法以及活动检测的进展。&lt;br&gt;   - 为每种模态提供详细的数据集描述，并总结最新的HAR系统，比较基准数据集上的结果。&lt;br&gt;&lt;h4&gt;6. 未来研究方向&lt;/h4&gt;   - 提供有见地的观察和有效的未来研究方向建议，以推动HAR领域的发展。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human Activity Recognition (HAR) systems aim to understand human behaviour
and assign a label to each action, attracting significant attention in computer
vision due to their wide range of applications. HAR can leverage various data
modalities, such as RGB images and video, skeleton, depth, infrared, point
cloud, event stream, audio, acceleration, and radar signals. Each modality
provides unique and complementary information suited to different application
scenarios. Consequently, numerous studies have investigated diverse approaches
for HAR using these modalities. This paper presents a comprehensive survey of
the latest advancements in HAR from 2014 to 2024, focusing on machine learning
(ML) and deep learning (DL) approaches categorized by input data modalities. We
review both single-modality and multi-modality techniques, highlighting
fusion-based and co-learning frameworks. Additionally, we cover advancements in
hand-crafted action features, methods for recognizing human-object
interactions, and activity detection. Our survey includes a detailed dataset
description for each modality and a summary of the latest HAR systems, offering
comparative results on benchmark datasets. Finally, we provide insightful
observations and propose effective future research directions in HAR.</description>
      <guid isPermaLink="false">2409.09678v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>GPN: Generative Point-based NeRF</title>
      <link>http://arxiv.org/abs/2404.08312v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 现代注册设备扫描真实场景时，通常会生成不完整的点云表示。&lt;br&gt;   - 不完整的原因包括部分扫描、3D遮挡和动态光照条件的限制。&lt;br&gt;&lt;h4&gt;2. 现有研究的局限性&lt;/h4&gt;   - 目前关于处理不完整点云的研究主要集中在点云补全上。&lt;br&gt;   - 这些现有方法未能确保补全后的点云与捕获图像在颜色和几何一致性方面的匹配。&lt;br&gt;&lt;h4&gt;3. 研究目标&lt;/h4&gt;   - 提出使用生成点云基础的NeRF（GPN）来重建和修复部分点云，充分利用扫描图像和相应的重建云。&lt;br&gt;&lt;h4&gt;4. 技术创新&lt;/h4&gt;   - 修复后的点云能够在高空间分辨率下实现与捕获图像的多视图一致性。&lt;br&gt;   - 针对单一场景的细调，通过引入自编码器架构来优化全局潜在条件，同时保持多视图一致性。&lt;br&gt;&lt;h4&gt;5. 结果&lt;/h4&gt;   - 生成的点云在平滑性、合理性和与部分扫描图像的几何一致性方面表现优异。&lt;br&gt;&lt;h4&gt;6. 实验验证&lt;/h4&gt;   - 在ShapeNet上进行的广泛实验表明，本研究的性能与其他先进的点云基础神经场景渲染和编辑方法相竞争。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-04-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/forestsen/GPN&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Scanning real-life scenes with modern registration devices typically gives
incomplete point cloud representations, primarily due to the limitations of
partial scanning, 3D occlusions, and dynamic light conditions. Recent works on
processing incomplete point clouds have always focused on point cloud
completion. However, these approaches do not ensure consistency between the
completed point cloud and the captured images regarding color and geometry. We
propose using Generative Point-based NeRF (GPN) to reconstruct and repair a
partial cloud by fully utilizing the scanning images and the corresponding
reconstructed cloud. The repaired point cloud can achieve multi-view
consistency with the captured images at high spatial resolution. For the
finetunes of a single scene, we optimize the global latent condition by
incorporating an Auto-Decoder architecture while retaining multi-view
consistency. As a result, the generated point clouds are smooth, plausible, and
geometrically consistent with the partial scanning images. Extensive
experiments on ShapeNet demonstrate that our works achieve competitive
performances to the other state-of-the-art point cloud-based neural scene
rendering and editing performances.</description>
      <guid isPermaLink="false">2404.08312v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Human-like Affective Cognition in Foundation Models</title>
      <link>http://arxiv.org/abs/2409.11733v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 理解情感是人际互动和体验的基础。&lt;br&gt;   - 人类能够轻松地从情境或面部表情中推断情感，反之亦然。&lt;br&gt;&lt;h4&gt;2. 研究目的&lt;/h4&gt;   - 本文旨在评估现代人工智能在情感推断方面的能力。&lt;br&gt;&lt;h4&gt;3. 评估框架&lt;/h4&gt;   - 引入了一个评估框架，用于测试基础模型的情感认知能力。&lt;br&gt;   - 该框架基于心理学理论，生成了1280个多样化的场景，探讨评估、情感、表达和结果之间的关系。&lt;br&gt;&lt;h4&gt;4. 模型与人类的比较&lt;/h4&gt;   - 评估了基础模型（如GPT-4、Claude-3、Gemini-1.5-Pro）和567名人类参与者的能力。&lt;br&gt;   - 结果显示，基础模型与人类直觉的匹配度较高，甚至在某些情况下超越了人类的共识。&lt;br&gt;&lt;h4&gt;5. “超人类”表现&lt;/h4&gt;   - 在某些条件下，模型的预测能力优于普通人类，表现出“超人类”的特征。&lt;br&gt;&lt;h4&gt;6. 推理方式的影响&lt;/h4&gt;   - 所有模型在使用链式推理时表现更佳，表明这种推理方式对情感理解有积极影响。&lt;br&gt;&lt;h4&gt;7. 结论&lt;/h4&gt;   - 这些结果表明，基础模型在情感及其对信念和行为影响的理解上已接近人类水平。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding emotions is fundamental to human interaction and experience.
Humans easily infer emotions from situations or facial expressions, situations
from emotions, and do a variety of other \emph{affective cognition}. How adept
is modern AI at these inferences? We introduce an evaluation framework for
testing affective cognition in foundation models. Starting from psychological
theory, we generate 1,280 diverse scenarios exploring relationships between
appraisals, emotions, expressions, and outcomes. We evaluate the abilities of
foundation models (GPT-4, Claude-3, Gemini-1.5-Pro) and humans (N = 567) across
carefully selected conditions. Our results show foundation models tend to agree
with human intuitions, matching or exceeding interparticipant agreement. In
some conditions, models are ``superhuman'' -- they better predict modal human
judgements than the average human. All models benefit from chain-of-thought
reasoning. This suggests foundation models have acquired a human-like
understanding of emotions and their influence on beliefs and behavior.</description>
      <guid isPermaLink="false">2409.11733v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>HGL: Hierarchical Geometry Learning for Test-time Adaptation in 3D Point Cloud Segmentation</title>
      <link>http://arxiv.org/abs/2407.12387v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 3D点云分割因其广泛应用而受到重视。&lt;br&gt;   - 模型在动态场景中的泛化能力受到测试数据与训练数据分布差异的影响。&lt;br&gt;&lt;h4&gt;2. 测试时适应（TTA）&lt;/h4&gt;   - 为提高在不同场景下的鲁棒性和适应性，最近引入了测试时适应方法。&lt;br&gt;   - 目前大多数TTA方法主要针对图像，适用于点云的方法相对有限。&lt;br&gt;&lt;h4&gt;3. 层次几何学习（HGL）框架&lt;/h4&gt;   - 本文提出了一个新颖的HGL框架，专注于3D点云分割中的TTA。&lt;br&gt;   - HGL由三个互补模块组成，分别对应局部、全局和时间学习。&lt;br&gt;&lt;h4&gt;4. 模块构建&lt;/h4&gt;   - **局部几何学习模块**：用于生成伪标签。&lt;br&gt;   - **全局几何原型模块**：从全局角度构建原型，以细化伪标签。&lt;br&gt;   - **时间一致性正则化模块**：用于减轻负迁移现象。&lt;br&gt;&lt;h4&gt;5. 实验结果&lt;/h4&gt;   - 在四个数据集上进行广泛实验，验证HGL的有效性和优越性。&lt;br&gt;   - 特别是在SynLiDAR到SemanticKITTI任务中，HGL实现了46.91%的平均交并比（mIoU），比GIPSO提高了3.0%，并显著减少了80%的适应时间。&lt;br&gt;&lt;h4&gt;6. 代码可用性&lt;/h4&gt;   - 研究代码已发布，链接为：[GitHub - tpzou/HGL](https://github.com/tpzou/HGL)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-07-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/tpzou/hgl&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D point cloud segmentation has received significant interest for its growing
applications. However, the generalization ability of models suffers in dynamic
scenarios due to the distribution shift between test and training data. To
promote robustness and adaptability across diverse scenarios, test-time
adaptation (TTA) has recently been introduced. Nevertheless, most existing TTA
methods are developed for images, and limited approaches applicable to point
clouds ignore the inherent hierarchical geometric structures in point cloud
streams, i.e., local (point-level), global (object-level), and temporal
(frame-level) structures. In this paper, we delve into TTA in 3D point cloud
segmentation and propose a novel Hierarchical Geometry Learning (HGL)
framework. HGL comprises three complementary modules from local, global to
temporal learning in a bottom-up manner.Technically, we first construct a local
geometry learning module for pseudo-label generation. Next, we build prototypes
from the global geometry perspective for pseudo-label fine-tuning. Furthermore,
we introduce a temporal consistency regularization module to mitigate negative
transfer. Extensive experiments on four datasets demonstrate the effectiveness
and superiority of our HGL. Remarkably, on the SynLiDAR to SemanticKITTI task,
HGL achieves an overall mIoU of 46.91\%, improving GIPSO by 3.0\% and
significantly reducing the required adaptation time by 80\%. The code is
available at https://github.com/tpzou/HGL.</description>
      <guid isPermaLink="false">2407.12387v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Rotation-Invariant Transformer for Point Cloud Matching</title>
      <link>http://arxiv.org/abs/2303.08231v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to CVPR 2023&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 内在旋转不变性的重要性&lt;/h4&gt;   - 内在旋转不变性是使用手工特征描述符进行点云匹配的核心。&lt;br&gt;   - 近期的深度匹配算法通过数据增强获得外在的旋转不变性，但这种方法存在局限。&lt;br&gt;&lt;h4&gt;2. 现有方法的不足&lt;/h4&gt;   - 由于增强旋转的数量有限，无法覆盖连续的SO(3)空间，这导致方法在面对少见旋转时表现不稳定。&lt;br&gt;&lt;h4&gt;3. RoITr的提出&lt;/h4&gt;   - 本文提出了RoITr（旋转不变Transformer），旨在应对点云匹配任务中的姿态变化。&lt;br&gt;&lt;h4&gt;4. 局部级别的贡献&lt;/h4&gt;   - 引入了一种基于注意力机制的模型，结合点对特征（PPF）坐标，以描述姿态不变的几何形状。&lt;br&gt;   - 在此基础上构建了一个新颖的基于注意力的编码器-解码器架构。&lt;br&gt;&lt;h4&gt;5. 全局级别的贡献&lt;/h4&gt;   - 提出了一个全局Transformer，具备旋转不变的跨帧空间意识，通过自注意力机制学习。&lt;br&gt;   - 该机制显著提高了特征的区分度，使模型在低重叠情况下更具鲁棒性。&lt;br&gt;&lt;h4&gt;6. 实验结果&lt;/h4&gt;   - 在刚性和非刚性公共基准测试上进行实验，RoITr在低重叠场景下超越了所有现有的最先进模型。&lt;br&gt;   - 在具有挑战性的3DLoMatch基准测试上，当旋转增大时，RoITr在内部比率（Inlier Ratio）和配准召回率（Registration Recall）上分别超过现有方法至少13和5个百分点。&lt;br&gt;&lt;h4&gt;7. 总结&lt;/h4&gt;   - RoITr通过创新的局部和全局方法有效解决了点云匹配中的旋转不变性问题，展示了其在低重叠情况下的优势。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2023-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/haoyu94/roitr&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The intrinsic rotation invariance lies at the core of matching point clouds
with handcrafted descriptors. However, it is widely despised by recent deep
matchers that obtain the rotation invariance extrinsically via data
augmentation. As the finite number of augmented rotations can never span the
continuous SO(3) space, these methods usually show instability when facing
rotations that are rarely seen. To this end, we introduce RoITr, a
Rotation-Invariant Transformer to cope with the pose variations in the point
cloud matching task. We contribute both on the local and global levels.
Starting from the local level, we introduce an attention mechanism embedded
with Point Pair Feature (PPF)-based coordinates to describe the pose-invariant
geometry, upon which a novel attention-based encoder-decoder architecture is
constructed. We further propose a global transformer with rotation-invariant
cross-frame spatial awareness learned by the self-attention mechanism, which
significantly improves the feature distinctiveness and makes the model robust
with respect to the low overlap. Experiments are conducted on both the rigid
and non-rigid public benchmarks, where RoITr outperforms all the
state-of-the-art models by a considerable margin in the low-overlapping
scenarios. Especially when the rotations are enlarged on the challenging
3DLoMatch benchmark, RoITr surpasses the existing methods by at least 13 and 5
percentage points in terms of Inlier Ratio and Registration Recall,
respectively.</description>
      <guid isPermaLink="false">2303.08231v3</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Learning for Character Detection in Ancient Greek Papyri</title>
      <link>http://arxiv.org/abs/2409.10156v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究目的&lt;/h4&gt;   - 本论文研究SimCLR对希腊字母识别的有效性，特别关注不同增强技术的影响。&lt;br&gt;&lt;h4&gt;2. 预训练与微调&lt;/h4&gt;   - 使用Alpub数据集进行SimCLR的预训练，然后在较小的ICDAR数据集上进行微调，以比较SimCLR与传统基线模型的表现。&lt;br&gt;&lt;h4&gt;3. 比较的基线模型&lt;/h4&gt;   - 研究中比较了三种模型：&lt;br&gt;     1. 使用交叉熵损失的基线模型。&lt;br&gt;     2. 带分类层的三元嵌入模型。&lt;br&gt;     3. 带分类层的SimCLR预训练模型。&lt;br&gt;&lt;h4&gt;4. 数据增强策略&lt;/h4&gt;   - 重点探讨了不同的数据增强策略，这对SimCLR的训练过程至关重要。&lt;br&gt;   - 初步训练中使用93种增强方法，最终通过统计t检验选择出表现最好的四种增强。&lt;br&gt;&lt;h4&gt;5. 训练过程&lt;/h4&gt;   - SimCLR在Alpub数据集上进行预训练，之后在ICDAR数据集上微调。&lt;br&gt;   - 三元损失模型也经历类似的过程，先在四种最佳增强上预训练，再微调于ICDAR。&lt;br&gt;&lt;h4&gt;6. 实验结果&lt;/h4&gt;   - 实验表明，SimCLR在字母识别任务中的表现不如基线模型。&lt;br&gt;   - 交叉熵损失的基线模型表现优于SimCLR和三元损失模型。&lt;br&gt;&lt;h4&gt;7. 研究贡献&lt;/h4&gt;   - 本研究详细评估了对比学习在字母识别中的应用，突显了SimCLR的局限性，同时强调了传统监督学习模型的优势。&lt;br&gt;&lt;h4&gt;8. 潜在问题&lt;/h4&gt;   - 认为SimCLR的裁剪策略可能导致输入图像的语义偏移，从而降低训练效果，尽管预训练数据集规模较大。&lt;br&gt;&lt;h4&gt;9. 代码发布&lt;/h4&gt;   - 研究代码已公开，链接为：[GitHub - MT_augmentation_and_contrastive_learning](https://github.com/DIVA-DIA/MT_augmentation_and_contrastive_learning/)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This thesis investigates the effectiveness of SimCLR, a contrastive learning
technique, in Greek letter recognition, focusing on the impact of various
augmentation techniques. We pretrain the SimCLR backbone using the Alpub
dataset (pretraining dataset) and fine-tune it on a smaller ICDAR dataset
(finetuning dataset) to compare SimCLR's performance against traditional
baseline models, which use cross-entropy and triplet loss functions.
Additionally, we explore the role of different data augmentation strategies,
essential for the SimCLR training process. Methodologically, we examine three
primary approaches: (1) a baseline model using cross-entropy loss, (2) a
triplet embedding model with a classification layer, and (3) a SimCLR
pretrained model with a classification layer. Initially, we train the baseline,
triplet, and SimCLR models using 93 augmentations on ResNet-18 and ResNet-50
networks with the ICDAR dataset. From these, the top four augmentations are
selected using a statistical t-test. Pretraining of SimCLR is conducted on the
Alpub dataset, followed by fine-tuning on the ICDAR dataset. The triplet loss
model undergoes a similar process, being pretrained on the top four
augmentations before fine-tuning on ICDAR. Our experiments show that SimCLR
does not outperform the baselines in letter recognition tasks. The baseline
model with cross-entropy loss demonstrates better performance than both SimCLR
and the triplet loss model. This study provides a detailed evaluation of
contrastive learning for letter recognition, highlighting SimCLR's limitations
while emphasizing the strengths of traditional supervised learning models in
this task. We believe SimCLR's cropping strategies may cause a semantic shift
in the input image, reducing training effectiveness despite the large
pretraining dataset. Our code is available at
https://github.com/DIVA-DIA/MT_augmentation_and_contrastive_learning/.</description>
      <guid isPermaLink="false">2409.10156v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Digital Twin Tracking Dataset (DTTD): A New RGB+Depth 3D Dataset for Longer-Range Object Tracking Applications</title>
      <link>http://arxiv.org/abs/2302.05991v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 数字双胞胎概念&lt;/h4&gt;   - 数字双胞胎是将真实物体与其数字化对应物结合的技术。&lt;br&gt;   - 该技术在增强现实（AR）、自主系统和用户界面/用户体验（UI/UX）等领域有广泛应用。&lt;br&gt;&lt;h4&gt;2. 实时3D物体追踪的重要性&lt;/h4&gt;   - 高效的数字双胞胎系统需要实时且准确的3D物体追踪。&lt;br&gt;   - 目前的研究大多集中在机器人抓取领域，使用旧款深度传感器。&lt;br&gt;&lt;h4&gt;3. 现有性能指标的局限性&lt;/h4&gt;   - 传统的性能指标可能不适用于AR等其他数字双胞胎应用。&lt;br&gt;&lt;h4&gt;4. 新数据集的创建&lt;/h4&gt;   - 本研究创建了一个名为“数字双胞胎追踪数据集（DTTD）”的新RGB-D数据集。&lt;br&gt;   - 该数据集旨在促进相关研究，并扩展潜在解决方案，以实现更长距离和毫米级的定位精度。&lt;br&gt;&lt;h4&gt;5. 数据采集技术&lt;/h4&gt;   - 选择最新的微软Azure Kinect作为最先进的飞行时间（ToF）相机，以减少点云噪声。&lt;br&gt;&lt;h4&gt;6. 数据集内容&lt;/h4&gt;   - 数据集包含103个场景，涵盖10种常见的丰富纹理的商品。&lt;br&gt;   - 每帧数据都有像素级语义分割和真实物体姿态的标注，后者由商业运动捕捉系统提供。&lt;br&gt;&lt;h4&gt;7. 实验与分析&lt;/h4&gt;   - 通过模型级和数据集级的广泛实验，证明了DTTD能够帮助研究人员开发未来的物体追踪方法，并分析新挑战。&lt;br&gt;&lt;h4&gt;8. 开源发布&lt;/h4&gt;   - 数据集、数据生成、标注和模型评估的流程已作为开源代码公开发布，链接为：[GitHub - DTTDv1](https://github.com/augcog/DTTDv1)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2023-02-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/augcog/dttdv1&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Digital twin is a problem of augmenting real objects with their digital
counterparts. It can underpin a wide range of applications in augmented reality
(AR), autonomy, and UI/UX. A critical component in a good digital-twin system
is real-time, accurate 3D object tracking. Most existing works solve 3D object
tracking through the lens of robotic grasping, employ older generations of
depth sensors, and measure performance metrics that may not apply to other
digital-twin applications such as in AR. In this work, we create a novel RGB-D
dataset, called Digital Twin Tracking Dataset (DTTD), to enable further
research of the problem and extend potential solutions towards longer ranges
and mm localization accuracy. To reduce point cloud noise from the input
source, we select the latest Microsoft Azure Kinect as the state-of-the-art
time-of-flight (ToF) camera. In total, 103 scenes of 10 common off-the-shelf
objects with rich textures are recorded, with each frame annotated with a
per-pixel semantic segmentation and ground-truth object poses provided by a
commercial motion capturing system. Through extensive experiments with
model-level and dataset-level analysis, we demonstrate that DTTD can help
researchers develop future object tracking methods and analyze new challenges.
The dataset, data generation, annotation, and model evaluation pipeline are
made publicly available as open source code at:
https://github.com/augcog/DTTDv1.</description>
      <guid isPermaLink="false">2302.05991v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>MFCLIP: Multi-modal Fine-grained CLIP for Generalizable Diffusion Face Forgery Detection</title>
      <link>http://arxiv.org/abs/2409.09724v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 照片级真实感人脸生成技术的快速发展引发了社会和学术界的重大关注，迫切需要鲁棒且具通用性的面部伪造检测（FFD）技术。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 现有方法主要通过图像模态捕捉面部伪造模式，但未充分探索细粒度噪声和文本等其他模态，限制了模型的泛化能力。&lt;br&gt;&lt;h4&gt;3. 检测挑战&lt;/h4&gt;   - 大多数FFD方法主要识别由生成对抗网络（GAN）生成的面部图像，但在检测未见过的扩散合成图像方面存在困难。&lt;br&gt;&lt;h4&gt;4. 研究目标&lt;/h4&gt;   - 本文旨在利用前沿基础模型对比语言-图像预训练（CLIP）实现通用的扩散面部伪造检测（DFFD）。&lt;br&gt;&lt;h4&gt;5. 模型提出&lt;/h4&gt;   - 提出了新颖的多模态细粒度CLIP（MFCLIP）模型，通过语言引导的面部伪造表示学习，挖掘图像-噪声模态下的全面且细粒度的伪造痕迹。&lt;br&gt;&lt;h4&gt;6. 细粒度语言编码器&lt;/h4&gt;   - 设计了一种细粒度语言编码器（FLE），从层次化文本提示中提取细粒度的全局语言特征。&lt;br&gt;&lt;h4&gt;7. 多模态视觉编码器&lt;/h4&gt;   - 设计了一种多模态视觉编码器（MVE），捕捉全局图像伪造嵌入和从最丰富的图块中提取的细粒度噪声伪造模式，并将其整合以挖掘一般视觉伪造痕迹。&lt;br&gt;&lt;h4&gt;8. 样本对注意机制&lt;/h4&gt;   - 构建了创新的即插即用样本对注意（SPA）方法，强调相关负样本对并抑制无关样本对，使跨模态样本对能够进行更灵活的对齐。&lt;br&gt;&lt;h4&gt;9. 实验结果&lt;/h4&gt;   - 大量实验和可视化结果表明，我们的模型在不同设置下，如跨生成器、跨伪造和跨数据集评估中，优于现有技术。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid development of photo-realistic face generation methods has raised
significant concerns in society and academia, highlighting the urgent need for
robust and generalizable face forgery detection (FFD) techniques. Although
existing approaches mainly capture face forgery patterns using image modality,
other modalities like fine-grained noises and texts are not fully explored,
which limits the generalization capability of the model. In addition, most FFD
methods tend to identify facial images generated by GAN, but struggle to detect
unseen diffusion-synthesized ones. To address the limitations, we aim to
leverage the cutting-edge foundation model, contrastive language-image
pre-training (CLIP), to achieve generalizable diffusion face forgery detection
(DFFD). In this paper, we propose a novel multi-modal fine-grained CLIP
(MFCLIP) model, which mines comprehensive and fine-grained forgery traces
across image-noise modalities via language-guided face forgery representation
learning, to facilitate the advancement of DFFD. Specifically, we devise a
fine-grained language encoder (FLE) that extracts fine global language features
from hierarchical text prompts. We design a multi-modal vision encoder (MVE) to
capture global image forgery embeddings as well as fine-grained noise forgery
patterns extracted from the richest patch, and integrate them to mine general
visual forgery traces. Moreover, we build an innovative plug-and-play sample
pair attention (SPA) method to emphasize relevant negative pairs and suppress
irrelevant ones, allowing cross-modality sample pairs to conduct more flexible
alignment. Extensive experiments and visualizations show that our model
outperforms the state of the arts on different settings like cross-generator,
cross-forgery, and cross-dataset evaluations.</description>
      <guid isPermaLink="false">2409.09724v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Multi-V2X: A Large Scale Multi-modal Multi-penetration-rate Dataset for Cooperative Perception</title>
      <link>http://arxiv.org/abs/2409.04980v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 4 figures, 5 tables&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 通过车辆与一切（V2X）技术的合作感知因其克服遮挡和增强远程感知的潜力而受到广泛关注。&lt;br&gt;&lt;h4&gt;2. 现有数据集问题&lt;/h4&gt;   - 现有的真实世界数据集存在可通信代理数量少的问题，而合成数据集通常仅涵盖车辆。&lt;br&gt;&lt;h4&gt;3. 连接与自动驾驶车辆的渗透率&lt;/h4&gt;   - 连接与自动驾驶车辆（CAVs）的渗透率是部署合作感知技术的关键因素，但尚未得到充分探讨。&lt;br&gt;&lt;h4&gt;4. 数据集介绍&lt;/h4&gt;   - 提出Multi-V2X，这是一个大规模、多模态、多渗透率的数据集，用于V2X感知。&lt;br&gt;&lt;h4&gt;5. 数据生成方法&lt;/h4&gt;   - 通过共同模拟SUMO和CARLA，在模拟城镇中为大量车辆和路边单元（RSUs）配备传感器并收集全面的感知数据。&lt;br&gt;&lt;h4&gt;6. 渗透率调整&lt;/h4&gt;   - 通过将一些装备车辆伪装为普通车辆，可以获得具有特定CAV渗透率的数据集。&lt;br&gt;&lt;h4&gt;7. 数据集规模&lt;/h4&gt;   - Multi-V2X数据集包含549k RGB帧、146k LiDAR帧和4,219k标注的3D边界框，覆盖六个类别。&lt;br&gt;&lt;h4&gt;8. 渗透率和通信范围&lt;/h4&gt;   - 最高CAV渗透率达到86.21%，在通信范围内最多可有31个代理，给选择合作代理带来了新挑战。&lt;br&gt;&lt;h4&gt;9. 基准测试&lt;/h4&gt;   - 提供了合作3D目标检测任务的全面基准测试。&lt;br&gt;&lt;h4&gt;10. 数据与代码可用性&lt;/h4&gt;    - 数据和代码可在 [GitHub](https://github.com/RadetzkyLi/Multi-V2X) 上获取。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cooperative perception through vehicle-to-everything (V2X) has garnered
significant attention in recent years due to its potential to overcome
occlusions and enhance long-distance perception. Great achievements have been
made in both datasets and algorithms. However, existing real-world datasets are
limited by the presence of few communicable agents, while synthetic datasets
typically cover only vehicles. More importantly, the penetration rate of
connected and autonomous vehicles (CAVs) , a critical factor for the deployment
of cooperative perception technologies, has not been adequately addressed. To
tackle these issues, we introduce Multi-V2X, a large-scale, multi-modal,
multi-penetration-rate dataset for V2X perception. By co-simulating SUMO and
CARLA, we equip a substantial number of cars and roadside units (RSUs) in
simulated towns with sensor suites, and collect comprehensive sensing data.
Datasets with specified CAV penetration rates can be obtained by masking some
equipped cars as normal vehicles. In total, our Multi-V2X dataset comprises
549k RGB frames, 146k LiDAR frames, and 4,219k annotated 3D bounding boxes
across six categories. The highest possible CAV penetration rate reaches
86.21%, with up to 31 agents in communication range, posing new challenges in
selecting agents to collaborate with. We provide comprehensive benchmarks for
cooperative 3D object detection tasks. Our data and code are available at
https://github.com/RadetzkyLi/Multi-V2X .</description>
      <guid isPermaLink="false">2409.04980v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Forecasting Market Prices using DL with Data Augmentation and Meta-learning: ARIMA still wins!</title>
      <link>http://arxiv.org/abs/2110.10233v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Camera Ready Version for ICBINB Workshop @ NeurIPS 2021&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 深度学习技术已成功应用于时间序列预测，并在许多标准基准数据集上相较于传统技术表现出色。&lt;br&gt;&lt;h4&gt;2. 研究目的&lt;/h4&gt;   - 本文提供了深度学习技术在金融市场价格预测中的综合比较研究。&lt;br&gt;&lt;h4&gt;3. 基准测试&lt;/h4&gt;   - 对先进的深度学习基线模型（如NBeats等）进行基准测试，数据来源于货币市场和股票市场。&lt;br&gt;&lt;h4&gt;4. 合成数据生成&lt;/h4&gt;   - 使用基于模糊逻辑的模型生成合成数据，模型的需求由技术规则（如移动平均线）驱动，这些规则常被交易者使用。&lt;br&gt;&lt;h4&gt;5. 数据增强&lt;/h4&gt;   - 在合成数据上对基线技术进行基准测试，并利用合成数据进行数据增强。&lt;br&gt;&lt;h4&gt;6. 元学习应用&lt;/h4&gt;   - 应用基于梯度的元学习来考虑金融时间序列的非平稳性。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 尽管进行了广泛的实验，令人惊讶的结果是，标准的ARIMA模型在数据增强或元学习的情况下仍然优于深度学习模型。&lt;br&gt;&lt;h4&gt;8. 结论与推测&lt;/h4&gt;   - 最后，作者对为何会出现这种情况进行了推测。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2021-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep-learning techniques have been successfully used for time-series
forecasting and have often shown superior performance on many standard
benchmark datasets as compared to traditional techniques. Here we present a
comprehensive and comparative study of performance of deep-learning techniques
for forecasting prices in financial markets. We benchmark state-of-the-art
deep-learning baselines, such as NBeats, etc., on data from currency as well as
stock markets. We also generate synthetic data using a fuzzy-logic based model
of demand driven by technical rules such as moving averages, which are often
used by traders. We benchmark the baseline techniques on this synthetic data as
well as use it for data augmentation. We also apply gradient-based
meta-learning to account for non-stationarity of financial time-series. Our
extensive experiments notwithstanding, the surprising result is that the
standard ARIMA models outperforms deep-learning even using data augmentation or
meta-learning. We conclude by speculating as to why this might be the case.</description>
      <guid isPermaLink="false">2110.10233v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>EyeCLIP: A visual-language foundation model for multi-modal ophthalmic image analysis</title>
      <link>http://arxiv.org/abs/2409.06644v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 早期检测眼病（如青光眼、黄斑变性和糖尿病视网膜病变）对预防视力丧失至关重要。&lt;br&gt;&lt;h4&gt;2. 现有技术局限&lt;/h4&gt;   - 尽管人工智能基础模型在解决这些挑战方面具有很大潜力，但现有的眼科基础模型主要集中于单一模态，而眼病诊断需要多种模态的信息。&lt;br&gt;&lt;h4&gt;3. 多视角信息的重要性&lt;/h4&gt;   - 充分利用同一患者的多视角信息是一个关键但常被忽视的方面。&lt;br&gt;&lt;h4&gt;4. 疾病特征的长尾性质&lt;/h4&gt;   - 由于眼科疾病的长尾特性，标准的完全监督或无监督学习方法常常难以奏效。&lt;br&gt;&lt;h4&gt;5. 整合临床文本&lt;/h4&gt;   - 因此，整合临床文本以捕捉更广泛的疾病谱系是必要的。&lt;br&gt;&lt;h4&gt;6. 方法提出&lt;/h4&gt;   - 提出了EyeCLIP，这是一种使用超过277万张多模态眼科图像和部分文本数据开发的视觉-语言基础模型。&lt;br&gt;&lt;h4&gt;7. 预训练策略&lt;/h4&gt;   - 为充分利用大量的多模态未标记和标记数据，介绍了一种结合自监督重建、多模态图像对比学习和图像-文本对比学习的预训练策略，以学习多模态的共享表示。&lt;br&gt;&lt;h4&gt;8. 评估与成果&lt;/h4&gt;   - 通过14个基准数据集的评估，EyeCLIP能转移到涉及眼科和系统性疾病的多种下游任务，并在疾病分类、视觉问答和跨模态检索中实现了最先进的性能。&lt;br&gt;&lt;h4&gt;9. 方法优势&lt;/h4&gt;   - EyeCLIP在以前的方法基础上取得了显著进展，特别是在现实世界的长尾场景中展示了少样本甚至无样本的能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Early detection of eye diseases like glaucoma, macular degeneration, and
diabetic retinopathy is crucial for preventing vision loss. While artificial
intelligence (AI) foundation models hold significant promise for addressing
these challenges, existing ophthalmic foundation models primarily focus on a
single modality, whereas diagnosing eye diseases requires multiple modalities.
A critical yet often overlooked aspect is harnessing the multi-view information
across various modalities for the same patient. Additionally, due to the
long-tail nature of ophthalmic diseases, standard fully supervised or
unsupervised learning approaches often struggle. Therefore, it is essential to
integrate clinical text to capture a broader spectrum of diseases. We propose
EyeCLIP, a visual-language foundation model developed using over 2.77 million
multi-modal ophthalmology images with partial text data. To fully leverage the
large multi-modal unlabeled and labeled data, we introduced a pretraining
strategy that combines self-supervised reconstructions, multi-modal image
contrastive learning, and image-text contrastive learning to learn a shared
representation of multiple modalities. Through evaluation using 14 benchmark
datasets, EyeCLIP can be transferred to a wide range of downstream tasks
involving ocular and systemic diseases, achieving state-of-the-art performance
in disease classification, visual question answering, and cross-modal
retrieval. EyeCLIP represents a significant advancement over previous methods,
especially showcasing few-shot, even zero-shot capabilities in real-world
long-tail scenarios.</description>
      <guid isPermaLink="false">2409.06644v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Seeking the Sufficiency and Necessity Causal Features in Multimodal Representation Learning</title>
      <link>http://arxiv.org/abs/2408.16577v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究主题&lt;/h4&gt;   - 本文探讨了具有高概率必要与充分原因（PNS）表示的学习，旨在增强深度学习模型的能力。&lt;br&gt;&lt;h4&gt;2. 任务定义&lt;/h4&gt;   - 该任务涉及识别既是充分（保证结果发生）又是必要（没有它结果无法发生）的因果特征。&lt;br&gt;&lt;h4&gt;3. 现有研究局限&lt;/h4&gt;   - 当前研究主要集中在单一模态数据上，扩展PNS学习到多模态环境面临重大挑战。&lt;br&gt;&lt;h4&gt;4. 挑战分析&lt;/h4&gt;   - 在多模态背景下，需要重新考虑PNS可识别性的条件，即外生性（Exogeneity）和单调性（Monotonicity），因为充分和必要的因果特征分布在不同模态之间。&lt;br&gt;&lt;h4&gt;5. 解决方案&lt;/h4&gt;   - 提出将多模态表示概念化为模态不变和模态特定的组成部分。&lt;br&gt;&lt;h4&gt;6. 可识别性分析&lt;/h4&gt;   - 针对每个组成部分分析PNS可识别性，同时确保非平凡的PNS估计。&lt;br&gt;&lt;h4&gt;7. 优化目标&lt;/h4&gt;   - 形成可处理的优化目标，使多模态模型能够学习高PNS表示，从而增强预测性能。&lt;br&gt;&lt;h4&gt;8. 实验验证&lt;/h4&gt;   - 实验结果证明了该方法在合成数据和真实数据上的有效性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning representations with a high Probability of Necessary and Sufficient
Causes (PNS) has been shown to enhance deep learning models' ability. This task
involves identifying causal features that are both sufficient (guaranteeing the
outcome) and necessary (without which the outcome cannot occur). However,
current research predominantly focuses on unimodal data, and extending PNS
learning to multimodal settings presents significant challenges. The challenges
arise as the conditions for PNS identifiability, Exogeneity and Monotonicity,
need to be reconsidered in a multimodal context, where sufficient and necessary
causal features are distributed across different modalities. To address this,
we first propose conceptualizing multimodal representations as comprising
modality-invariant and modality-specific components. We then analyze PNS
identifiability for each component, while ensuring non-trivial PNS estimation.
Finally, we formulate tractable optimization objectives that enable multimodal
models to learn high-PNS representations, thereby enhancing their predictive
performance. Experiments demonstrate the effectiveness of our method on both
synthetic and real-world data.</description>
      <guid isPermaLink="false">2408.16577v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Correspondence-Free SE(3) Point Cloud Registration in RKHS via Unsupervised Equivariant Learning</title>
      <link>http://arxiv.org/abs/2407.20223v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, to be published in ECCV 2024&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究主题&lt;/h4&gt;   - 本文提出了一种稳健的无监督SE(3)点云配准方法，不需要点对应关系。&lt;br&gt;&lt;h4&gt;2. 方法框架&lt;/h4&gt;   - 将点云视为在重现核Hilbert空间（RKHS）中的函数，利用SE(3)等变特征进行直接特征空间配准。&lt;br&gt;&lt;h4&gt;3. 新颖性&lt;/h4&gt;   - 提出了新的RKHS距离度量，能够在噪声、离群点和不对称数据中提供可靠的性能。&lt;br&gt;&lt;h4&gt;4. 训练方法&lt;/h4&gt;   - 采用无监督训练方法，有效处理有限的真实数据，便于适应真实数据集。&lt;br&gt;&lt;h4&gt;5. 实验结果&lt;/h4&gt;   - 提出的算法在合成数据集（ModelNet40）和现实世界数据集（ETH3D）上，相较于传统和监督方法，显示出更高的配准准确性。&lt;br&gt;&lt;h4&gt;6. 研究贡献&lt;/h4&gt;   - 据我们所知，这是首次成功使用等变方法进行真实RGB-D里程计数据配准。&lt;br&gt;&lt;h4&gt;7. 代码可用性&lt;/h4&gt;   - 相关代码可在 [EquivAlign](https://sites.google.com/view/eccv24-equivalign) 获取。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces a robust unsupervised SE(3) point cloud registration
method that operates without requiring point correspondences. The method frames
point clouds as functions in a reproducing kernel Hilbert space (RKHS),
leveraging SE(3)-equivariant features for direct feature space registration. A
novel RKHS distance metric is proposed, offering reliable performance amidst
noise, outliers, and asymmetrical data. An unsupervised training approach is
introduced to effectively handle limited ground truth data, facilitating
adaptation to real datasets. The proposed method outperforms classical and
supervised methods in terms of registration accuracy on both synthetic
(ModelNet40) and real-world (ETH3D) noisy, outlier-rich datasets. To our best
knowledge, this marks the first instance of successful real RGB-D odometry data
registration using an equivariant method. The code is available at
{https://sites.google.com/view/eccv24-equivalign}</description>
      <guid isPermaLink="false">2407.20223v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Few-Shot Learning: Expanding ID Cards Presentation Attack Detection to Unknown ID Countries</title>
      <link>http://arxiv.org/abs/2409.06842v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究主题&lt;/h4&gt;   - 本文提出了一种用于检测身份证上的展示攻击（Presentation Attacks）的少样本学习（Few-shot Learning, FSL）方法，适用于远程验证系统并可扩展到新国家。&lt;br&gt;&lt;h4&gt;2. 研究背景&lt;/h4&gt;   - 分析了原型网络（Prototypical Networks）在西班牙和智利的文档上的性能，作为基线。&lt;br&gt;&lt;h4&gt;3. 扩展能力评估&lt;/h4&gt;   - 评估了该方法在新国家（如阿根廷和哥斯达黎加）上的泛化能力。&lt;br&gt;&lt;h4&gt;4. 针对展示攻击的挑战&lt;/h4&gt;   - 特别针对屏幕显示展示攻击的挑战。&lt;br&gt;&lt;h4&gt;5. 方法论&lt;/h4&gt;   - 利用卷积架构和嵌入在原型网络中的元学习原则，构建了一个在少样本示例上表现出高效能的模型。&lt;br&gt;&lt;h4&gt;6. 实验结果&lt;/h4&gt;   - 研究表明，只需五个独特身份和每个新国家少于100张图像即可实现竞争性能。&lt;br&gt;&lt;h4&gt;7. 研究意义&lt;/h4&gt;   - 该研究为未知攻击提供了新的见解，开启了针对身份证的新型泛化展示攻击检测的可能性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper proposes a Few-shot Learning (FSL) approach for detecting
Presentation Attacks on ID Cards deployed in a remote verification system and
its extension to new countries. Our research analyses the performance of
Prototypical Networks across documents from Spain and Chile as a baseline and
measures the extension of generalisation capabilities of new ID Card countries
such as Argentina and Costa Rica. Specifically targeting the challenge of
screen display presentation attacks. By leveraging convolutional architectures
and meta-learning principles embodied in Prototypical Networks, we have crafted
a model that demonstrates high efficacy with Few-shot examples. This research
reveals that competitive performance can be achieved with as Few-shots as five
unique identities and with under 100 images per new country added. This opens a
new insight for novel generalised Presentation Attack Detection on ID cards to
unknown attacks.</description>
      <guid isPermaLink="false">2409.06842v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Uncertainty and Prediction Quality Estimation for Semantic Segmentation via Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2409.11373v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 3 figures, submitted to BMVC "Workshop on Robust
  Recognition in the Open World" (https://rrow2024.github.io/call-for-papers)&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 在安全关键应用（如汽车感知或医学成像）中，使用深度神经网络（DNN）进行语义分割时，实时估计其性能非常重要，例如通过不确定性估计或预测质量估计。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 之前的研究主要在像素级别进行不确定性估计。另一些研究采用了连通分量（分段）视角，通过所谓的元分类和回归来进行对象级的不确定性和预测质量估计。&lt;br&gt;&lt;h4&gt;3. 个体处理问题&lt;/h4&gt;   - 在这些方法中，每个预测的段被单独考虑，以估计其不确定性或预测质量，但忽视了邻近段可能提供的额外信息。&lt;br&gt;&lt;h4&gt;4. 研究创新&lt;/h4&gt;   - 本文研究了如何利用邻近段的信息来评估给定预测段的质量，基于段级不确定性指标。&lt;br&gt;&lt;h4&gt;5. 使用图神经网络&lt;/h4&gt;   - 采用图神经网络（GNN）来建模给定段质量与其自身指标及其邻近段指标之间的关系。&lt;br&gt;&lt;h4&gt;6. 性能比较&lt;/h4&gt;   - 比较了不同的GNN架构，取得了显著的性能提升。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; When employing deep neural networks (DNNs) for semantic segmentation in
safety-critical applications like automotive perception or medical imaging, it
is important to estimate their performance at runtime, e.g. via uncertainty
estimates or prediction quality estimates. Previous works mostly performed
uncertainty estimation on pixel-level. In a line of research, a
connected-component-wise (segment-wise) perspective was taken, approaching
uncertainty estimation on an object-level by performing so-called meta
classification and regression to estimate uncertainty and prediction quality,
respectively. In those works, each predicted segment is considered individually
to estimate its uncertainty or prediction quality. However, the neighboring
segments may provide additional hints on whether a given predicted segment is
of high quality, which we study in the present work. On the basis of
uncertainty indicating metrics on segment-level, we use graph neural networks
(GNNs) to model the relationship of a given segment's quality as a function of
the given segment's metrics as well as those of its neighboring segments. We
compare different GNN architectures and achieve a notable performance
improvement.</description>
      <guid isPermaLink="false">2409.11373v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>A Robust Probability-based Joint Registration Method of Multiple Point Clouds Considering Local Consistency</title>
      <link>http://arxiv.org/abs/2409.09682v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to ICRA 2025&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 在机器人检测中，多个点云的联合配准是估计测量部件（如螺旋桨中的多个叶片）之间变换关系的重要技术。&lt;br&gt;&lt;h4&gt;2. 问题描述&lt;/h4&gt;   - 数据中的噪声和离群点会显著影响配准性能，进而影响对应关系的正确性。&lt;br&gt;&lt;h4&gt;3. 提出的解决方案&lt;/h4&gt;   - 本文将局部一致性属性纳入基于概率的联合配准方法中，以解决上述问题。&lt;br&gt;&lt;h4&gt;4. 模型框架&lt;/h4&gt;   - 将每个测量点集视为未知高斯混合模型（GMM）的样本，并将配准问题框架化为估计概率模型。&lt;br&gt;&lt;h4&gt;5. 优化过程&lt;/h4&gt;   - 通过将局部一致性纳入优化过程，增强后验分布的稳健性和准确性，这些分布直接决定了配准结果的一对多对应关系。&lt;br&gt;&lt;h4&gt;6. 算法实现&lt;/h4&gt;   - 通过期望最大化（EM）算法推导出变换和概率参数的有效闭式解。&lt;br&gt;&lt;h4&gt;7. 实验验证&lt;/h4&gt;   - 大量实验表明，该方法在存在噪声和离群点的情况下，性能优于现有方法，达到了高准确性和稳健性。&lt;br&gt;&lt;h4&gt;8. 代码可用性&lt;/h4&gt;   - 相关代码将发布在GitHub上，链接为 [JPRLC_registration](https://github.com/sulingjie/JPRLC_registration)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In robotic inspection, joint registration of multiple point clouds is an
essential technique for estimating the transformation relationships between
measured parts, such as multiple blades in a propeller. However, the presence
of noise and outliers in the data can significantly impair the registration
performance by affecting the correctness of correspondences. To address this
issue, we incorporate local consistency property into the probability-based
joint registration method. Specifically, each measured point set is treated as
a sample from an unknown Gaussian Mixture Model (GMM), and the registration
problem is framed as estimating the probability model. By incorporating local
consistency into the optimization process, we enhance the robustness and
accuracy of the posterior distributions, which represent the one-to-all
correspondences that directly determine the registration results. Effective
closed-form solution for transformation and probability parameters are derived
with Expectation-Maximization (EM) algorithm. Extensive experiments demonstrate
that our method outperforms the existing methods, achieving high accuracy and
robustness with the existence of noise and outliers. The code will be available
at https://github.com/sulingjie/JPRLC_registration.</description>
      <guid isPermaLink="false">2409.09682v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Towards Unified Representation of Multi-Modal Pre-training for 3D Understanding via Differentiable Rendering</title>
      <link>http://arxiv.org/abs/2404.13619v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 目前最先进的3D模型在识别任务中表现出色，但通常依赖于大规模数据集和明确的类别集合。&lt;br&gt;&lt;h4&gt;2. 多模态预训练的进展&lt;/h4&gt;   - 最近的多模态预训练进展显示，通过对齐3D形状与其2D RGB或深度图像的特征，可以有效学习3D表示。&lt;br&gt;&lt;h4&gt;3. 现有框架的局限性&lt;/h4&gt;   - 现有框架通常只依赖于RGB或深度图像，限制了其在3D应用中利用全面多模态数据的有效性。&lt;br&gt;&lt;h4&gt;4. 提出的新框架&lt;/h4&gt;   - 本文提出了DR-Point，这是一个三模态预训练框架，通过利用来自每种模态的物体三元组学习统一表示，包括RGB图像、深度图像和3D点云。&lt;br&gt;&lt;h4&gt;5. 解决三元组稀缺性&lt;/h4&gt;   - 为了解决三元组稀缺的问题，DR-Point采用可微渲染技术生成多种深度图像。这一方法不仅增加了深度图像的供给，还提高了重建点云的准确性。&lt;br&gt;&lt;h4&gt;6. Transformer骨干网络的学习&lt;/h4&gt;   - 通过使用有限数量的合成生成三元组，DR-Point有效学习与RGB-深度图像空间无缝对齐的3D表示空间。&lt;br&gt;&lt;h4&gt;7. 实验验证&lt;/h4&gt;   - 大量实验表明，DR-Point在多种下游任务中超越了现有自监督学习方法，任务包括3D物体分类、部件分割、点云补全、语义分割和检测。&lt;br&gt;&lt;h4&gt;8. 消融研究&lt;/h4&gt;   - 消融研究验证了DR-Point在增强点云理解方面的有效性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-04-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; State-of-the-art 3D models, which excel in recognition tasks, typically
depend on large-scale datasets and well-defined category sets. Recent advances
in multi-modal pre-training have demonstrated potential in learning 3D
representations by aligning features from 3D shapes with their 2D RGB or depth
counterparts. However, these existing frameworks often rely solely on either
RGB or depth images, limiting their effectiveness in harnessing a comprehensive
range of multi-modal data for 3D applications. To tackle this challenge, we
present DR-Point, a tri-modal pre-training framework that learns a unified
representation of RGB images, depth images, and 3D point clouds by pre-training
with object triplets garnered from each modality. To address the scarcity of
such triplets, DR-Point employs differentiable rendering to obtain various
depth images. This approach not only augments the supply of depth images but
also enhances the accuracy of reconstructed point clouds, thereby promoting the
representative learning of the Transformer backbone. Subsequently, using a
limited number of synthetically generated triplets, DR-Point effectively learns
a 3D representation space that aligns seamlessly with the RGB-Depth image
space. Our extensive experiments demonstrate that DR-Point outperforms existing
self-supervised learning methods in a wide range of downstream tasks, including
3D object classification, part segmentation, point cloud completion, semantic
segmentation, and detection. Additionally, our ablation studies validate the
effectiveness of DR-Point in enhancing point cloud understanding.</description>
      <guid isPermaLink="false">2404.13619v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Dual-level Adaptive Self-Labeling for Novel Class Discovery in Point Cloud Segmentation</title>
      <link>http://arxiv.org/abs/2407.12489v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ECCV2024&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究主题&lt;/h4&gt;   - 论文聚焦于点云分割中的新类别发现，目的是基于已知类别的语义知识发现新类别。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 现有工作提出了一种在线逐点聚类方法，但它对新类别施加了简化的相等类大小约束，以避免退化解决方案。&lt;br&gt;&lt;h4&gt;3. 新类别的不平衡分布&lt;/h4&gt;   - 点云中新类别的固有不平衡分布通常会违反相等类大小的约束，导致聚类效果不佳。&lt;br&gt;&lt;h4&gt;4. 上下文信息的缺失&lt;/h4&gt;   - 逐点聚类忽视了物体的丰富空间上下文信息，这会导致语义分割的表示能力不足。&lt;br&gt;&lt;h4&gt;5. 提出的新策略&lt;/h4&gt;   - 本文提出了一种新颖的自标记策略，在模型训练过程中自适应生成高质量的伪标签，以应对不平衡类别的问题。&lt;br&gt;&lt;h4&gt;6. 双层表示法&lt;/h4&gt;   - 开发了一个双层表示法，将区域一致性融入点级分类器学习中，从而减少生成分割时的噪声。&lt;br&gt;&lt;h4&gt;7. 实验验证&lt;/h4&gt;   - 在两个广泛使用的数据集（SemanticKITTI和SemanticPOSS）上进行了广泛实验，结果表明该方法在性能上显著优于现有最先进的方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-07-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/RikkiXu/NCD_PC&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We tackle the novel class discovery in point cloud segmentation, which
discovers novel classes based on the semantic knowledge of seen classes.
Existing work proposes an online point-wise clustering method with a simplified
equal class-size constraint on the novel classes to avoid degenerate solutions.
However, the inherent imbalanced distribution of novel classes in point clouds
typically violates the equal class-size constraint. Moreover, point-wise
clustering ignores the rich spatial context information of objects, which
results in less expressive representation for semantic segmentation. To address
the above challenges, we propose a novel self-labeling strategy that adaptively
generates high-quality pseudo-labels for imbalanced classes during model
training. In addition, we develop a dual-level representation that incorporates
regional consistency into the point-level classifier learning, reducing noise
in generated segmentation. Finally, we conduct extensive experiments on two
widely used datasets, SemanticKITTI and SemanticPOSS, and the results show our
method outperforms the state of the art by a large margin.</description>
      <guid isPermaLink="false">2407.12489v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>A Multi-modal Garden Dataset and Hybrid 3D Dense Reconstruction Framework Based on Panoramic Stereo Images for a Trimming Robot</title>
      <link>http://arxiv.org/abs/2305.06278v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  32 pages&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 恢复户外环境的表面网格对农业机器人在任务规划和远程可视化中至关重要。&lt;br&gt;&lt;h4&gt;2. 解决方案概述&lt;/h4&gt;   - 本文提出了一种基于新设计的全景立体相机以及一个包含三个融合模块的混合软件框架的解决方案。&lt;br&gt;&lt;h4&gt;3. 全景立体相机&lt;/h4&gt;   - 全景立体相机呈五边形，由五对立体视觉相机组成，能够流式传输同步的全景立体图像。&lt;br&gt;&lt;h4&gt;4. 融合模块&lt;/h4&gt;   - **视差融合模块**：&lt;br&gt;     - 使用多种立体视觉算法从校正后的立体图像生成初始视差图。&lt;br&gt;     - 将这些初始视差图和强度图像输入到视差融合网络中，以生成精细化的视差图。&lt;br&gt;&lt;h4&gt;5. 姿态融合模块&lt;/h4&gt;   - 采用两阶段的全局-粗到细策略：&lt;br&gt;     - **第一阶段**：通过全局点云匹配算法将每对全视图点云注册，估计全局姿态图中的边的变换，实现回环闭合。&lt;br&gt;     - **第二阶段**：使用局部点云匹配算法匹配不同节点中的单视图点云。&lt;br&gt;&lt;h4&gt;6. 姿态优化&lt;/h4&gt;   - 根据三条提出的规则局部优化全局姿态图中所有对应边的姿态，构建精细化的姿态图。&lt;br&gt;&lt;h4&gt;7. 体积融合模块&lt;/h4&gt;   - 使用所有节点的全局姿态将单视图点云整合入体积，生成整个花园的网格。&lt;br&gt;&lt;h4&gt;8. 实验验证&lt;/h4&gt;   - 提出的框架及其三个融合模块在真实的户外花园数据集上进行了测试，展示了其性能的优越性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2023-05-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/canpu999/trimbot-wageningen-slam-dataset&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recovering an outdoor environment's surface mesh is vital for an agricultural
robot during task planning and remote visualization. Our proposed solution is
based on a newly-designed panoramic stereo camera along with a hybrid novel
software framework that consists of three fusion modules. The panoramic stereo
camera with a pentagon shape consists of 5 stereo vision camera pairs to stream
synchronized panoramic stereo images for the following three fusion modules. In
the disparity fusion module, rectified stereo images produce the initial
disparity maps using multiple stereo vision algorithms. Then, these initial
disparity maps, along with the intensity images, are input into a disparity
fusion network to produce refined disparity maps. Next, the refined disparity
maps are converted into full-view point clouds or single-view point clouds for
the pose fusion module. The pose fusion module adopts a two-stage
global-coarse-to-local-fine strategy. In the first stage, each pair of
full-view point clouds is registered by a global point cloud matching algorithm
to estimate the transformation for a global pose graph's edge, which
effectively implements loop closure. In the second stage, a local point cloud
matching algorithm is used to match single-view point clouds in different
nodes. Next, we locally refine the poses of all corresponding edges in the
global pose graph using three proposed rules, thus constructing a refined pose
graph. The refined pose graph is optimized to produce a global pose trajectory
for volumetric fusion. In the volumetric fusion module, the global poses of all
the nodes are used to integrate the single-view point clouds into the volume to
produce the mesh of the whole garden. The proposed framework and its three
fusion modules are tested on a real outdoor garden dataset to show the
superiority of the performance.</description>
      <guid isPermaLink="false">2305.06278v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Robust image representations with counterfactual contrastive learning</title>
      <link>http://arxiv.org/abs/2409.10365v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Code available at
  https://github.com/biomedia-mira/counterfactual-contrastive/&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 对比预训练的优势&lt;/h4&gt;   - 对比预训练能够显著提高模型的泛化能力和下游任务的表现。&lt;br&gt;&lt;h4&gt;2. 表示学习的质量依赖于数据增强&lt;/h4&gt;   - 学习到的表示质量高度依赖于生成正样本对的数据增强策略。&lt;br&gt;&lt;h4&gt;3. 正样本对的要求&lt;/h4&gt;   - 正对比样本应保持语义意义，同时去除与数据获取领域相关的不必要变化。&lt;br&gt;&lt;h4&gt;4. 传统对比方法的局限&lt;/h4&gt;   - 传统的对比管道通过预定义的通用图像变换模拟领域变化，但这些变换未必能真实反映医学成像中的重要变化（如扫描仪差异）。&lt;br&gt;&lt;h4&gt;5. 新方法的提出&lt;/h4&gt;   - 本文引入反事实对比学习（counterfactual contrastive learning），一个利用因果图像合成的最新进展来创建对比正样本对的新框架。&lt;br&gt;&lt;h4&gt;6. 方法评估&lt;/h4&gt;   - 在五个数据集（包括胸部X光和乳腺X光）上评估该方法，使用了两个已建立的对比目标（SimCLR和DINO-v2），结果显示其在对获取偏移的鲁棒性方面优于标准对比学习。&lt;br&gt;&lt;h4&gt;7. 下游任务表现&lt;/h4&gt;   - 反事实对比学习在分布内和外部数据集上均实现了更优的下游表现，尤其是在使用训练集中代表性不足的扫描仪获取的图像上。&lt;br&gt;&lt;h4&gt;8. 扩展性实验&lt;/h4&gt;   - 进一步实验表明，该框架不仅限于获取偏移，采用反事实对比学习训练的模型在生物性别的子群体表现上也显著改善。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/biomedia-mira/counterfactual-contrastive&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Contrastive pretraining can substantially increase model generalisation and
downstream performance. However, the quality of the learned representations is
highly dependent on the data augmentation strategy applied to generate positive
pairs. Positive contrastive pairs should preserve semantic meaning while
discarding unwanted variations related to the data acquisition domain.
Traditional contrastive pipelines attempt to simulate domain shifts through
pre-defined generic image transformations. However, these do not always mimic
realistic and relevant domain variations for medical imaging such as scanner
differences. To tackle this issue, we herein introduce counterfactual
contrastive learning, a novel framework leveraging recent advances in causal
image synthesis to create contrastive positive pairs that faithfully capture
relevant domain variations. Our method, evaluated across five datasets
encompassing both chest radiography and mammography data, for two established
contrastive objectives (SimCLR and DINO-v2), outperforms standard contrastive
learning in terms of robustness to acquisition shift. Notably, counterfactual
contrastive learning achieves superior downstream performance on both
in-distribution and on external datasets, especially for images acquired with
scanners under-represented in the training set. Further experiments show that
the proposed framework extends beyond acquisition shifts, with models trained
with counterfactual contrastive learning substantially improving subgroup
performance across biological sex.</description>
      <guid isPermaLink="false">2409.10365v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>V2V4Real: A Real-world Large-scale Dataset for Vehicle-to-Vehicle Cooperative Perception</title>
      <link>http://arxiv.org/abs/2303.07601v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CVPR2023. Website link:
  https://research.seas.ucla.edu/mobility-lab/v2v4real&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 现代自主车辆感知系统对遮挡非常敏感，且在长距离感知方面能力不足，这阻碍了达到五级（Level 5）自动驾驶的进展。&lt;br&gt;&lt;h4&gt;2. V2V合作感知的潜力&lt;/h4&gt;   - 最近的研究表明，车辆间（V2V）合作感知系统具有革命性潜力，可以推动自主驾驶行业的发展。&lt;br&gt;&lt;h4&gt;3. 数据集的必要性&lt;/h4&gt;   - 缺乏真实世界数据集是该领域进展的主要障碍。&lt;br&gt;&lt;h4&gt;4. V2V4Real数据集的介绍&lt;/h4&gt;   - 本文提出了V2V4Real，这是首个大规模真实世界的多模态数据集，专门用于V2V感知。&lt;br&gt;&lt;h4&gt;5. 数据收集方式&lt;/h4&gt;   - 数据由两辆配备多模态传感器的车辆在多种场景下共同驾驶收集。&lt;br&gt;&lt;h4&gt;6. 数据集规模&lt;/h4&gt;   - V2V4Real覆盖410公里的驾驶区域，包含：&lt;br&gt;     - 20,000帧LiDAR数据&lt;br&gt;     - 40,000帧RGB图像&lt;br&gt;     - 240,000个3D边界框注释，涉及五个类别&lt;br&gt;     - 涵盖所有行驶路线的高清地图（HD Maps）&lt;br&gt;&lt;h4&gt;7. 感知任务&lt;/h4&gt;   - V2V4Real引入了三个感知任务：&lt;br&gt;     - 合作3D物体检测&lt;br&gt;     - 合作3D物体跟踪&lt;br&gt;     - Sim2Real领域适应&lt;br&gt;&lt;h4&gt;8. 基准测试&lt;/h4&gt;   - 提供了对近期合作感知算法在这三个任务上的全面基准测试。&lt;br&gt;&lt;h4&gt;9. 数据集获取&lt;/h4&gt;   - V2V4Real数据集可在指定链接获取：[V2V4Real Dataset](https://research.seas.ucla.edu/mobility-lab/v2v4real/)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2023-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/ucla-mobility/v2v4real&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern perception systems of autonomous vehicles are known to be sensitive to
occlusions and lack the capability of long perceiving range. It has been one of
the key bottlenecks that prevents Level 5 autonomy. Recent research has
demonstrated that the Vehicle-to-Vehicle (V2V) cooperative perception system
has great potential to revolutionize the autonomous driving industry. However,
the lack of a real-world dataset hinders the progress of this field. To
facilitate the development of cooperative perception, we present V2V4Real, the
first large-scale real-world multi-modal dataset for V2V perception. The data
is collected by two vehicles equipped with multi-modal sensors driving together
through diverse scenarios. Our V2V4Real dataset covers a driving area of 410
km, comprising 20K LiDAR frames, 40K RGB frames, 240K annotated 3D bounding
boxes for 5 classes, and HDMaps that cover all the driving routes. V2V4Real
introduces three perception tasks, including cooperative 3D object detection,
cooperative 3D object tracking, and Sim2Real domain adaptation for cooperative
perception. We provide comprehensive benchmarks of recent cooperative
perception algorithms on three tasks. The V2V4Real dataset can be found at
https://research.seas.ucla.edu/mobility-lab/v2v4real/.</description>
      <guid isPermaLink="false">2303.07601v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Distribution Discrepancy and Feature Heterogeneity for Active 3D Object Detection</title>
      <link>http://arxiv.org/abs/2409.05425v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to CoRL 2024&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 基于LiDAR的3D物体检测是自主驾驶和机器人技术发展的关键技术。&lt;br&gt;   - 数据注释的高成本限制了该技术的进展。&lt;br&gt;&lt;h4&gt;2. 新方法的提出&lt;/h4&gt;   - 本文提出了一种新颖有效的主动学习（AL）方法，称为“分布差异与特征异质性”（DDFH）。&lt;br&gt;&lt;h4&gt;3. 方法的核心思想&lt;/h4&gt;   - DDFH同时考虑几何特征和模型嵌入，从实例级和帧级两个角度评估信息。&lt;br&gt;&lt;h4&gt;4. 分布差异（Distribution Discrepancy）&lt;/h4&gt;   - 评估未标记和已标记分布之间的差异和新颖性，使得模型能够在有限数据下高效学习。&lt;br&gt;&lt;h4&gt;5. 特征异质性（Feature Heterogeneity）&lt;/h4&gt;   - 确保帧内实例特征的异质性，维持特征多样性，避免冗余或相似实例，从而降低注释成本。&lt;br&gt;&lt;h4&gt;6. 信息聚合&lt;/h4&gt;   - 通过分位数变换（Quantile Transform）高效聚合多个指标，提供统一的信息量度。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 大量实验表明，DDFH在KITTI和Waymo数据集上的表现超过当前最先进的方法（SOTA）。&lt;br&gt;&lt;h4&gt;8. 成本效益&lt;/h4&gt;   - DDFH有效降低了边界框注释成本，减少了56.3%，并且在一阶段和两阶段模型中均表现出强大的鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/Coolshanlan/DDFH-active-3Ddet&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LiDAR-based 3D object detection is a critical technology for the development
of autonomous driving and robotics. However, the high cost of data annotation
limits its advancement. We propose a novel and effective active learning (AL)
method called Distribution Discrepancy and Feature Heterogeneity (DDFH), which
simultaneously considers geometric features and model embeddings, assessing
information from both the instance-level and frame-level perspectives.
Distribution Discrepancy evaluates the difference and novelty of instances
within the unlabeled and labeled distributions, enabling the model to learn
efficiently with limited data. Feature Heterogeneity ensures the heterogeneity
of intra-frame instance features, maintaining feature diversity while avoiding
redundant or similar instances, thus minimizing annotation costs. Finally,
multiple indicators are efficiently aggregated using Quantile Transform,
providing a unified measure of informativeness. Extensive experiments
demonstrate that DDFH outperforms the current state-of-the-art (SOTA) methods
on the KITTI and Waymo datasets, effectively reducing the bounding box
annotation cost by 56.3% and showing robustness when working with both
one-stage and two-stage models.</description>
      <guid isPermaLink="false">2409.05425v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>EMHI: A Multimodal Egocentric Human Motion Dataset with HMD and Body-Worn IMUs</title>
      <link>http://arxiv.org/abs/2408.17168v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 基于可穿戴传感器的自我中心人体姿态估计（HPE）对虚拟现实（VR）和增强现实（AR）应用至关重要。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 目前的方法主要依赖于自我中心视图图像或稀疏的惯性测量单元（IMU）信号，这导致由于自遮挡或传感器稀疏和漂移而产生的不准确性。&lt;br&gt;   - 缺乏包含这两种模态的真实世界数据集是该领域进展的主要障碍。&lt;br&gt;&lt;h4&gt;3. 提出的数据集&lt;/h4&gt;   - 本文提出了EMHI，一个多模态的自我中心运动数据集，包含头戴显示器（HMD）和身体佩戴的IMU数据，所有数据均在真实的VR产品套件下收集。&lt;br&gt;&lt;h4&gt;4. 数据集特点&lt;/h4&gt;   - EMHI提供了来自头戴设备下倾摄像头的同步立体图像和来自身体佩戴传感器的IMU数据，以及SMPL格式的姿态注释。&lt;br&gt;   - 数据集包含885个序列，涵盖58名受试者执行39种动作，总录制时长约为28.5小时。&lt;br&gt;&lt;h4&gt;5. 注释评估&lt;/h4&gt;   - 通过与基于光学标记的SMPL拟合结果进行比较，评估了数据集的注释准确性。&lt;br&gt;&lt;h4&gt;6. 新方法的介绍&lt;/h4&gt;   - 提出了MEPoser，一个新的基线方法，用于多模态自我中心HPE，采用多模态融合编码器、时间特征编码器和基于MLP的回归头。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 在EMHI数据集上的实验表明，MEPoser优于现有的单模态方法，展示了该数据集在解决自我中心HPE问题中的价值。&lt;br&gt;&lt;h4&gt;8. 研究贡献&lt;/h4&gt;   - 本文认为EMHI的数据集和方法的发布将推动自我中心HPE的研究，并加速该技术在VR/AR产品中的实际应用。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Egocentric human pose estimation (HPE) using wearable sensors is essential
for VR/AR applications. Most methods rely solely on either egocentric-view
images or sparse Inertial Measurement Unit (IMU) signals, leading to
inaccuracies due to self-occlusion in images or the sparseness and drift of
inertial sensors. Most importantly, the lack of real-world datasets containing
both modalities is a major obstacle to progress in this field. To overcome the
barrier, we propose EMHI, a multimodal \textbf{E}gocentric human
\textbf{M}otion dataset with \textbf{H}ead-Mounted Display (HMD) and body-worn
\textbf{I}MUs, with all data collected under the real VR product suite.
Specifically, EMHI provides synchronized stereo images from downward-sloping
cameras on the headset and IMU data from body-worn sensors, along with pose
annotations in SMPL format. This dataset consists of 885 sequences captured by
58 subjects performing 39 actions, totaling about 28.5 hours of recording. We
evaluate the annotations by comparing them with optical marker-based SMPL
fitting results. To substantiate the reliability of our dataset, we introduce
MEPoser, a new baseline method for multimodal egocentric HPE, which employs a
multimodal fusion encoder, temporal feature encoder, and MLP-based regression
heads. The experiments on EMHI show that MEPoser outperforms existing
single-modal methods and demonstrates the value of our dataset in solving the
problem of egocentric HPE. We believe the release of EMHI and the method could
advance the research of egocentric HPE and expedite the practical
implementation of this technology in VR/AR products.</description>
      <guid isPermaLink="false">2408.17168v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Fast Data-Driven Adaptation of Radar Detection via Meta-Learning</title>
      <link>http://arxiv.org/abs/2112.01780v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Presented at the 2021 Asilomar Conference on Signals, Systems, and
  Computers&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究问题&lt;/h4&gt;   - 本文解决了在有限训练数据下快速学习雷达检测器的问题。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 当前的数据驱动雷达检测方法在操作环境变化时通常需要重新训练，这会导致数据收集和训练时间的大量开销。&lt;br&gt;&lt;h4&gt;3. 提出的新方法&lt;/h4&gt;   - 本文提出两种新颖的基于深度学习的方法，使检测器能够基于少量新环境数据样本快速适应。&lt;br&gt;&lt;h4&gt;4. 整合先前知识&lt;/h4&gt;   - 提出的两种方法以不同方式整合了关于先前遇到的雷达操作环境的先验知识。&lt;br&gt;&lt;h4&gt;5. 方法一：迁移学习&lt;/h4&gt;   - 第一个方法基于迁移学习：首先预训练一个检测器，使其在先前观察的环境中表现良好，然后将其适应到当前特定环境。&lt;br&gt;&lt;h4&gt;6. 方法二：元学习&lt;/h4&gt;   - 第二个方法通过元学习显式地针对少样本训练：基于之前环境的数据，找到一个共同初始化，使其能够快速适应新环境。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 数值结果验证了两种提出方法相比于基于无先验知识训练的传统方法的优势。&lt;br&gt;&lt;h4&gt;8. 性能比较&lt;/h4&gt;   - 当杂波为高斯分布时，基于元学习的检测器优于基于迁移学习的检测器。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2021-12-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper addresses the problem of fast learning of radar detectors with a
limited amount of training data. In current data-driven approaches for radar
detection, re-training is generally required when the operating environment
changes, incurring large overhead in terms of data collection and training
time. In contrast, this paper proposes two novel deep learning-based approaches
that enable fast adaptation of detectors based on few data samples from a new
environment. The proposed methods integrate prior knowledge regarding
previously encountered radar operating environments in two different ways. One
approach is based on transfer learning: it first pre-trains a detector such
that it works well on data collected in previously observed environments, and
then it adapts the pre-trained detector to the specific current environment.
The other approach targets explicitly few-shot training via meta-learning:
based on data from previous environments, it finds a common initialization that
enables fast adaptation to a new environment. Numerical results validate the
benefits of the proposed two approaches compared with the conventional method
based on training with no prior knowledge. Furthermore, the meta-learning-based
detector outperforms the transfer learning-based detector when the clutter is
Gaussian.</description>
      <guid isPermaLink="false">2112.01780v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>MaFreeI2P: A Matching-Free Image-to-Point Cloud Registration Paradigm with Active Camera Pose Retrieval</title>
      <link>http://arxiv.org/abs/2408.02392v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to IEEE Conference on Multimedia Expo 2024&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究主题&lt;/h4&gt;   - 图像与点云配准旨在估计它们之间的相对相机姿态，但由于数据模态之间的差异，这仍然是一个开放的问题。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 最近的基于匹配的方法通过建立2D-3D对应关系来解决此问题，但这些方法存在信息损失的问题。&lt;br&gt;&lt;h4&gt;3. 新方法的提出&lt;/h4&gt;   - 本文提出了一种无匹配的范式，称为MaFreeI2P，旨在解决上述问题。&lt;br&gt;&lt;h4&gt;4. 关键思路&lt;/h4&gt;   - 我们的关键见解是通过对比点云和查询图像之间的几何特征，主动检索SE(3)空间中的相机姿态。&lt;br&gt;&lt;h4&gt;5. 实现步骤&lt;/h4&gt;   - 首先，采样一组候选相机姿态，并使用跨模态特征构建它们的成本体积。&lt;br&gt;   - 成本体积优于匹配方法，能保留更多信息，其特征相似性隐含地反映了采样姿态的置信水平。&lt;br&gt;&lt;h4&gt;6. 相似性评估函数&lt;/h4&gt;   - 采用卷积网络自适应地构建相似性评估函数，输入的成本体积通过过滤和基于姿态的加权进一步改进。&lt;br&gt;&lt;h4&gt;7. 姿态更新与收敛&lt;/h4&gt;   - 根据相似性评分更新相机姿态，并采用启发式策略迭代缩小姿态采样空间以实现收敛。&lt;br&gt;&lt;h4&gt;8. 实验结果&lt;/h4&gt;   - MaFreeI2P在KITTI-Odometry和Apollo-DaoxiangLake数据集上实现了非常有竞争力的配准精度和召回率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Image-to-point cloud registration seeks to estimate their relative camera
pose, which remains an open question due to the data modality gaps. The recent
matching-based methods tend to tackle this by building 2D-3D correspondences.
In this paper, we reveal the information loss inherent in these methods and
propose a matching-free paradigm, named MaFreeI2P. Our key insight is to
actively retrieve the camera pose in SE(3) space by contrasting the geometric
features between the point cloud and the query image. To achieve this, we first
sample a set of candidate camera poses and construct their cost volume using
the cross-modal features. Superior to matching, cost volume can preserve more
information and its feature similarity implicitly reflects the confidence level
of the sampled poses. Afterwards, we employ a convolutional network to
adaptively formulate a similarity assessment function, where the input cost
volume is further improved by filtering and pose-based weighting. Finally, we
update the camera pose based on the similarity scores, and adopt a heuristic
strategy to iteratively shrink the pose sampling space for convergence. Our
MaFreeI2P achieves a very competitive registration accuracy and recall on the
KITTI-Odometry and Apollo-DaoxiangLake datasets.</description>
      <guid isPermaLink="false">2408.02392v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Fast Medical Shape Reconstruction via Meta-learned Implicit Neural Representations</title>
      <link>http://arxiv.org/abs/2409.07100v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 高效快速的解剖结构重建在临床实践中至关重要。&lt;br&gt;   - 降低检索和处理时间有助于在关键情况下增强响应速度和决策能力，并支持互动手术规划和导航。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 最近的方法通过利用隐式神经函数解决医学形状重建问题，但在泛化能力和计算时间方面表现不佳，尤其在实时应用中至关重要。&lt;br&gt;&lt;h4&gt;3. 提出的解决方案&lt;/h4&gt;   - 本文提出利用元学习（meta-learning）来改善网络参数初始化，以减少推理时间，提升效率，同时保持高精度。&lt;br&gt;&lt;h4&gt;4. 数据集评估&lt;/h4&gt;   - 在三个涵盖不同解剖形状和模态的公共数据集上评估了该方法，包括CT和MRI数据集。&lt;br&gt;&lt;h4&gt;5. 实验结果&lt;/h4&gt;   - 实验结果表明，模型能够处理各种输入配置，如不同方向和间距的稀疏切片。&lt;br&gt;&lt;h4&gt;6. 泛化能力&lt;/h4&gt;   - 该方法展示了强大的可迁移能力，能够泛化到训练时未观察到的形状领域，表明其适应性强。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Efficient and fast reconstruction of anatomical structures plays a crucial
role in clinical practice. Minimizing retrieval and processing times not only
potentially enhances swift response and decision-making in critical scenarios
but also supports interactive surgical planning and navigation. Recent methods
attempt to solve the medical shape reconstruction problem by utilizing implicit
neural functions. However, their performance suffers in terms of generalization
and computation time, a critical metric for real-time applications. To address
these challenges, we propose to leverage meta-learning to improve the network
parameters initialization, reducing inference time by an order of magnitude
while maintaining high accuracy. We evaluate our approach on three public
datasets covering different anatomical shapes and modalities, namely CT and
MRI. Our experimental results show that our model can handle various input
configurations, such as sparse slices with different orientations and spacings.
Additionally, we demonstrate that our method exhibits strong transferable
capabilities in generalizing to shape domains unobserved at training time.</description>
      <guid isPermaLink="false">2409.07100v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Novelty Detection Methods Benchmarking with Wavelet Decomposition</title>
      <link>http://arxiv.org/abs/2409.07135v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To be published in the 8th International Conference on System
  Reliability and Safety. Sicily, Italy - November 20-22, 2024. 15 pages, 7
  figures, 4 tables&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究主题&lt;/h4&gt;   - 新颖性检测是多个工程领域中的关键任务。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 许多新颖性检测方法依赖于监督或半监督学习，这需要标记数据集进行训练。&lt;br&gt;   - 获取标记数据通常昂贵且耗时。&lt;br&gt;&lt;h4&gt;3. 无监督学习的优势&lt;/h4&gt;   - 无监督学习是一种强大的替代方案，可以在不需要标记样本的情况下进行新颖性检测。&lt;br&gt;&lt;h4&gt;4. 算法比较&lt;/h4&gt;   - 本研究比较了多种无监督机器学习算法在新颖性检测中的应用，强调了它们在振动传感背景下的优缺点。&lt;br&gt;&lt;h4&gt;5. 框架创新&lt;/h4&gt;   - 提出的框架使用连续度量，与大多数传统方法仅标记异常样本而不量化异常程度不同。&lt;br&gt;&lt;h4&gt;6. 数据集的构建&lt;/h4&gt;   - 收集了一个新的数据集，来自于特定频率振动的执行器，以基准测试算法并评估框架。&lt;br&gt;&lt;h4&gt;7. 新条件的引入&lt;/h4&gt;   - 通过改变输入波形信号引入新条件，以测试算法的适应性。&lt;br&gt;&lt;h4&gt;8. 研究发现&lt;/h4&gt;   - 研究结果提供了关于无监督学习技术在现实世界新颖性检测应用中的适应性和鲁棒性的宝贵见解。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/pic4ser/unsupervised-novelty-detection-methods-benchmarking-with-wavelet-decomposition&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Novelty detection is a critical task in various engineering fields. Numerous
approaches to novelty detection rely on supervised or semi-supervised learning,
which requires labelled datasets for training. However, acquiring labelled
data, when feasible, can be expensive and time-consuming. For these reasons,
unsupervised learning is a powerful alternative that allows performing novelty
detection without needing labelled samples. In this study, numerous
unsupervised machine learning algorithms for novelty detection are compared,
highlighting their strengths and weaknesses in the context of vibration
sensing. The proposed framework uses a continuous metric, unlike most
traditional methods that merely flag anomalous samples without quantifying the
degree of anomaly. Moreover, a new dataset is gathered from an actuator
vibrating at specific frequencies to benchmark the algorithms and evaluate the
framework. Novel conditions are introduced by altering the input wave signal.
Our findings offer valuable insights into the adaptability and robustness of
unsupervised learning techniques for real-world novelty detection applications.</description>
      <guid isPermaLink="false">2409.07135v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>DENSER: 3D Gaussians Splatting for Scene Reconstruction of Dynamic Urban Environments</title>
      <link>http://arxiv.org/abs/2409.10041v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究主题&lt;/h4&gt;   - 本文介绍了DENSER，一个高效且有效的方法，利用3D高斯点云（3DGS）重建动态城市环境。&lt;br&gt;&lt;h4&gt;2. 背景与挑战&lt;/h4&gt;   - 现有的方法，包括使用神经辐射场（NeRF）和3DGS的场景重建，已在相对复杂的动态场景中取得了良好结果。&lt;br&gt;   - 然而，建模前景物体的动态外观仍然具有挑战性，限制了这些方法捕捉场景细节的能力，尤其是远处的动态物体。&lt;br&gt;&lt;h4&gt;3. DENSER框架的创新&lt;/h4&gt;   - DENSER框架显著增强了动态物体的表现，并准确建模动态物体在驾驶场景中的外观。&lt;br&gt;&lt;h4&gt;4. 新方法的引入&lt;/h4&gt;   - 本文提出了一种新方法，动态估计球谐函数（SH）基，通过小波变换来整合，以更好地表示动态物体的外观，提升空间和时间上的表现。&lt;br&gt;&lt;h4&gt;5. 形状表示的改善&lt;/h4&gt;   - DENSER通过在多个场景帧中密集化点云，增强了物体形状的表示，从而加速模型训练的收敛。&lt;br&gt;&lt;h4&gt;6. 实验结果&lt;/h4&gt;   - 在KITTI数据集上的广泛评估表明，该方法显著优于现有的最先进方法，成绩差距明显。&lt;br&gt;&lt;h4&gt;7. 代码与模型共享&lt;/h4&gt;   - 源代码和模型将上传至GitHub仓库，链接为[GitHub - sntubix/denser](https://github.com/sntubix/denser)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents DENSER, an efficient and effective approach leveraging 3D
Gaussian splatting (3DGS) for the reconstruction of dynamic urban environments.
While several methods for photorealistic scene representations, both implicitly
using neural radiance fields (NeRF) and explicitly using 3DGS have shown
promising results in scene reconstruction of relatively complex dynamic scenes,
modeling the dynamic appearance of foreground objects tend to be challenging,
limiting the applicability of these methods to capture subtleties and details
of the scenes, especially far dynamic objects. To this end, we propose DENSER,
a framework that significantly enhances the representation of dynamic objects
and accurately models the appearance of dynamic objects in the driving scene.
Instead of directly using Spherical Harmonics (SH) to model the appearance of
dynamic objects, we introduce and integrate a new method aiming at dynamically
estimating SH bases using wavelets, resulting in better representation of
dynamic objects appearance in both space and time. Besides object appearance,
DENSER enhances object shape representation through densification of its point
cloud across multiple scene frames, resulting in faster convergence of model
training. Extensive evaluations on KITTI dataset show that the proposed
approach significantly outperforms state-of-the-art methods by a wide margin.
Source codes and models will be uploaded to this repository
https://github.com/sntubix/denser</description>
      <guid isPermaLink="false">2409.10041v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Building-PCC: Building Point Cloud Completion Benchmarks</title>
      <link>http://arxiv.org/abs/2404.15644v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 随着3D传感技术的快速发展，获取物体的3D形状信息变得越来越方便。&lt;br&gt;   - 激光雷达（Lidar）技术能够准确捕捉远距离物体的3D信息，因此被广泛应用于城市场景的3D数据收集。&lt;br&gt;&lt;h4&gt;2. 数据问题&lt;/h4&gt;   - 收集到的点云数据常常由于遮挡、信号吸收和镜面反射等因素而表现出不完整性。&lt;br&gt;&lt;h4&gt;3. 研究目的&lt;/h4&gt;   - 本文探讨了点云补全技术在处理这些不完整数据中的应用。&lt;br&gt;&lt;h4&gt;4. 新基准的建立&lt;/h4&gt;   - 建立了一个新的现实世界基准**Building-PCC**数据集，用于评估现有深度学习方法在城市建筑点云补全任务中的表现。&lt;br&gt;&lt;h4&gt;5. 评估与挑战&lt;/h4&gt;   - 通过对不同方法的综合评估，分析了建筑点云补全面临的关键挑战，旨在促进3D地理信息应用领域的创新。&lt;br&gt;&lt;h4&gt;6. 代码可获取性&lt;/h4&gt;   - 本文提供的源代码可在GitHub上获取，链接为[Building-PCC GitHub Repository](https://github.com/tudelft3d/Building-PCC-Building-Point-Cloud-Completion-Benchmarks.git)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-04-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/tudelft3d/building-pcc-building-point-cloud-completion-benchmarks&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid advancement of 3D sensing technologies, obtaining 3D shape
information of objects has become increasingly convenient. Lidar technology,
with its capability to accurately capture the 3D information of objects at long
distances, has been widely applied in the collection of 3D data in urban
scenes. However, the collected point cloud data often exhibit incompleteness
due to factors such as occlusion, signal absorption, and specular reflection.
This paper explores the application of point cloud completion technologies in
processing these incomplete data and establishes a new real-world benchmark
Building-PCC dataset, to evaluate the performance of existing deep learning
methods in the task of urban building point cloud completion. Through a
comprehensive evaluation of different methods, we analyze the key challenges
faced in building point cloud completion, aiming to promote innovation in the
field of 3D geoinformation applications. Our source code is available at
https://github.com/tudelft3d/Building-PCC-Building-Point-Cloud-Completion-Benchmarks.git.</description>
      <guid isPermaLink="false">2404.15644v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>SegPoint: Segment Any Point Cloud via Large Language Model</title>
      <link>http://arxiv.org/abs/2407.13761v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ECCV 2024, Project Page: https://heshuting555.github.io/SegPoint&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 尽管3D点云分割取得了显著进展，现有方法主要针对特定任务，依赖明确的指令来识别目标，缺乏在统一框架中推断和理解隐含用户意图的能力。&lt;br&gt;&lt;h4&gt;2. 提出的模型&lt;/h4&gt;   - 本文提出了一种名为**SegPoint**的模型，利用多模态大语言模型（LLM）的推理能力，生成针对多个任务的点级分割掩膜。&lt;br&gt;&lt;h4&gt;3. 任务范围&lt;/h4&gt;   - SegPoint适用于以下四种任务：&lt;br&gt;     1. 3D指令分割&lt;br&gt;     2. 3D指代分割&lt;br&gt;     3. 3D语义分割&lt;br&gt;     4. 3D开放词汇语义分割&lt;br&gt;&lt;h4&gt;4. 新基准的引入&lt;/h4&gt;   - 为了推动3D指令研究，推出了新基准**Instruct3D**，用于评估来自复杂和隐含指令文本的分割性能，包含2565对点云和指令的数据对。&lt;br&gt;&lt;h4&gt;5. 实验结果&lt;/h4&gt;   - 实验结果表明，SegPoint在现有基准（如ScanRefer和ScanNet）上表现竞争力，同时在Instruct3D数据集上取得了出色的结果。&lt;br&gt;&lt;h4&gt;6. 模型创新性&lt;/h4&gt;   - 据我们所知，SegPoint是第一个在单一框架内处理这些多样化分割任务的模型，且表现令人满意。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-07-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite significant progress in 3D point cloud segmentation, existing methods
primarily address specific tasks and depend on explicit instructions to
identify targets, lacking the capability to infer and understand implicit user
intentions in a unified framework. In this work, we propose a model, called
SegPoint, that leverages the reasoning capabilities of a multi-modal Large
Language Model (LLM) to produce point-wise segmentation masks across a diverse
range of tasks: 1) 3D instruction segmentation, 2) 3D referring segmentation,
3) 3D semantic segmentation, and 4) 3D open-vocabulary semantic segmentation.
To advance 3D instruction research, we introduce a new benchmark, Instruct3D,
designed to evaluate segmentation performance from complex and implicit
instructional texts, featuring 2,565 point cloud-instruction pairs. Our
experimental results demonstrate that SegPoint achieves competitive performance
on established benchmarks such as ScanRefer for referring segmentation and
ScanNet for semantic segmentation, while delivering outstanding outcomes on the
Instruct3D dataset. To our knowledge, SegPoint is the first model to address
these varied segmentation tasks within a single framework, achieving
satisfactory performance.</description>
      <guid isPermaLink="false">2407.13761v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>SIGMA: Scale-Invariant Global Sparse Shape Matching</title>
      <link>http://arxiv.org/abs/2308.08393v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究目的&lt;/h4&gt;   - 提出了一个新颖的混合整数规划（MIP）模型，用于生成高度非刚性形状的精确稀疏对应关系。&lt;br&gt;&lt;h4&gt;2. 新概念引入&lt;/h4&gt;   - 引入了投影拉普拉斯-贝尔特拉米算子（PLBO），该算子结合了内在和外在几何信息，用于测量由预测的对应关系引起的形变质量。&lt;br&gt;&lt;h4&gt;3. 模型整合&lt;/h4&gt;   - 将PLBO与方向感知正则化器结合，形成一个新的MIP模型，该模型可以针对许多实际问题求解到全局最优。&lt;br&gt;&lt;h4&gt;4. 方法优势&lt;/h4&gt;   - 相较于以往方法：&lt;br&gt;     - 该方法在刚性变换和全局缩放下是可证明的无关性。&lt;br&gt;     - 无需初始化，具备最优性保证。&lt;br&gt;     - 能够扩展到高分辨率网格，且在实证上观察到线性时间复杂度。&lt;br&gt;&lt;h4&gt;5. 实验结果&lt;/h4&gt;   - 在多个具有挑战性的3D数据集上展示了最先进的稀疏非刚性匹配结果，包括处理不一致网格的数据。&lt;br&gt;&lt;h4&gt;6. 应用范围&lt;/h4&gt;   - 该方法还可用于网格与点云之间的匹配应用。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2023-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a novel mixed-integer programming (MIP) formulation for generating
precise sparse correspondences for highly non-rigid shapes. To this end, we
introduce a projected Laplace-Beltrami operator (PLBO) which combines intrinsic
and extrinsic geometric information to measure the deformation quality induced
by predicted correspondences. We integrate the PLBO, together with an
orientation-aware regulariser, into a novel MIP formulation that can be solved
to global optimality for many practical problems. In contrast to previous
methods, our approach is provably invariant to rigid transformations and global
scaling, initialisation-free, has optimality guarantees, and scales to high
resolution meshes with (empirically observed) linear time. We show
state-of-the-art results for sparse non-rigid matching on several challenging
3D datasets, including data with inconsistent meshing, as well as applications
in mesh-to-point-cloud matching.</description>
      <guid isPermaLink="false">2308.08393v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>LEROjD: Lidar Extended Radar-Only Object Detection</title>
      <link>http://arxiv.org/abs/2409.05564v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication as ECCV 2024&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 精确的3D物体检测对于自动驾驶至关重要。尽管激光雷达（LiDAR）传感器非常适合此任务，但其成本高昂，并且在恶劣天气条件下表现有限。&lt;br&gt;&lt;h4&gt;2. 替代方案&lt;/h4&gt;   - 3+1D成像雷达传感器提供了一种成本效益高且更为稳健的替代方案，但面临低分辨率和高测量噪声的挑战。&lt;br&gt;&lt;h4&gt;3. 数据集的现状&lt;/h4&gt;   - 现有的3+1D成像雷达数据集包含雷达和激光雷达数据，允许跨模态模型的改进。尽管推断时不应使用激光雷达，但它可以辅助雷达-only物体检测器的训练。&lt;br&gt;&lt;h4&gt;4. 知识转移策略&lt;/h4&gt;   - 探索了两种从激光雷达到雷达领域的知识转移策略：&lt;br&gt;     1. **多阶段训练**：通过逐步减少激光点云数据进行训练。&lt;br&gt;     2. **跨模态知识蒸馏**：通过初始化学生模型的权重为教师模型的权重来提高性能。&lt;br&gt;&lt;h4&gt;5. 多阶段训练方法&lt;/h4&gt;   - 在多阶段过程中，考察了三种不同的点云稀疏方法。&lt;br&gt;&lt;h4&gt;6. 实验结果&lt;/h4&gt;   - 实验结果表明，使用多阶段训练方法，平均精度（mAP）性能提升最多可达4.2个百分点；使用知识蒸馏法则可提升最多3.9个百分点。&lt;br&gt;&lt;h4&gt;7. 方法的通用性&lt;/h4&gt;   - 这些方法的主要优点在于其适用于其他3D物体检测网络，而无需改变其架构，且在两种不同的物体检测器上进行了验证。&lt;br&gt;&lt;h4&gt;8. 代码可用性&lt;/h4&gt;   - 相关代码可在GitHub上获取，链接为 [LEROjD GitHub](https://github.com/rst-tu-dortmund/lerojd)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/rst-tu-dortmund/lerojd&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate 3D object detection is vital for automated driving. While lidar
sensors are well suited for this task, they are expensive and have limitations
in adverse weather conditions. 3+1D imaging radar sensors offer a
cost-effective, robust alternative but face challenges due to their low
resolution and high measurement noise. Existing 3+1D imaging radar datasets
include radar and lidar data, enabling cross-modal model improvements. Although
lidar should not be used during inference, it can aid the training of
radar-only object detectors. We explore two strategies to transfer knowledge
from the lidar to the radar domain and radar-only object detectors: 1.
multi-stage training with sequential lidar point cloud thin-out, and 2.
cross-modal knowledge distillation. In the multi-stage process, three thin-out
methods are examined. Our results show significant performance gains of up to
4.2 percentage points in mean Average Precision with multi-stage training and
up to 3.9 percentage points with knowledge distillation by initializing the
student with the teacher's weights. The main benefit of these approaches is
their applicability to other 3D object detection networks without altering
their architecture, as we show by analyzing it on two different object
detectors. Our code is available at https://github.com/rst-tu-dortmund/lerojd</description>
      <guid isPermaLink="false">2409.05564v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Modeling Continuous Motion for 3D Point Cloud Object Tracking</title>
      <link>http://arxiv.org/abs/2303.07605v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 3D单目标跟踪（SOT）利用LiDAR点云在自动驾驶和机器人等多个应用中至关重要。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 目前的方法主要依赖于外观匹配或仅在两个连续帧内进行运动建模，忽视了物体在3D空间中的长距离连续运动特性。&lt;br&gt;&lt;h4&gt;3. 提出的方法&lt;/h4&gt;   - 本文提出了一种新颖的方法，将每个轨迹视为一个连续流：每个时间戳仅将当前帧输入网络，与存储在记忆库中的多帧历史特征进行交互，从而有效利用顺序信息。&lt;br&gt;&lt;h4&gt;4. 交叉帧信息传递&lt;/h4&gt;   - 设计了一种混合注意力机制，以实现有效的跨帧信息传递，考虑长距离关系建模和局部几何特征提取。&lt;br&gt;&lt;h4&gt;5. 增强多帧特征的利用&lt;/h4&gt;   - 提出了对比序列增强策略，利用真实轨迹增强训练序列，以对比方式提升对假阳性的区分能力。&lt;br&gt;&lt;h4&gt;6. 实验结果&lt;/h4&gt;   - 大量实验表明，所提出的方法在多个基准测试中显著超越了现有最先进的方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2023-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The task of 3D single object tracking (SOT) with LiDAR point clouds is
crucial for various applications, such as autonomous driving and robotics.
However, existing approaches have primarily relied on appearance matching or
motion modeling within only two successive frames, thereby overlooking the
long-range continuous motion property of objects in 3D space. To address this
issue, this paper presents a novel approach that views each tracklet as a
continuous stream: at each timestamp, only the current frame is fed into the
network to interact with multi-frame historical features stored in a memory
bank, enabling efficient exploitation of sequential information. To achieve
effective cross-frame message passing, a hybrid attention mechanism is designed
to account for both long-range relation modeling and local geometric feature
extraction. Furthermore, to enhance the utilization of multi-frame features for
robust tracking, a contrastive sequence enhancement strategy is proposed, which
uses ground truth tracklets to augment training sequences and promote
discrimination against false positives in a contrastive manner. Extensive
experiments demonstrate that the proposed method outperforms the
state-of-the-art method by significant margins on multiple benchmarks.</description>
      <guid isPermaLink="false">2303.07605v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Learning Semi-Supervised Medical Image Segmentation from Spatial Registration</title>
      <link>http://arxiv.org/abs/2409.10422v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 半监督医学图像分割在使用有限标注数据和丰富未标注数据训练模型方面显示出潜力。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 当前最先进的方法忽视了一种潜在的无监督语义信息来源：图像体积之间的空间配准变换。&lt;br&gt;&lt;h4&gt;3. 提出的方法&lt;/h4&gt;   - 本文提出了CCT-R，一种结合配准信息的对比交叉教学框架。&lt;br&gt;&lt;h4&gt;4. 模块介绍&lt;/h4&gt;   - CCT-R包含两个新提出的模块：&lt;br&gt;     - **注册监督损失（RSL）**：利用标注和未标注体积对之间的变换所获得的分割知识，提供额外的伪标签来源。&lt;br&gt;     - **注册增强正采样（REPS）**：通过使用配准变换识别跨体积的解剖对应正例，从而增强对比学习。&lt;br&gt;&lt;h4&gt;5. 实验结果&lt;/h4&gt;   - 在两个具有挑战性的医学分割基准上进行的实验表明，CCT-R在多种半监督设置下的有效性和优越性，甚至在仅有一个标注案例的情况下也表现良好。&lt;br&gt;&lt;h4&gt;6. 代码可用性&lt;/h4&gt;   - 相关代码已发布在GitHub上，链接为 [CCT-R GitHub](https://github.com/kathyliu579/ContrastiveCross-teachingWithRegistration)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semi-supervised medical image segmentation has shown promise in training
models with limited labeled data and abundant unlabeled data. However,
state-of-the-art methods ignore a potentially valuable source of unsupervised
semantic information -- spatial registration transforms between image volumes.
To address this, we propose CCT-R, a contrastive cross-teaching framework
incorporating registration information. To leverage the semantic information
available in registrations between volume pairs, CCT-R incorporates two
proposed modules: Registration Supervision Loss (RSL) and Registration-Enhanced
Positive Sampling (REPS). The RSL leverages segmentation knowledge derived from
transforms between labeled and unlabeled volume pairs, providing an additional
source of pseudo-labels. REPS enhances contrastive learning by identifying
anatomically-corresponding positives across volumes using registration
transforms. Experimental results on two challenging medical segmentation
benchmarks demonstrate the effectiveness and superiority of CCT-R across
various semi-supervised settings, with as few as one labeled case. Our code is
available at
https://github.com/kathyliu579/ContrastiveCross-teachingWithRegistration.</description>
      <guid isPermaLink="false">2409.10422v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Comparative Analysis of Modality Fusion Approaches for Audio-Visual Person Identification and Verification</title>
      <link>http://arxiv.org/abs/2409.00562v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been submitted to a conference&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 多模态学习涉及整合来自不同模态的信息，以增强学习和理解能力。&lt;br&gt;&lt;h4&gt;2. 研究目标&lt;/h4&gt;   - 本文比较三种模态融合策略在身份识别和验证中的表现，处理的模态包括声音和面部。&lt;br&gt;&lt;h4&gt;3. 声音模态处理&lt;/h4&gt;   - 使用一维卷积神经网络进行x-vector提取，以处理声音数据。&lt;br&gt;&lt;h4&gt;4. 面部模态处理&lt;/h4&gt;   - 利用预训练的VGGFace2网络和迁移学习方法处理面部模态。&lt;br&gt;&lt;h4&gt;5. 语音表示&lt;/h4&gt;   - 采用gammatonegram作为语音表示，并与预训练的Darknet19网络结合。&lt;br&gt;&lt;h4&gt;6. 评估方法&lt;/h4&gt;   - 提出的系统通过K折交叉验证技术在VoxCeleb2数据集的118名说话者上进行评估。&lt;br&gt;&lt;h4&gt;7. 比较评估&lt;/h4&gt;   - 在相同条件下，对单模态和三种提议的多模态策略进行比较评估。&lt;br&gt;&lt;h4&gt;8. 实验结果&lt;/h4&gt;   - gammatonegram与面部特征的特征融合策略在身份识别任务中达到最高性能，准确率为98.37%。&lt;br&gt;   - 然而，在验证任务中，面部特征与x-vector的连接仅达到了0.62%的等错误率（EER）。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal learning involves integrating information from various modalities
to enhance learning and comprehension. We compare three modality fusion
strategies in person identification and verification by processing two
modalities: voice and face. In this paper, a one-dimensional convolutional
neural network is employed for x-vector extraction from voice, while the
pre-trained VGGFace2 network and transfer learning are utilized for face
modality. In addition, gammatonegram is used as speech representation in
engagement with the Darknet19 pre-trained network. The proposed systems are
evaluated using the K-fold cross-validation technique on the 118 speakers of
the test set of the VoxCeleb2 dataset. The comparative evaluations are done for
single-modality and three proposed multimodal strategies in equal situations.
Results demonstrate that the feature fusion strategy of gammatonegram and
facial features achieves the highest performance, with an accuracy of 98.37% in
the person identification task. However, concatenating facial features with the
x-vector reaches 0.62% for EER in verification tasks.</description>
      <guid isPermaLink="false">2409.00562v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Simple Meta-Learning: Multi-Purpose Models for Multi-Domain, Active and Continual Few-Shot Learning</title>
      <link>http://arxiv.org/abs/2201.05151v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 数据生成方法在机器学习文献中显示出通过增加稀疏标签生成额外信息丰富的训练示例的良好结果，但在图数据中的研究较少。&lt;br&gt;&lt;h4&gt;2. 复杂性挑战&lt;/h4&gt;   - 图数据的节点间存在复杂的依赖关系，这使得在图中应用数据生成方法变得更加复杂。&lt;br&gt;&lt;h4&gt;3. 提出的方法&lt;/h4&gt;   - 本文提出了一种新颖的节点生成方法，通过将一小部分高质量合成节点注入图中，作为额外的标记节点，优化标记信息的传播。&lt;br&gt;&lt;h4&gt;4. 框架设计&lt;/h4&gt;   - 该框架与图学习和下游分类技术是正交的，因此兼容大多数流行的图预训练（自监督学习）、半监督学习和元学习方法。&lt;br&gt;&lt;h4&gt;5. 优化问题&lt;/h4&gt;   - 贡献在于通过解决一个新颖的优化问题来设计生成的节点集，优化目标包括：&lt;br&gt;     - **最小化分类损失**：确保训练准确性。&lt;br&gt;     - **最大化标签传播**：确保低置信度节点的高质量传播。&lt;br&gt;&lt;h4&gt;6. 理论分析&lt;/h4&gt;   - 理论上证明上述双重优化能够最大化节点分类的全局置信度。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 实验表明，与14个基线模型相比，在10个公开数据集上实现了统计上显著的性能提升。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2022-01-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/plai-group/simple-cnaps&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern deep learning requires large-scale extensively labelled datasets for
training. Few-shot learning aims to alleviate this issue by learning
effectively from few labelled examples. In previously proposed few-shot visual
classifiers, it is assumed that the feature manifold, where classifier
decisions are made, has uncorrelated feature dimensions and uniform feature
variance. In this work, we focus on addressing the limitations arising from
this assumption by proposing a variance-sensitive class of models that operates
in a low-label regime. The first method, Simple CNAPS, employs a hierarchically
regularized Mahalanobis-distance based classifier combined with a state of the
art neural adaptive feature extractor to achieve strong performance on
Meta-Dataset, mini-ImageNet and tiered-ImageNet benchmarks. We further extend
this approach to a transductive learning setting, proposing Transductive CNAPS.
This transductive method combines a soft k-means parameter refinement procedure
with a two-step task encoder to achieve improved test-time classification
accuracy using unlabelled data. Transductive CNAPS achieves state of the art
performance on Meta-Dataset. Finally, we explore the use of our methods (Simple
and Transductive) for "out of the box" continual and active learning. Extensive
experiments on large scale benchmarks illustrate robustness and versatility of
this, relatively speaking, simple class of models. All trained model
checkpoints and corresponding source codes have been made publicly available.</description>
      <guid isPermaLink="false">2201.05151v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Virtual Node Generation for Node Classification in Sparsely-Labeled Graphs</title>
      <link>http://arxiv.org/abs/2409.07712v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 对话主题分割在各种对话建模任务中起着至关重要的作用。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 最先进的无监督对话主题分割（DTS）方法通过相邻话语匹配和伪分割，从对话数据中学习主题感知的语篇表示。然而，在多轮对话中，话语常常存在共指或省略，这可能会影响相邻话语匹配任务的语义相似性计算。&lt;br&gt;&lt;h4&gt;3. 研究目标&lt;/h4&gt;   - 为了充分利用对话关系中的有用线索，本文提出了一种新颖的无监督对话主题分割方法，结合了话语重写（UR）技术与无监督学习算法。&lt;br&gt;&lt;h4&gt;4. 方法创新&lt;/h4&gt;   - 通过重写对话来恢复共指和省略的词，从而有效利用未标记对话中的有用线索。&lt;br&gt;&lt;h4&gt;5. 模型名称&lt;/h4&gt;   - 提出的模型称为话语重写主题分割模型（UR-DTS）。&lt;br&gt;&lt;h4&gt;6. 性能提升&lt;/h4&gt;   - 与现有的无监督模型相比，UR-DTS显著提高了主题分割的准确性。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 在DialSeg711数据集上，模型的绝对误差分数提高了约6%，绝对误差分数达11.42%，WD（Wang Distance）达12.97%。&lt;br&gt;   - 在Doc2Dial数据集上，绝对误差分数和WD分别提高了约3%和2%，绝对误差分数达到35.17%，WD达到38.49%。&lt;br&gt;&lt;h4&gt;8. 研究结论&lt;/h4&gt;   - 这些结果表明，模型在捕捉对话主题的细微差别方面非常有效，同时也展现了利用未标记对话的有用性和挑战。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the broader machine learning literature, data-generation methods
demonstrate promising results by generating additional informative training
examples via augmenting sparse labels. Such methods are less studied in graphs
due to the intricate dependencies among nodes in complex topology structures.
This paper presents a novel node generation method that infuses a small set of
high-quality synthesized nodes into the graph as additional labeled nodes to
optimally expand the propagation of labeled information. By simply infusing
additional nodes, the framework is orthogonal to the graph learning and
downstream classification techniques, and thus is compatible with most popular
graph pre-training (self-supervised learning), semi-supervised learning, and
meta-learning methods. The contribution lies in designing the generated node
set by solving a novel optimization problem. The optimization places the
generated nodes in a manner that: (1) minimizes the classification loss to
guarantee training accuracy and (2) maximizes label propagation to
low-confidence nodes in the downstream task to ensure high-quality propagation.
Theoretically, we show that the above dual optimization maximizes the global
confidence of node classification. Our Experiments demonstrate statistically
significant performance improvements over 14 baselines on 10 publicly available
datasets.</description>
      <guid isPermaLink="false">2409.07712v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>An Unsupervised Dialogue Topic Segmentation Model Based on Utterance Rewriting</title>
      <link>http://arxiv.org/abs/2409.07672v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  in Chinese language&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 对话主题分割在各种对话建模任务中起着至关重要的作用。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 最先进的无监督对话主题分割（DTS）方法通过相邻话语匹配和伪分割，从对话数据中学习主题感知的语篇表示。然而，在多轮对话中，话语常常存在共指或省略，这可能会影响相邻话语匹配任务的语义相似性计算。&lt;br&gt;&lt;h4&gt;3. 研究目标&lt;/h4&gt;   - 为了充分利用对话关系中的有用线索，本文提出了一种新颖的无监督对话主题分割方法，结合了话语重写（UR）技术与无监督学习算法。&lt;br&gt;&lt;h4&gt;4. 方法创新&lt;/h4&gt;   - 通过重写对话来恢复共指和省略的词，从而有效利用未标记对话中的有用线索。&lt;br&gt;&lt;h4&gt;5. 模型名称&lt;/h4&gt;   - 提出的模型称为话语重写主题分割模型（UR-DTS）。&lt;br&gt;&lt;h4&gt;6. 性能提升&lt;/h4&gt;   - 与现有的无监督模型相比，UR-DTS显著提高了主题分割的准确性。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 在DialSeg711数据集上，模型的绝对误差分数提高了约6%，绝对误差分数达11.42%，WD（Wang Distance）达12.97%。&lt;br&gt;   - 在Doc2Dial数据集上，绝对误差分数和WD分别提高了约3%和2%，绝对误差分数达到35.17%，WD达到38.49%。&lt;br&gt;&lt;h4&gt;8. 研究结论&lt;/h4&gt;   - 这些结果表明，模型在捕捉对话主题的细微差别方面非常有效，同时也展现了利用未标记对话的有用性和挑战。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dialogue topic segmentation plays a crucial role in various types of dialogue
modeling tasks. The state-of-the-art unsupervised DTS methods learn topic-aware
discourse representations from conversation data through adjacent discourse
matching and pseudo segmentation to further mine useful clues in unlabeled
conversational relations. However, in multi-round dialogs, discourses often
have co-references or omissions, leading to the fact that direct use of these
discourses for representation learning may negatively affect the semantic
similarity computation in the neighboring discourse matching task. In order to
fully utilize the useful cues in conversational relations, this study proposes
a novel unsupervised dialog topic segmentation method that combines the
Utterance Rewriting (UR) technique with an unsupervised learning algorithm to
efficiently utilize the useful cues in unlabeled dialogs by rewriting the
dialogs in order to recover the co-referents and omitted words. Compared with
existing unsupervised models, the proposed Discourse Rewriting Topic
Segmentation Model (UR-DTS) significantly improves the accuracy of topic
segmentation. The main finding is that the performance on DialSeg711 improves
by about 6% in terms of absolute error score and WD, achieving 11.42% in terms
of absolute error score and 12.97% in terms of WD. on Doc2Dial the absolute
error score and WD improves by about 3% and 2%, respectively, resulting in SOTA
reaching 35.17% in terms of absolute error score and 38.49% in terms of WD.
This shows that the model is very effective in capturing the nuances of
conversational topics, as well as the usefulness and challenges of utilizing
unlabeled conversations.</description>
      <guid isPermaLink="false">2409.07672v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>DiFSD: Ego-Centric Fully Sparse Paradigm with Uncertainty Denoising and Iterative Refinement for Efficient End-to-End Autonomous Driving</title>
      <link>http://arxiv.org/abs/2409.09777v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 当前的端到端自动驾驶方法通过统一模块设计（如感知、预测和规划）来处理各种任务。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 尽管现有系统在规划导向的精神下经过优化，但缺乏自我中心设计的端到端驾驶系统仍面临性能不佳和效率低下的问题，这主要归因于光栅化场景表示学习和冗余信息传输。&lt;br&gt;&lt;h4&gt;3. 新方法的提出&lt;/h4&gt;   - 本文回顾了人类驾驶行为，提出了一种自我中心的完全稀疏范式，称为DiFSD，用于端到端自驾驶。&lt;br&gt;&lt;h4&gt;4. DiFSD的结构&lt;/h4&gt;   - DiFSD主要由三个模块组成：稀疏感知、层次交互和迭代运动规划。&lt;br&gt;&lt;h4&gt;5. 稀疏感知模块&lt;/h4&gt;   - 该模块基于稀疏场景表示进行检测、跟踪和在线地图构建。&lt;br&gt;&lt;h4&gt;6. 层次交互模块&lt;/h4&gt;   - 旨在从粗到细选择最接近车道内的车辆/静态物体（CIPV / CIPS），并利用额外的几何先验信息。&lt;br&gt;&lt;h4&gt;7. 迭代运动规划模块&lt;/h4&gt;   - 考虑选定的交互代理和自我车辆的联合运动预测，输出的多模态自我轨迹以迭代方式优化。&lt;br&gt;&lt;h4&gt;8. 不确定性建模&lt;/h4&gt;   - 引入位置级运动扩散和轨迹级规划去噪，以促进整个框架的训练稳定性和收敛性。&lt;br&gt;&lt;h4&gt;9. 实验结果&lt;/h4&gt;   - 在nuScenes数据集上进行的广泛实验表明，DiFSD在规划性能和效率方面表现优越，平均L2误差降低了66%，碰撞率降低了77%，同时运行效率提高了8.2倍。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current end-to-end autonomous driving methods resort to unifying modular
designs for various tasks (e.g. perception, prediction and planning). Although
optimized in a planning-oriented spirit with a fully differentiable framework,
existing end-to-end driving systems without ego-centric designs still suffer
from unsatisfactory performance and inferior efficiency, owing to the
rasterized scene representation learning and redundant information
transmission. In this paper, we revisit the human driving behavior and propose
an ego-centric fully sparse paradigm, named DiFSD, for end-to-end self-driving.
Specifically, DiFSD mainly consists of sparse perception, hierarchical
interaction and iterative motion planner. The sparse perception module performs
detection, tracking and online mapping based on sparse representation of the
driving scene. The hierarchical interaction module aims to select the Closest
In-Path Vehicle / Stationary (CIPV / CIPS) from coarse to fine, benefiting from
an additional geometric prior. As for the iterative motion planner, both
selected interactive agents and ego-vehicle are considered for joint motion
prediction, where the output multi-modal ego-trajectories are optimized in an
iterative fashion. Besides, both position-level motion diffusion and
trajectory-level planning denoising are introduced for uncertainty modeling,
thus facilitating the training stability and convergence of the whole
framework. Extensive experiments conducted on nuScenes dataset demonstrate the
superior planning performance and great efficiency of DiFSD, which
significantly reduces the average L2 error by \textbf{66\%} and collision rate
by \textbf{77\%} than UniAD while achieves \textbf{8.2$\times$} faster running
efficiency.</description>
      <guid isPermaLink="false">2409.09777v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Efficient 3D Shape Generation via Diffusion Mamba with Bidirectional SSMs</title>
      <link>http://arxiv.org/abs/2406.05038v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 最近的序列建模进展催生了Mamba架构，以选择性状态空间方法著称，提供了高效处理长序列的可能性。&lt;br&gt;&lt;h4&gt;2. 应用局限&lt;/h4&gt;   - Mamba架构在高分辨率3D形状生成中的应用仍然未得到充分探讨。&lt;br&gt;&lt;h4&gt;3. 传统方法的挑战&lt;/h4&gt;   - 传统的扩散变压器（DiT）使用自注意力机制，尽管潜力巨大，但由于输入长度增加导致的注意力操作的立方复杂性，面临可扩展性挑战，尤其是在处理高分辨率体素时。&lt;br&gt;&lt;h4&gt;4. 新架构的提出&lt;/h4&gt;   - 为了解决上述挑战，本文提出了一种新的扩散架构——Diffusion Mamba（DiM-3D），专门用于3D点云生成。&lt;br&gt;&lt;h4&gt;5. 注意力机制的替代&lt;/h4&gt;   - DiM-3D摒弃了传统的注意力机制，利用Mamba架构的固有效率，使得序列长度的复杂性保持线性。&lt;br&gt;&lt;h4&gt;6. 性能优势&lt;/h4&gt;   - DiM-3D的推理时间快速，计算需求显著降低，以减少的Gflops量化，从而解决了先前模型的关键可扩展性问题。&lt;br&gt;&lt;h4&gt;7. 实证结果&lt;/h4&gt;   - 在ShapeNet基准测试中，DiM-3D在生成高保真和多样化的3D形状方面实现了最先进的性能。&lt;br&gt;&lt;h4&gt;8. 任务表现&lt;/h4&gt;   - DiM-3D在3D点云补全等任务中表现出色，不仅证明了模型的可扩展性，还强调了其在生成细致、高分辨率体素方面的效率。&lt;br&gt;&lt;h4&gt;9. 结论与影响&lt;/h4&gt;   - 通过这些发现，展示了Diffusion Mamba框架在3D形状生成中的卓越可扩展性和效率，为该领域设定了新的标准，并为未来高分辨率3D建模技术的探索铺平了道路。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-06-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in sequence modeling have led to the development of the
Mamba architecture, noted for its selective state space approach, offering a
promising avenue for efficient long sequence handling. However, its application
in 3D shape generation, particularly at high resolutions, remains
underexplored. Traditional diffusion transformers (DiT) with self-attention
mechanisms, despite their potential, face scalability challenges due to the
cubic complexity of attention operations as input length increases. This
complexity becomes a significant hurdle when dealing with high-resolution voxel
sizes. To address this challenge, we introduce a novel diffusion architecture
tailored for 3D point clouds generation-Diffusion Mamba (DiM-3D). This
architecture forgoes traditional attention mechanisms, instead utilizing the
inherent efficiency of the Mamba architecture to maintain linear complexity
with respect to sequence length. DiM-3D is characterized by fast inference
times and substantially lower computational demands, quantified in reduced
Gflops, thereby addressing the key scalability issues of prior models. Our
empirical results on the ShapeNet benchmark demonstrate that DiM-3D achieves
state-of-the-art performance in generating high-fidelity and diverse 3D shapes.
Additionally, DiM-3D shows superior capabilities in tasks like 3D point cloud
completion. This not only proves the model's scalability but also underscores
its efficiency in generating detailed, high-resolution voxels necessary for
advanced 3D shape modeling, particularly excelling in environments requiring
high-resolution voxel sizes. Through these findings, we illustrate the
exceptional scalability and efficiency of the Diffusion Mamba framework in 3D
shape generation, setting a new standard for the field and paving the way for
future explorations in high-resolution 3D modeling technologies.</description>
      <guid isPermaLink="false">2406.05038v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>SuPerPM: A Large Deformation-Robust Surgical Perception Framework Based on Deep Point Matching Learned from Physical Constrained Simulation Data</title>
      <link>http://arxiv.org/abs/2309.13863v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 使用外科工具操作组织时，通常会导致较大的形变，而现有的跟踪和重建算法未能有效应对这些形变。&lt;br&gt;&lt;h4&gt;2. 跟踪误差来源&lt;/h4&gt;   - 在大形变过程中，跟踪误差的主要来源是观测传感器测量与先前跟踪场景之间的数据关联错误。&lt;br&gt;&lt;h4&gt;3. 提出的新框架&lt;/h4&gt;   - 本文提出了一种外科感知框架SuPerPM，利用基于学习的非刚性点云匹配进行数据关联，以适应更大的形变。&lt;br&gt;&lt;h4&gt;4. 学习模型的挑战&lt;/h4&gt;   - 学习模型通常需要具有真实点云对应关系的训练数据，但在外科环境中收集这些数据具有挑战性，甚至不切实际。&lt;br&gt;&lt;h4&gt;5. 模型调优的方法&lt;/h4&gt;   - 为了调优学习模型，收集了外科机器人操作软组织的内窥镜数据，并在不同时间点之间建立点云对应关系作为真实值。&lt;br&gt;&lt;h4&gt;6. 物理约束的应用&lt;/h4&gt;   - 通过采用基于位置的动力学（PBD）模拟，确保这些对应关系符合物理约束。&lt;br&gt;&lt;h4&gt;7. 性能验证&lt;/h4&gt;   - 在多个具有大形变特征的外科数据集上验证了该框架，表现优于最先进的外科场景跟踪算法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2023-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Manipulation of tissue with surgical tools often results in large
deformations that current methods in tracking and reconstructing algorithms
have not effectively addressed. A major source of tracking errors during large
deformations stems from wrong data association between observed sensor
measurements with previously tracked scene. To mitigate this issue, we present
a surgical perception framework, SuPerPM, that leverages learning-based
non-rigid point cloud matching for data association, thus accommodating larger
deformations. The learning models typically require training data with ground
truth point cloud correspondences, which is challenging or even impractical to
collect in surgical environments. Thus, for tuning the learning model, we
gather endoscopic data of soft tissue being manipulated by a surgical robot and
then establish correspondences between point clouds at different time points to
serve as ground truth. This was achieved by employing a position-based dynamics
(PBD) simulation to ensure that the correspondences adhered to physical
constraints. The proposed framework is demonstrated on several challenging
surgical datasets that are characterized by large deformations, achieving
superior performance over state-of-the-art surgical scene tracking algorithms.</description>
      <guid isPermaLink="false">2309.13863v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Scale Disparity of Instances in Interactive Point Cloud Segmentation</title>
      <link>http://arxiv.org/abs/2407.14009v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by 2024 IEEE/RSJ International Conference on Intelligent
  Robots and Systems&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 交互式点云分割已成为理解3D场景的关键任务，允许用户通过简单的交互（如点击）来指导分割模型，从而显著减少为多样场景和新类别定制模型所需的努力。&lt;br&gt;&lt;h4&gt;2. 实例定义的差异&lt;/h4&gt;   - 在交互式分割中，实例的含义与实例分割有所不同，用户可能希望分割不同规模的“事物”（thing）和“物质”（stuff）类别。&lt;br&gt;&lt;h4&gt;3. 现有方法的局限性&lt;/h4&gt;   - 现有方法主要集中于“事物”类别，忽略了“物质”类别的分割以及由规模差异带来的挑战。&lt;br&gt;&lt;h4&gt;4. 提出的新模型&lt;/h4&gt;   - 本文提出了ClickFormer，一种创新的交互式点云分割模型，能够准确分割“事物”和“物质”类别的实例。&lt;br&gt;&lt;h4&gt;5. 查询增强模块&lt;/h4&gt;   - 采用查询增强模块，通过全局查询采样策略来增强点击查询，实现不同实例规模下的一致性能。&lt;br&gt;&lt;h4&gt;6. 全局注意力机制&lt;/h4&gt;   - 在查询-体素变换器中采用全局注意力，以降低生成误报的风险，并进行其他网络结构改进，进一步提升模型的分割性能。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 实验表明，ClickFormer在室内和室外数据集上的表现优于现有的交互式点云分割方法，在开放世界环境中提供了更准确的分割结果，并减少了用户点击次数。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-07-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Interactive point cloud segmentation has become a pivotal task for
understanding 3D scenes, enabling users to guide segmentation models with
simple interactions such as clicks, therefore significantly reducing the effort
required to tailor models to diverse scenarios and new categories. However, in
the realm of interactive segmentation, the meaning of instance diverges from
that in instance segmentation, because users might desire to segment instances
of both thing and stuff categories that vary greatly in scale. Existing methods
have focused on thing categories, neglecting the segmentation of stuff
categories and the difficulties arising from scale disparity. To bridge this
gap, we propose ClickFormer, an innovative interactive point cloud segmentation
model that accurately segments instances of both thing and stuff categories. We
propose a query augmentation module to augment click queries by a global query
sampling strategy, thus maintaining consistent performance across different
instance scales. Additionally, we employ global attention in the query-voxel
transformer to mitigate the risk of generating false positives, along with
several other network structure improvements to further enhance the model's
segmentation performance. Experiments demonstrate that ClickFormer outperforms
existing interactive point cloud segmentation methods across both indoor and
outdoor datasets, providing more accurate segmentation results with fewer user
clicks in an open-world setting.</description>
      <guid isPermaLink="false">2407.14009v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Semi-Supervised 3D Object Detection with Chanel Augmentation using Transformation Equivariance</title>
      <link>http://arxiv.org/abs/2409.06583v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to 2024 IEEE International Conference on Image Processing
  (ICIP)&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 精确的3D物体检测对于自动驾驶汽车和机器人安全有效地导航和与环境互动至关重要。&lt;br&gt;&lt;h4&gt;2. 数据和标注的挑战&lt;/h4&gt;   - 3D检测器的性能依赖于数据量和标注，而获取这些数据和标注成本昂贵，因此对有限标注数据的训练需求日益增长。&lt;br&gt;&lt;h4&gt;3. 提出的新框架&lt;/h4&gt;   - 本文探索了一种新颖的教师-学生框架，采用通道增强（channel augmentation）进行3D半监督物体检测。&lt;br&gt;&lt;h4&gt;4. 教师-学生自监督学习（SSL）方法&lt;/h4&gt;   - 传统的教师-学生SSL通常对教师网络采用弱增强，对学生网络采用强增强。&lt;br&gt;&lt;h4&gt;5. 通道增强的应用&lt;/h4&gt;   - 本研究对两个网络都应用了多种通道增强，使用变换等变性检测器（TED）进行操作。&lt;br&gt;&lt;h4&gt;6. TED的优势&lt;/h4&gt;   - TED允许探索点云上的不同增强组合，并有效聚合多通道变换等变性特征。&lt;br&gt;&lt;h4&gt;7. 训练稳定性&lt;/h4&gt;   - 通过对教师网络采用固定的通道增强，学生网络可以在可靠的伪标签上稳定训练。&lt;br&gt;&lt;h4&gt;8. 数据多样性&lt;/h4&gt;   - 强通道增强可以丰富数据多样性，提高网络对变换的鲁棒性，增强学生网络的泛化性能。&lt;br&gt;&lt;h4&gt;9. 基准和适应性&lt;/h4&gt;   - 使用最先进的分层监督作为基线，并将其双阈值适应于TED，称为通道IoU一致性。&lt;br&gt;&lt;h4&gt;10. 实验结果&lt;/h4&gt;    - 在KITTI数据集上评估的方法显著提升了性能，超越了最先进的3D半监督物体检测模型。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate 3D object detection is crucial for autonomous vehicles and robots to
navigate and interact with the environment safely and effectively. Meanwhile,
the performance of 3D detector relies on the data size and annotation which is
expensive. Consequently, the demand of training with limited labeled data is
growing. We explore a novel teacher-student framework employing channel
augmentation for 3D semi-supervised object detection. The teacher-student SSL
typically adopts a weak augmentation and strong augmentation to teacher and
student, respectively. In this work, we apply multiple channel augmentations to
both networks using the transformation equivariance detector (TED). The TED
allows us to explore different combinations of augmentation on point clouds and
efficiently aggregates multi-channel transformation equivariance features. In
principle, by adopting fixed channel augmentations for the teacher network, the
student can train stably on reliable pseudo-labels. Adopting strong channel
augmentations can enrich the diversity of data, fostering robustness to
transformations and enhancing generalization performance of the student
network. We use SOTA hierarchical supervision as a baseline and adapt its
dual-threshold to TED, which is called channel IoU consistency. We evaluate our
method with KITTI dataset, and achieved a significant performance leap,
surpassing SOTA 3D semi-supervised object detection models.</description>
      <guid isPermaLink="false">2409.06583v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Cross-Organ and Cross-Scanner Adenocarcinoma Segmentation using Rein to Fine-tune Vision Foundation Models</title>
      <link>http://arxiv.org/abs/2409.11752v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 数字病理学领域在肿瘤分割方面取得了显著进展，但器官、组织制备方法和图像采集过程的变化可能导致数字病理图像之间的领域差异。&lt;br&gt;&lt;h4&gt;2. 研究目的&lt;/h4&gt;   - 针对上述问题，本文提出使用Rein，一种微调方法，来高效地微调各种视觉基础模型（VFM）。&lt;br&gt;&lt;h4&gt;3. Rein方法的核心&lt;/h4&gt;   - Rein的核心是一个可学习的标记集，这些标记与实例直接关联，提高了每层的实例级功能。&lt;br&gt;&lt;h4&gt;4. 挑战背景&lt;/h4&gt;   - 研究基于MICCAI 2024跨器官和跨扫描仪腺癌分割挑战（COSAS2024）的数据环境。&lt;br&gt;&lt;h4&gt;5. 实验结果&lt;/h4&gt;   - 大量实验表明，Rein微调的VFM在挑战中取得了令人满意的结果。&lt;br&gt;&lt;h4&gt;6. 具体模型应用&lt;/h4&gt;   - 使用Rein对ConvNeXt和DINOv2进行了微调。&lt;br&gt;&lt;h4&gt;7. 性能表现&lt;/h4&gt;   - ConvNeXt在任务1的初步测试阶段和最终测试阶段分别取得了0.7719和0.7557的得分。&lt;br&gt;   - DINOv2在任务2的初步测试阶段和最终测试阶段分别取得了0.8848和0.8192的得分。&lt;br&gt;&lt;h4&gt;8. 代码可用性&lt;/h4&gt;   - 相关代码可在GitHub上获取。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, significant progress has been made in tumor segmentation
within the field of digital pathology. However, variations in organs, tissue
preparation methods, and image acquisition processes can lead to domain
discrepancies among digital pathology images. To address this problem, in this
paper, we use Rein, a fine-tuning method, to parametrically and efficiently
fine-tune various vision foundation models (VFMs) for MICCAI 2024 Cross-Organ
and Cross-Scanner Adenocarcinoma Segmentation (COSAS2024). The core of Rein
consists of a set of learnable tokens, which are directly linked to instances,
improving functionality at the instance level in each layer. In the data
environment of the COSAS2024 Challenge, extensive experiments demonstrate that
Rein fine-tuned the VFMs to achieve satisfactory results. Specifically, we used
Rein to fine-tune ConvNeXt and DINOv2. Our team used the former to achieve
scores of 0.7719 and 0.7557 on the preliminary test phase and final test phase
in task1, respectively, while the latter achieved scores of 0.8848 and 0.8192
on the preliminary test phase and final test phase in task2. Code is available
at GitHub.</description>
      <guid isPermaLink="false">2409.11752v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>TrajectoryFormer: 3D Object Tracking Transformer with Predictive Trajectory Hypotheses</title>
      <link>http://arxiv.org/abs/2306.05888v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICCV 2023&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 3D多目标跟踪（MOT）在自动驾驶车辆和服务机器人等多个应用中至关重要。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 目前的跟踪-检测范式在3D MOT上取得了重要进展，但仅依赖当前帧的检测框进行轨迹与框的关联，这限制了对被检测器漏掉的物体的恢复能力。&lt;br&gt;&lt;h4&gt;3. 提出的新框架&lt;/h4&gt;   - 本文提出了TrajectoryFormer，一个基于点云的3D MOT框架。&lt;br&gt;&lt;h4&gt;4. 恢复漏检物体的方法&lt;/h4&gt;   - 为了恢复被检测器漏掉的物体，TrajectoryFormer生成多个轨迹假设，使用混合候选框，包括时间预测框和当前帧检测框，以进行轨迹-框关联。&lt;br&gt;&lt;h4&gt;5. 时间预测框的优势&lt;/h4&gt;   - 预测框能够传播物体的历史轨迹信息到当前帧，从而使网络可以容忍短期的漏检。&lt;br&gt;&lt;h4&gt;6. 特征嵌入的优化&lt;/h4&gt;   - 结合长期物体运动特征和短期物体外观特征，创建每个假设的特征嵌入，从而减少空间-时间编码的计算开销。&lt;br&gt;&lt;h4&gt;7. 信息交互模块&lt;/h4&gt;   - 引入全局-局部交互模块，促进所有假设之间的信息交互，并建模它们的空间关系，从而提高假设的估计准确性。&lt;br&gt;&lt;h4&gt;8. 性能表现&lt;/h4&gt;   - TrajectoryFormer在Waymo 3D MOT基准测试中实现了最先进的性能。&lt;br&gt;&lt;h4&gt;9. 代码可用性&lt;/h4&gt;   - 相关代码可在GitHub上获取，地址为 [https://github.com/poodarchu/EFG](https://github.com/poodarchu/EFG)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2023-06-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/poodarchu/efg&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D multi-object tracking (MOT) is vital for many applications including
autonomous driving vehicles and service robots. With the commonly used
tracking-by-detection paradigm, 3D MOT has made important progress in recent
years. However, these methods only use the detection boxes of the current frame
to obtain trajectory-box association results, which makes it impossible for the
tracker to recover objects missed by the detector. In this paper, we present
TrajectoryFormer, a novel point-cloud-based 3D MOT framework. To recover the
missed object by detector, we generates multiple trajectory hypotheses with
hybrid candidate boxes, including temporally predicted boxes and current-frame
detection boxes, for trajectory-box association. The predicted boxes can
propagate object's history trajectory information to the current frame and thus
the network can tolerate short-term miss detection of the tracked objects. We
combine long-term object motion feature and short-term object appearance
feature to create per-hypothesis feature embedding, which reduces the
computational overhead for spatial-temporal encoding. Additionally, we
introduce a Global-Local Interaction Module to conduct information interaction
among all hypotheses and models their spatial relations, leading to accurate
estimation of hypotheses. Our TrajectoryFormer achieves state-of-the-art
performance on the Waymo 3D MOT benchmarks. Code is available at
https://github.com/poodarchu/EFG .</description>
      <guid isPermaLink="false">2306.05888v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Meta-learning Spiking Neural Networks with Surrogate Gradient Descent</title>
      <link>http://arxiv.org/abs/2201.10777v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to IOP Neuromorphic Computing and Engineering for peer
  review&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 自适应的“终身”学习在边缘计算和在线任务执行中是人工智能研究的一个理想目标。&lt;br&gt;&lt;h4&gt;2. 神经形态硬件的优势&lt;/h4&gt;   - 实现脉冲神经网络（SNN）的神经形态硬件尤其吸引人，因为其实时、基于事件的局部计算范式适合边缘实现和快速学习。&lt;br&gt;&lt;h4&gt;3. 学习过程的挑战&lt;/h4&gt;   - 传统SNN训练的长期和迭代学习特性与神经形态硬件的物理特性和实时操作不兼容。&lt;br&gt;&lt;h4&gt;4. 双层学习的应用&lt;/h4&gt;   - 双层学习（如元学习）在深度学习中越来越多地被使用，以克服这些限制。&lt;br&gt;&lt;h4&gt;5. 方法论&lt;/h4&gt;   - 本研究展示了在SNN中应用基于梯度的元学习，使用代理梯度方法来近似脉冲阈值函数以进行梯度估计。&lt;br&gt;&lt;h4&gt;6. 代理梯度的优势&lt;/h4&gt;   - 代理梯度可使模型具有二次可微性，从而能够使用如模型无关元学习（MAML）等成熟的二阶梯度元学习方法。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 使用MAML进行元训练的SNN在基于事件的元数据集上达到或超过了使用MAML进行元训练的传统人工神经网络（ANN）的性能。&lt;br&gt;&lt;h4&gt;8. 元学习的具体优势&lt;/h4&gt;   - 本研究展示了元学习带来的特定优势：快速学习而不需要高精度的权重或梯度。&lt;br&gt;&lt;h4&gt;9. 实际应用的潜力&lt;/h4&gt;   - 结果强调了元学习技术在将神经形态学习技术应用于现实世界问题中的重要性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2022-01-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Adaptive "life-long" learning at the edge and during online task performance
is an aspirational goal of AI research. Neuromorphic hardware implementing
Spiking Neural Networks (SNNs) are particularly attractive in this regard, as
their real-time, event-based, local computing paradigm makes them suitable for
edge implementations and fast learning. However, the long and iterative
learning that characterizes state-of-the-art SNN training is incompatible with
the physical nature and real-time operation of neuromorphic hardware. Bi-level
learning, such as meta-learning is increasingly used in deep learning to
overcome these limitations. In this work, we demonstrate gradient-based
meta-learning in SNNs using the surrogate gradient method that approximates the
spiking threshold function for gradient estimations. Because surrogate
gradients can be made twice differentiable, well-established, and effective
second-order gradient meta-learning methods such as Model Agnostic Meta
Learning (MAML) can be used. We show that SNNs meta-trained using MAML match or
exceed the performance of conventional ANNs meta-trained with MAML on
event-based meta-datasets. Furthermore, we demonstrate the specific advantages
that accrue from meta-learning: fast learning without the requirement of high
precision weights or gradients. Our results emphasize how meta-learning
techniques can become instrumental for deploying neuromorphic learning
technologies on real-world problems.</description>
      <guid isPermaLink="false">2201.10777v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Influence of Early through Late Fusion on Pancreas Segmentation from Imperfectly Registered Multimodal MRI</title>
      <link>http://arxiv.org/abs/2409.04563v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13.5 pages of manuscript content&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 多模态融合在胰腺分割中有潜力，但在模型中进行融合的位置仍然是一个未解的问题。&lt;br&gt;&lt;h4&gt;2. 对齐挑战&lt;/h4&gt;   - 在分析成对的不完全对齐图像时，存在两个主要挑战：&lt;br&gt;     - 胰腺的可变形性。&lt;br&gt;     - 呼吸导致的腹部变形。&lt;br&gt;   - 即使经过图像配准，相关的变形仍然通常无法纠正。&lt;br&gt;&lt;h4&gt;3. 研究目的&lt;/h4&gt;   - 考察从早期到晚期的融合对胰腺分割的影响。&lt;br&gt;&lt;h4&gt;4. 数据集&lt;/h4&gt;   - 使用353对T2加权（T2w）和T1加权（T1w）腹部MR图像，来自163名受试者，并附有胰腺标签。&lt;br&gt;&lt;h4&gt;5. 图像配准&lt;/h4&gt;   - 使用图像配准方法（deeds）对图像对进行对齐。&lt;br&gt;&lt;h4&gt;6. 模型训练&lt;/h4&gt;   - 训练一系列基本的UNet模型，测试不同的融合点，从早期到晚期，以评估其对不完全对齐图像分割性能的影响。&lt;br&gt;&lt;h4&gt;7. 融合点的泛化&lt;/h4&gt;   - 评估nnUNet中融合点的泛化能力。&lt;br&gt;&lt;h4&gt;8. 基准结果&lt;/h4&gt;   - 基本UNet模型的单模态T2w基线Dice得分为0.73，而nnUNet模型的相同基线得分为0.80。&lt;br&gt;&lt;h4&gt;9. 最佳融合策略&lt;/h4&gt;   - 对于基本UNet，最佳融合方法发生在编码器的中间（早期/中期融合），较基线提高了0.0125的Dice得分。&lt;br&gt;   - 对于nnUNet，最佳融合方法是在模型之前进行的简单图像拼接（早期融合），较基线提高了0.0021的Dice得分。&lt;br&gt;&lt;h4&gt;10. 性能提升的局限性&lt;/h4&gt;    - 在特定块中的融合可以提高性能，但最佳融合块是模型特定的，且增益较小。&lt;br&gt;&lt;h4&gt;11. 设计的重要性&lt;/h4&gt;    - 在不完全对齐的数据集上，融合是一个复杂的问题，设计的艺术对于揭示潜在的见解至关重要。&lt;br&gt;&lt;h4&gt;12. 未来方向&lt;/h4&gt;    - 需要未来的创新更好地解决在腹部图像对的不完全对齐情况下的融合问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/masilab/influence_of_fusion_on_pancreas_segmentation&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal fusion promises better pancreas segmentation. However, where to
perform fusion in models is still an open question. It is unclear if there is a
best location to fuse information when analyzing pairs of imperfectly aligned
images. Two main alignment challenges in this pancreas segmentation study are
1) the pancreas is deformable and 2) breathing deforms the abdomen. Even after
image registration, relevant deformations are often not corrected. We examine
how early through late fusion impacts pancreas segmentation. We used 353 pairs
of T2-weighted (T2w) and T1-weighted (T1w) abdominal MR images from 163
subjects with accompanying pancreas labels. We used image registration (deeds)
to align the image pairs. We trained a collection of basic UNets with different
fusion points, spanning from early to late, to assess how early through late
fusion influenced segmentation performance on imperfectly aligned images. We
assessed generalization of fusion points on nnUNet. The single-modality T2w
baseline using a basic UNet model had a Dice score of 0.73, while the same
baseline on the nnUNet model achieved 0.80. For the basic UNet, the best fusion
approach occurred in the middle of the encoder (early/mid fusion), which led to
a statistically significant improvement of 0.0125 on Dice score compared to the
baseline. For the nnUNet, the best fusion approach was na\"ive image
concatenation before the model (early fusion), which resulted in a
statistically significant Dice score increase of 0.0021 compared to baseline.
Fusion in specific blocks can improve performance, but the best blocks for
fusion are model specific, and the gains are small. In imperfectly registered
datasets, fusion is a nuanced problem, with the art of design remaining vital
for uncovering potential insights. Future innovation is needed to better
address fusion in cases of imperfect alignment of abdominal image pairs.</description>
      <guid isPermaLink="false">2409.04563v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>E-QUARTIC: Energy Efficient Edge Ensemble of Convolutional Neural Networks for Resource-Optimized Learning</title>
      <link>http://arxiv.org/abs/2409.08369v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by the 30th Asia and South Pacific Design Automation
  Conference (ASP-DAC 2025)&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 集成学习是一种元学习方法，通过结合多个学习者的预测来提高准确性和鲁棒性。&lt;br&gt;&lt;h4&gt;2. 现有问题&lt;/h4&gt;   - 使用卷积神经网络（CNN）进行模型集成会导致高内存和计算开销，限制了其在嵌入式系统中的部署。&lt;br&gt;&lt;h4&gt;3. 嵌入式系统的特性&lt;/h4&gt;   - 嵌入式设备通常配备小型电池供电，可能还包括从环境中提取能量的能量收集模块。&lt;br&gt;&lt;h4&gt;4. 新方法的提出&lt;/h4&gt;   - 提出了E-QUARTIC，一个新颖的能效边缘集成框架，旨在构建针对基于人工智能（AI）的嵌入式系统的CNN集成。&lt;br&gt;&lt;h4&gt;5. 性能提升&lt;/h4&gt;   - 该设计在准确性上优于单实例CNN基线和最先进的边缘AI解决方案，同时适应不同的能量条件，并保持类似的内存需求。&lt;br&gt;&lt;h4&gt;6. 能量感知模型选择&lt;/h4&gt;   - 利用设计的集成的多CNN结构，在能量收集AI系统中实施能量感知的模型选择策略。&lt;br&gt;&lt;h4&gt;7. 系统性能&lt;/h4&gt;   - 结果表明，所提出的解决方案通过将系统故障率降低多达40%，同时确保更高的平均输出质量，优于现有的最先进方法。&lt;br&gt;&lt;h4&gt;8. 并行训练与推理&lt;/h4&gt;   - 最终，研究表明所提出的设计能够在边缘设备上实现并行的设备训练和高质量推理执行，性能和能量开销限制在0.04%以下。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ensemble learning is a meta-learning approach that combines the predictions
of multiple learners, demonstrating improved accuracy and robustness.
Nevertheless, ensembling models like Convolutional Neural Networks (CNNs)
result in high memory and computing overhead, preventing their deployment in
embedded systems. These devices are usually equipped with small batteries that
provide power supply and might include energy-harvesting modules that extract
energy from the environment. In this work, we propose E-QUARTIC, a novel Energy
Efficient Edge Ensembling framework to build ensembles of CNNs targeting
Artificial Intelligence (AI)-based embedded systems. Our design outperforms
single-instance CNN baselines and state-of-the-art edge AI solutions, improving
accuracy and adapting to varying energy conditions while maintaining similar
memory requirements. Then, we leverage the multi-CNN structure of the designed
ensemble to implement an energy-aware model selection policy in
energy-harvesting AI systems. We show that our solution outperforms the
state-of-the-art by reducing system failure rate by up to 40% while ensuring
higher average output qualities. Ultimately, we show that the proposed design
enables concurrent on-device training and high-quality inference execution at
the edge, limiting the performance and energy overheads to less than 0.04%.</description>
      <guid isPermaLink="false">2409.08369v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Self-Supervised Syllable Discovery Based on Speaker-Disentangled HuBERT</title>
      <link>http://arxiv.org/abs/2409.10103v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IEEE SLT 2024&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 自监督语音表示学习对从未转录的音频中提取有意义特征变得越来越重要。&lt;br&gt;&lt;h4&gt;2. 近期进展&lt;/h4&gt;   - 近期的进展强调了从与语言单位相关的特征中导出离散符号的潜力，使得在多种任务中能够进行无文本训练。&lt;br&gt;&lt;h4&gt;3. SD-HuBERT方法&lt;/h4&gt;   - 句子级自蒸馏的预训练HuBERT（SD-HuBERT）通过自注意力层从中间Transformer层提取的潜在语音帧表示中引导音节结构。&lt;br&gt;&lt;h4&gt;4. CLS标记的问题&lt;/h4&gt;   - 在SD-HuBERT中，句子级表示通过使用特殊的CLS标记从语音帧特征中累积。然而，观察到CLS标记中聚合的信息与说话者身份的相关性高于与语言内容的相关性。&lt;br&gt;&lt;h4&gt;5. 提出的新方法&lt;/h4&gt;   - 为了解决这一问题，提出了一种仅基于语音的自监督微调方法，旨在将音节单位与说话者信息分离。&lt;br&gt;&lt;h4&gt;6. 数据增强和训练目标&lt;/h4&gt;   - 该方法引入说话者扰动作为数据增强，并采用帧级训练目标，防止CLS标记聚合副语言信息。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 实验结果表明，该方法在Librispeech上大多数音节分割和音节单位质量指标上超过了当前最先进的方法，强调了其在促进语音模型内音节组织方面的有效性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/ryota-komatsu/speaker_disentangled_hubert&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised speech representation learning has become essential for
extracting meaningful features from untranscribed audio. Recent advances
highlight the potential of deriving discrete symbols from the features
correlated with linguistic units, which enables text-less training across
diverse tasks. In particular, sentence-level Self-Distillation of the
pretrained HuBERT (SD-HuBERT) induces syllabic structures within latent speech
frame representations extracted from an intermediate Transformer layer. In
SD-HuBERT, sentence-level representation is accumulated from speech frame
features through self-attention layers using a special CLS token. However, we
observe that the information aggregated in the CLS token correlates more with
speaker identity than with linguistic content. To address this, we propose a
speech-only self-supervised fine-tuning approach that separates syllabic units
from speaker information. Our method introduces speaker perturbation as data
augmentation and adopts a frame-level training objective to prevent the CLS
token from aggregating paralinguistic information. Experimental results show
that our approach surpasses the current state-of-the-art method in most
syllable segmentation and syllabic unit quality metrics on Librispeech,
underscoring its effectiveness in promoting syllabic organization within
speech-only models.</description>
      <guid isPermaLink="false">2409.10103v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>SPAC: Sampling-based Progressive Attribute Compression for Dense Point Clouds</title>
      <link>http://arxiv.org/abs/2409.10293v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  136pages, 13 figures&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究目标&lt;/h4&gt;   - 提出了一个端到端的密集点云属性压缩方法。&lt;br&gt;&lt;h4&gt;2. 方法组成&lt;/h4&gt;   - 该方法结合了多个模块：&lt;br&gt;     - **频率采样模块**：使用汉明窗和快速傅里叶变换（FFT）提取点云的高频成分。&lt;br&gt;     - **自适应尺度特征提取模块**：借助几何信息进行特征提取。&lt;br&gt;     - **全局超先验熵模型**：用于熵编码。&lt;br&gt;&lt;h4&gt;3. 频率采样过程&lt;/h4&gt;   - 原始点云与采样点云之间的差异被划分为多个子点云。&lt;br&gt;   - 这些子点云使用八叉树进行分区，提供结构化输入以进行特征提取。&lt;br&gt;&lt;h4&gt;4. 特征提取模块&lt;/h4&gt;   - 整合了自适应卷积层，并利用偏移注意力捕捉局部和全局特征。&lt;br&gt;&lt;h4&gt;5. 特征精炼&lt;/h4&gt;   - 使用几何辅助属性特征精炼模块对提取的属性特征进行细化。&lt;br&gt;&lt;h4&gt;6. 熵编码&lt;/h4&gt;   - 引入全局超先验模型，通过从最深层（基础层）向其他层传播超先验参数，提高编码效率。&lt;br&gt;&lt;h4&gt;7. 解码器设计&lt;/h4&gt;   - 使用镜像网络逐步恢复特征，通过反卷积层重建颜色属性。&lt;br&gt;&lt;h4&gt;8. 编码效率&lt;/h4&gt;   - 方法在低比特率下编码基础层信息，并逐步添加增强层信息以提高重建精度。&lt;br&gt;&lt;h4&gt;9. 实验结果&lt;/h4&gt;   - 相较于最新的G-PCC测试模型（TMC13v23），在MPEG常见测试条件（CTCs）下，该方法在MPEG类别固体数据集上实现了Y分量平均比特率减少24.58%（YUV组合为21.23%），在MPEG类别密集数据集上实现了Y分量减少22.48%（YUV组合为17.19%）。&lt;br&gt;&lt;h4&gt;10. 贡献&lt;/h4&gt;    - 这是第一个在这些数据集上表现优于G-PCC标准的基于学习的编解码器，符合MPEG CTCs。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose an end-to-end attribute compression method for dense point clouds.
The proposed method combines a frequency sampling module, an adaptive scale
feature extraction module with geometry assistance, and a global hyperprior
entropy model. The frequency sampling module uses a Hamming window and the Fast
Fourier Transform to extract high-frequency components of the point cloud. The
difference between the original point cloud and the sampled point cloud is
divided into multiple sub-point clouds. These sub-point clouds are then
partitioned using an octree, providing a structured input for feature
extraction. The feature extraction module integrates adaptive convolutional
layers and uses offset-attention to capture both local and global features.
Then, a geometry-assisted attribute feature refinement module is used to refine
the extracted attribute features. Finally, a global hyperprior model is
introduced for entropy encoding. This model propagates hyperprior parameters
from the deepest (base) layer to the other layers, further enhancing the
encoding efficiency. At the decoder, a mirrored network is used to
progressively restore features and reconstruct the color attribute through
transposed convolutional layers. The proposed method encodes base layer
information at a low bitrate and progressively adds enhancement layer
information to improve reconstruction accuracy. Compared to the latest G-PCC
test model (TMC13v23) under the MPEG common test conditions (CTCs), the
proposed method achieved an average Bjontegaard delta bitrate reduction of
24.58% for the Y component (21.23% for YUV combined) on the MPEG Category Solid
dataset and 22.48% for the Y component (17.19% for YUV combined) on the MPEG
Category Dense dataset. This is the first instance of a learning-based codec
outperforming the G-PCC standard on these datasets under the MPEG CTCs.</description>
      <guid isPermaLink="false">2409.10293v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Latent Space Score-based Diffusion Model for Probabilistic Multivariate Time Series Imputation</title>
      <link>http://arxiv.org/abs/2409.08917v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, conference&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 准确补全的重要性&lt;/h4&gt;   - 准确的缺失数据补全对于下游任务的可靠性和成功至关重要。&lt;br&gt;&lt;h4&gt;2. 扩散模型的关注&lt;/h4&gt;   - 最近，扩散模型在数据补全领域受到广泛关注，但存在一些局限性。&lt;br&gt;&lt;h4&gt;3. 现有模型的局限&lt;/h4&gt;   - 现有扩散模型忽视了从观察数据中派生的低维潜在分布，这限制了模型的生成能力。&lt;br&gt;   - 处理原始缺失数据时，由于缺乏标签，问题变得尤为复杂。&lt;br&gt;&lt;h4&gt;4. 新方法的提出&lt;/h4&gt;   - 提出了潜在空间基于评分的扩散模型（LSSDM），用于概率性的多变量时间序列补全。&lt;br&gt;&lt;h4&gt;5. 潜在空间的利用&lt;/h4&gt;   - 观察值被投影到低维潜在空间，通过无监督学习方法重建缺失数据的粗略值，而不需要真实值。&lt;br&gt;&lt;h4&gt;6. 条件扩散模型的应用&lt;/h4&gt;   - 重建的值被输入到条件扩散模型中，以获得时间序列的精确补全值。&lt;br&gt;&lt;h4&gt;7. LSSDM的优势&lt;/h4&gt;   - LSSDM不仅能够识别潜在分布，还能无缝整合扩散模型，以获取高保真度的补全值并评估数据集的不确定性。&lt;br&gt;&lt;h4&gt;8. 实验结果&lt;/h4&gt;   - 实验结果表明，LSSDM在补全性能上优于其他方法，同时提供了更好的补全机制解释和不确定性分析。&lt;br&gt;&lt;h4&gt;9. 代码链接&lt;/h4&gt;   - 相关代码可在GitHub上找到，链接为 [LSSDM_imputation](https://github.com/gorgen2020/LSSDM_imputation)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/gorgen2020/LSSDM_imputation&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate imputation is essential for the reliability and success of
downstream tasks. Recently, diffusion models have attracted great attention in
this field. However, these models neglect the latent distribution in a
lower-dimensional space derived from the observed data, which limits the
generative capacity of the diffusion model. Additionally, dealing with the
original missing data without labels becomes particularly problematic. To
address these issues, we propose the Latent Space Score-Based Diffusion Model
(LSSDM) for probabilistic multivariate time series imputation. Observed values
are projected onto low-dimensional latent space and coarse values of the
missing data are reconstructed without knowing their ground truth values by
this unsupervised learning approach. Finally, the reconstructed values are fed
into a conditional diffusion model to obtain the precise imputed values of the
time series. In this way, LSSDM not only possesses the power to identify the
latent distribution but also seamlessly integrates the diffusion model to
obtain the high-fidelity imputed values and assess the uncertainty of the
dataset. Experimental results demonstrate that LSSDM achieves superior
imputation performance while also providing a better explanation and
uncertainty analysis of the imputation mechanism. The website of the code is
\textit{https://github.com/gorgen2020/LSSDM\_imputation}.</description>
      <guid isPermaLink="false">2409.08917v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>DMF-Net: Image-Guided Point Cloud Completion with Dual-Channel Modality Fusion and Shape-Aware Upsampling Transformer</title>
      <link>http://arxiv.org/abs/2406.17319v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究任务&lt;/h4&gt;   - 本文研究单视图图像引导的点云补全任务。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 现有方法通过显式或隐式地融合图像信息与点云，已取得良好效果，但未能充分平衡两种模态的信息。&lt;br&gt;&lt;h4&gt;3. 模态的重要性&lt;/h4&gt;   - 图像提供全局形状信息，而部分点云则包含丰富的局部细节。因此，在进行模态融合时，两种模态都需要得到同等重视。&lt;br&gt;&lt;h4&gt;4. 新方法的提出&lt;/h4&gt;   - 提出了新颖的双通道模态融合网络（DMF-Net），采用粗到细的方式进行图像引导的点云补全。&lt;br&gt;&lt;h4&gt;5. 网络结构&lt;/h4&gt;   - **第一阶段**：DMF-Net接收部分点云和对应图像作为输入，恢复粗略的点云。&lt;br&gt;   - **第二阶段**：使用形状感知上采样变换器对粗略点云进行两次上采样，以获得密集且完整的点云。&lt;br&gt;&lt;h4&gt;6. 实验结果&lt;/h4&gt;   - 大量定量和定性实验结果表明，DMF-Net在ShapeNet-ViPC数据集上超越了现有的单模态和多模态点云补全方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-06-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper we study the task of a single-view image-guided point cloud
completion. Existing methods have got promising results by fusing the
information of image into point cloud explicitly or implicitly. However, given
that the image has global shape information and the partial point cloud has
rich local details, We believe that both modalities need to be given equal
attention when performing modality fusion. To this end, we propose a novel
dual-channel modality fusion network for image-guided point cloud
completion(named DMF-Net), in a coarse-to-fine manner. In the first stage,
DMF-Net takes a partial point cloud and corresponding image as input to recover
a coarse point cloud. In the second stage, the coarse point cloud will be
upsampled twice with shape-aware upsampling transformer to get the dense and
complete point cloud. Extensive quantitative and qualitative experimental
results show that DMF-Net outperforms the state-of-the-art unimodal and
multimodal point cloud completion works on ShapeNet-ViPC dataset.</description>
      <guid isPermaLink="false">2406.17319v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Knowledge Adaptation Network for Few-Shot Class-Incremental Learning</title>
      <link>http://arxiv.org/abs/2409.11770v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages;6 figures&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 少样本类增量学习（FSCIL）旨在使用少量样本逐步识别新类别，同时保持对之前学习的类别的性能。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 许多现有方法通过均值特征简单初始化分类器权重，但这种策略在新类别表现上存在不足，因为新类别的表示通常较弱且有偏差。&lt;br&gt;&lt;h4&gt;3. 提出的解决方案&lt;/h4&gt;   - 本文从两个方面解决这一问题：&lt;br&gt;     1. **使用基础模型**：利用基础模型CLIP作为网络的基础，为每个类别提供通用表示。&lt;br&gt;     2. **知识适配模块**：提出知识适配器（KA）模块，汇总训练数据中的特定知识，并将其融合到通用表示中，以生成更可靠和全面的实例表示。&lt;br&gt;&lt;h4&gt;4. 增量伪情节学习机制&lt;/h4&gt;   - 为了将基础类别学习到的知识调整到即将到来的类别，提出了增量伪情节学习（IPEL）机制，通过模拟实际的FSCIL过程实现。&lt;br&gt;&lt;h4&gt;5. 方法名称与效果&lt;/h4&gt;   - 所提出的方法称为知识适应网络（KANet），在包括CIFAR100、CUB200和ImageNet-R等多个数据集上表现出竞争力的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Few-shot class-incremental learning (FSCIL) aims to incrementally recognize
new classes using a few samples while maintaining the performance on previously
learned classes. One of the effective methods to solve this challenge is to
construct prototypical evolution classifiers. Despite the advancement achieved
by most existing methods, the classifier weights are simply initialized using
mean features. Because representations for new classes are weak and biased, we
argue such a strategy is suboptimal. In this paper, we tackle this issue from
two aspects. Firstly, thanks to the development of foundation models, we employ
a foundation model, the CLIP, as the network pedestal to provide a general
representation for each class. Secondly, to generate a more reliable and
comprehensive instance representation, we propose a Knowledge Adapter (KA)
module that summarizes the data-specific knowledge from training data and fuses
it into the general representation. Additionally, to tune the knowledge learned
from the base classes to the upcoming classes, we propose a mechanism of
Incremental Pseudo Episode Learning (IPEL) by simulating the actual FSCIL.
Taken together, our proposed method, dubbed as Knowledge Adaptation Network
(KANet), achieves competitive performance on a wide range of datasets,
including CIFAR100, CUB200, and ImageNet-R.</description>
      <guid isPermaLink="false">2409.11770v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Fine-grained Metrics for Point Cloud Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2407.21289v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  PRCV 2024&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 常见的不平衡现象&lt;/h4&gt;   - 在点云语义分割数据集中，观察到两种常见的不平衡：&lt;br&gt;     1. **类别不平衡**：某些对象的出现频率高于其他对象。&lt;br&gt;     2. **大小不平衡**：某些对象占据的点数多于其他对象。&lt;br&gt;&lt;h4&gt;2. 现有评估指标的偏见&lt;/h4&gt;   - 由于上述不平衡，现有的评估指标往往偏向于大类别和大型对象。&lt;br&gt;&lt;h4&gt;3. 新指标的提出&lt;/h4&gt;   - 本文建议使用细粒度的mIoU（平均交并比）和mAcc（平均准确率）来更全面地评估点云分割算法，以解决不平衡问题。&lt;br&gt;&lt;h4&gt;4. 统计信息的丰富性&lt;/h4&gt;   - 细粒度指标提供了更丰富的统计信息，有助于更好地理解模型和数据集的表现。&lt;br&gt;&lt;h4&gt;5. 减少偏见&lt;/h4&gt;   - 这些新指标有助于减少现有语义分割指标对大型对象的偏见。&lt;br&gt;&lt;h4&gt;6. 应用实例&lt;/h4&gt;   - 提出的指标用于训练和评估多种语义分割算法，涵盖三种不同的室内和室外语义分割数据集。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Two forms of imbalances are commonly observed in point cloud semantic
segmentation datasets: (1) category imbalances, where certain objects are more
prevalent than others; and (2) size imbalances, where certain objects occupy
more points than others. Because of this, the majority of categories and large
objects are favored in the existing evaluation metrics. This paper suggests
fine-grained mIoU and mAcc for a more thorough assessment of point cloud
segmentation algorithms in order to address these issues. Richer statistical
information is provided for models and datasets by these fine-grained metrics,
which also lessen the bias of current semantic segmentation metrics towards
large objects. The proposed metrics are used to train and assess various
semantic segmentation algorithms on three distinct indoor and outdoor semantic
segmentation datasets.</description>
      <guid isPermaLink="false">2407.21289v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Cross-Modal Self-Supervised Learning with Effective Contrastive Units for LiDAR Point Clouds</title>
      <link>http://arxiv.org/abs/2409.06827v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  IROS 2024&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 3D感知的重要性&lt;/h4&gt;   - LiDAR点云中的3D感知对自动驾驶车辆在3D环境中正确行动至关重要。&lt;br&gt;&lt;h4&gt;2. 手动标注的挑战&lt;/h4&gt;   - 手动标注点云数据既困难又昂贵。&lt;br&gt;&lt;h4&gt;3. 自监督预训练的兴起&lt;/h4&gt;   - 近年来，对3D感知模型的自监督预训练引起了越来越多的兴趣。&lt;br&gt;&lt;h4&gt;4. 对比学习的成功&lt;/h4&gt;   - 受图像对比学习成功的启发，目前的方法主要在点云上进行对比预训练。&lt;br&gt;&lt;h4&gt;5. 传感器的多样性&lt;/h4&gt;   - 自动驾驶车辆通常配备多个传感器，包括摄像头和LiDAR，这为感知提供了多模态数据。&lt;br&gt;&lt;h4&gt;6. 对比学习的系统研究&lt;/h4&gt;   - 系统研究了单模态、跨模态和多模态的点云对比学习，结果显示跨模态学习优于其他选择。&lt;br&gt;&lt;h4&gt;7. 训练源的差异&lt;/h4&gt;   - 考虑到2D图像和3D点云之间的巨大差异，如何为LiDAR设计更有效的对比单元仍然不明确。&lt;br&gt;&lt;h4&gt;8. 新型对比单元的提出&lt;/h4&gt;   - 提出了实例感知和相似性平衡的对比单元，专门为自动驾驶点云定制。&lt;br&gt;&lt;h4&gt;9. 实验结果&lt;/h4&gt;   - 广泛的实验表明，该方法在LiDAR基础的3D目标检测和3D语义分割等下游感知任务上，相较于多种点云模型实现了显著的性能提升。&lt;br&gt;&lt;h4&gt;10. 基准测试&lt;/h4&gt;    - 该研究在四个流行基准测试上进行验证，包括Waymo Open Dataset、nuScenes、SemanticKITTI和ONCE。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/qcraftai/cross-modal-ssl&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D perception in LiDAR point clouds is crucial for a self-driving vehicle to
properly act in 3D environment. However, manually labeling point clouds is hard
and costly. There has been a growing interest in self-supervised pre-training
of 3D perception models. Following the success of contrastive learning in
images, current methods mostly conduct contrastive pre-training on point clouds
only. Yet an autonomous driving vehicle is typically supplied with multiple
sensors including cameras and LiDAR. In this context, we systematically study
single modality, cross-modality, and multi-modality for contrastive learning of
point clouds, and show that cross-modality wins over other alternatives. In
addition, considering the huge difference between the training sources in 2D
images and 3D point clouds, it remains unclear how to design more effective
contrastive units for LiDAR. We therefore propose the instance-aware and
similarity-balanced contrastive units that are tailored for self-driving point
clouds. Extensive experiments reveal that our approach achieves remarkable
performance gains over various point cloud models across the downstream
perception tasks of LiDAR based 3D object detection and 3D semantic
segmentation on the four popular benchmarks including Waymo Open Dataset,
nuScenes, SemanticKITTI and ONCE.</description>
      <guid isPermaLink="false">2409.06827v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Metappearance: Meta-Learning for Visual Appearance Reproduction</title>
      <link>http://arxiv.org/abs/2204.08993v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted as a journal paper to SIGGRAPH Asia 2022. Also see the
  project page at https://mfischer-ucl.github.io/metappearance&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 现有的视觉外观再现方法&lt;/h4&gt;   - 当前主要有两种使用机器学习（ML）再现视觉外观的方法。&lt;br&gt;&lt;h4&gt;2. 第一种方法&lt;/h4&gt;   - 训练模型以在不同实例上进行泛化，如数据集中不同的图像。&lt;br&gt;   - 这种“一次性”方法提供快速推断，但在质量上往往不够理想。&lt;br&gt;&lt;h4&gt;3. 第二种方法&lt;/h4&gt;   - 不在任务间泛化，而是对单个问题实例进行过拟合，例如材料的闪光图像。&lt;br&gt;   - 这些方法提供高质量结果，但训练时间较长。&lt;br&gt;&lt;h4&gt;4. 提出新方法&lt;/h4&gt;   - 建议结合这两种技术，通过元学习（meta-learning）实现端到端的解决方案。&lt;br&gt;   - 在内循环中对单个问题实例进行过拟合，同时在外循环中学习如何在多个样本中高效执行这一过程。&lt;br&gt;&lt;h4&gt;5. 元学习的形式化&lt;/h4&gt;   - 推导出所需的形式化方法，使元学习能够应用于多种视觉外观再现问题，包括纹理、BRDFs、svBRDFs、光照以及场景的整个光传输。&lt;br&gt;&lt;h4&gt;6. 参数分析&lt;/h4&gt;   - 分析元学习参数对视觉外观的多个方面的影响，并为不同任务提供具体指导。&lt;br&gt;&lt;h4&gt;7. Metappearance的优势&lt;/h4&gt;   - Metappearance在运行时间上仅需传统过拟合方法的一小部分，且能够实现与其相似的视觉质量，同时保持一般模型的适应性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2022-04-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3550454.3555458&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/mfischer-ucl/metappearance&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; There currently exist two main approaches to reproducing visual appearance
using Machine Learning (ML): The first is training models that generalize over
different instances of a problem, e.g., different images of a dataset. As
one-shot approaches, these offer fast inference, but often fall short in
quality. The second approach does not train models that generalize across
tasks, but rather over-fit a single instance of a problem, e.g., a flash image
of a material. These methods offer high quality, but take long to train. We
suggest to combine both techniques end-to-end using meta-learning: We over-fit
onto a single problem instance in an inner loop, while also learning how to do
so efficiently in an outer-loop across many exemplars. To this end, we derive
the required formalism that allows applying meta-learning to a wide range of
visual appearance reproduction problems: textures, BRDFs, svBRDFs, illumination
or the entire light transport of a scene. The effects of meta-learning
parameters on several different aspects of visual appearance are analyzed in
our framework, and specific guidance for different tasks is provided.
Metappearance enables visual quality that is similar to over-fit approaches in
only a fraction of their runtime while keeping the adaptivity of general
models.</description>
      <guid isPermaLink="false">2204.08993v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>S.T.A.R.-Track: Latent Motion Models for End-to-End 3D Object Tracking with Adaptive Spatio-Temporal Appearance Representations</title>
      <link>http://arxiv.org/abs/2306.17602v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  \c{opyright} 2023 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 跟踪方法背景&lt;/h4&gt;   - 本文遵循“通过注意力跟踪”的范式，提出了一种以对象为中心的、基于变换器的三维跟踪框架。&lt;br&gt;&lt;h4&gt;2. 传统跟踪方法的局限性&lt;/h4&gt;   - 传统的模型基础跟踪方法通过几何运动模型来整合对象和自我运动之间的几何效应。&lt;br&gt;&lt;h4&gt;3. 新方法的提出&lt;/h4&gt;   - 提出S.T.A.R.-Track，使用了一种新颖的潜在运动模型（LMM），该模型可以直接在潜在空间中调整对象查询，以考虑视角和光照条件的变化。&lt;br&gt;&lt;h4&gt;4. 几何运动建模&lt;/h4&gt;   - 同时，S.T.A.R.-Track明确建模几何运动，保持传统方法的优势。&lt;br&gt;&lt;h4&gt;5. 可学习的轨迹嵌入&lt;/h4&gt;   - 结合了一种新颖的可学习轨迹嵌入，帮助建模轨迹的存在概率，从而增强跟踪效果。&lt;br&gt;&lt;h4&gt;6. 通用性&lt;/h4&gt;   - 该框架是通用的，可以与任何基于查询的检测器集成，灵活性高。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 在nuScenes基准测试上进行了广泛实验，证明了该方法的优势，尤其是在DETR3D基础的跟踪器中表现出色，同时显著减少了轨迹的身份切换数量。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2023-06-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/LRA.2023.3342552&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Following the tracking-by-attention paradigm, this paper introduces an
object-centric, transformer-based framework for tracking in 3D. Traditional
model-based tracking approaches incorporate the geometric effect of object- and
ego motion between frames with a geometric motion model. Inspired by this, we
propose S.T.A.R.-Track, which uses a novel latent motion model (LMM) to
additionally adjust object queries to account for changes in viewing direction
and lighting conditions directly in the latent space, while still modeling the
geometric motion explicitly. Combined with a novel learnable track embedding
that aids in modeling the existence probability of tracks, this results in a
generic tracking framework that can be integrated with any query-based
detector. Extensive experiments on the nuScenes benchmark demonstrate the
benefits of our approach, showing \ac{sota} performance for DETR3D-based
trackers while drastically reducing the number of identity switches of tracks
at the same time.</description>
      <guid isPermaLink="false">2306.17602v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>A Survey of Multimodal Composite Editing and Retrieval</title>
      <link>http://arxiv.org/abs/2409.05405v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 3 figures, and 11 tables&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 信息丰富性与多样性&lt;/h4&gt;   - 现实世界中信息种类繁多，涉及多种模态（如文本、图像、音频等）。&lt;br&gt;   - 研究的重点是如何理解和利用这些多种数据类型来改善检索系统。&lt;br&gt;&lt;h4&gt;2. 多模态复合检索&lt;/h4&gt;   - 多模态复合检索结合了不同的模态，以提供更准确、个性化和上下文相关的结果。&lt;br&gt;&lt;h4&gt;3. 调查内容&lt;/h4&gt;   - 本文深入探讨多模态复合编辑和检索，涵盖图像-文本复合编辑、图像-文本复合检索及其他多模态复合检索。&lt;br&gt;&lt;h4&gt;4. 系统性组织&lt;/h4&gt;   - 系统性地整理了应用场景、方法、基准、实验和未来方向。&lt;br&gt;&lt;h4&gt;5. 多模态学习的重要性&lt;/h4&gt;   - 多模态学习在大型模型时代是一个热门话题，相关文献也在不断增加。&lt;br&gt;&lt;h4&gt;6. 文献综述的独特性&lt;/h4&gt;   - 这是首次对多模态复合检索文献的全面回顾，及时补充了现有的多模态融合研究。&lt;br&gt;&lt;h4&gt;7. 项目页面链接&lt;/h4&gt;   - 提供了项目页面链接，帮助读者快速跟踪该领域的最新进展：[GitHub项目页面](https://github.com/fuxianghuang1/Multimodal-Composite-Editing-and-Retrieval)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/fuxianghuang1/multimodal-composite-editing-and-retrieval&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the real world, where information is abundant and diverse across different
modalities, understanding and utilizing various data types to improve retrieval
systems is a key focus of research. Multimodal composite retrieval integrates
diverse modalities such as text, image and audio, etc. to provide more
accurate, personalized, and contextually relevant results. To facilitate a
deeper understanding of this promising direction, this survey explores
multimodal composite editing and retrieval in depth, covering image-text
composite editing, image-text composite retrieval, and other multimodal
composite retrieval. In this survey, we systematically organize the application
scenarios, methods, benchmarks, experiments, and future directions. Multimodal
learning is a hot topic in large model era, and have also witnessed some
surveys in multimodal learning and vision-language models with transformers
published in the PAMI journal. To the best of our knowledge, this survey is the
first comprehensive review of the literature on multimodal composite retrieval,
which is a timely complement of multimodal fusion to existing reviews. To help
readers' quickly track this field, we build the project page for this survey,
which can be found at
https://github.com/fuxianghuang1/Multimodal-Composite-Editing-and-Retrieval.</description>
      <guid isPermaLink="false">2409.05405v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Rethinking Meta-Learning from a Learning Lens</title>
      <link>http://arxiv.org/abs/2409.08474v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 元学习是一种强大的方法，通过利用以前任务的知识来解决新任务。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限&lt;/h4&gt;   - 主流方法主要关注训练一个良好泛化的模型初始化，然后在有限数据和更新的情况下适应不同任务，但这可能导致模型在训练任务上过拟合。&lt;br&gt;&lt;h4&gt;3. 过拟合原因&lt;/h4&gt;   - 以前的方法将过拟合归因于数据缺乏，通常使用数据增强来解决这一问题，但受到训练充分性和增强策略有效性的限制。&lt;br&gt;&lt;h4&gt;4. 研究重点&lt;/h4&gt;   - 本研究关注元学习的“学习如何学习”策略，探讨造成错误的原因及如何在不改变环境的情况下消除这些错误。&lt;br&gt;&lt;h4&gt;5. 算法过程重新思考&lt;/h4&gt;   - 从“学习”的角度重新思考元学习的算法过程，通过理论和实证分析发现：&lt;br&gt;     - (i) 该范式面临过拟合和欠拟合的风险；&lt;br&gt;     - (ii) 适应于不同任务的模型相互促进，且任务越相似，效果越明显。&lt;br&gt;&lt;h4&gt;6. 提出的解决方案&lt;/h4&gt;   - 基于上述见解，提出利用任务关系来校准元学习的优化过程，并开发了一种称为Task Relation Learner（TRLearner）的即插即用方法。&lt;br&gt;&lt;h4&gt;7. 方法实现步骤&lt;/h4&gt;   - 首先，从提取的任务特定元数据中获取任务关系矩阵；&lt;br&gt;   - 然后，使用获得的矩阵和关系感知一致性正则化来指导优化。&lt;br&gt;&lt;h4&gt;8. 实验结果&lt;/h4&gt;   - 大量理论和实证分析证明了TRLearner的有效性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Meta-learning has emerged as a powerful approach for leveraging knowledge
from previous tasks to solve new tasks. The mainstream methods focus on
training a well-generalized model initialization, which is then adapted to
different tasks with limited data and updates. However, it pushes the model
overfitting on the training tasks. Previous methods mainly attributed this to
the lack of data and used augmentations to address this issue, but they were
limited by sufficient training and effective augmentation strategies. In this
work, we focus on the more fundamental ``learning to learn'' strategy of
meta-learning to explore what causes errors and how to eliminate these errors
without changing the environment. Specifically, we first rethink the
algorithmic procedure of meta-learning from a ``learning'' lens. Through
theoretical and empirical analyses, we find that (i) this paradigm faces the
risk of both overfitting and underfitting and (ii) the model adapted to
different tasks promote each other where the effect is stronger if the tasks
are more similar. Based on this insight, we propose using task relations to
calibrate the optimization process of meta-learning and propose a plug-and-play
method called Task Relation Learner (TRLearner) to achieve this goal.
Specifically, it first obtains task relation matrices from the extracted
task-specific meta-data. Then, it uses the obtained matrices with
relation-aware consistency regularization to guide optimization. Extensive
theoretical and empirical analyses demonstrate the effectiveness of TRLearner.</description>
      <guid isPermaLink="false">2409.08474v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Point2Graph: An End-to-end Point Cloud-based 3D Open-Vocabulary Scene Graph for Robot Navigation</title>
      <link>http://arxiv.org/abs/2409.10350v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 9 figures&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 当前的开放词汇场景图生成算法高度依赖3D场景点云数据和带姿态的RGB-D图像，因此在缺乏RGB-D图像或相机姿态的情况下应用受限。&lt;br&gt;&lt;h4&gt;2. 研究目标&lt;/h4&gt;   - 提出Point2Graph，一个新颖的端到端点云基础的3D开放词汇场景图生成框架，消除了对带姿态RGB-D图像序列的需求。&lt;br&gt;&lt;h4&gt;3. 框架结构&lt;/h4&gt;   - 该框架为分层结构，包括房间和物体检测/分割以及开放词汇分类。&lt;br&gt;&lt;h4&gt;4. 房间层方法&lt;/h4&gt;   - 利用几何边界检测算法与学习区域检测相结合的优势，进行房间分割，并创建“Snap-Lookup”框架用于开放词汇房间分类。&lt;br&gt;&lt;h4&gt;5. 物体层方法&lt;/h4&gt;   - 创建一个端到端的管道，仅基于3D点云数据检测和分类3D物体。&lt;br&gt;&lt;h4&gt;6. 实验评估&lt;/h4&gt;   - 评估结果表明，Point2Graph框架在广泛使用的真实场景数据集上，能够超越当前最先进的开放词汇物体和房间分割与分类算法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current open-vocabulary scene graph generation algorithms highly rely on both
3D scene point cloud data and posed RGB-D images and thus have limited
applications in scenarios where RGB-D images or camera poses are not readily
available. To solve this problem, we propose Point2Graph, a novel end-to-end
point cloud-based 3D open-vocabulary scene graph generation framework in which
the requirement of posed RGB-D image series is eliminated. This hierarchical
framework contains room and object detection/segmentation and open-vocabulary
classification. For the room layer, we leverage the advantage of merging the
geometry-based border detection algorithm with the learning-based region
detection to segment rooms and create a "Snap-Lookup" framework for
open-vocabulary room classification. In addition, we create an end-to-end
pipeline for the object layer to detect and classify 3D objects based solely on
3D point cloud data. Our evaluation results show that our framework can
outperform the current state-of-the-art (SOTA) open-vocabulary object and room
segmentation and classification algorithm on widely used real-scene datasets.</description>
      <guid isPermaLink="false">2409.10350v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Active Learning to Guide Labeling Efforts for Question Difficulty Estimation</title>
      <link>http://arxiv.org/abs/2409.09258v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published as a workshop paper at ECML-PKDD 2024&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 最近，使用自然语言处理技术进行问题难度估计（QDE）的研究激增。&lt;br&gt;&lt;h4&gt;2. 现有方法&lt;/h4&gt;   - 基于Transformer的神经网络在QDE中取得了最先进的性能，主要通过监督学习方法，但在无监督学习方面的研究较少。&lt;br&gt;&lt;h4&gt;3. 监督与无监督方法的比较&lt;/h4&gt;   - **监督方法**：侧重于预测性能，但需要大量标记数据。&lt;br&gt;   - **无监督方法**：不需要标记数据，但依赖于不同且计算成本高的评估指标。&lt;br&gt;&lt;h4&gt;4. 研究创新&lt;/h4&gt;   - 本文通过探索主动学习为QDE搭建桥梁，提出一种监督的“人机协作”方法，旨在在匹配最先进模型性能的同时，减少标记工作量。&lt;br&gt;&lt;h4&gt;5. 主动学习过程&lt;/h4&gt;   - 迭代地在标记子集上训练，仅对最具信息量的未标记数据点获取人类专家的标签。&lt;br&gt;&lt;h4&gt;6. 新颖的获取函数&lt;/h4&gt;   - 提出了一种新颖的获取函数PowerVariance，用于将最具信息量的样本添加到标记集，这是对在分类中流行的PowerBALD函数的回归扩展。&lt;br&gt;&lt;h4&gt;7. 模型选择&lt;/h4&gt;   - 使用DistilBERT进行QDE，通过应用蒙特卡洛丢弃法捕获未标记样本中的认知不确定性，以识别信息丰富的样本。&lt;br&gt;&lt;h4&gt;8. 实验结果&lt;/h4&gt;   - 实验表明，使用PowerVariance获取的主动学习方法在仅标记10%训练数据后，性能接近完全监督模型。&lt;br&gt;&lt;h4&gt;9. 应用前景&lt;/h4&gt;   - 提出的 methodology 促进了教育资源的负责任使用，使QDE工具对课程教师更加可及，并对个性化支持系统和问答工具等其他应用具有潜力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, there has been a surge in research on Question Difficulty
Estimation (QDE) using natural language processing techniques.
Transformer-based neural networks achieve state-of-the-art performance,
primarily through supervised methods but with an isolated study in unsupervised
learning. While supervised methods focus on predictive performance, they
require abundant labeled data. On the other hand, unsupervised methods do not
require labeled data but rely on a different evaluation metric that is also
computationally expensive in practice. This work bridges the research gap by
exploring active learning for QDE, a supervised human-in-the-loop approach
striving to minimize the labeling efforts while matching the performance of
state-of-the-art models. The active learning process iteratively trains on a
labeled subset, acquiring labels from human experts only for the most
informative unlabeled data points. Furthermore, we propose a novel acquisition
function PowerVariance to add the most informative samples to the labeled set,
a regression extension to the PowerBALD function popular in classification. We
employ DistilBERT for QDE and identify informative samples by applying Monte
Carlo dropout to capture epistemic uncertainty in unlabeled samples. The
experiments demonstrate that active learning with PowerVariance acquisition
achieves a performance close to fully supervised models after labeling only 10%
of the training data. The proposed methodology promotes the responsible use of
educational resources, makes QDE tools more accessible to course instructors,
and is promising for other applications such as personalized support systems
and question-answering tools.</description>
      <guid isPermaLink="false">2409.09258v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>CMR-Agent: Learning a Cross-Modal Agent for Iterative Image-to-Point Cloud Registration</title>
      <link>http://arxiv.org/abs/2408.02394v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS) 2024&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究目标&lt;/h4&gt;   - 研究图像与点云配准，旨在确定RGB图像相对于点云的相对相机姿态。&lt;br&gt;&lt;h4&gt;2. 应用背景&lt;/h4&gt;   - 在预构建的LiDAR地图中，相机定位中扮演重要角色。&lt;br&gt;&lt;h4&gt;3. 现有方法的不足&lt;/h4&gt;   - 大多数基于学习的方法在特征空间中建立2D-3D点对应关系，但缺乏迭代优化的反馈机制，导致准确性和可解释性较差。&lt;br&gt;&lt;h4&gt;4. 新方法提出&lt;/h4&gt;   - 提出将配准过程重新表述为迭代的马尔可夫决策过程，允许基于每个中间状态逐步调整相机姿态。&lt;br&gt;&lt;h4&gt;5. 强化学习应用&lt;/h4&gt;   - 采用强化学习开发跨模态配准代理（CMR-Agent），并使用模仿学习初始化其配准策略，以提高训练的稳定性和快速启动。&lt;br&gt;&lt;h4&gt;6. 状态表示&lt;/h4&gt;   - 提出了2D-3D混合状态表示，充分利用RGB图像的细粒度特征，同时减少由于相机视锥空间截断导致的无用中立状态。&lt;br&gt;&lt;h4&gt;7. 框架设计&lt;/h4&gt;   - 整体框架设计良好，能够有效重用一次性跨模态嵌入，避免重复和耗时的特征提取。&lt;br&gt;&lt;h4&gt;8. 实验结果&lt;/h4&gt;   - 在KITTI-Odometry和NuScenes数据集上的广泛实验表明，CMR-Agent在配准中实现了具有竞争力的准确性和效率。&lt;br&gt;&lt;h4&gt;9. 迭代效率&lt;/h4&gt;   - 一旦完成一次性嵌入，每次迭代仅需几毫秒。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Image-to-point cloud registration aims to determine the relative camera pose
of an RGB image with respect to a point cloud. It plays an important role in
camera localization within pre-built LiDAR maps. Despite the modality gaps,
most learning-based methods establish 2D-3D point correspondences in feature
space without any feedback mechanism for iterative optimization, resulting in
poor accuracy and interpretability. In this paper, we propose to reformulate
the registration procedure as an iterative Markov decision process, allowing
for incremental adjustments to the camera pose based on each intermediate
state. To achieve this, we employ reinforcement learning to develop a
cross-modal registration agent (CMR-Agent), and use imitation learning to
initialize its registration policy for stability and quick-start of the
training. According to the cross-modal observations, we propose a 2D-3D hybrid
state representation that fully exploits the fine-grained features of RGB
images while reducing the useless neutral states caused by the spatial
truncation of camera frustum. Additionally, the overall framework is
well-designed to efficiently reuse one-shot cross-modal embeddings, avoiding
repetitive and time-consuming feature extraction. Extensive experiments on the
KITTI-Odometry and NuScenes datasets demonstrate that CMR-Agent achieves
competitive accuracy and efficiency in registration. Once the one-shot
embeddings are completed, each iteration only takes a few milliseconds.</description>
      <guid isPermaLink="false">2408.02394v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>The T05 System for The VoiceMOS Challenge 2024: Transfer Learning from Deep Image Classifier to Naturalness MOS Prediction of High-Quality Synthetic Speech</title>
      <link>http://arxiv.org/abs/2409.09305v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IEEE SLT 2024. Our MOS prediction system (UTMOSv2) is
  available in https://github.com/sarulab-speech/UTMOSv2&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 系统介绍&lt;/h4&gt;   - 介绍了用于2024年VoiceMOS Challenge (VMC) 的系统，称为T05。&lt;br&gt;&lt;h4&gt;2. 挑战目标&lt;/h4&gt;   - T05系统针对VMC 2024 Track 1，重点在于准确预测高质量合成语音的自然性均值意见分数（MOS）。&lt;br&gt;&lt;h4&gt;3. 特征提取器&lt;/h4&gt;   - 系统结合了一个预训练的自监督学习（SSL）语音特征提取器和一个预训练的图像特征提取器，以捕捉合成语音在语音谱图中的差异。&lt;br&gt;&lt;h4&gt;4. MOS预测器训练&lt;/h4&gt;   - 首先分别训练两个MOS预测器，分别使用SSL特征和谱图特征。&lt;br&gt;&lt;h4&gt;5. 预测器微调&lt;/h4&gt;   - 然后，通过融合两种提取的特征来微调这两个预测器，以提高MOS预测的准确性。&lt;br&gt;&lt;h4&gt;6. 性能表现&lt;/h4&gt;   - 在VMC 2024 Track 1中，T05系统在16项评估指标中取得了7项第一名和9项第二名，与排名第三及以下的系统相比具有显著差距。&lt;br&gt;&lt;h4&gt;7. 消融研究&lt;/h4&gt;   - 报告了消融研究的结果，探讨系统中的关键因素。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/sarulab-speech/utmosv2&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present our system (denoted as T05) for the VoiceMOS Challenge (VMC) 2024.
Our system was designed for the VMC 2024 Track 1, which focused on the accurate
prediction of naturalness mean opinion score (MOS) for high-quality synthetic
speech. In addition to a pretrained self-supervised learning (SSL)-based speech
feature extractor, our system incorporates a pretrained image feature extractor
to capture the difference of synthetic speech observed in speech spectrograms.
We first separately train two MOS predictors that use either of an SSL-based or
spectrogram-based feature. Then, we fine-tune the two predictors for better MOS
prediction using the fusion of two extracted features. In the VMC 2024 Track 1,
our T05 system achieved first place in 7 out of 16 evaluation metrics and
second place in the remaining 9 metrics, with a significant difference compared
to those ranked third and below. We also report the results of our ablation
study to investigate essential factors of our system.</description>
      <guid isPermaLink="false">2409.09305v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Explicitly Guided Information Interaction Network for Cross-modal Point Cloud Completion</title>
      <link>http://arxiv.org/abs/2407.02887v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ECCV 2024&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究目标&lt;/h4&gt;   - 探索一种新框架，EGIInet（Explicitly Guided Information Interaction Network），用于视图引导的点云完成（ViPC）任务。&lt;br&gt;&lt;h4&gt;2. 任务描述&lt;/h4&gt;   - 目标是从部分点云和单幅图像恢复完整的点云。&lt;br&gt;&lt;h4&gt;3. 方法优势&lt;/h4&gt;   - 相较于以往依赖输入图像全局语义的方法，EGIInet高效结合了两种模态的信息，利用了完成任务的几何特性。&lt;br&gt;&lt;h4&gt;4. 关键创新&lt;/h4&gt;   - 提出了显式引导的信息交互策略，支持模态对齐，以改善点云完成效果。&lt;br&gt;&lt;h4&gt;5. 特征编码&lt;/h4&gt;   - 与以往方法仅分别使用2D和3D骨干网络编码特征不同，EGIInet统一了编码过程，以促进模态对齐。&lt;br&gt;&lt;h4&gt;6. 信息识别&lt;/h4&gt;   - 新的引导信息交互策略帮助网络识别图像中的关键信息，从而为完成提供更好的指导。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 广泛实验表明该框架的有效性，在基准数据集上实现了新的状态-of-the-art（相较XMFnet提升16% CD），且使用的参数数量少于以往方法。&lt;br&gt;&lt;h4&gt;8. 资源获取&lt;/h4&gt;   - 预训练模型和代码可在 [GitHub](https://github.com/WHU-USI3DV/EGIInet) 上获取。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-07-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/whu-usi3dv/egiinet&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we explore a novel framework, EGIInet (Explicitly Guided
Information Interaction Network), a model for View-guided Point cloud
Completion (ViPC) task, which aims to restore a complete point cloud from a
partial one with a single view image. In comparison with previous methods that
relied on the global semantics of input images, EGIInet efficiently combines
the information from two modalities by leveraging the geometric nature of the
completion task. Specifically, we propose an explicitly guided information
interaction strategy supported by modal alignment for point cloud completion.
First, in contrast to previous methods which simply use 2D and 3D backbones to
encode features respectively, we unified the encoding process to promote modal
alignment. Second, we propose a novel explicitly guided information interaction
strategy that could help the network identify critical information within
images, thus achieving better guidance for completion. Extensive experiments
demonstrate the effectiveness of our framework, and we achieved a new
state-of-the-art (+16% CD over XMFnet) in benchmark datasets despite using
fewer parameters than the previous methods. The pre-trained model and code and
are available at https://github.com/WHU-USI3DV/EGIInet.</description>
      <guid isPermaLink="false">2407.02887v3</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>BOTT: Box Only Transformer Tracker for 3D Object Tracking</title>
      <link>http://arxiv.org/abs/2308.08753v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 3D对象跟踪是自动驾驶中的重要任务。&lt;br&gt;   - 经典的基于卡尔曼滤波的方法仍然是最流行的解决方案。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 这些方法需要手工设计运动模型，无法充分利用不断增长的数据量。&lt;br&gt;&lt;h4&gt;3. 提出的新方法&lt;/h4&gt;   - 提出了Box Only Transformer Tracker (BOTT)，通过将时间窗口内的所有3D框作为输入，学习连接来自不同帧的同一对象的3D框。&lt;br&gt;&lt;h4&gt;4. 技术细节&lt;/h4&gt;   - 应用变换器自注意力机制，促进所有框之间的信息交流，以学习全局信息的框嵌入。&lt;br&gt;   - 使用这些学习到的嵌入之间的相似性来链接同一对象的框。&lt;br&gt;&lt;h4&gt;5. 跟踪模式&lt;/h4&gt;   - BOTT可以无缝支持在线和离线跟踪模式。&lt;br&gt;&lt;h4&gt;6. 工程效率&lt;/h4&gt;   - 其简洁性显著减少了传统基于卡尔曼滤波方法所需的工程工作。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 实验表明，BOTT在两个最大的3D多目标跟踪基准上表现具有竞争力：&lt;br&gt;     - 在nuScenes验证和测试集上分别为69.9和66.7 AMOTA。&lt;br&gt;     - 在Waymo Open Dataset验证和测试集上分别为56.45和59.57 MOTA L2。&lt;br&gt;&lt;h4&gt;8. 结论&lt;/h4&gt;   - 本研究表明，通过使用变换器直接从3D框学习特征进行3D对象跟踪是一种简单而有效的方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2023-08-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tracking 3D objects is an important task in autonomous driving. Classical
Kalman Filtering based methods are still the most popular solutions. However,
these methods require handcrafted designs in motion modeling and can not
benefit from the growing data amounts. In this paper, Box Only Transformer
Tracker (BOTT) is proposed to learn to link 3D boxes of the same object from
the different frames, by taking all the 3D boxes in a time window as input.
Specifically, transformer self-attention is applied to exchange information
between all the boxes to learn global-informative box embeddings. The
similarity between these learned embeddings can be used to link the boxes of
the same object. BOTT can be used for both online and offline tracking modes
seamlessly. Its simplicity enables us to significantly reduce engineering
efforts required by traditional Kalman Filtering based methods. Experiments
show BOTT achieves competitive performance on two largest 3D MOT benchmarks:
69.9 and 66.7 AMOTA on nuScenes validation and test splits, respectively, 56.45
and 59.57 MOTA L2 on Waymo Open Dataset validation and test splits,
respectively. This work suggests that tracking 3D objects by learning features
directly from 3D boxes using transformers is a simple yet effective way.</description>
      <guid isPermaLink="false">2308.08753v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Trainable Pointwise Decoder Module for Point Cloud Segmentation</title>
      <link>http://arxiv.org/abs/2408.01548v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  No comments&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 点云分割（Point Cloud Segmentation, PCS）旨在对每个点进行预测，使机器人和自动驾驶汽车能够理解环境。&lt;br&gt;&lt;h4&gt;2. 范围图的优势&lt;/h4&gt;   - 范围图是大规模户外点云的密集表示，基于图像构建的分割模型通常执行效率较高。&lt;br&gt;&lt;h4&gt;3. 问题描述&lt;/h4&gt;   - 将点云投影到范围图上不可避免地会丢失点，因每个图像坐标仅保留一个点，尽管多个点投影到同一位置。&lt;br&gt;   - 难以为被丢弃的点分配正确的预测，尤其是当这些点属于与保留点不同的类别时。&lt;br&gt;&lt;h4&gt;4. 现有方法的局限性&lt;/h4&gt;   - 现有后处理方法（如K近邻搜索和核点卷积KPConv）无法以端到端的方式与模型训练，且对不同密度的户外点云处理效果不佳，导致模型性能亚优。&lt;br&gt;&lt;h4&gt;5. 提出的新方法&lt;/h4&gt;   - 提出可训练的逐点解码模块（Pointwise Decoder Module, PDM）作为后处理方法，聚合邻域的加权特征，然后为查询点做最终预测。&lt;br&gt;&lt;h4&gt;6. 数据增强策略&lt;/h4&gt;   - 引入虚拟范围图引导的复制-旋转-粘贴（VRCrop）策略进行数据增强。&lt;br&gt;   - VRCrop限制点的总数，消除增强点云中的不良伪影。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 使用PDM和VRCrop后，现有基于范围图的分割模型在SemanticKITTI、SemanticPOSS和nuScenes数据集上表现优于其对手。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud segmentation (PCS) aims to make per-point predictions and enables
robots and autonomous driving cars to understand the environment. The range
image is a dense representation of a large-scale outdoor point cloud, and
segmentation models built upon the image commonly execute efficiently. However,
the projection of the point cloud onto the range image inevitably leads to
dropping points because, at each image coordinate, only one point is kept
despite multiple points being projected onto the same location. More
importantly, it is challenging to assign correct predictions to the dropped
points that belong to the classes different from the kept point class. Besides,
existing post-processing methods, such as K-nearest neighbor (KNN) search and
kernel point convolution (KPConv), cannot be trained with the models in an
end-to-end manner or cannot process varying-density outdoor point clouds well,
thereby enabling the models to achieve sub-optimal performance. To alleviate
this problem, we propose a trainable pointwise decoder module (PDM) as the
post-processing approach, which gathers weighted features from the neighbors
and then makes the final prediction for the query point. In addition, we
introduce a virtual range image-guided copy-rotate-paste (VRCrop) strategy in
data augmentation. VRCrop constrains the total number of points and eliminates
undesirable artifacts in the augmented point cloud. With PDM and VRCrop,
existing range image-based segmentation models consistently perform better than
their counterparts on the SemanticKITTI, SemanticPOSS, and nuScenes datasets.</description>
      <guid isPermaLink="false">2408.01548v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>A Property Encoder for Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2409.11554v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  conference paper&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 图机器学习，特别是图神经网络，依赖于节点特征。&lt;br&gt;   - 许多现实世界系统（如社交网络和生物网络）由于隐私问题、数据不完整或采集限制，常常缺乏节点特征。&lt;br&gt;&lt;h4&gt;2. 现有方法&lt;/h4&gt;   - 研究人员通常使用结构和位置编码来构建节点特征。&lt;br&gt;   - 这些特征的长度依赖于被编码属性中的最大值（如最高节点度），在一些应用（如无尺度网络）中可能非常大。&lt;br&gt;&lt;h4&gt;3. 编码方案的局限性&lt;/h4&gt;   - 现有编码方案通常仅适用于分类数据，无法编码返回其他类型值的度量。&lt;br&gt;&lt;h4&gt;4. 提出的新方法&lt;/h4&gt;   - 本文介绍了一种新颖的通用编码器，称为PropEnc，能够从任何给定的图度量构建表达性的节点嵌入。&lt;br&gt;   - PropEnc结合直方图构建和反向索引编码，提供了一种灵活的节点特征初始化方法。&lt;br&gt;&lt;h4&gt;5. 灵活性与有效性&lt;/h4&gt;   - PropEnc支持在维度和输入类型上的灵活编码，展示了其在多种应用中的有效性。&lt;br&gt;   - 该方法可以在低维空间中编码度量，有效避免稀疏性问题，提高模型效率。&lt;br&gt;&lt;h4&gt;6. 功能与适应性&lt;/h4&gt;   - PropEnc能够构建的节点特征可以精确复制独热编码（one-hot encoding），或在不同设置下紧密逼近索引。&lt;br&gt;&lt;h4&gt;7. 实验验证&lt;/h4&gt;   - 在缺乏节点特征的多个社交网络上进行图分类的广泛评估支持了我们的假设。&lt;br&gt;   - 实证结果明确表明，PropEnc是从多种图度量构建节点特征的高效而有效的机制。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph machine learning, particularly using graph neural networks,
fundamentally relies on node features. Nevertheless, numerous real-world
systems, such as social and biological networks, often lack node features due
to various reasons, including privacy concerns, incomplete or missing data, and
limitations in data collection. In such scenarios, researchers typically resort
to methods like structural and positional encoding to construct node features.
However, the length of such features is contingent on the maximum value within
the property being encoded, for example, the highest node degree, which can be
exceedingly large in applications like scale-free networks. Furthermore, these
encoding schemes are limited to categorical data and might not be able to
encode metrics returning other type of values. In this paper, we introduce a
novel, universally applicable encoder, termed PropEnc, which constructs
expressive node embedding from any given graph metric. PropEnc leverages
histogram construction combined with reverse index encoding, offering a
flexible method for node features initialization. It supports flexible encoding
in terms of both dimensionality and type of input, demonstrating its
effectiveness across diverse applications. PropEnc allows encoding metrics in
low-dimensional space which effectively avoids the issue of sparsity and
enhances the efficiency of the models. We show that \emph{PropEnc} can
construct node features that either exactly replicate one-hot encoding or
closely approximate indices under various settings. Our extensive evaluations
in graph classification setting across multiple social networks that lack node
features support our hypothesis. The empirical results conclusively demonstrate
that PropEnc is both an efficient and effective mechanism for constructing node
features from diverse set of graph metrics.</description>
      <guid isPermaLink="false">2409.11554v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Few-Shot Learning of Compact Models via Task-Specific Meta Distillation</title>
      <link>http://arxiv.org/abs/2210.09922v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted by WACV'2023&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 本文探讨了小模型的少样本学习新问题。&lt;br&gt;   - 元学习是一种常见的少样本学习方法。&lt;br&gt;&lt;h4&gt;2. 现有假设的挑战&lt;/h4&gt;   - 以往的元学习研究通常假设元训练期间的模型架构与最终部署的模型架构相同。&lt;br&gt;   - 然而，最终部署时常需要模型较小，但小模型通常缺乏有效适应新任务的能力。&lt;br&gt;&lt;h4&gt;3. 元训练背景&lt;/h4&gt;   - 元训练通常在服务器上进行，能够使用大数据集和强大的计算资源。&lt;br&gt;&lt;h4&gt;4. 提出的方法&lt;/h4&gt;   - 提出任务特定的元蒸馏方法，同时学习两个模型：一个大型教师模型和一个小型学生模型。&lt;br&gt;   - 这两个模型在元训练期间共同学习。&lt;br&gt;&lt;h4&gt;5. 适应新任务&lt;/h4&gt;   - 在元测试阶段，首先将教师模型适应新任务，然后使用适应后的教师模型指导学生模型的适应。&lt;br&gt;   - 最终部署使用适应后的学生模型。&lt;br&gt;&lt;h4&gt;6. 实验验证&lt;/h4&gt;   - 在少样本图像分类中，采用模型无关元学习（MAML）来验证方法的有效性。&lt;br&gt;   - 结果显示，所提方法在多个基准数据集上优于其他替代方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2022-10-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We consider a new problem of few-shot learning of compact models.
Meta-learning is a popular approach for few-shot learning. Previous work in
meta-learning typically assumes that the model architecture during
meta-training is the same as the model architecture used for final deployment.
In this paper, we challenge this basic assumption. For final deployment, we
often need the model to be small. But small models usually do not have enough
capacity to effectively adapt to new tasks. In the mean time, we often have
access to the large dataset and extensive computing power during meta-training
since meta-training is typically performed on a server. In this paper, we
propose task-specific meta distillation that simultaneously learns two models
in meta-learning: a large teacher model and a small student model. These two
models are jointly learned during meta-training. Given a new task during
meta-testing, the teacher model is first adapted to this task, then the adapted
teacher model is used to guide the adaptation of the student model. The adapted
student model is used for final deployment. We demonstrate the effectiveness of
our approach in few-shot image classification using model-agnostic
meta-learning (MAML). Our proposed method outperforms other alternatives on
several benchmark datasets.</description>
      <guid isPermaLink="false">2210.09922v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>A Hybrid Meta-Learning and Multi-Armed Bandit Approach for Context-Specific Multi-Objective Recommendation Optimization</title>
      <link>http://arxiv.org/abs/2409.08752v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 在线市场的推荐系统面临平衡多重目标的挑战，以满足客户、供应商和平台等多个利益相关者的需求。&lt;br&gt;&lt;h4&gt;2. 方法提出&lt;/h4&gt;   - 本文介绍了Juggler-MAB，一种结合元学习与多臂赌博机（Multi-Armed Bandits, MAB）的混合方法，以解决现有多利益相关者推荐系统的局限性。&lt;br&gt;&lt;h4&gt;3. 框架扩展&lt;/h4&gt;   - Juggler-MAB扩展了Juggler框架，通过元学习预测效用和补偿调整的最佳权重，并结合MAB组件进行实时、上下文特定的调整。&lt;br&gt;&lt;h4&gt;4. 两阶段方法&lt;/h4&gt;   - 采用两阶段方法：首先由Juggler提供初始权重预测，然后通过MAB进行快速适应的调整，以应对用户行为和市场条件的迅速变化。&lt;br&gt;&lt;h4&gt;5. 上下文特征利用&lt;/h4&gt;   - 系统利用上下文特征（如设备类型和品牌）进行细粒度权重调整，基于特定细分市场进行优化。&lt;br&gt;&lt;h4&gt;6. 实验评估&lt;/h4&gt;   - 使用来自Expedia住宿预订平台的60万个搜索数据集开发仿真框架来评估方法。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 结果显示，Juggler-MAB在所有指标上均优于原始Juggler模型：&lt;br&gt;     - NDCG提升2.9%。&lt;br&gt;     - 后悔率降低13.7%。&lt;br&gt;     - 最佳臂选择率提高9.8%。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recommender systems in online marketplaces face the challenge of balancing
multiple objectives to satisfy various stakeholders, including customers,
providers, and the platform itself. This paper introduces Juggler-MAB, a hybrid
approach that combines meta-learning with Multi-Armed Bandits (MAB) to address
the limitations of existing multi-stakeholder recommendation systems. Our
method extends the Juggler framework, which uses meta-learning to predict
optimal weights for utility and compensation adjustments, by incorporating a
MAB component for real-time, context-specific refinements. We present a
two-stage approach where Juggler provides initial weight predictions, followed
by MAB-based adjustments that adapt to rapid changes in user behavior and
market conditions. Our system leverages contextual features such as device type
and brand to make fine-grained weight adjustments based on specific segments.
To evaluate our approach, we developed a simulation framework using a dataset
of 0.6 million searches from Expedia's lodging booking platform. Results show
that Juggler-MAB outperforms the original Juggler model across all metrics,
with NDCG improvements of 2.9%, a 13.7% reduction in regret, and a 9.8%
improvement in best arm selection rate.</description>
      <guid isPermaLink="false">2409.08752v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Emotion Recognition with Vision-language Prompting and Modality Dropout</title>
      <link>http://arxiv.org/abs/2409.07078v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 本文提出了解决方案，针对第二届多模态情感识别挑战赛第一轨道（MER2024-SEMI）。&lt;br&gt;&lt;h4&gt;2. 目标&lt;/h4&gt;   - 提高情感识别的准确性和泛化性能。&lt;br&gt;&lt;h4&gt;3. 主要方法&lt;/h4&gt;   - **EmoVCLIP模型**：&lt;br&gt;     - 基于CLIP模型进行微调，采用视觉-语言提示学习，专为视频情感识别任务设计。&lt;br&gt;     - 通过提示学习，EmoVCLIP提升了预训练CLIP在情感视频上的表现。&lt;br&gt;&lt;h4&gt;4. 多模态融合问题&lt;/h4&gt;   - 为解决多模态融合中的模态依赖问题，采用模态丢弃（modality dropout）以实现鲁棒的信息融合。&lt;br&gt;&lt;h4&gt;5. 情感信息提取&lt;/h4&gt;   - 建议使用GPT-4作为Baichuan的提示，以帮助更好地提取情感信息。&lt;br&gt;&lt;h4&gt;6. 自我训练策略&lt;/h4&gt;   - 利用未标记的视频，通过模型生成高置信度的伪标签，并将其纳入训练集，以增强模型训练。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 实验结果表明，模型在MER2024-SEMI轨道中排名第一，在测试集上达到90.15%的准确率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we present our solution for the Second Multimodal Emotion
Recognition Challenge Track 1(MER2024-SEMI). To enhance the accuracy and
generalization performance of emotion recognition, we propose several methods
for Multimodal Emotion Recognition. Firstly, we introduce EmoVCLIP, a model
fine-tuned based on CLIP using vision-language prompt learning, designed for
video-based emotion recognition tasks. By leveraging prompt learning on CLIP,
EmoVCLIP improves the performance of pre-trained CLIP on emotional videos.
Additionally, to address the issue of modality dependence in multimodal fusion,
we employ modality dropout for robust information fusion. Furthermore, to aid
Baichuan in better extracting emotional information, we suggest using GPT-4 as
the prompt for Baichuan. Lastly, we utilize a self-training strategy to
leverage unlabeled videos. In this process, we use unlabeled videos with
high-confidence pseudo-labels generated by our model and incorporate them into
the training set. Experimental results demonstrate that our model ranks 1st in
the MER2024-SEMI track, achieving an accuracy of 90.15% on the test set.</description>
      <guid isPermaLink="false">2409.07078v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Evaluating and Improving the Robustness of LiDAR-based Localization and Mapping</title>
      <link>http://arxiv.org/abs/2409.10824v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - LiDAR是常用的传感器，用于同时定位与地图构建（SLAM）和基于地图的全局定位。&lt;br&gt;   - SLAM和基于地图的定位对自主系统的独立操作至关重要，尤其在GNSS信号不可用或不可靠时。&lt;br&gt;&lt;h4&gt;2. 现有成就&lt;/h4&gt;   - 现有的最先进（SOTA）LiDAR SLAM系统可实现0.5%的误差（即每100米误差0.5米）。&lt;br&gt;   - 基于地图的定位可达到厘米级的全局定位精度。&lt;br&gt;&lt;h4&gt;3. 研究问题&lt;/h4&gt;   - 目前尚不清楚这些系统在不同常见LiDAR数据损坏情况下的鲁棒性。&lt;br&gt;&lt;h4&gt;4. 研究方法&lt;/h4&gt;   - 对五个最先进的LiDAR定位系统进行了广泛评估，测试了18种常见场景级LiDAR点云数据（PCD）损坏。&lt;br&gt;&lt;h4&gt;5. 主要发现&lt;/h4&gt;   - **鲁棒性差异**：LiDAR定位的鲁棒性因类别而异。&lt;br&gt;   - **SLAM方法**：&lt;br&gt;     - 手工设计的方法一般对大多数损坏类型具有鲁棒性，但对特定损坏极为脆弱（误差高达80%）。&lt;br&gt;     - 学习基于的方法对大多数损坏类型较为脆弱。&lt;br&gt;   - **基于地图的全局定位**：最先进的方法对所有应用的损坏表现出抗性。&lt;br&gt;&lt;h4&gt;6. 去噪与重训练效果&lt;/h4&gt;   - 简单的双边滤波去噪能有效消除噪声引起的损坏，但对密度基础的损坏无效。&lt;br&gt;   - 重训练对抗学习基于SLAM的所有类型损坏更为有效。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/boyang9602/LiDARLocRobustness&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LiDAR is one of the most commonly adopted sensors for simultaneous
localization and mapping (SLAM) and map-based global localization. SLAM and
map-based localization are crucial for the independent operation of autonomous
systems, especially when external signals such as GNSS are unavailable or
unreliable. While state-of-the-art (SOTA) LiDAR SLAM systems could achieve 0.5%
(i.e., 0.5m per 100m) of errors and map-based localization could achieve
centimeter-level global localization, it is still unclear how robust they are
under various common LiDAR data corruptions. In this work, we extensively
evaluated five SOTA LiDAR-based localization systems under 18 common
scene-level LiDAR point cloud data (PCD) corruptions. We found that the
robustness of LiDAR-based localization varies significantly depending on the
category. For SLAM, hand-crafted methods are in general robust against most
types of corruption, while being extremely vulnerable (up to +80% errors) to a
specific corruption. Learning-based methods are vulnerable to most types of
corruptions. For map-based global localization, we found that the SOTA is
resistant to all applied corruptions. Finally, we found that simple Bilateral
Filter denoising effectively eliminates noise-based corruption but is not
helpful in density-based corruption. Re-training is more effective in defending
learning-based SLAM against all types of corruption.</description>
      <guid isPermaLink="false">2409.10824v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Disentangling Visual Priors: Unsupervised Learning of Scene Interpretations with Compositional Autoencoder</title>
      <link>http://arxiv.org/abs/2409.09716v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 现代深度学习架构缺乏有效捕捉和处理基本视觉概念（如物体、形状、几何变换及其他高层结构）的方法。&lt;br&gt;&lt;h4&gt;2. 方法提出&lt;/h4&gt;   - 提出了一个神经符号架构，使用特定领域的语言来捕捉图像形成的选择性先验，包括物体形状、外观、分类和几何变换。&lt;br&gt;&lt;h4&gt;3. 模板程序&lt;/h4&gt;   - 在该语言中表达模板程序，并通过卷积神经网络提取的特征学习其参数化。&lt;br&gt;&lt;h4&gt;4. 执行与评估&lt;/h4&gt;   - 执行参数化程序生成几何原语，渲染并评估其与场景内容的一致性，通过自关联和梯度训练进行优化。&lt;br&gt;&lt;h4&gt;5. 实验对比&lt;/h4&gt;   - 将该方法与基准方法在合成基准上进行比较，展示其在以下方面的能力：&lt;br&gt;     - 解构图像形成过程的特定方面。&lt;br&gt;     - 从小数据中学习。&lt;br&gt;     - 在噪声存在时进行推理修正。&lt;br&gt;     - 实现超出样本的泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1007/978-3-031-71167-1_13&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Contemporary deep learning architectures lack principled means for capturing
and handling fundamental visual concepts, like objects, shapes, geometric
transforms, and other higher-level structures. We propose a neurosymbolic
architecture that uses a domain-specific language to capture selected priors of
image formation, including object shape, appearance, categorization, and
geometric transforms. We express template programs in that language and learn
their parameterization with features extracted from the scene by a
convolutional neural network. When executed, the parameterized program produces
geometric primitives which are rendered and assessed for correspondence with
the scene content and trained via auto-association with gradient. We confront
our approach with a baseline method on a synthetic benchmark and demonstrate
its capacity to disentangle selected aspects of the image formation process,
learn from small data, correct inference in the presence of noise, and
out-of-sample generalization.</description>
      <guid isPermaLink="false">2409.09716v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>On the Generalizability of Foundation Models for Crop Type Mapping</title>
      <link>http://arxiv.org/abs/2409.09451v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 基础模型通过自监督和弱监督学习进行预训练，显示出在语言理解、文本生成和图像识别等下游任务中的强大迁移学习能力。&lt;br&gt;&lt;h4&gt;2. 地球观测领域的应用&lt;/h4&gt;   - 最近，地球观测（EO）领域直接在多光谱卫星影像（如Sentinel-2）上预训练了若干基础模型，应用于精准农业、野火和干旱监测及自然灾害响应等。&lt;br&gt;&lt;h4&gt;3. 研究问题&lt;/h4&gt;   - 目前对这些模型在新地理位置的泛化能力研究较少，且存在地理偏见的潜在担忧，即在数据丰富的发达国家训练的模型在数据稀缺的发展中国家效果不佳。&lt;br&gt;&lt;h4&gt;4. 研究目标&lt;/h4&gt;   - 调查流行的EO基础模型在农业领域向新地理区域转移的能力，特别关注农作实践差异和类别不平衡带来的挑战。&lt;br&gt;&lt;h4&gt;5. 数据集选择&lt;/h4&gt;   - 选择六个跨五大洲的作物分类数据集，标准化数据集大小并协调类别，重点关注四种主要谷物：玉米、大豆、水稻和小麦。&lt;br&gt;&lt;h4&gt;6. 模型比较&lt;/h4&gt;   - 比较三种流行的基础模型，分别在SSL4EO-S12、SatlasPretrain和ImageNet上进行预训练，并使用分布内（ID）和分布外（OOD）评估。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 针对Sentinel-2专门设计的预训练权重（如SSL4EO-S12）在性能上优于通用的预训练权重（如ImageNet）。&lt;br&gt;   - 当只有10到100个ID训练样本时，使用OOD数据的预训练效果最为显著。&lt;br&gt;&lt;h4&gt;8. 应用前景&lt;/h4&gt;   - 在OOD和有限ID数据上进行迁移学习和预训练显示出良好的应用前景，因为许多发展中国家缺乏作物类型标签。&lt;br&gt;&lt;h4&gt;9. 开源资源&lt;/h4&gt;   - 所有协调的数据集和实验代码均为开源，可供下载。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models pre-trained using self-supervised and weakly-supervised
learning have shown powerful transfer learning capabilities on various
downstream tasks, including language understanding, text generation, and image
recognition. Recently, the Earth observation (EO) field has produced several
foundation models pre-trained directly on multispectral satellite imagery
(e.g., Sentinel-2) for applications like precision agriculture, wildfire and
drought monitoring, and natural disaster response. However, few studies have
investigated the ability of these models to generalize to new geographic
locations, and potential concerns of geospatial bias -- models trained on
data-rich developed countries not transferring well to data-scarce developing
countries -- remain. We investigate the ability of popular EO foundation models
to transfer to new geographic regions in the agricultural domain, where
differences in farming practices and class imbalance make transfer learning
particularly challenging. We first select six crop classification datasets
across five continents, normalizing for dataset size and harmonizing classes to
focus on four major cereal grains: maize, soybean, rice, and wheat. We then
compare three popular foundation models, pre-trained on SSL4EO-S12,
SatlasPretrain, and ImageNet, using in-distribution (ID) and
out-of-distribution (OOD) evaluation. Experiments show that pre-trained weights
designed explicitly for Sentinel-2, such as SSL4EO-S12, outperform general
pre-trained weights like ImageNet. Furthermore, the benefits of pre-training on
OOD data are the most significant when only 10--100 ID training samples are
used. Transfer learning and pre-training with OOD and limited ID data show
promising applications, as many developing regions have scarce crop type
labels. All harmonized datasets and experimental code are open-source and
available for download.</description>
      <guid isPermaLink="false">2409.09451v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>LoopSplat: Loop Closure by Registering 3D Gaussian Splats</title>
      <link>http://arxiv.org/abs/2408.10154v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://loopsplat.github.io/&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 基于3D高斯点云（3D Gaussian Splats, 3DGS）的同时定位与地图构建（SLAM）近年来在生成更准确、密集的3D场景地图方面显示出潜力。&lt;br&gt;&lt;h4&gt;2. 现有问题&lt;/h4&gt;   - 现有的3DGS方法未能有效解决场景的全局一致性问题，例如循环闭合和全局束调整。&lt;br&gt;&lt;h4&gt;3. 方法提出&lt;/h4&gt;   - 提出了LoopSplat方法，该方法以RGB-D图像为输入，使用3DGS子地图进行密集映射和帧到模型的跟踪。&lt;br&gt;&lt;h4&gt;4. 循环闭合机制&lt;/h4&gt;   - LoopSplat在线触发循环闭合，并通过3DGS注册直接计算子地图之间的相对循环边约束，提高了效率和准确性，相较于传统的全局到局部点云注册方法。&lt;br&gt;&lt;h4&gt;5. 全局一致性&lt;/h4&gt;   - 使用稳健的姿态图优化公式，将子地图刚性对齐，以实现全局一致性。&lt;br&gt;&lt;h4&gt;6. 实验评估&lt;/h4&gt;   - 在合成的Replica数据集以及真实世界的TUM-RGBD、ScanNet和ScanNet++数据集上进行评估，显示出在跟踪、映射和渲染方面与现有密集RGB-D SLAM方法相比具有竞争力或优越的性能。&lt;br&gt;&lt;h4&gt;7. 代码可用性&lt;/h4&gt;   - 相关代码可在 [loopsplat.github.io](http://loopsplat.github.io) 获取。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/GradientSpaces/LoopSplat&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Simultaneous Localization and Mapping (SLAM) based on 3D Gaussian Splats
(3DGS) has recently shown promise towards more accurate, dense 3D scene maps.
However, existing 3DGS-based methods fail to address the global consistency of
the scene via loop closure and/or global bundle adjustment. To this end, we
propose LoopSplat, which takes RGB-D images as input and performs dense mapping
with 3DGS submaps and frame-to-model tracking. LoopSplat triggers loop closure
online and computes relative loop edge constraints between submaps directly via
3DGS registration, leading to improvements in efficiency and accuracy over
traditional global-to-local point cloud registration. It uses a robust pose
graph optimization formulation and rigidly aligns the submaps to achieve global
consistency. Evaluation on the synthetic Replica and real-world TUM-RGBD,
ScanNet, and ScanNet++ datasets demonstrates competitive or superior tracking,
mapping, and rendering compared to existing methods for dense RGB-D SLAM. Code
is available at loopsplat.github.io.</description>
      <guid isPermaLink="false">2408.10154v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>T-CorresNet: Template Guided 3D Point Cloud Completion with Correspondence Pooling Query Generation Strategy</title>
      <link>http://arxiv.org/abs/2407.05008v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ECCV 2024&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 点云在自动驾驶和制造业等实际应用中广泛使用。&lt;br&gt;   - 由于视角限制、扫描仪分辨率和遮挡等原因，点云常常存在不完整的问题。&lt;br&gt;&lt;h4&gt;2. 研究目标&lt;/h4&gt;   - 针对点云的缺失部分进行预测，解决点云不完整的问题。&lt;br&gt;&lt;h4&gt;3. 方法提出&lt;/h4&gt;   - 提出了一个新颖的点云补全方法。&lt;br&gt;&lt;h4&gt;4. 方法步骤&lt;/h4&gt;   - **使用球形模板**：利用球形模板引导生成粗略完整的模板，并通过将其嵌入部分输入，调整模板以最佳匹配输入。&lt;br&gt;   - **动态查询生成**：通过对应池查询生成器（Corres-Pooling）来细化粗略模板，生成动态查询令牌，用于预测完整的点云代理。&lt;br&gt;&lt;h4&gt;5. 生成完整点云&lt;/h4&gt;   - 根据细化后的模板和预测的点代理，使用FoldingNet生成完整的点云，遵循从粗到细的范式。&lt;br&gt;&lt;h4&gt;6. 实验结果&lt;/h4&gt;   - 实验结果表明，T-CorresNet在多个基准测试中优于最先进的方法。&lt;br&gt;&lt;h4&gt;7. 代码可用性&lt;/h4&gt;   - 相关代码可在 [GitHub](https://github.com/df-boy/T-CorresNet) 获取。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-07-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/df-boy/t-corresnet&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point clouds are commonly used in various practical applications such as
autonomous driving and the manufacturing industry. However, these point clouds
often suffer from incompleteness due to limited perspectives, scanner
resolution and occlusion. Therefore the prediction of missing parts performs a
crucial task. In this paper, we propose a novel method for point cloud
completion. We utilize a spherical template to guide the generation of the
coarse complete template and generate the dynamic query tokens through a
correspondence pooling (Corres-Pooling) query generator. Specifically, we first
generate the coarse complete template by embedding a Gaussian spherical
template into the partial input and transforming the template to best match the
input. Then we use the Corres-Pooling query generator to refine the coarse
template and generate dynamic query tokens which could be used to predict the
complete point proxies. Finally, we generate the complete point cloud with a
FoldingNet following the coarse-to-fine paradigm, according to the fine
template and the predicted point proxies. Experimental results demonstrate that
our T-CorresNet outperforms the state-of-the-art methods on several benchmarks.
Our Codes are available at https://github.com/df-boy/T-CorresNet.</description>
      <guid isPermaLink="false">2407.05008v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Delving into Motion-Aware Matching for Monocular 3D Object Tracking</title>
      <link>http://arxiv.org/abs/2308.11607v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICCV 2023. Code is available at
  https://github.com/kuanchihhuang/MoMA-M3T&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 近年来，单目3D物体检测的进步为基于低成本相机传感器的3D多物体跟踪任务提供了支持。&lt;br&gt;&lt;h4&gt;2. 研究发现&lt;/h4&gt;   - 物体在不同时间帧的运动线索对3D多物体跟踪至关重要，而现有的单目方法对此关注较少。&lt;br&gt;&lt;h4&gt;3. 框架提出&lt;/h4&gt;   - 提出了一个运动感知框架MoMA-M3T，专注于单目3D多物体跟踪（MOT）。&lt;br&gt;&lt;h4&gt;4. 组件构成&lt;/h4&gt;   - 框架主要由三个运动感知组件组成：&lt;br&gt;     - **运动特征表示**：将物体相对于所有目标轨迹在特征空间中的可能移动表示为运动特征。&lt;br&gt;     - **时空建模**：通过运动变换器以时空视角建模历史目标轨迹。&lt;br&gt;     - **运动感知匹配模块**：将历史目标轨迹与当前观测关联，生成最终跟踪结果。&lt;br&gt;&lt;h4&gt;5. 实验验证&lt;/h4&gt;   - 在nuScenes和KITTI数据集上进行了广泛实验，表明MoMA-M3T在性能上与最先进的方法具有竞争力。&lt;br&gt;&lt;h4&gt;6. 灵活性&lt;/h4&gt;   - 所提出的跟踪器具有灵活性，可以轻松集成到现有的基于图像的3D物体检测器中，而无需重新训练。&lt;br&gt;&lt;h4&gt;7. 代码与模型&lt;/h4&gt;   - 相关代码和模型可在 [GitHub](https://github.com/kuanchihhuang/MoMA-M3T) 获取。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2023-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/kuanchihhuang/moma-m3t&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances of monocular 3D object detection facilitate the 3D
multi-object tracking task based on low-cost camera sensors. In this paper, we
find that the motion cue of objects along different time frames is critical in
3D multi-object tracking, which is less explored in existing monocular-based
approaches. In this paper, we propose a motion-aware framework for monocular 3D
MOT. To this end, we propose MoMA-M3T, a framework that mainly consists of
three motion-aware components. First, we represent the possible movement of an
object related to all object tracklets in the feature space as its motion
features. Then, we further model the historical object tracklet along the time
frame in a spatial-temporal perspective via a motion transformer. Finally, we
propose a motion-aware matching module to associate historical object tracklets
and current observations as final tracking results. We conduct extensive
experiments on the nuScenes and KITTI datasets to demonstrate that our MoMA-M3T
achieves competitive performance against state-of-the-art methods. Moreover,
the proposed tracker is flexible and can be easily plugged into existing
image-based 3D object detectors without re-training. Code and models are
available at https://github.com/kuanchihhuang/MoMA-M3T.</description>
      <guid isPermaLink="false">2308.11607v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Context-Conditioned Spatio-Temporal Predictive Learning for Reliable V2V Channel Prediction</title>
      <link>http://arxiv.org/abs/2409.09978v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 实现可靠的多维车与车（V2V）信道状态信息（CSI）预测对优化依赖瞬时CSI的下游任务至关重要。&lt;br&gt;&lt;h4&gt;2. 研究目标&lt;/h4&gt;   - 本文扩展了传统预测方法，专注于四维（4D）CSI，包括时间、带宽和天线（发射和接收）空间的预测。&lt;br&gt;&lt;h4&gt;3. 重要性&lt;/h4&gt;   - 这样全面的框架对于应对智能交通系统中动态的移动环境至关重要，需要捕捉不同领域中的时间和空间依赖关系。&lt;br&gt;&lt;h4&gt;4. 方法创新&lt;/h4&gt;   - 提出了新的上下文条件时空预测学习方法，利用因果卷积长短期记忆网络（CA-ConvLSTM）有效捕捉4D CSI数据中的依赖关系。&lt;br&gt;&lt;h4&gt;5. 增强机制&lt;/h4&gt;   - 引入上下文条件注意机制，提高时空记忆更新的效率。&lt;br&gt;&lt;h4&gt;6. 自适应学习方案&lt;/h4&gt;   - 提出了一种针对递归网络的自适应元学习方案，以减轻累积预测误差的问题。&lt;br&gt;&lt;h4&gt;7. 实验验证&lt;/h4&gt;   - 在三种不同的几何配置和移动场景中进行了实证研究，验证了所提方法。&lt;br&gt;&lt;h4&gt;8. 性能表现&lt;/h4&gt;   - 结果表明，所提方法在各种几何条件下超越了现有最先进的预测模型，表现优越。&lt;br&gt;&lt;h4&gt;9. 鲁棒性与适应性&lt;/h4&gt;   - 显示元学习框架显著提升了基于递归的预测模型在高度挑战的跨几何设置中的性能，强调了其鲁棒性和适应性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Achieving reliable multidimensional Vehicle-to-Vehicle (V2V) channel state
information (CSI) prediction is both challenging and crucial for optimizing
downstream tasks that depend on instantaneous CSI. This work extends
traditional prediction approaches by focusing on four-dimensional (4D) CSI,
which includes predictions over time, bandwidth, and antenna (TX and RX) space.
Such a comprehensive framework is essential for addressing the dynamic nature
of mobility environments within intelligent transportation systems,
necessitating the capture of both temporal and spatial dependencies across
diverse domains. To address this complexity, we propose a novel
context-conditioned spatiotemporal predictive learning method. This method
leverages causal convolutional long short-term memory (CA-ConvLSTM) to
effectively capture dependencies within 4D CSI data, and incorporates
context-conditioned attention mechanisms to enhance the efficiency of
spatiotemporal memory updates. Additionally, we introduce an adaptive
meta-learning scheme tailored for recurrent networks to mitigate the issue of
accumulative prediction errors. We validate the proposed method through
empirical studies conducted across three different geometric configurations and
mobility scenarios. Our results demonstrate that the proposed approach
outperforms existing state-of-the-art predictive models, achieving superior
performance across various geometries. Moreover, we show that the meta-learning
framework significantly enhances the performance of recurrent-based predictive
models in highly challenging cross-geometry settings, thus highlighting its
robustness and adaptability.</description>
      <guid isPermaLink="false">2409.09978v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Non-Rigid Shape Registration via Deep Functional Maps Prior</title>
      <link>http://arxiv.org/abs/2311.04494v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS2023&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究框架&lt;/h4&gt;   - 提出了一个基于学习的非刚性形状配准框架，无需对应监督。&lt;br&gt;&lt;h4&gt;2. 传统方法的局限&lt;/h4&gt;   - 传统形状配准技术依赖于外部接近性诱导的对应关系，面对大规模内在变形时容易失败。&lt;br&gt;&lt;h4&gt;3. 谱映射方法的优势&lt;/h4&gt;   - 谱映射方法通过将形状嵌入到几何或学习的高维空间中，解决了形状对齐的挑战。&lt;br&gt;&lt;h4&gt;4. 潜在问题&lt;/h4&gt;   - 由于依赖于抽象的非线性嵌入方案，谱映射方法对扰动或不相关输入易受影响。&lt;br&gt;&lt;h4&gt;5. 框架创新&lt;/h4&gt;   - 本文的框架结合了两者的优点，通过高维嵌入学习的对应关系，引导源网格朝向目标点云变形。&lt;br&gt;&lt;h4&gt;6. 动态更新&lt;/h4&gt;   - 对应关系根据中间配准动态更新，并通过一致性先验进行过滤，从而增强整体管道的鲁棒性。&lt;br&gt;&lt;h4&gt;7. 训练与通用性&lt;/h4&gt;   - 为减轻对外部对齐输入的需求，训练了一个方向回归器，使用一组与DFM训练形状无关的对齐合成形状。&lt;br&gt;&lt;h4&gt;8. 实验结果&lt;/h4&gt;   - 实验表明，使用少量训练形状（有限变异性），该管道在非刚性点云匹配的多个基准上达到最先进的结果，并在未见的挑战性形状对之间提供高质量的对应关系。&lt;br&gt;&lt;h4&gt;9. 适用范围&lt;/h4&gt;   - 该方法在面对显著的外部和内部变形时表现优越，而传统配准方法和内在方法在此情况下通常不起作用。&lt;br&gt;&lt;h4&gt;10. 代码链接&lt;/h4&gt;    - 相关代码可在 [GitHub](https://github.com/rqhuang88/DFR) 获取。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2023-11-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we propose a learning-based framework for non-rigid shape
registration without correspondence supervision. Traditional shape registration
techniques typically rely on correspondences induced by extrinsic proximity,
therefore can fail in the presence of large intrinsic deformations. Spectral
mapping methods overcome this challenge by embedding shapes into, geometric or
learned, high-dimensional spaces, where shapes are easier to align. However,
due to the dependency on abstract, non-linear embedding schemes, the latter can
be vulnerable with respect to perturbed or alien input. In light of this, our
framework takes the best of both worlds. Namely, we deform source mesh towards
the target point cloud, guided by correspondences induced by high-dimensional
embeddings learned from deep functional maps (DFM). In particular, the
correspondences are dynamically updated according to the intermediate
registrations and filtered by consistency prior, which prominently robustify
the overall pipeline. Moreover, in order to alleviate the requirement of
extrinsically aligned input, we train an orientation regressor on a set of
aligned synthetic shapes independent of the training shapes for DFM. Empirical
results show that, with as few as dozens of training shapes of limited
variability, our pipeline achieves state-of-the-art results on several
benchmarks of non-rigid point cloud matching, but also delivers high-quality
correspondences between unseen challenging shape pairs that undergo both
significant extrinsic and intrinsic deformations, in which case neither
traditional registration methods nor intrinsic methods work. The code is
available at https://github.com/rqhuang88/DFR.</description>
      <guid isPermaLink="false">2311.04494v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>What to align in multimodal contrastive learning?</title>
      <link>http://arxiv.org/abs/2409.07402v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 人类通过多感官整合感知世界，结合不同模态的信息以调整行为。&lt;br&gt;&lt;h4&gt;2. 对比学习的优势&lt;/h4&gt;   - 对比学习为多模态自监督学习提供了有效解决方案，通过将每种模态视为同一实体的不同视角，学习在共享表示空间中对齐不同模态的特征。&lt;br&gt;&lt;h4&gt;3. 现有方法的局限&lt;/h4&gt;   - 传统方法仅学习模态之间的共享或冗余信息，无法充分捕捉多模态交互的其他形式。&lt;br&gt;&lt;h4&gt;4. 新方法提出&lt;/h4&gt;   - 本文提出了CoMM（对比多模态学习策略），允许在单一多模态空间中进行模态之间的沟通。&lt;br&gt;&lt;h4&gt;5. 方法创新&lt;/h4&gt;   - CoMM通过最大化这些多模态特征的增强版本之间的互信息来对齐多模态表示，而非强加交叉或内部模态约束。&lt;br&gt;&lt;h4&gt;6. 理论分析&lt;/h4&gt;   - 理论分析表明，这种方法自然产生共享、协同和独特的信息项，从而估计超越冗余的多模态交互。&lt;br&gt;&lt;h4&gt;7. 实验验证&lt;/h4&gt;   - 在受控环境中测试CoMM，证明其有效捕捉模态之间的冗余、独特和协同信息。&lt;br&gt;   - 在一系列真实世界的设置中，CoMM学习复杂的多模态交互，并在六个多模态基准测试中取得了最先进的结果。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humans perceive the world through multisensory integration, blending the
information of different modalities to adapt their behavior. Contrastive
learning offers an appealing solution for multimodal self-supervised learning.
Indeed, by considering each modality as a different view of the same entity, it
learns to align features of different modalities in a shared representation
space. However, this approach is intrinsically limited as it only learns shared
or redundant information between modalities, while multimodal interactions can
arise in other ways. In this work, we introduce CoMM, a Contrastive MultiModal
learning strategy that enables the communication between modalities in a single
multimodal space. Instead of imposing cross- or intra- modality constraints, we
propose to align multimodal representations by maximizing the mutual
information between augmented versions of these multimodal features. Our
theoretical analysis shows that shared, synergistic and unique terms of
information naturally emerge from this formulation, allowing us to estimate
multimodal interactions beyond redundancy. We test CoMM both in a controlled
and in a series of real-world settings: in the former, we demonstrate that CoMM
effectively captures redundant, unique and synergistic information between
modalities. In the latter, CoMM learns complex multimodal interactions and
achieves state-of-the-art results on the six multimodal benchmarks.</description>
      <guid isPermaLink="false">2409.07402v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Learning in Memristor-based Neuromorphic Systems</title>
      <link>http://arxiv.org/abs/2409.10887v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in SiPS 2024&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 脉冲神经网络是第三代人工神经网络，作为神经元模型的重要家族，解决了现代反向传播深度网络面临的一些关键限制。&lt;br&gt;&lt;h4&gt;2. 主要问题&lt;/h4&gt;   - 现代深度网络存在高能耗和被广泛批评的生物不合理性等问题。&lt;br&gt;&lt;h4&gt;3. 研究目标&lt;/h4&gt;   - 本文设计并研究了对比信号依赖可塑性（CSDP）的概念验证实例，这是一种基于前向信号的神经形态学习方法，避免了反向传播。&lt;br&gt;&lt;h4&gt;4. 实验结果&lt;/h4&gt;   - 实验模拟表明，CSDP的硬件实现能够学习简单的逻辑函数，而无需复杂的梯度计算。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spiking neural networks, the third generation of artificial neural networks,
have become an important family of neuron-based models that sidestep many of
the key limitations facing modern-day backpropagation-trained deep networks,
including their high energy inefficiency and long-criticized biological
implausibility. In this work, we design and investigate a proof-of-concept
instantiation of contrastive-signal-dependent plasticity (CSDP), a neuromorphic
form of forward-forward-based, backpropagation-free learning. Our experimental
simulations demonstrate that a hardware implementation of CSDP is capable of
learning simple logic functions without the need to resort to complex gradient
calculations.</description>
      <guid isPermaLink="false">2409.10887v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Machine Learning for Analyzing Atomic Force Microscopy (AFM) Images Generated from Polymer Blends</title>
      <link>http://arxiv.org/abs/2409.11438v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  39 pages, 13 figures, 4 tables&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究目的&lt;/h4&gt;   - 本文提出了一种新的机器学习工作流程，采用无监督学习技术识别聚合物薄膜的原子力显微镜（AFM）图像中的不同领域。&lt;br&gt;&lt;h4&gt;2. 工作流程目标&lt;/h4&gt;   - 目标是自动识别两种聚合物领域的空间位置，尽量减少人工干预，并计算领域大小分布，以帮助确定材料的相分离状态（宏相或微相有序或无序领域）。&lt;br&gt;&lt;h4&gt;3. 文献回顾&lt;/h4&gt;   - 简要回顾了在其他领域（如计算机视觉和信号处理）中使用的现有方法，这些方法可应用于聚合物科学和工程中的相关任务。&lt;br&gt;&lt;h4&gt;4. 方法测试&lt;/h4&gt;   - 在AFM图像数据集上测试了计算机视觉和信号处理方法，评估每种方法在领域分割任务中的优缺点。&lt;br&gt;&lt;h4&gt;5. 最佳方法发现&lt;/h4&gt;   - 对于领域分割任务，发现使用离散傅里叶变换（DFT）或离散余弦变换（DCT）结合方差统计作为特征的工作流程效果最佳。&lt;br&gt;&lt;h4&gt;6. 深度学习方法表现&lt;/h4&gt;   - 流行的ResNet50深度学习方法在AFM图像的领域分割任务中的表现相对较差，未能超越基于DFT和DCT的工作流程。&lt;br&gt;&lt;h4&gt;7. 领域大小分布计算&lt;/h4&gt;   - 对144幅输入AFM图像，使用现有的Porespy Python包从DFT工作流程的输出计算领域大小分布。&lt;br&gt;&lt;h4&gt;8. 共享信息与代码&lt;/h4&gt;   - 本文中共享的信息和开源代码可为聚合物和软材料领域的研究人员提供指导，帮助他们进行AFM图像的自动分析，适用于具有晶体或非晶领域、领域之间界面锐利或粗糙、或微相和宏相分离的样本。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper we present a new machine learning workflow with unsupervised
learning techniques to identify domains within atomic force microscopy images
obtained from polymer films. The goal of the workflow is to identify the
spatial location of the two types of polymer domains with little to no manual
intervention and calculate the domain size distributions which in turn can help
qualify the phase separated state of the material as macrophase or microphase
ordered or disordered domains. We briefly review existing approaches used in
other fields, computer vision and signal processing that can be applicable for
the above tasks that happen frequently in the field of polymer science and
engineering. We then test these approaches from computer vision and signal
processing on the AFM image dataset to identify the strengths and limitations
of each of these approaches for our first task. For our first domain
segmentation task, we found that the workflow using discrete Fourier transform
or discrete cosine transform with variance statistics as the feature works the
best. The popular ResNet50 deep learning approach from computer vision field
exhibited relatively poorer performance in the domain segmentation task for our
AFM images as compared to the DFT and DCT based workflows. For the second task,
for each of 144 input AFM images, we then used an existing porespy python
package to calculate the domain size distribution from the output of that image
from DFT based workflow. The information and open source codes we share in this
paper can serve as a guide for researchers in the polymer and soft materials
fields who need ML modeling and workflows for automated analyses of AFM images
from polymer samples that may have crystalline or amorphous domains, sharp or
rough interfaces between domains, or micro or macrophase separated domains.</description>
      <guid isPermaLink="false">2409.11438v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>LVBA: LiDAR-Visual Bundle Adjustment for RGB Point Cloud Mapping</title>
      <link>http://arxiv.org/abs/2409.10868v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 具有准确颜色的点云地图在机器人技术和映射应用中至关重要。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限&lt;/h4&gt;   - 目前的RGB着色地图生成方法主要依赖于基于滤波的估计或滑动窗口优化的实时定位，可能缺乏准确性和全局一致性。&lt;br&gt;&lt;h4&gt;3. 新方法介绍&lt;/h4&gt;   - 本文提出了一种新颖的全局LiDAR-视觉束调整（BA）方法，称为LVBA，旨在提高RGB点云映射的质量，超越现有基准。&lt;br&gt;&lt;h4&gt;4. 方法流程&lt;/h4&gt;   - LVBA首先通过全局LiDAR束调整优化LiDAR位姿，然后结合来自LiDAR点云的平面特征进行摄影视觉束调整，以优化相机位姿。&lt;br&gt;&lt;h4&gt;5. 创新算法&lt;/h4&gt;   - 为了解决构建优化问题时地图点遮挡的挑战，LVBA实现了一种新颖的LiDAR辅助全局可见性算法。&lt;br&gt;&lt;h4&gt;6. 实验评估&lt;/h4&gt;   - 为评估LVBA的有效性，进行了广泛实验，将其映射质量与现有最先进的基准（如R$^3$LIVE和FAST-LIVO）进行比较。&lt;br&gt;&lt;h4&gt;7. 研究结果&lt;/h4&gt;   - 实验结果证明，LVBA能够高效重建高保真、准确的RGB点云地图，优于同行方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud maps with accurate color are crucial in robotics and mapping
applications. Existing approaches for producing RGB-colorized maps are
primarily based on real-time localization using filter-based estimation or
sliding window optimization, which may lack accuracy and global consistency. In
this work, we introduce a novel global LiDAR-Visual bundle adjustment (BA)
named LVBA to improve the quality of RGB point cloud mapping beyond existing
baselines. LVBA first optimizes LiDAR poses via a global LiDAR BA, followed by
a photometric visual BA incorporating planar features from the LiDAR point
cloud for camera pose optimization. Additionally, to address the challenge of
map point occlusions in constructing optimization problems, we implement a
novel LiDAR-assisted global visibility algorithm in LVBA. To evaluate the
effectiveness of LVBA, we conducted extensive experiments by comparing its
mapping quality against existing state-of-the-art baselines (i.e., R$^3$LIVE
and FAST-LIVO). Our results prove that LVBA can proficiently reconstruct
high-fidelity, accurate RGB point cloud maps, outperforming its counterparts.</description>
      <guid isPermaLink="false">2409.10868v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>A Comparative Study of Open Source Computer Vision Models for Application on Small Data: The Case of CFRP Tape Laying</title>
      <link>http://arxiv.org/abs/2409.10104v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 在工业制造领域，人工智能（AI）正发挥越来越重要的作用，从自动化现有流程到帮助开发新材料和技术。&lt;br&gt;&lt;h4&gt;2. 主要挑战&lt;/h4&gt;   - 在较小的实验性流程中，训练数据的可用性有限，这对在小数据环境中训练AI模型提出了挑战。&lt;br&gt;&lt;h4&gt;3. 研究目的&lt;/h4&gt;   - 本文探讨了迁移学习的潜力，特别是调查开发功能性AI模型所需的最小数据量。&lt;br&gt;&lt;h4&gt;4. 应用案例&lt;/h4&gt;   - 研究以航空制造中使用光学传感器的碳纤维增强聚合物（CFRP）带铺设的质量控制为例。&lt;br&gt;&lt;h4&gt;5. 研究方法&lt;/h4&gt;   - 分析不同开源计算机视觉模型在训练数据持续减少情况下的表现。&lt;br&gt;&lt;h4&gt;6. 研究结果&lt;/h4&gt;   - 结果表明，成功训练AI模型所需的数据量可以大幅减少，并且使用较小的模型不一定会导致性能下降。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the realm of industrial manufacturing, Artificial Intelligence (AI) is
playing an increasing role, from automating existing processes to aiding in the
development of new materials and techniques. However, a significant challenge
arises in smaller, experimental processes characterized by limited training
data availability, questioning the possibility to train AI models in such small
data contexts. In this work, we explore the potential of Transfer Learning to
address this challenge, specifically investigating the minimum amount of data
required to develop a functional AI model. For this purpose, we consider the
use case of quality control of Carbon Fiber Reinforced Polymer (CFRP) tape
laying in aerospace manufacturing using optical sensors. We investigate the
behavior of different open-source computer vision models with a continuous
reduction of the training data. Our results show that the amount of data
required to successfully train an AI model can be drastically reduced, and the
use of smaller models does not necessarily lead to a loss of performance.</description>
      <guid isPermaLink="false">2409.10104v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Informed, Constrained, Aligned: A Field Analysis on Degeneracy-aware Point Cloud Registration in the Wild</title>
      <link>http://arxiv.org/abs/2408.11809v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to IEEE Transactions on Field Robotics&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - ICP（迭代最近点）注册算法在基于LiDAR的机器人定位中已被使用近十年，但在几何条件不良的环境中，其可靠性可能下降。&lt;br&gt;&lt;h4&gt;2. 现有问题&lt;/h4&gt;   - 现代SLAM解决方案中，ICP可能会在几何条件不佳的环境中表现不佳，当前的解决方案主要依赖外部里程计等额外信息来替代退化的优化方向或在传感器融合中添加约束。&lt;br&gt;&lt;h4&gt;3. 研究目的&lt;/h4&gt;   - 本文首次在大规模文献中调查并比较新旧的退化缓解方法，以提高基于LiDAR的定位的鲁棒性，并分析这些方法在退化环境中的有效性。&lt;br&gt;&lt;h4&gt;4. 主要研究内容&lt;/h4&gt;   - 提出了三个研究方向：&lt;br&gt;     - 将不同类型的约束纳入ICP算法。&lt;br&gt;     - 使用主动或被动的退化缓解技术的效果。&lt;br&gt;     - 在LiDAR退化环境中选择使用全局点云注册方法对ICP问题的影响。&lt;br&gt;&lt;h4&gt;5. 实验验证&lt;/h4&gt;   - 研究结果通过多个真实世界和模拟实验进行验证。&lt;br&gt;&lt;h4&gt;6. 分析结果&lt;/h4&gt;   - 在没有可靠外部估计辅助的情况下，主动优化的退化缓解方法被证明是必要且有利的。&lt;br&gt;   - 在优化之前或期间引入退化感知硬约束比事后添加约束的效果更好。&lt;br&gt;   - 通过启发式微调参数，软约束在复杂的不良条件场景中可以提供相同或更好的结果。&lt;br&gt;&lt;h4&gt;7. 资源共享&lt;/h4&gt;   - 本文中使用的实现已公开分享给社区。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The ICP registration algorithm has been a preferred method for LiDAR-based
robot localization for nearly a decade. However, even in modern SLAM solutions,
ICP can degrade and become unreliable in geometrically ill-conditioned
environments. Current solutions primarily focus on utilizing additional sources
of information, such as external odometry, to either replace the degenerate
directions of the optimization solution or add additional constraints in a
sensor-fusion setup afterward. In response, this work investigates and compares
new and existing degeneracy mitigation methods for robust LiDAR-based
localization and analyzes the efficacy of these approaches in degenerate
environments for the first time in the literature at this scale. Specifically,
this work proposes and investigates i) the incorporation of different types of
constraints into the ICP algorithm, ii) the effect of using active or passive
degeneracy mitigation techniques, and iii) the choice of utilizing global point
cloud registration methods on the ill-conditioned ICP problem in LiDAR
degenerate environments. The study results are validated through multiple
real-world field and simulated experiments. The analysis shows that active
optimization degeneracy mitigation is necessary and advantageous in the absence
of reliable external estimate assistance for LiDAR-SLAM. Furthermore,
introducing degeneracy-aware hard constraints in the optimization before or
during the optimization is shown to perform better in the wild than by
including the constraints after. Moreover, with heuristic fine-tuned
parameters, soft constraints can provide equal or better results in complex
ill-conditioned scenarios. The implementations used in the analysis of this
work are made publicly available to the community.</description>
      <guid isPermaLink="false">2408.11809v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>jina-embeddings-v3: Multilingual Embeddings With Task LoRA</title>
      <link>http://arxiv.org/abs/2409.10173v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, pp11-13 references, pp14-20 appendix and experiment tables&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 模型介绍&lt;/h4&gt;   - 引入了jina-embeddings-v3，这是一个新颖的文本嵌入模型，拥有5.7亿个参数。&lt;br&gt;&lt;h4&gt;2. 性能表现&lt;/h4&gt;   - 该模型在多语言数据和长上下文检索任务上表现出最先进的性能，支持最长达到8192个标记的上下文长度。&lt;br&gt;&lt;h4&gt;3. 适应性设计&lt;/h4&gt;   - 模型包含了一组任务特定的低秩适应（LoRA）适配器，用于生成高质量的嵌入，适用于查询-文档检索、聚类、分类和文本匹配。&lt;br&gt;&lt;h4&gt;4. 学习方法&lt;/h4&gt;   - 集成了Matryoshka表示学习到训练过程中，允许灵活截断嵌入维度而不影响性能。&lt;br&gt;&lt;h4&gt;5. 评估结果&lt;/h4&gt;   - 在MTEB基准上评估表明，jina-embeddings-v3在英语任务上超越了OpenAI和Cohere的最新专有嵌入，同时在所有多语言任务上表现优于multilingual-e5-large-instruct。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce jina-embeddings-v3, a novel text embedding model with 570
million parameters, achieves state-of-the-art performance on multilingual data
and long-context retrieval tasks, supporting context lengths of up to 8192
tokens. The model includes a set of task-specific Low-Rank Adaptation (LoRA)
adapters to generate high-quality embeddings for query-document retrieval,
clustering, classification, and text matching. Additionally, Matryoshka
Representation Learning is integrated into the training process, allowing
flexible truncation of embedding dimensions without compromising performance.
Evaluation on the MTEB benchmark shows that jina-embeddings-v3 outperforms the
latest proprietary embeddings from OpenAI and Cohere on English tasks, while
achieving superior performance compared to multilingual-e5-large-instruct
across all multilingual tasks.</description>
      <guid isPermaLink="false">2409.10173v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>DuInNet: Dual-Modality Feature Interaction for Point Cloud Completion</title>
      <link>http://arxiv.org/abs/2407.07374v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under Review, 13 pages, 7 figures&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 为进一步推动多模态点云补全的发展，本文贡献了一个大规模的多模态点云补全基准数据集ModelNet-MPC。&lt;br&gt;&lt;h4&gt;2. 数据集特点&lt;/h4&gt;   - ModelNet-MPC包含近400,000对高质量的点云和渲染图像，涵盖40个类别，具有更丰富的形状类别和更为多样的测试数据。&lt;br&gt;&lt;h4&gt;3. 任务设置&lt;/h4&gt;   - 除了完全监督的点云补全任务外，ModelNet-MPC还提出了两个额外任务：去噪补全和零-shot学习补全，以模拟真实世界场景，并验证当前方法对噪声的鲁棒性和跨类别的迁移能力。&lt;br&gt;&lt;h4&gt;4. 现有方法的局限&lt;/h4&gt;   - 现有的多模态补全管道通常采用单向融合机制，忽略了图像模态中包含的形状先验信息。&lt;br&gt;&lt;h4&gt;5. 新方法提出&lt;/h4&gt;   - 本文提出了双模态特征交互网络（DuInNet），该网络通过迭代交互点云和图像之间的特征，学习形状的几何和纹理特征。&lt;br&gt;&lt;h4&gt;6. 适应性设计&lt;/h4&gt;   - 为适应不同任务（如完全监督、去噪和零-shot学习点云补全），提出了一种自适应点生成器，以不同权重生成块状的完整点云。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 在ShapeNet-ViPC和ModelNet-MPC基准上的广泛实验表明，DuInNet在所有补全任务中相较于最先进的方法表现出更强的优越性、鲁棒性和迁移能力。&lt;br&gt;&lt;h4&gt;8. 数据和代码发布&lt;/h4&gt;   - 代码和数据集将很快发布。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-07-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; To further promote the development of multimodal point cloud completion, we
contribute a large-scale multimodal point cloud completion benchmark
ModelNet-MPC with richer shape categories and more diverse test data, which
contains nearly 400,000 pairs of high-quality point clouds and rendered images
of 40 categories. Besides the fully supervised point cloud completion task, two
additional tasks including denoising completion and zero-shot learning
completion are proposed in ModelNet-MPC, to simulate real-world scenarios and
verify the robustness to noise and the transfer ability across categories of
current methods. Meanwhile, considering that existing multimodal completion
pipelines usually adopt a unidirectional fusion mechanism and ignore the shape
prior contained in the image modality, we propose a Dual-Modality Feature
Interaction Network (DuInNet) in this paper. DuInNet iteratively interacts
features between point clouds and images to learn both geometric and texture
characteristics of shapes with the dual feature interactor. To adapt to
specific tasks such as fully supervised, denoising, and zero-shot learning
point cloud completions, an adaptive point generator is proposed to generate
complete point clouds in blocks with different weights for these two
modalities. Extensive experiments on the ShapeNet-ViPC and ModelNet-MPC
benchmarks demonstrate that DuInNet exhibits superiority, robustness and
transfer ability in all completion tasks over state-of-the-art methods. The
code and dataset will be available soon.</description>
      <guid isPermaLink="false">2407.07374v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Synchronize Feature Extracting and Matching: A Single Branch Framework for 3D Object Tracking</title>
      <link>http://arxiv.org/abs/2308.12549v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2023&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - Siamese网络已成为3D LiDAR目标追踪的基准框架，利用共享参数编码器分别提取模板和搜索区域的特征。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限&lt;/h4&gt;   - 该范式依赖于额外的匹配网络来建模模板和搜索区域的交叉相关性或相似性。&lt;br&gt;&lt;h4&gt;3. 新方法提出&lt;/h4&gt;   - 本文抛弃传统的Siamese范式，提出了一种新颖的单分支框架SyncTrack，旨在同步特征提取和匹配，从而避免对模板和搜索区域重复前向编码器，并引入额外的匹配网络参数。&lt;br&gt;&lt;h4&gt;4. 同步机制&lt;/h4&gt;   - 同步机制基于Transformer的动态亲和性，并提供了相关性的理论分析。&lt;br&gt;&lt;h4&gt;5. 新策略引入&lt;/h4&gt;   - 基于同步机制，提出了一种新的注意力点采样策略（APST），在Transformer层中取代随机或最远点采样（FPS）方法，通过关注模板和搜索区域之间的关系进行采样。&lt;br&gt;&lt;h4&gt;6. 优势分析&lt;/h4&gt;   - 这种方法将逐点采样与特征学习相结合，有助于聚合更多独特和几何特征，以便在稀疏点的情况下进行追踪。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 在两个基准数据集（KITTI和NuScenes）上的广泛实验表明，SyncTrack在实时追踪中达到了最先进的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2023-08-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Siamese network has been a de facto benchmark framework for 3D LiDAR object
tracking with a shared-parametric encoder extracting features from template and
search region, respectively. This paradigm relies heavily on an additional
matching network to model the cross-correlation/similarity of the template and
search region. In this paper, we forsake the conventional Siamese paradigm and
propose a novel single-branch framework, SyncTrack, synchronizing the feature
extracting and matching to avoid forwarding encoder twice for template and
search region as well as introducing extra parameters of matching network. The
synchronization mechanism is based on the dynamic affinity of the Transformer,
and an in-depth analysis of the relevance is provided theoretically. Moreover,
based on the synchronization, we introduce a novel Attentive Points-Sampling
strategy into the Transformer layers (APST), replacing the random/Farthest
Points Sampling (FPS) method with sampling under the supervision of attentive
relations between the template and search region. It implies connecting
point-wise sampling with the feature learning, beneficial to aggregating more
distinctive and geometric features for tracking with sparse points. Extensive
experiments on two benchmark datasets (KITTI and NuScenes) show that SyncTrack
achieves state-of-the-art performance in real-time tracking.</description>
      <guid isPermaLink="false">2308.12549v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>AALF: Almost Always Linear Forecasting</title>
      <link>http://arxiv.org/abs/2409.10142v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 最近的时间序列预测工作越来越依赖深度学习模型的高预测能力。&lt;br&gt;&lt;h4&gt;2. 模型复杂性问题&lt;/h4&gt;   - 随着模型复杂性的增加，对模型决策过程的理解缺乏，这在高风险决策中是一个问题。&lt;br&gt;&lt;h4&gt;3. 简单模型的有效性&lt;/h4&gt;   - 简单、可解释的预测方法（如线性模型）在性能上有时与深度学习方法不相上下，甚至可以表现得很好。&lt;br&gt;&lt;h4&gt;4. 研究观点&lt;/h4&gt;   - 我们认为简单模型在大多数情况下已经足够，预测性能可以通过仅对特定预测选择深度学习方法来提高，从而增强预测过程的整体可解释性。&lt;br&gt;&lt;h4&gt;5. 方法提出&lt;/h4&gt;   - 提出了一个新颖的在线模型选择框架，利用元学习来识别这些预测，仅在必要时使用不可解释的大型模型。&lt;br&gt;&lt;h4&gt;6. 实证研究&lt;/h4&gt;   - 在多个真实世界数据集上的广泛实证研究表明，所提出的选择方法在大多数情况下优于现有的最先进在线模型选择方法。&lt;br&gt;&lt;h4&gt;7. 结论&lt;/h4&gt;   - 几乎总是选择简单的线性模型进行预测会获得具有竞争力的性能，表明在时间序列预测中，对不透明的黑箱模型的需求比最近的研究所暗示的要小。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent works for time-series forecasting more and more leverage the high
predictive power of Deep Learning models. With this increase in model
complexity, however, comes a lack in understanding of the underlying model
decision process, which is problematic for high-stakes decision making. At the
same time, simple, interpretable forecasting methods such as Linear Models can
still perform very well, sometimes on-par, with Deep Learning approaches. We
argue that simple models are good enough most of the time, and forecasting
performance can be improved by choosing a Deep Learning method only for certain
predictions, increasing the overall interpretability of the forecasting
process. In this context, we propose a novel online model selection framework
which uses meta-learning to identify these predictions and only rarely uses a
non-interpretable, large model. An extensive empirical study on various
real-world datasets shows that our selection methodology outperforms
state-of-the-art online model selections methods in most cases. We find that
almost always choosing a simple Linear Model for forecasting results in
competitive performance, suggesting that the need for opaque black-box models
in time-series forecasting is smaller than recent works would suggest.</description>
      <guid isPermaLink="false">2409.10142v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>A Comprehensive Survey on Deep Multimodal Learning with Missing Modality</title>
      <link>http://arxiv.org/abs/2409.07825v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Work in progress; open to discussion; planning to submit to ACM CSUR
  in September&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 在多模态模型训练和推理过程中，数据样本可能缺失某些模态，导致模型性能受损。&lt;br&gt;&lt;h4&gt;2. 成因分析&lt;/h4&gt;   - 模态缺失的原因包括传感器限制、成本约束、隐私问题、数据丢失以及时间和空间因素。&lt;br&gt;&lt;h4&gt;3. 研究主题&lt;/h4&gt;   - 本文综述了缺失模态的多模态学习（MLMM）的最新进展，重点关注深度学习技术。&lt;br&gt;&lt;h4&gt;4. 综述特性&lt;/h4&gt;   - 这是首个全面的综述，涵盖了历史背景以及MLMM与标准多模态学习设置之间的区别。&lt;br&gt;&lt;h4&gt;5. 方法分析&lt;/h4&gt;   - 进行了对当前MLMM方法、应用和数据集的详细分析。&lt;br&gt;&lt;h4&gt;6. 挑战与未来方向&lt;/h4&gt;   - 讨论了该领域中的挑战和潜在的未来发展方向。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; During multimodal model training and reasoning, data samples may miss certain
modalities and lead to compromised model performance due to sensor limitations,
cost constraints, privacy concerns, data loss, and temporal and spatial
factors. This survey provides an overview of recent progress in Multimodal
Learning with Missing Modality (MLMM), focusing on deep learning techniques. It
is the first comprehensive survey that covers the historical background and the
distinction between MLMM and standard multimodal learning setups, followed by a
detailed analysis of current MLMM methods, applications, and datasets,
concluding with a discussion about challenges and potential future directions
in the field.</description>
      <guid isPermaLink="false">2409.07825v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Continual Learning of Conjugated Visual Representations through Higher-order Motion Flows</title>
      <link>http://arxiv.org/abs/2409.11441v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Currently under review&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 从连续的视觉信息流中学习神经网络面临多重挑战，主要由于数据的非独立同分布（non-i.i.d.）特性。&lt;br&gt;&lt;h4&gt;2. 研究机会&lt;/h4&gt;   - 这种数据流的特性提供了开发与信息流一致的表示的创新机会。&lt;br&gt;&lt;h4&gt;3. 研究主题&lt;/h4&gt;   - 本文探讨了无监督的持续学习，专注于受多种运动诱导约束的像素级特征，称之为运动共轭特征表示。&lt;br&gt;&lt;h4&gt;4. 方法创新&lt;/h4&gt;   - 与现有方法不同，运动并不是一个给定信号（如真实值或外部模块估计），而是通过渐进和自主学习过程的结果，发生在特征层次的不同级别。&lt;br&gt;&lt;h4&gt;5. 运动流估计&lt;/h4&gt;   - 多种运动流通过神经网络估计，并以不同的抽象层次进行特征化，从传统的光流到源自高层特征的潜在信号，称之为高阶运动。&lt;br&gt;&lt;h4&gt;6. 学习一致性&lt;/h4&gt;   - 持续学习以发展一致的多阶流和表示容易导致简单的解决方案，为此引入了一种自监督对比损失，该损失基于流诱导的相似性，具有空间感知特性。&lt;br&gt;&lt;h4&gt;7. 模型评估&lt;/h4&gt;   - 在光逼真的合成流和真实视频上评估模型，与基于Transformer的预训练最先进特征提取器和最近的无监督学习模型进行比较，显著优于这些替代方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning with neural networks from a continuous stream of visual information
presents several challenges due to the non-i.i.d. nature of the data. However,
it also offers novel opportunities to develop representations that are
consistent with the information flow. In this paper we investigate the case of
unsupervised continual learning of pixel-wise features subject to multiple
motion-induced constraints, therefore named motion-conjugated feature
representations. Differently from existing approaches, motion is not a given
signal (either ground-truth or estimated by external modules), but is the
outcome of a progressive and autonomous learning process, occurring at various
levels of the feature hierarchy. Multiple motion flows are estimated with
neural networks and characterized by different levels of abstractions, spanning
from traditional optical flow to other latent signals originating from
higher-level features, hence called higher-order motions. Continuously learning
to develop consistent multi-order flows and representations is prone to trivial
solutions, which we counteract by introducing a self-supervised contrastive
loss, spatially-aware and based on flow-induced similarity. We assess our model
on photorealistic synthetic streams and real-world videos, comparing to
pre-trained state-of-the art feature extractors (also based on Transformers)
and to recent unsupervised learning models, significantly outperforming these
alternatives.</description>
      <guid isPermaLink="false">2409.11441v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Mixture of Experts Fusion for Fake Audio Detection Using Frozen wav2vec 2.0</title>
      <link>http://arxiv.org/abs/2409.11909v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  submitted to ICASSP2025&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 语音合成技术对说话人验证系统构成了严重威胁。&lt;br&gt;&lt;h4&gt;2. 现有方法&lt;/h4&gt;   - 当前最有效的假音频检测方法利用预训练模型，结合来自不同层的特征进一步提升检测性能。&lt;br&gt;&lt;h4&gt;3. 主要问题&lt;/h4&gt;   - 许多现有的融合方法需要对预训练模型进行微调，导致训练时间过长，并在面对新语音合成技术时阻碍模型迭代。&lt;br&gt;&lt;h4&gt;4. 方法提出&lt;/h4&gt;   - 本文提出了一种基于专家混合（Mixture of Experts）的特征融合方法，从层特征中提取与假音频检测相关的特征。&lt;br&gt;&lt;h4&gt;5. 方法细节&lt;/h4&gt;   - 该方法通过一个基于最后一层特征的门控网络引导特征的提取和集成，同时保持预训练模型的冻结状态。&lt;br&gt;&lt;h4&gt;6. 实验验证&lt;/h4&gt;   - 在ASVspoof2019和ASVspoof2021数据集上的实验表明，所提出的方法在性能上与需要微调的方法相当。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Speech synthesis technology has posed a serious threat to speaker
verification systems.
  Currently, the most effective fake audio detection methods utilize pretrained
models, and integrating features from various layers of pretrained model
further enhances detection performance.
  However, most of the previously proposed fusion methods require fine-tuning
the pretrained models, resulting in excessively long training times and
hindering model iteration when facing new speech synthesis technology.
  To address this issue, this paper proposes a feature fusion method based on
the Mixture of Experts, which extracts and integrates features relevant to fake
audio detection from layer features, guided by a gating network based on the
last layer feature, while freezing the pretrained model.
  Experiments conducted on the ASVspoof2019 and ASVspoof2021 datasets
demonstrate that the proposed method achieves competitive performance compared
to those requiring fine-tuning.</description>
      <guid isPermaLink="false">2409.11909v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Fair Anomaly Detection For Imbalanced Groups</title>
      <link>http://arxiv.org/abs/2409.10951v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 异常检测（AD）在许多实际应用中得到广泛研究，包括金融诈骗检测和网络安全入侵检测等。&lt;br&gt;&lt;h4&gt;2. 问题识别&lt;/h4&gt;   - 由于受到保护组和未保护组之间的不平衡性，以及正常样本和异常样本的不平衡分布，现有异常检测方法的学习目标往往集中在占主导地位的未保护组上。&lt;br&gt;&lt;h4&gt;3. 公平性的重要性&lt;/h4&gt;   - 许多研究者意识到在异常检测中确保模型公平性的重要性，但现有的公平异常检测方法在不平衡场景下常常错误地将大多数正常样本标记为异常，特别是在未保护组数量更多的情况下。&lt;br&gt;&lt;h4&gt;4. 原因分析&lt;/h4&gt;   - 这种现象源于学习目标设计不当，统计上更关注学习频繁模式（即未保护组），而忽视了代表性不足的模式（即保护组）。&lt;br&gt;&lt;h4&gt;5. 方法提出&lt;/h4&gt;   - 提出了FairAD，一种针对不平衡场景的公平异常检测方法。&lt;br&gt;&lt;h4&gt;6. 方法组成&lt;/h4&gt;   - 包含一个公平感知对比学习模块和一个重平衡自编码器模块，分别确保公平性和处理不平衡数据问题。&lt;br&gt;&lt;h4&gt;7. 理论分析&lt;/h4&gt;   - 提供了理论分析，表明所提出的对比学习正则化能够保证组公平性。&lt;br&gt;&lt;h4&gt;8. 实验验证&lt;/h4&gt;   - 实证研究表明，FairAD在多个实际数据集上表现出有效性和高效性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Anomaly detection (AD) has been widely studied for decades in many real-world
applications, including fraud detection in finance, and intrusion detection for
cybersecurity, etc. Due to the imbalanced nature between protected and
unprotected groups and the imbalanced distributions of normal examples and
anomalies, the learning objectives of most existing anomaly detection methods
tend to solely concentrate on the dominating unprotected group. Thus, it has
been recognized by many researchers about the significance of ensuring model
fairness in anomaly detection. However, the existing fair anomaly detection
methods tend to erroneously label most normal examples from the protected group
as anomalies in the imbalanced scenario where the unprotected group is more
abundant than the protected group. This phenomenon is caused by the improper
design of learning objectives, which statistically focus on learning the
frequent patterns (i.e., the unprotected group) while overlooking the
under-represented patterns (i.e., the protected group). To address these
issues, we propose FairAD, a fairness-aware anomaly detection method targeting
the imbalanced scenario. It consists of a fairness-aware contrastive learning
module and a rebalancing autoencoder module to ensure fairness and handle the
imbalanced data issue, respectively. Moreover, we provide the theoretical
analysis that shows our proposed contrastive learning regularization guarantees
group fairness. Empirical studies demonstrate the effectiveness and efficiency
of FairAD across multiple real-world datasets.</description>
      <guid isPermaLink="false">2409.10951v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Direct-CP: Directed Collaborative Perception for Connected and Autonomous Vehicles via Proactive Attention</title>
      <link>http://arxiv.org/abs/2409.08840v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 协作感知（CP）利用连接和自主车辆（CAV）中的视觉数据来增强自我车辆的视场（FoV）。&lt;br&gt;&lt;h4&gt;2. 现有问题&lt;/h4&gt;   - 当前的CP方法在扩展自我车辆的360度感知范围时，面临两个主要挑战：&lt;br&gt;     - 在交通分布不均的区域，关注交通较少的方向收益有限。&lt;br&gt;     - 在通信预算有限的情况下，将过多带宽分配给不太关键的方向会降低在重要区域的感知准确性。&lt;br&gt;&lt;h4&gt;3. 方法提出&lt;/h4&gt;   - 提出了Direct-CP，一个主动且方向感知的CP系统，旨在改善特定方向的CP性能。&lt;br&gt;&lt;h4&gt;4. 核心思想&lt;/h4&gt;   - 使自我车辆主动信号其感兴趣的方向，并重新调整注意力，以增强局部方向的CP表现。&lt;br&gt;&lt;h4&gt;5. 机制设计&lt;/h4&gt;   - 提出了一个基于RSU（路边单元）的方向遮罩机制，帮助自我车辆识别重要方向。&lt;br&gt;   - 设计了方向感知选择性注意模块，根据自我车辆的方向优先级、通信预算和CAV的位置信息智能聚合相关特征。&lt;br&gt;&lt;h4&gt;6. 损失函数&lt;/h4&gt;   - 引入了方向加权检测损失（DWLoss），用于捕捉方向CP结果与真实值之间的偏差，以便有效训练模型。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 在V2X-Sim 2.0数据集上的广泛实验表明，该方法在感兴趣方向上实现了19.8%的局部感知准确率提升，以及比现有最先进方法高出2.5%的整体感知准确率，特别是在协作3D物体检测任务中表现优异。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Collaborative perception (CP) leverages visual data from connected and
autonomous vehicles (CAV) to enhance an ego vehicle's field of view (FoV).
Despite recent progress, current CP methods expand the ego vehicle's 360-degree
perceptual range almost equally, which faces two key challenges. Firstly, in
areas with uneven traffic distribution, focusing on directions with little
traffic offers limited benefits. Secondly, under limited communication budgets,
allocating excessive bandwidth to less critical directions lowers the
perception accuracy in more vital areas. To address these issues, we propose
Direct-CP, a proactive and direction-aware CP system aiming at improving CP in
specific directions. Our key idea is to enable an ego vehicle to proactively
signal its interested directions and readjust its attention to enhance local
directional CP performance. To achieve this, we first propose an RSU-aided
direction masking mechanism that assists an ego vehicle in identifying vital
directions. Additionally, we design a direction-aware selective attention
module to wisely aggregate pertinent features based on ego vehicle's
directional priorities, communication budget, and the positional data of CAVs.
Moreover, we introduce a direction-weighted detection loss (DWLoss) to capture
the divergence between directional CP outcomes and the ground truth,
facilitating effective model training. Extensive experiments on the V2X-Sim 2.0
dataset demonstrate that our approach achieves 19.8\% higher local perception
accuracy in interested directions and 2.5\% higher overall perception accuracy
than the state-of-the-art methods in collaborative 3D object detection tasks.</description>
      <guid isPermaLink="false">2409.08840v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Hierarchical Graph Pooling Based on Minimum Description Length</title>
      <link>http://arxiv.org/abs/2409.10263v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 图池化是深度图表示学习中的一个重要环节。&lt;br&gt;&lt;h4&gt;2. 方法提出&lt;/h4&gt;   - 本文提出了MapEqPool，这是一种考虑现实世界图的固有层次结构的原则性池化操作符。&lt;br&gt;&lt;h4&gt;3. 理论基础&lt;/h4&gt;   - MapEqPool基于“映射方程”，这是一种用于社区检测的信息论目标函数，基于最小描述长度原理，能够自然地实现奥卡姆剃刀原则，平衡模型复杂性和拟合度。&lt;br&gt;&lt;h4&gt;4. 性能评估&lt;/h4&gt;   - 通过与多种基线方法在标准图分类数据集上的实证比较，展示了MapEqPool的竞争性性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph pooling is an essential part of deep graph representation learning. We
introduce MapEqPool, a principled pooling operator that takes the inherent
hierarchical structure of real-world graphs into account. MapEqPool builds on
the map equation, an information-theoretic objective function for community
detection based on the minimum description length principle which naturally
implements Occam's razor and balances between model complexity and fit. We
demonstrate MapEqPool's competitive performance with an empirical comparison
against various baselines across standard graph classification datasets.</description>
      <guid isPermaLink="false">2409.10263v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>UMERegRobust - Universal Manifold Embedding Compatible Features for Robust Point Cloud Registration</title>
      <link>http://arxiv.org/abs/2408.12380v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ECCV 2024&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 本文采用了通用流形嵌入（UME）框架，用于刚性变换的估计，并对其进行了扩展，以适应部分重叠和不同采样点云的场景。&lt;br&gt;&lt;h4&gt;2. UME框架概述&lt;/h4&gt;   - UME是一种方法论，旨在将同一对象的观测值（通过刚性变换相关联）映射到一个低维线性子空间，从而获得变换不变的表示。&lt;br&gt;&lt;h4&gt;3. 矩阵形式特性&lt;/h4&gt;   - 该过程的矩阵形式具有协变（即等变）特性，确保与变换保持一致性。&lt;br&gt;&lt;h4&gt;4. 扩展方法&lt;/h4&gt;   - 本文通过引入UME兼容的特征提取方法、独特的UME对比损失以及采样均衡器，扩展了UME框架。&lt;br&gt;&lt;h4&gt;5. 注册管道&lt;/h4&gt;   - 这些组件被集成到一个全面且稳健的注册管道中，命名为UMERegRobust。&lt;br&gt;&lt;h4&gt;6. 基准测试&lt;/h4&gt;   - 提出了RotKITTI注册基准，专门用于评估涉及大旋转的注册方法。&lt;br&gt;&lt;h4&gt;7. 性能评估&lt;/h4&gt;   - UMERegRobust在KITTI基准上表现优于现有最先进的技术（SOTA），特别是在严格精度（1°，10cm）下，平均提高9%。&lt;br&gt;   - 在RotKITTI基准上，显著优于最新的SOTA方法，提升幅度达到45%。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/yuvalh9/umeregrobust&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we adopt the Universal Manifold Embedding (UME) framework for
the estimation of rigid transformations and extend it, so that it can
accommodate scenarios involving partial overlap and differently sampled point
clouds. UME is a methodology designed for mapping observations of the same
object, related by rigid transformations, into a single low-dimensional linear
subspace. This process yields a transformation-invariant representation of the
observations, with its matrix form representation being covariant (i.e.
equivariant) with the transformation. We extend the UME framework by
introducing a UME-compatible feature extraction method augmented with a unique
UME contrastive loss and a sampling equalizer. These components are integrated
into a comprehensive and robust registration pipeline, named UMERegRobust. We
propose the RotKITTI registration benchmark, specifically tailored to evaluate
registration methods for scenarios involving large rotations. UMERegRobust
achieves better than state-of-the-art performance on the KITTI benchmark,
especially when strict precision of (1{\deg}, 10cm) is considered (with an
average gain of +9%), and notably outperform SOTA methods on the RotKITTI
benchmark (with +45% gain compared the most recent SOTA method).</description>
      <guid isPermaLink="false">2408.12380v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>The Interstate-24 3D Dataset: a new benchmark for 3D multi-camera vehicle tracking</title>
      <link>http://arxiv.org/abs/2308.14833v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究目的&lt;/h4&gt;   - 本文提出了一个新的视频数据集，旨在支持城市高速公路交通监测中的多摄像头3D物体跟踪。&lt;br&gt;&lt;h4&gt;2. 数据集特点&lt;/h4&gt;   - 数据集来自3个场景，每个场景至少包含16个摄像头录制的视频，总长度为57分钟。&lt;br&gt;&lt;h4&gt;3. 注释信息&lt;/h4&gt;   - 数据集中包含877,000个3D边界框和相应的物体轨迹，针对每个摄像头视野进行了全面准确的标注，并结合成空间和时间连续的车辆轨迹集。&lt;br&gt;&lt;h4&gt;4. 基准测试&lt;/h4&gt;   - 结合现有算法对多个3D多摄像头跟踪管道在该数据集上的表现进行基准测试。&lt;br&gt;&lt;h4&gt;5. 挑战性分析&lt;/h4&gt;   - 结果表明，由于高速移动物体在摄像头间匹配的难度以及在拥堵交通中可能出现的严重遮挡，数据集具有挑战性，可能会影响数百帧的跟踪。&lt;br&gt;&lt;h4&gt;6. 研究意义&lt;/h4&gt;   - 本研究旨在推动准确自动的车辆轨迹提取算法的发展，这对理解自动驾驶技术对交通安全和效率的影响至关重要。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2023-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work presents a novel video dataset recorded from overlapping highway
traffic cameras along an urban interstate, enabling multi-camera 3D object
tracking in a traffic monitoring context. Data is released from 3 scenes
containing video from at least 16 cameras each, totaling 57 minutes in length.
877,000 3D bounding boxes and corresponding object tracklets are fully and
accurately annotated for each camera field of view and are combined into a
spatially and temporally continuous set of vehicle trajectories for each scene.
Lastly, existing algorithms are combined to benchmark a number of 3D
multi-camera tracking pipelines on the dataset, with results indicating that
the dataset is challenging due to the difficulty of matching objects traveling
at high speeds across cameras and heavy object occlusion, potentially for
hundreds of frames, during congested traffic. This work aims to enable the
development of accurate and automatic vehicle trajectory extraction algorithms,
which will play a vital role in understanding impacts of autonomous vehicle
technologies on the safety and efficiency of traffic.</description>
      <guid isPermaLink="false">2308.14833v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>A Multimodal Approach for Fluid Overload Prediction: Integrating Lung Ultrasound and Clinical Data</title>
      <link>http://arxiv.org/abs/2409.08790v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 1 figure, 1 table&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 在透析患者中，管理液体平衡至关重要，管理不当可能导致严重并发症。&lt;br&gt;&lt;h4&gt;2. 方法提出&lt;/h4&gt;   - 本文提出一种多模态方法，结合肺部超声图像的视觉特征和临床数据，以增强对体内多余液体的预测。&lt;br&gt;&lt;h4&gt;3. 框架设计&lt;/h4&gt;   - 采用独立编码器提取每种模态的特征，并通过跨域注意机制将其结合，以捕捉互补信息。&lt;br&gt;&lt;h4&gt;4. 任务框架&lt;/h4&gt;   - 将预测任务框定为分类任务，模型在性能上显著优于回归方法。&lt;br&gt;&lt;h4&gt;5. 性能比较&lt;/h4&gt;   - 结果表明，多模态模型在性能上始终优于单模态模型，尤其是在注意机制优先考虑表格数据时。&lt;br&gt;&lt;h4&gt;6. 样本生成&lt;/h4&gt;   - 伪样本生成有助于缓解分类不平衡问题，最终实现最高准确率88.31%。&lt;br&gt;&lt;h4&gt;7. 研究意义&lt;/h4&gt;   - 本研究强调了多模态学习在透析患者液体过载管理中的有效性，为改善临床结果提供了有价值的见解。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Managing fluid balance in dialysis patients is crucial, as improper
management can lead to severe complications. In this paper, we propose a
multimodal approach that integrates visual features from lung ultrasound images
with clinical data to enhance the prediction of excess body fluid. Our
framework employs independent encoders to extract features for each modality
and combines them through a cross-domain attention mechanism to capture
complementary information. By framing the prediction as a classification task,
the model achieves significantly better performance than regression. The
results demonstrate that multimodal models consistently outperform
single-modality models, particularly when attention mechanisms prioritize
tabular data. Pseudo-sample generation further contributes to mitigating the
imbalanced classification problem, achieving the highest accuracy of 88.31%.
This study underscores the effectiveness of multimodal learning for fluid
overload management in dialysis patients, offering valuable insights for
improved clinical outcomes.</description>
      <guid isPermaLink="false">2409.08790v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>LLM-wrapper: Black-Box Semantic-Aware Adaptation of Vision-Language Foundation Models</title>
      <link>http://arxiv.org/abs/2409.11919v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  EVAL-FoMo workshop, ECCV 2024&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 视觉语言模型（VLMs）在多种任务上表现出色，但其零-shot能力相较于专门或微调的模型有限。&lt;br&gt;&lt;h4&gt;2. 微调的局限性&lt;/h4&gt;   - 微调VLMs需要对模型架构和权重的“白盒”访问，并且需要设计微调目标和优化超参数的专业知识，这些都因模型和下游任务而异。&lt;br&gt;&lt;h4&gt;3. 方法提出&lt;/h4&gt;   - 本文提出了LLM-wrapper，一种通过利用大型语言模型（LLMs）以“黑盒”方式适应VLMs的新方法，旨在推理其输出。&lt;br&gt;&lt;h4&gt;4. 任务验证&lt;/h4&gt;   - 在指称表达理解（REC）这一具有挑战性的开放词汇任务上验证了LLM-wrapper的有效性，该任务需要空间和语义推理。&lt;br&gt;&lt;h4&gt;5. 性能提升&lt;/h4&gt;   - 该方法显著提升了现成模型的性能，结果与经典微调方法相比具有竞争力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision Language Models (VLMs) have shown impressive performances on numerous
tasks but their zero-shot capabilities can be limited compared to dedicated or
fine-tuned models. Yet, fine-tuning VLMs comes with limitations as it requires
`white-box' access to the model's architecture and weights as well as expertise
to design the fine-tuning objectives and optimize the hyper-parameters, which
are specific to each VLM and downstream task. In this work, we propose
LLM-wrapper, a novel approach to adapt VLMs in a `black-box' manner by
leveraging large language models (LLMs) so as to reason on their outputs. We
demonstrate the effectiveness of LLM-wrapper on Referring Expression
Comprehension (REC), a challenging open-vocabulary task that requires spatial
and semantic reasoning. Our approach significantly boosts the performance of
off-the-shelf models, resulting in competitive results when compared with
classic fine-tuning.</description>
      <guid isPermaLink="false">2409.11919v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>GSTran: Joint Geometric and Semantic Coherence for Point Cloud Segmentation</title>
      <link>http://arxiv.org/abs/2408.11558v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICPR 2024&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 在点云分割任务中，学习有意义的局部和全局信息仍然是一个挑战。&lt;br&gt;&lt;h4&gt;2. 局部信息问题&lt;/h4&gt;   - 之前的研究在使用局部信息时，未能区分不同类别的邻居信息，可能导致查询点的特征受到损害。&lt;br&gt;&lt;h4&gt;3. 全局信息问题&lt;/h4&gt;   - 在利用全局信息时，长距离上下文依赖的建模不准确也会影响模型性能。&lt;br&gt;&lt;h4&gt;4. 方法提出&lt;/h4&gt;   - 本文提出了GSTran，一种专门为分割任务设计的新型变换网络。&lt;br&gt;&lt;h4&gt;5. 网络组件&lt;/h4&gt;   - GSTran主要由两个核心组件组成：&lt;br&gt;     - **局部几何变换器**：显式计算局部区域内的几何差异，增强与几何相似邻居点的关联，同时抑制与其他邻居的关联。&lt;br&gt;     - **全局语义变换器**：设计了一个多头投票策略，评估整个空间范围内的语义相似性，精确捕捉上下文依赖。&lt;br&gt;&lt;h4&gt;6. 实验验证&lt;/h4&gt;   - 在ShapeNetPart和S3DIS基准测试上的实验表明，该方法有效且优于其他算法。&lt;br&gt;&lt;h4&gt;7. 代码可用性&lt;/h4&gt;   - 相关代码可在GitHub上获取，链接为[https://github.com/LAB123-tech/GSTran](https://github.com/LAB123-tech/GSTran)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/lab123-tech/gstran&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning meaningful local and global information remains a challenge in point
cloud segmentation tasks. When utilizing local information, prior studies
indiscriminately aggregates neighbor information from different classes to
update query points, potentially compromising the distinctive feature of query
points. In parallel, inaccurate modeling of long-distance contextual
dependencies when utilizing global information can also impact model
performance. To address these issues, we propose GSTran, a novel transformer
network tailored for the segmentation task. The proposed network mainly
consists of two principal components: a local geometric transformer and a
global semantic transformer. In the local geometric transformer module, we
explicitly calculate the geometric disparity within the local region. This
enables amplifying the affinity with geometrically similar neighbor points
while suppressing the association with other neighbors. In the global semantic
transformer module, we design a multi-head voting strategy. This strategy
evaluates semantic similarity across the entire spatial range, facilitating the
precise capture of contextual dependencies. Experiments on ShapeNetPart and
S3DIS benchmarks demonstrate the effectiveness of the proposed method, showing
its superiority over other algorithms. The code is available at
https://github.com/LAB123-tech/GSTran.</description>
      <guid isPermaLink="false">2408.11558v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Deep Optimal Transport: A Practical Algorithm for Photo-realistic Image Restoration</title>
      <link>http://arxiv.org/abs/2306.02342v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 提出一种图像恢复算法，能够在测试时控制感知质量和均方误差（MSE）之间的权衡。&lt;br&gt;&lt;h4&gt;2. 算法特点&lt;/h4&gt;   - 该算法为少量样本学习：仅需大约十张模型恢复的图像，便能显著改善新恢复图像的感知质量和/或MSE，而无需进一步训练。&lt;br&gt;&lt;h4&gt;3. 理论动机&lt;/h4&gt;   - 研究基于最新的理论结果，探讨最小均方误差（MMSE）预测器与在完美感知质量约束下最小化MSE的预测器之间的关系。&lt;br&gt;&lt;h4&gt;4. 最优传输&lt;/h4&gt;   - 证明了在感知质量约束下，后者可以通过对前者的输出进行最优传输来获得，使其分布与源数据匹配。&lt;br&gt;&lt;h4&gt;5. 实现方法&lt;/h4&gt;   - 为了改善原本训练以最小化MSE的预测器的感知质量，采用变分自编码器的潜在空间中的线性变换来近似最优传输，这一过程通过经验均值和协方差以闭式形式计算。&lt;br&gt;&lt;h4&gt;6. 超越理论&lt;/h4&gt;   - 发现对最初训练以实现高感知质量的模型应用相同的程序，通常能进一步改善其感知质量。&lt;br&gt;&lt;h4&gt;7. 结果插值&lt;/h4&gt;   - 通过将结果与模型的原始输出进行插值，可以在牺牲感知质量的情况下改善MSE。&lt;br&gt;&lt;h4&gt;8. 应用示例&lt;/h4&gt;   - 在多种降解条件下，对任意维度的一般内容图像展示了该方法的有效性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2023-06-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/theoad/dot-dmax&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose an image restoration algorithm that can control the perceptual
quality and/or the mean square error (MSE) of any pre-trained model, trading
one over the other at test time. Our algorithm is few-shot: Given about a dozen
images restored by the model, it can significantly improve the perceptual
quality and/or the MSE of the model for newly restored images without further
training. Our approach is motivated by a recent theoretical result that links
between the minimum MSE (MMSE) predictor and the predictor that minimizes the
MSE under a perfect perceptual quality constraint. Specifically, it has been
shown that the latter can be obtained by optimally transporting the output of
the former, such that its distribution matches the source data. Thus, to
improve the perceptual quality of a predictor that was originally trained to
minimize MSE, we approximate the optimal transport by a linear transformation
in the latent space of a variational auto-encoder, which we compute in
closed-form using empirical means and covariances. Going beyond the theory, we
find that applying the same procedure on models that were initially trained to
achieve high perceptual quality, typically improves their perceptual quality
even further. And by interpolating the results with the original output of the
model, we can improve their MSE on the expense of perceptual quality. We
illustrate our method on a variety of degradations applied to general content
images of arbitrary dimensions.</description>
      <guid isPermaLink="false">2306.02342v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>TrajSSL: Trajectory-Enhanced Semi-Supervised 3D Object Detection</title>
      <link>http://arxiv.org/abs/2409.10901v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 半监督3D物体检测是一种常用策略，旨在解决大规模自动驾驶感知数据集手动标注的挑战。&lt;br&gt;&lt;h4&gt;2. 伪标注方法&lt;/h4&gt;   - 伪标注的半监督学习采用教师-学生框架，利用机器生成的伪标注与少量手动标注的数据集结合进行训练。&lt;br&gt;&lt;h4&gt;3. 研究问题&lt;/h4&gt;   - 本文旨在通过利用驾驶场景中的长期时间信息来提高伪标注质量。&lt;br&gt;&lt;h4&gt;4. 方法细节&lt;/h4&gt;   - 具体来说，利用预训练的运动预测模型在伪标注数据上生成物体轨迹，以进一步增强学生模型的训练。&lt;br&gt;&lt;h4&gt;5. 伪标注质量提升&lt;/h4&gt;   - 该方法通过两种方式提高伪标注质量：&lt;br&gt;     - **抑制假阳性**：通过建立运动预测输出在多个帧之间的一致性来减少假阳性伪标注。&lt;br&gt;     - **补偿假阴性**：直接将预测的物体轨迹插入伪标注场景中，以弥补假阴性检测。&lt;br&gt;&lt;h4&gt;6. 实验验证&lt;/h4&gt;   - 在nuScenes数据集上的实验表明，该方法有效提高了标准半监督方法在多种设置下的表现。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semi-supervised 3D object detection is a common strategy employed to
circumvent the challenge of manually labeling large-scale autonomous driving
perception datasets. Pseudo-labeling approaches to semi-supervised learning
adopt a teacher-student framework in which machine-generated pseudo-labels on a
large unlabeled dataset are used in combination with a small manually-labeled
dataset for training. In this work, we address the problem of improving
pseudo-label quality through leveraging long-term temporal information captured
in driving scenes. More specifically, we leverage pre-trained
motion-forecasting models to generate object trajectories on pseudo-labeled
data to further enhance the student model training. Our approach improves
pseudo-label quality in two distinct manners: first, we suppress false positive
pseudo-labels through establishing consistency across multiple frames of motion
forecasting outputs. Second, we compensate for false negative detections by
directly inserting predicted object tracks into the pseudo-labeled scene.
Experiments on the nuScenes dataset demonstrate the effectiveness of our
approach, improving the performance of standard semi-supervised approaches in a
variety of settings.</description>
      <guid isPermaLink="false">2409.10901v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Sight View Constraint for Robust Point Cloud Registration</title>
      <link>http://arxiv.org/abs/2409.05065v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 部分到部分点云配准（Partial PCR）仍然是一项具有挑战性的任务，特别是在重叠率低的情况下。&lt;br&gt;&lt;h4&gt;2. 现有问题&lt;/h4&gt;   - 相较于全到全的配准任务，部分PCR的目标仍未明确定义，没有可靠的度量标准能够识别真实的变换。&lt;br&gt;&lt;h4&gt;3. 挑战识别&lt;/h4&gt;   - 识别这一点作为部分PCR任务中最根本的挑战。&lt;br&gt;&lt;h4&gt;4. 新方法提出&lt;/h4&gt;   - 提出一种新颖的通用视角约束（Sight View Constraint, SVC），旨在明确识别错误的变换，从而增强现有PCR方法的鲁棒性。&lt;br&gt;&lt;h4&gt;5. 实验验证&lt;/h4&gt;   - 大量实验验证了SVC在室内和室外场景中的有效性。&lt;br&gt;&lt;h4&gt;6. 性能提升&lt;/h4&gt;   - 在具有挑战性的3DLoMatch数据集上，该方法将配准召回率从78%提升至82%，实现了最新的研究成果。&lt;br&gt;&lt;h4&gt;7. 研究意义&lt;/h4&gt;   - 本研究强调了部分PCR决策版本问题的重要性，可能为部分PCR问题提供新的见解。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Partial to Partial Point Cloud Registration (partial PCR) remains a
challenging task, particularly when dealing with a low overlap rate. In
comparison to the full-to-full registration task, we find that the objective of
partial PCR is still not well-defined, indicating no metric can reliably
identify the true transformation. We identify this as the most fundamental
challenge in partial PCR tasks. In this paper, instead of directly seeking the
optimal transformation, we propose a novel and general Sight View Constraint
(SVC) to conclusively identify incorrect transformations, thereby enhancing the
robustness of existing PCR methods. Extensive experiments validate the
effectiveness of SVC on both indoor and outdoor scenes. On the challenging
3DLoMatch dataset, our approach increases the registration recall from 78\% to
82\%, achieving the state-of-the-art result. This research also highlights the
significance of the decision version problem of partial PCR, which has the
potential to provide novel insights into the partial PCR problem.</description>
      <guid isPermaLink="false">2409.05065v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>GS-Net: Generalizable Plug-and-Play 3D Gaussian Splatting Module</title>
      <link>http://arxiv.org/abs/2409.11307v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 3D Gaussian Splatting (3DGS)结合了基于原始体素的表示和体积渲染技术，实现了实时高质量渲染。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 3DGS模型通常对单场景训练过拟合，且对高斯椭球的初始化非常敏感，这些椭球是从结构光束（SfM）点云中启发性推导的，限制了模型的泛化能力和实用性。&lt;br&gt;&lt;h4&gt;3. 新方法提出&lt;/h4&gt;   - 提出GS-Net，一个可泛化的即插即用3DGS模块，能够从稀疏的SfM点云中密集化高斯椭球，增强几何结构表示。&lt;br&gt;&lt;h4&gt;4. 创新点&lt;/h4&gt;   - GS-Net是首个具有跨场景泛化能力的即插即用3DGS模块。&lt;br&gt;&lt;h4&gt;5. 新数据集介绍&lt;/h4&gt;   - 引入CARLA-NVS数据集，包含额外的相机视角，用于全面评估重建和渲染质量。&lt;br&gt;&lt;h4&gt;6. 实验结果&lt;/h4&gt;   - 大量实验表明，将GS-Net应用于3DGS后，传统视角的PSNR提升了2.08 dB，新的视角提升了1.86 dB，确认了该方法的有效性和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Gaussian Splatting (3DGS) integrates the strengths of primitive-based
representations and volumetric rendering techniques, enabling real-time,
high-quality rendering. However, 3DGS models typically overfit to single-scene
training and are highly sensitive to the initialization of Gaussian ellipsoids,
heuristically derived from Structure from Motion (SfM) point clouds, which
limits both generalization and practicality. To address these limitations, we
propose GS-Net, a generalizable, plug-and-play 3DGS module that densifies
Gaussian ellipsoids from sparse SfM point clouds, enhancing geometric structure
representation. To the best of our knowledge, GS-Net is the first plug-and-play
3DGS module with cross-scene generalization capabilities. Additionally, we
introduce the CARLA-NVS dataset, which incorporates additional camera
viewpoints to thoroughly evaluate reconstruction and rendering quality.
Extensive experiments demonstrate that applying GS-Net to 3DGS yields a PSNR
improvement of 2.08 dB for conventional viewpoints and 1.86 dB for novel
viewpoints, confirming the method's effectiveness and robustness.</description>
      <guid isPermaLink="false">2409.11307v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>World of Forms: Deformable Geometric Templates for One-Shot Surface Meshing in Coronary CT Angiography</title>
      <link>http://arxiv.org/abs/2409.11837v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to Medical Image Analysis&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 基于深度学习的医学图像分割和表面网格生成通常涉及从图像到分割再到网格的顺序流程。&lt;br&gt;&lt;h4&gt;2. 当前挑战&lt;/h4&gt;   - 这种流程通常需要大量训练数据，并且对先前的几何知识利用有限，可能导致拓扑不一致和在数据稀缺情况下的性能不佳。&lt;br&gt;&lt;h4&gt;3. 新方法提出&lt;/h4&gt;   - 提出一种数据高效的深度学习方法，直接生成3D解剖对象表面网格，利用几何先验信息。&lt;br&gt;&lt;h4&gt;4. 方法细节&lt;/h4&gt;   - 采用多分辨率图神经网络在一个先验几何模板上操作，该模板会变形以适应感兴趣的对象边界。&lt;br&gt;&lt;h4&gt;5. 模板使用&lt;/h4&gt;   - 说明不同的模板可以用于不同的表面网格生成目标。&lt;br&gt;&lt;h4&gt;6. 预训练策略&lt;/h4&gt;   - 引入一种新颖的掩蔽自编码器预训练策略，专门针对3D球形数据。&lt;br&gt;&lt;h4&gt;7. 性能表现&lt;/h4&gt;   - 在一次性分割任务中，该方法在心包、左心室腔和左心室肌层的分割上优于nnUNet。&lt;br&gt;&lt;h4&gt;8. 对比结果&lt;/h4&gt;   - 方法在多平面重组图像的腔体分割任务中也表现优于其他方法。&lt;br&gt;&lt;h4&gt;9. 网格质量&lt;/h4&gt;   - 结果表明，生成的网格质量与或优于使用“marching cubes”后处理的体素掩膜预测，同时在网格三角化先验的选择上保持灵活性。&lt;br&gt;&lt;h4&gt;10. 应用前景&lt;/h4&gt;    - 该方法为更准确和拓扑一致的3D医学对象表面网格生成铺平了道路。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning-based medical image segmentation and surface mesh generation
typically involve a sequential pipeline from image to segmentation to meshes,
often requiring large training datasets while making limited use of prior
geometric knowledge. This may lead to topological inconsistencies and
suboptimal performance in low-data regimes. To address these challenges, we
propose a data-efficient deep learning method for direct 3D anatomical object
surface meshing using geometric priors. Our approach employs a multi-resolution
graph neural network that operates on a prior geometric template which is
deformed to fit object boundaries of interest. We show how different templates
may be used for the different surface meshing targets, and introduce a novel
masked autoencoder pretraining strategy for 3D spherical data. The proposed
method outperforms nnUNet in a one-shot setting for segmentation of the
pericardium, left ventricle (LV) cavity and the LV myocardium. Similarly, the
method outperforms other lumen segmentation operating on multi-planar
reformatted images. Results further indicate that mesh quality is on par with
or improves upon marching cubes post-processing of voxel mask predictions,
while remaining flexible in the choice of mesh triangulation prior, thus paving
the way for more accurate and topologically consistent 3D medical object
surface meshing.</description>
      <guid isPermaLink="false">2409.11837v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>MacDiff: Unified Skeleton Modeling with Masked Conditional Diffusion</title>
      <link>http://arxiv.org/abs/2409.10473v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ECCV 2024&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 自监督学习在基于骨架的人类动作理解中已被证明有效。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 先前的方法依赖于对比学习，存在假阴性问题；或者基于重建，学习到过多无关的低级线索，导致对下游任务的表示能力有限。&lt;br&gt;&lt;h4&gt;3. 生成学习的进展&lt;/h4&gt;   - 近年来，生成学习取得了显著进展，作为一种自然且有意义的前置任务，用于建模一般的数据分布。&lt;br&gt;&lt;h4&gt;4. 生成模型的探索不足&lt;/h4&gt;   - 生成模型的表示学习能力尚未得到充分探索，尤其是在处理具有空间稀疏性和时间冗余的骨架数据时。&lt;br&gt;&lt;h4&gt;5. 新方法提出&lt;/h4&gt;   - 提出**Masked Conditional Diffusion (MacDiff)**，作为统一的骨架建模框架。&lt;br&gt;&lt;h4&gt;6. 方法细节&lt;/h4&gt;   - 首次利用扩散模型作为有效的骨架表示学习工具，通过条件训练扩散解码器与语义编码器提取的表示相结合。&lt;br&gt;&lt;h4&gt;7. 随机掩蔽技术&lt;/h4&gt;   - 在编码器输入上应用随机掩蔽，引入信息瓶颈，去除骨架的冗余信息。&lt;br&gt;&lt;h4&gt;8. 理论支持&lt;/h4&gt;   - 理论上证明了我们的生成目标涉及对比学习目标，能够对齐掩蔽和噪声视图，同时促使表示互补，为噪声视图提供支持，从而提高泛化性能。&lt;br&gt;&lt;h4&gt;9. 性能表现&lt;/h4&gt;   - MacDiff在表示学习基准上实现了最先进的性能，同时保持生成任务的能力。&lt;br&gt;&lt;h4&gt;10. 数据增强应用&lt;/h4&gt;    - 利用扩散模型进行数据增强，在标注数据稀缺的场景中显著提升了微调性能。&lt;br&gt;&lt;h4&gt;11. 项目链接&lt;/h4&gt;    - 项目可访问链接：[MacDiff项目页面](https://lehongwu.github.io/ECCV24MacDiff/)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised learning has proved effective for skeleton-based human action
understanding. However, previous works either rely on contrastive learning that
suffers false negative problems or are based on reconstruction that learns too
much unessential low-level clues, leading to limited representations for
downstream tasks. Recently, great advances have been made in generative
learning, which is naturally a challenging yet meaningful pretext task to model
the general underlying data distributions. However, the representation learning
capacity of generative models is under-explored, especially for the skeletons
with spacial sparsity and temporal redundancy. To this end, we propose Masked
Conditional Diffusion (MacDiff) as a unified framework for human skeleton
modeling. For the first time, we leverage diffusion models as effective
skeleton representation learners. Specifically, we train a diffusion decoder
conditioned on the representations extracted by a semantic encoder. Random
masking is applied to encoder inputs to introduce a information bottleneck and
remove redundancy of skeletons. Furthermore, we theoretically demonstrate that
our generative objective involves the contrastive learning objective which
aligns the masked and noisy views. Meanwhile, it also enforces the
representation to complement for the noisy view, leading to better
generalization performance. MacDiff achieves state-of-the-art performance on
representation learning benchmarks while maintaining the competence for
generative tasks. Moreover, we leverage the diffusion model for data
augmentation, significantly enhancing the fine-tuning performance in scenarios
with scarce labeled data. Our project is available at
https://lehongwu.github.io/ECCV24MacDiff/.</description>
      <guid isPermaLink="false">2409.10473v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>CenterRadarNet: Joint 3D Object Detection and Tracking Framework using 4D FMCW Radar</title>
      <link>http://arxiv.org/abs/2311.01423v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 可靠的感知是确保安全自主驾驶和辅助驾驶的重要组成部分。&lt;br&gt;&lt;h4&gt;2. 技术优势&lt;/h4&gt;   - 汽车雷达（77至81 GHz）提供了抗天气干扰的传感能力，补充了基于视觉或激光雷达的自动驾驶系统。&lt;br&gt;&lt;h4&gt;3. 数据特性&lt;/h4&gt;   - 原始射频（RF）雷达张量包含丰富的时空语义信息，除了3D位置信息之外。&lt;br&gt;&lt;h4&gt;4. 现有方法的局限性&lt;/h4&gt;   - 大多数现有方法处理3D（多普勒-距离-方位）RF雷达张量，能够预测物体的位置信息、航向角和大小，但缺乏同时推断物体在3D空间中的大小、方向和身份的能力。&lt;br&gt;&lt;h4&gt;5. 新方法介绍&lt;/h4&gt;   - 本文提出了一种高效的联合架构，称为**CenterRadarNet**，旨在从4D（多普勒-距离-方位-高度）雷达数据中促进高分辨率表示学习，以实现3D目标检测和重新识别（re-ID）任务。&lt;br&gt;&lt;h4&gt;6. 架构特性&lt;/h4&gt;   - CenterRadarNet作为单阶段3D目标检测器，直接推断鸟瞰图（BEV）中的目标分布置信度图、相应的3D边界框属性和每个像素的外观嵌入。&lt;br&gt;&lt;h4&gt;7. 在线跟踪功能&lt;/h4&gt;   - 同时构建了一个在线跟踪器，利用学习到的外观嵌入进行重新识别。&lt;br&gt;&lt;h4&gt;8. 实验成果&lt;/h4&gt;   - CenterRadarNet在K-Radar 3D目标检测基准上达到了最先进的结果，并首次在K-Radar数据集V2上展示了使用雷达进行3D目标跟踪的结果。&lt;br&gt;&lt;h4&gt;9. 性能表现&lt;/h4&gt;   - 在多样化的驾驶场景中，CenterRadarNet表现出一致且可靠的性能，强调了其广泛的适用性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2023-11-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robust perception is a vital component for ensuring safe autonomous and
assisted driving. Automotive radar (77 to 81 GHz), which offers
weather-resilient sensing, provides a complementary capability to the vision-
or LiDAR-based autonomous driving systems. Raw radio-frequency (RF) radar
tensors contain rich spatiotemporal semantics besides 3D location information.
The majority of previous methods take in 3D (Doppler-range-azimuth) RF radar
tensors, allowing prediction of an object's location, heading angle, and size
in bird's-eye-view (BEV). However, they lack the ability to at the same time
infer objects' size, orientation, and identity in the 3D space. To overcome
this limitation, we propose an efficient joint architecture called
CenterRadarNet, designed to facilitate high-resolution representation learning
from 4D (Doppler-range-azimuth-elevation) radar data for 3D object detection
and re-identification (re-ID) tasks. As a single-stage 3D object detector,
CenterRadarNet directly infers the BEV object distribution confidence maps,
corresponding 3D bounding box attributes, and appearance embedding for each
pixel. Moreover, we build an online tracker utilizing the learned appearance
embedding for re-ID. CenterRadarNet achieves the state-of-the-art result on the
K-Radar 3D object detection benchmark. In addition, we present the first 3D
object-tracking result using radar on the K-Radar dataset V2. In diverse
driving scenarios, CenterRadarNet shows consistent, robust performance,
emphasizing its wide applicability.</description>
      <guid isPermaLink="false">2311.01423v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Interactive Masked Image Modeling for Multimodal Object Detection in Remote Sensing</title>
      <link>http://arxiv.org/abs/2409.08885v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 远程感知图像中的目标检测在多种地球观测应用中发挥着重要作用。&lt;br&gt;&lt;h4&gt;2. 挑战&lt;/h4&gt;   - 与自然场景图像的目标检测相比，远程感知中目标检测面临更大挑战，因为存在大量小型且常常几乎不可见的物体，分布在多样的地形中。&lt;br&gt;&lt;h4&gt;3. 解决方案&lt;/h4&gt;   - 提出使用多模态学习来整合不同数据模态的特征，从而提高检测准确性。&lt;br&gt;&lt;h4&gt;4. 数据限制&lt;/h4&gt;   - 然而，多模态学习的性能往往受到标注数据集规模有限的制约。&lt;br&gt;&lt;h4&gt;5. 预训练技术&lt;/h4&gt;   - 本文建议使用掩蔽图像建模（MIM）作为预训练技术，通过自监督学习利用未标注数据来增强检测性能。&lt;br&gt;&lt;h4&gt;6. 传统MIM的局限&lt;/h4&gt;   - 传统的MIM方法（如MAE）使用掩蔽标记而没有上下文信息，难以捕捉细粒度的细节，因为缺乏与图像其他部分的交互。&lt;br&gt;&lt;h4&gt;7. 新方法提出&lt;/h4&gt;   - 本文提出了一种新的交互式MIM方法，能够在不同标记之间建立交互，这对于远程感知中的目标检测尤其有益。&lt;br&gt;&lt;h4&gt;8. 实验验证&lt;/h4&gt;   - 大量消融研究和评估实验证明了我们方法的有效性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Object detection in remote sensing imagery plays a vital role in various
Earth observation applications. However, unlike object detection in natural
scene images, this task is particularly challenging due to the abundance of
small, often barely visible objects across diverse terrains. To address these
challenges, multimodal learning can be used to integrate features from
different data modalities, thereby improving detection accuracy. Nonetheless,
the performance of multimodal learning is often constrained by the limited size
of labeled datasets. In this paper, we propose to use Masked Image Modeling
(MIM) as a pre-training technique, leveraging self-supervised learning on
unlabeled data to enhance detection performance. However, conventional MIM such
as MAE which uses masked tokens without any contextual information, struggles
to capture the fine-grained details due to a lack of interactions with other
parts of image. To address this, we propose a new interactive MIM method that
can establish interactions between different tokens, which is particularly
beneficial for object detection in remote sensing. The extensive ablation
studies and evluation demonstrate the effectiveness of our approach.</description>
      <guid isPermaLink="false">2409.08885v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Advances and Challenges in Meta-Learning: A Technical Review</title>
      <link>http://arxiv.org/abs/2307.04722v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 元学习使学习系统能够从多个任务中获取知识，从而加速适应新任务和提高泛化能力。&lt;br&gt;&lt;h4&gt;2. 综述目的&lt;/h4&gt;   - 本文提供了元学习的全面技术概述，强调其在数据稀缺或获取成本高的实际应用中的重要性。&lt;br&gt;&lt;h4&gt;3. 研究范围&lt;/h4&gt;   - 论文涵盖了最先进的元学习方法，并探讨了元学习与以下领域之间的关系：&lt;br&gt;     - 多任务学习&lt;br&gt;     - 迁移学习&lt;br&gt;     - 领域适应与泛化&lt;br&gt;     - 自监督学习&lt;br&gt;     - 个性化联邦学习&lt;br&gt;     - 持续学习&lt;br&gt;&lt;h4&gt;4. 主题协同&lt;/h4&gt;   - 通过强调这些主题与元学习领域之间的协同作用，论文展示了一个领域的进展如何促进整体发展，并避免不必要的重复努力。&lt;br&gt;&lt;h4&gt;5. 高级主题探讨&lt;/h4&gt;   - 论文深入探讨了几个高级元学习主题，包括：&lt;br&gt;     - 从复杂的多模态任务分布中学习&lt;br&gt;     - 无监督元学习&lt;br&gt;     - 有效适应数据分布变化的学习&lt;br&gt;     - 持续元学习&lt;br&gt;&lt;h4&gt;6. 未来研究方向&lt;/h4&gt;   - 文章指出了元学习领域的开放问题和挑战，为未来的研究提供了方向。&lt;br&gt;&lt;h4&gt;7. 研究贡献&lt;/h4&gt;   - 通过综合最新的研究进展，本文提供了对元学习及其在各种机器学习应用中潜在影响的深入理解，旨在推动元学习的发展及其实际应用。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2023-07-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Meta-learning empowers learning systems with the ability to acquire knowledge
from multiple tasks, enabling faster adaptation and generalization to new
tasks. This review provides a comprehensive technical overview of
meta-learning, emphasizing its importance in real-world applications where data
may be scarce or expensive to obtain. The paper covers the state-of-the-art
meta-learning approaches and explores the relationship between meta-learning
and multi-task learning, transfer learning, domain adaptation and
generalization, self-supervised learning, personalized federated learning, and
continual learning. By highlighting the synergies between these topics and the
field of meta-learning, the paper demonstrates how advancements in one area can
benefit the field as a whole, while avoiding unnecessary duplication of
efforts. Additionally, the paper delves into advanced meta-learning topics such
as learning from complex multi-modal task distributions, unsupervised
meta-learning, learning to efficiently adapt to data distribution shifts, and
continual meta-learning. Lastly, the paper highlights open problems and
challenges for future research in the field. By synthesizing the latest
research developments, this paper provides a thorough understanding of
meta-learning and its potential impact on various machine learning
applications. We believe that this technical overview will contribute to the
advancement of meta-learning and its practical implications in addressing
real-world problems.</description>
      <guid isPermaLink="false">2307.04722v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Augment, Drop &amp; Swap: Improving Diversity in LLM Captions for Efficient Music-Text Representation Learning</title>
      <link>http://arxiv.org/abs/2409.11498v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To appear in the Proceedings of the 25th International Society for
  Music Information Retrieval Conference (ISMIR 2024)&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 音频-文本对比模型在音乐表示学习中已成为一种强有力的方法，但对关键设计选择对音乐-文本表示质量影响的了解仍然有限。&lt;br&gt;&lt;h4&gt;2. 研究目标&lt;/h4&gt;   - 本文旨在揭示在有限数据和计算预算下，设计选择对音乐-文本表示学习的影响，并通过实证观察建立更扎实的理解。&lt;br&gt;&lt;h4&gt;3. 研究维度&lt;/h4&gt;   - 研究从三个方面分析影响：&lt;br&gt;     - 基础编码器的选择&lt;br&gt;     - 训练数据的整理程度&lt;br&gt;     - 文本增强的使用&lt;br&gt;&lt;h4&gt;4. 主要发现&lt;/h4&gt;   - 数据整理是资源受限场景中音乐-文本对比训练中最重要的因素。&lt;br&gt;&lt;h4&gt;5. 新技术引入&lt;/h4&gt;   - 为了提高文本输入的多样性和描述性，提出了两种新技术：&lt;br&gt;     - **增强视图丢弃（Augmented View Dropout）**&lt;br&gt;     - **文本交换（TextSwap）**&lt;br&gt;&lt;h4&gt;6. 实验验证&lt;/h4&gt;   - 通过实验证明，这些新技术在不同的预训练方案、模型架构和下游数据分布中有效提升了性能。&lt;br&gt;&lt;h4&gt;7. 成本效益&lt;/h4&gt;   - 新方法的引入没有增加计算成本或需要额外的训练数据。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Audio-text contrastive models have become a powerful approach in music
representation learning. Despite their empirical success, however, little is
known about the influence of key design choices on the quality of music-text
representations learnt through this framework. In this work, we expose these
design choices within the constraints of limited data and computation budgets,
and establish a more solid understanding of their impact grounded in empirical
observations along three axes: the choice of base encoders, the level of
curation in training data, and the use of text augmentation. We find that data
curation is the single most important factor for music-text contrastive
training in resource-constrained scenarios. Motivated by this insight, we
introduce two novel techniques, Augmented View Dropout and TextSwap, which
increase the diversity and descriptiveness of text inputs seen in training.
Through our experiments we demonstrate that these are effective at boosting
performance across different pre-training regimes, model architectures, and
downstream data distributions, without incurring higher computational costs or
requiring additional training data.</description>
      <guid isPermaLink="false">2409.11498v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>D3Former: Jointly Learning Repeatable Dense Detectors and Feature-enhanced Descriptors via Saliency-guided Transformer</title>
      <link>http://arxiv.org/abs/2312.12970v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 6 figures&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 精确和具有代表性的匹配是解决点云配准问题的关键步骤。&lt;br&gt;&lt;h4&gt;2. 常见方法的局限性&lt;/h4&gt;   - 现有方法通常通过检测具有显著几何特征的关键点，并将这些关键点从一个点云帧映射到另一个帧。然而，这些方法受到采样关键点重复性的限制。&lt;br&gt;&lt;h4&gt;3. 新方法介绍&lt;/h4&gt;   - 本文提出了一种基于显著性引导的变换器，称为**D3Former**，该方法联合学习可重复的**D**ense **D**etectors和特征增强的**D**escriptors。&lt;br&gt;&lt;h4&gt;4. 模型组成&lt;/h4&gt;   - D3Former模型包括两个模块：&lt;br&gt;     - **特征增强描述子学习（FEDL）**：利用区域注意机制来增强特征的独特性。&lt;br&gt;     - **重复关键点检测学习（RKDL）**：专注于检测可重复的关键点，以增强匹配能力。&lt;br&gt;&lt;h4&gt;5. 实验结果&lt;/h4&gt;   - 在具有挑战性的室内和室外基准测试中，实验结果表明该方法在点云匹配方面始终优于当前最先进的方法。&lt;br&gt;&lt;h4&gt;6. 具体表现&lt;/h4&gt;   - 在3DLoMatch测试中，即使在低重叠率下，D3Former也持续优于近期发表的方法，如RoReg和RoITr。&lt;br&gt;&lt;h4&gt;7. 性能对比&lt;/h4&gt;   - 当提取的关键点数量减少到250时，RoReg、RoITr和我们的方法的配准召回率分别为64.3%、73.6%和76.5%。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2023-12-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Establishing accurate and representative matches is a crucial step in
addressing the point cloud registration problem. A commonly employed approach
involves detecting keypoints with salient geometric features and subsequently
mapping these keypoints from one frame of the point cloud to another. However,
methods within this category are hampered by the repeatability of the sampled
keypoints. In this paper, we introduce a saliency-guided trans\textbf{former},
referred to as \textit{D3Former}, which entails the joint learning of
repeatable \textbf{D}ense \textbf{D}etectors and feature-enhanced
\textbf{D}escriptors. The model comprises a Feature Enhancement Descriptor
Learning (FEDL) module and a Repetitive Keypoints Detector Learning (RKDL)
module. The FEDL module utilizes a region attention mechanism to enhance
feature distinctiveness, while the RKDL module focuses on detecting repeatable
keypoints to enhance matching capabilities. Extensive experimental results on
challenging indoor and outdoor benchmarks demonstrate that our proposed method
consistently outperforms state-of-the-art point cloud matching methods.
Notably, tests on 3DLoMatch, even with a low overlap ratio, show that our
method consistently outperforms recently published approaches such as RoReg and
RoITr. For instance, with the number of extracted keypoints reduced to 250, the
registration recall scores for RoReg, RoITr, and our method are 64.3\%, 73.6\%,
and 76.5\%, respectively.</description>
      <guid isPermaLink="false">2312.12970v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Graph Neural Network-State Predictive Information Bottleneck (GNN-SPIB) approach for learning molecular thermodynamics and kinetics</title>
      <link>http://arxiv.org/abs/2409.11843v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 分子动力学模拟提供了对原子运动的详细洞察，但面临时间尺度的限制。&lt;br&gt;&lt;h4&gt;2. 增强采样方法的挑战&lt;/h4&gt;   - 尽管增强采样方法已解决了一些挑战，但即使结合机器学习，它们通常依赖于预先选择的专家特征。&lt;br&gt;&lt;h4&gt;3. 新框架介绍&lt;/h4&gt;   - 本文提出了图神经网络-状态预测信息瓶颈（GNN-SPIB）框架，结合了图神经网络和状态预测信息瓶颈。&lt;br&gt;&lt;h4&gt;4. 自动学习表示&lt;/h4&gt;   - GNN-SPIB框架能够直接从原子坐标中自动学习低维表示。&lt;br&gt;&lt;h4&gt;5. 基准测试&lt;/h4&gt;   - 在三个基准系统上进行测试，表明该方法能够预测慢过程的基本结构、热力学和动力学信息。&lt;br&gt;&lt;h4&gt;6. 鲁棒性&lt;/h4&gt;   - 方法在不同系统中表现出稳定性和鲁棒性。&lt;br&gt;&lt;h4&gt;7. 复杂系统应用&lt;/h4&gt;   - 该方法在复杂系统中显示出潜力，能够有效进行增强采样，而无需预定义的反应坐标或输入特征。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Molecular dynamics simulations offer detailed insights into atomic motions
but face timescale limitations. Enhanced sampling methods have addressed these
challenges but even with machine learning, they often rely on pre-selected
expert-based features. In this work, we present the Graph Neural Network-State
Predictive Information Bottleneck (GNN-SPIB) framework, which combines graph
neural networks and the State Predictive Information Bottleneck to
automatically learn low-dimensional representations directly from atomic
coordinates. Tested on three benchmark systems, our approach predicts essential
structural, thermodynamic and kinetic information for slow processes,
demonstrating robustness across diverse systems. The method shows promise for
complex systems, enabling effective enhanced sampling without requiring
pre-defined reaction coordinates or input features.</description>
      <guid isPermaLink="false">2409.11843v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>RF-GML: Reference-Free Generative Machine Listener</title>
      <link>http://arxiv.org/abs/2409.10210v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Pre-review version submitted to ICASSP 2025&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究目的&lt;/h4&gt;   - 本文介绍了一种新颖的无参考（RF）音频质量度量标准，称为RF-Generative Machine Listener (RF-GML)，用于评估编码的单声道、立体声和双耳音频，采样率为48 kHz。&lt;br&gt;&lt;h4&gt;2. 模型基础&lt;/h4&gt;   - RF-GML基于现有的全参考（FR）生成机器监听器（GML），通过最小的架构修改进行迁移学习。&lt;br&gt;&lt;h4&gt;3. 生成能力&lt;/h4&gt;   - "生成"一词指的是该模型能够生成任意数量的模拟听感评分。&lt;br&gt;&lt;h4&gt;4. 预测准确性&lt;/h4&gt;   - 与现有的RF模型不同，RF-GML能够准确预测不同内容类型和编码的主观质量评分。&lt;br&gt;&lt;h4&gt;5. 广泛评估&lt;/h4&gt;   - 大量评估表明，RF-GML在对未编码音频的评级和区分不同编码伪影方面表现优越。&lt;br&gt;&lt;h4&gt;6. 应用价值&lt;/h4&gt;   - RF-GML的性能和多功能性使其成为在各种应用中评估和监测编码音频质量的有价值工具，且无需参考信号。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces a novel reference-free (RF) audio quality metric called
the RF-Generative Machine Listener (RF-GML), designed to evaluate coded mono,
stereo, and binaural audio at a 48 kHz sample rate. RF-GML leverages transfer
learning from a state-of-the-art full-reference (FR) Generative Machine
Listener (GML) with minimal architectural modifications. The term "generative"
refers to the model's ability to generate an arbitrary number of simulated
listening scores. Unlike existing RF models, RF-GML accurately predicts
subjective quality scores across diverse content types and codecs. Extensive
evaluations demonstrate its superiority in rating unencoded audio and
distinguishing different levels of coding artifacts. RF-GML's performance and
versatility make it a valuable tool for coded audio quality assessment and
monitoring in various applications, all without the need for a reference
signal.</description>
      <guid isPermaLink="false">2409.10210v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Correlation Siamese Transformer Network with Dense Connection for 3D Single Object Tracking</title>
      <link>http://arxiv.org/abs/2312.11051v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint version for IEEE Robotics and Automation Letters (RAL)&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 基于点云的3D物体跟踪是自动驾驶中的重要任务，尽管Siamese网络在3D跟踪方面取得了显著进展，但有效学习模板和搜索分支之间的关联仍然具有挑战性，尤其是在稀疏的激光雷达点云数据下。&lt;br&gt;&lt;h4&gt;2. 研究创新&lt;/h4&gt;   - 本文提出了一种多相关性Siamese Transformer网络，具有多个阶段，在每个阶段的末尾基于稀疏柱体进行特征相关性计算，而不是仅在网络中的一个点进行。&lt;br&gt;&lt;h4&gt;3. 自注意力与交叉注意力&lt;/h4&gt;   - 在每个阶段，首先对每个分支应用自注意力，以捕获非局部上下文信息，然后使用交叉注意力将模板信息注入到搜索区域中。&lt;br&gt;&lt;h4&gt;4. 特征学习策略&lt;/h4&gt;   - 这种策略使得搜索区域的特征学习能够关注模板，同时保持模板的个体特征不变。&lt;br&gt;&lt;h4&gt;5. 密集连接设计&lt;/h4&gt;   - 为了便于网络保留不同阶段学习的信息，并简化优化过程，搜索区域的初始输入稀疏柱体与每个阶段的输出之间进行密集连接，连接至所有后续阶段和目标定位网络。&lt;br&gt;&lt;h4&gt;6. 鸟瞰图特征映射&lt;/h4&gt;   - 目标定位网络将柱体转换为鸟瞰图（BEV）特征图，并使用小型密集连接卷积网络预测目标状态。&lt;br&gt;&lt;h4&gt;7. 深度监督&lt;/h4&gt;   - 在每个阶段添加深度监督，以进一步提升性能。&lt;br&gt;&lt;h4&gt;8. 实验评估&lt;/h4&gt;   - 提出的算法在KITTI、nuScenes和Waymo等流行数据集上进行了评估，实验结果显示方法相较于当前最先进的技术具有良好性能。&lt;br&gt;&lt;h4&gt;9. 消融研究&lt;/h4&gt;   - 提供了消融研究，以展示各个组件的有效性。&lt;br&gt;&lt;h4&gt;10. 代码可用性&lt;/h4&gt;    - 相关代码可在 [GitHub](https://github.com/liangp/MCSTN-3DSOT) 上获取。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2023-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/LRA.2023.3325715&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/liangp/mcstn-3dsot&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud-based 3D object tracking is an important task in autonomous
driving. Though great advances regarding Siamese-based 3D tracking have been
made recently, it remains challenging to learn the correlation between the
template and search branches effectively with the sparse LIDAR point cloud
data. Instead of performing correlation of the two branches at just one point
in the network, in this paper, we present a multi-correlation Siamese
Transformer network that has multiple stages and carries out feature
correlation at the end of each stage based on sparse pillars. More
specifically, in each stage, self-attention is first applied to each branch
separately to capture the non-local context information. Then, cross-attention
is used to inject the template information into the search area. This strategy
allows the feature learning of the search area to be aware of the template
while keeping the individual characteristics of the template intact. To enable
the network to easily preserve the information learned at different stages and
ease the optimization, for the search area, we densely connect the initial
input sparse pillars and the output of each stage to all subsequent stages and
the target localization network, which converts pillars to bird's eye view
(BEV) feature maps and predicts the state of the target with a small densely
connected convolution network. Deep supervision is added to each stage to
further boost the performance as well. The proposed algorithm is evaluated on
the popular KITTI, nuScenes, and Waymo datasets, and the experimental results
show that our method achieves promising performance compared with the
state-of-the-art. Ablation study that shows the effectiveness of each component
is provided as well. Code is available at
https://github.com/liangp/MCSTN-3DSOT.</description>
      <guid isPermaLink="false">2312.11051v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Self-supervised 3D Point Cloud Completion via Multi-view Adversarial Learning</title>
      <link>http://arxiv.org/abs/2407.09786v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages,8 figures&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 在实际场景中，扫描的点云常因遮挡问题而不完整，导致自监督点云补全的任务需要在没有完整地面真相的情况下重建缺失区域。&lt;br&gt;&lt;h4&gt;2. 现有方法问题&lt;/h4&gt;   - 当前的自监督方法通常依赖于多个部分观察的视图进行监督，或忽略了可以从给定部分点云中识别和利用的内在几何相似性。&lt;br&gt;&lt;h4&gt;3. 提出的方法&lt;/h4&gt;   - 本文提出了MAL-SPC框架，利用物体级别和类别特定的几何相似性来完成缺失结构的重建。&lt;br&gt;&lt;h4&gt;4. 无监督学习&lt;/h4&gt;   - MAL-SPC不需要任何3D完整监督，只需每个对象的单一部分点云。&lt;br&gt;&lt;h4&gt;5. 模式检索网络&lt;/h4&gt;   - 首先引入模式检索网络，检索部分输入和预测形状之间的相似位置和曲率模式，以此增强和细化重建结果。&lt;br&gt;&lt;h4&gt;6. 多视图深度图渲染&lt;/h4&gt;   - 将重建的完整形状渲染为多视图深度图，并设计对抗学习模块，从类别特定的单视图深度图像中学习目标形状的几何特性。&lt;br&gt;&lt;h4&gt;7. 密度感知半径估计算法&lt;/h4&gt;   - 为实现各向异性渲染，设计了一种密度感知半径估计算法，以提高渲染图像的质量。&lt;br&gt;&lt;h4&gt;8. 实验结果&lt;/h4&gt;   - MAL-SPC在与当前最先进方法的比较中，获得了最佳结果。&lt;br&gt;&lt;h4&gt;9. 代码发布&lt;/h4&gt;   - 相关源代码将公开发布在 [GitHub](https://github.com/ltwu6/malspc)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-07-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/ltwu6/malspc&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In real-world scenarios, scanned point clouds are often incomplete due to
occlusion issues. The task of self-supervised point cloud completion involves
reconstructing missing regions of these incomplete objects without the
supervision of complete ground truth. Current self-supervised methods either
rely on multiple views of partial observations for supervision or overlook the
intrinsic geometric similarity that can be identified and utilized from the
given partial point clouds. In this paper, we propose MAL-SPC, a framework that
effectively leverages both object-level and category-specific geometric
similarities to complete missing structures. Our MAL-SPC does not require any
3D complete supervision and only necessitates a single partial point cloud for
each object. Specifically, we first introduce a Pattern Retrieval Network to
retrieve similar position and curvature patterns between the partial input and
the predicted shape, then leverage these similarities to densify and refine the
reconstructed results. Additionally, we render the reconstructed complete shape
into multi-view depth maps and design an adversarial learning module to learn
the geometry of the target shape from category-specific single-view depth
images. To achieve anisotropic rendering, we design a density-aware radius
estimation algorithm to improve the quality of the rendered images. Our MAL-SPC
yields the best results compared to current state-of-the-art methods.We will
make the source code publicly available at \url{https://github.com/ltwu6/malspc</description>
      <guid isPermaLink="false">2407.09786v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Online Nonconvex Bilevel Optimization with Bregman Divergences</title>
      <link>http://arxiv.org/abs/2409.10470v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 双层优化方法在机器学习中变得越来越重要，特别是在超参数优化和元学习等任务中。&lt;br&gt;&lt;h4&gt;2. 在线双层优化&lt;/h4&gt;   - 与离线设置相比，在线双层优化（OBO）提供了一个更动态的框架，能够处理时变函数和顺序到达的数据。&lt;br&gt;&lt;h4&gt;3. 研究问题&lt;/h4&gt;   - 本研究针对在线非凸-强凸双层优化问题。&lt;br&gt;&lt;h4&gt;4. 新方法介绍&lt;/h4&gt;   - 在确定性设置中，提出了一种新颖的在线Bregman双层优化器（OBBO），利用自适应Bregman散度。&lt;br&gt;&lt;h4&gt;5. 性能提升&lt;/h4&gt;   - 证明OBBO通过新颖的超梯度误差分解方法改善了已知的双层局部遗憾的次线性速率，适应了问题的底层几何特性。&lt;br&gt;&lt;h4&gt;6. 随机上下文中的方法&lt;/h4&gt;   - 在随机上下文中，首次引入随机在线双层优化器（SOBBO），采用窗口平均方法更新外层变量，利用最近超梯度的加权平均进行随机近似。&lt;br&gt;&lt;h4&gt;7. 效果与优势&lt;/h4&gt;   - 该方法不仅实现了双层局部遗憾的次线性速率，还作为有效的方差减少策略，消除了在每个时间步需要额外随机梯度样本的需求。&lt;br&gt;&lt;h4&gt;8. 实验结果&lt;/h4&gt;   - 在在线超参数优化和在线元学习方面的实验表明，与现有的在线和离线双层基准相比，所提出的基于Bregman的算法在性能、效率和适应性上具有显著优势。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Bilevel optimization methods are increasingly relevant within machine
learning, especially for tasks such as hyperparameter optimization and
meta-learning. Compared to the offline setting, online bilevel optimization
(OBO) offers a more dynamic framework by accommodating time-varying functions
and sequentially arriving data. This study addresses the online
nonconvex-strongly convex bilevel optimization problem. In deterministic
settings, we introduce a novel online Bregman bilevel optimizer (OBBO) that
utilizes adaptive Bregman divergences. We demonstrate that OBBO enhances the
known sublinear rates for bilevel local regret through a novel hypergradient
error decomposition that adapts to the underlying geometry of the problem. In
stochastic contexts, we introduce the first stochastic online bilevel optimizer
(SOBBO), which employs a window averaging method for updating outer-level
variables using a weighted average of recent stochastic approximations of
hypergradients. This approach not only achieves sublinear rates of bilevel
local regret but also serves as an effective variance reduction strategy,
obviating the need for additional stochastic gradient samples at each timestep.
Experiments on online hyperparameter optimization and online meta-learning
highlight the superior performance, efficiency, and adaptability of our
Bregman-based algorithms compared to established online and offline bilevel
benchmarks.</description>
      <guid isPermaLink="false">2409.10470v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Speaker-IPL: Unsupervised Learning of Speaker Characteristics with i-Vector based Pseudo-Labels</title>
      <link>http://arxiv.org/abs/2409.10791v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to ICASSP 2025&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究主题&lt;/h4&gt;   - 迭代自我训练（IPL）是一种利用当前迭代改进模型为下一迭代提供伪标签的方法，已被证明能有效提升说话人表示的质量。&lt;br&gt;&lt;h4&gt;2. 背景&lt;/h4&gt;   - 近期在无监督说话人识别中的IPL应用起始于从复杂的自监督方法（如DINO）提取的表示。&lt;br&gt;&lt;h4&gt;3. 训练挑战&lt;/h4&gt;   - 训练强大的自监督模型并不简单，涉及超参数调优，且可能无法对域外数据进行良好的泛化。&lt;br&gt;&lt;h4&gt;4. 研究贡献&lt;/h4&gt;   - 本研究表明，简单且成熟的i-vector生成模型足以启动无监督学习的IPL过程，以学习说话人表示。&lt;br&gt;&lt;h4&gt;5. 组件影响分析&lt;/h4&gt;   - 系统研究了影响IPL过程的其他组件，包括初始模型、编码器、数据增强、聚类数量和聚类算法。&lt;br&gt;&lt;h4&gt;6. 实验发现&lt;/h4&gt;   - 即使使用简单且显著较弱的初始模型（i-vector），IPL仍能实现与最先进方法相当的说话人验证性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Iterative self-training, or iterative pseudo-labeling (IPL)--using an
improved model from the current iteration to provide pseudo-labels for the next
iteration--has proven to be a powerful approach to enhance the quality of
speaker representations. Recent applications of IPL in unsupervised speaker
recognition start with representations extracted from very elaborate
self-supervised methods (e.g., DINO). However, training such strong
self-supervised models is not straightforward (they require hyper-parameters
tuning and may not generalize to out-of-domain data) and, moreover, may not be
needed at all. To this end, we show the simple, well-studied, and established
i-vector generative model is enough to bootstrap the IPL process for
unsupervised learning of speaker representations. We also systematically study
the impact of other components on the IPL process, which includes the initial
model, the encoder, augmentations, the number of clusters, and the clustering
algorithm. Remarkably, we find that even with a simple and significantly weaker
initial model like i-vector, IPL can still achieve speaker verification
performance that rivals state-of-the-art methods.</description>
      <guid isPermaLink="false">2409.10791v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>SRIF: Semantic Shape Registration Empowered by Diffusion-based Image Morphing and Flow Estimation</title>
      <link>http://arxiv.org/abs/2409.11682v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究目标&lt;/h4&gt;   - 提出SRIF，一种基于扩散图像变形和流估计的新颖语义形状配准框架。&lt;br&gt;&lt;h4&gt;2. 方法概述&lt;/h4&gt;   - 对于一对外部对齐的形状，首先从多个视角渲染它们，然后利用基于扩散模型的图像插值框架生成它们之间的中间图像序列。&lt;br&gt;&lt;h4&gt;3. 动态3D高斯点云重建&lt;/h4&gt;   - 将生成的中间图像输入到动态3D高斯点云喷射框架中，进行重建和后处理，以符合图像变形处理。&lt;br&gt;&lt;h4&gt;4. 注册模块&lt;/h4&gt;   - 提出一种新的注册模块，用于估计连续归一化流，使源形状一致地变形为目标形状，使用中间点云作为弱指导。&lt;br&gt;&lt;h4&gt;5. 关键见解&lt;/h4&gt;   - 利用大型视觉模型（LVMs）关联形状，从而获取比传统特征提取和对齐更丰富的语义信息。&lt;br&gt;&lt;h4&gt;6. 结果&lt;/h4&gt;   - SRIF在处理挑战性形状对时实现高质量的密集对应关系，并在两者之间提供平滑且语义上有意义的插值。&lt;br&gt;&lt;h4&gt;7. 实证支持&lt;/h4&gt;   - 实验结果证实了该方法的有效性和优越性，以及特定设计选择的合理性。&lt;br&gt;&lt;h4&gt;8. 代码发布&lt;/h4&gt;   - 相关代码已在[GitHub](https://github.com/rqhuang88/SRIF)上公开。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we propose SRIF, a novel Semantic shape Registration framework
based on diffusion-based Image morphing and Flow estimation. More concretely,
given a pair of extrinsically aligned shapes, we first render them from
multi-views, and then utilize an image interpolation framework based on
diffusion models to generate sequences of intermediate images between them. The
images are later fed into a dynamic 3D Gaussian splatting framework, with which
we reconstruct and post-process for intermediate point clouds respecting the
image morphing processing. In the end, tailored for the above, we propose a
novel registration module to estimate continuous normalizing flow, which
deforms source shape consistently towards the target, with intermediate point
clouds as weak guidance. Our key insight is to leverage large vision models
(LVMs) to associate shapes and therefore obtain much richer semantic
information on the relationship between shapes than the ad-hoc feature
extraction and alignment. As a consequence, SRIF achieves high-quality dense
correspondences on challenging shape pairs, but also delivers smooth,
semantically meaningful interpolation in between. Empirical evidence justifies
the effectiveness and superiority of our method as well as specific design
choices. The code is released at https://github.com/rqhuang88/SRIF.</description>
      <guid isPermaLink="false">2409.11682v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Exploring Active Learning in Meta-Learning: Enhancing Context Set Labeling</title>
      <link>http://arxiv.org/abs/2311.02879v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ECCV2024&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 大多数元学习方法假设在测试时使用的（非常小的）上下文集是被动提供的。&lt;br&gt;&lt;h4&gt;2. 主动选择的潜力&lt;/h4&gt;   - 在某些情况下，可以主动选择标记哪些点，这种选择的潜在收益显著，但与典型的主动学习设置有重大差异。&lt;br&gt;&lt;h4&gt;3. 主动元学习的框架&lt;/h4&gt;   - 阐明了主动元学习如何用于标记上下文集，具体取决于元学习过程中的哪些部分使用主动学习。&lt;br&gt;&lt;h4&gt;4. 提出的算法&lt;/h4&gt;   - 提出了一种基于拟合高斯混合的自然算法，用于选择标记点；该算法虽然简单，但具有理论动机。&lt;br&gt;&lt;h4&gt;5. 实验结果&lt;/h4&gt;   - 在与多种元学习算法结合使用时，该算法在多个基准数据集上超越了最先进的主动学习方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2023-11-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Most meta-learning methods assume that the (very small) context set used to
establish a new task at test time is passively provided. In some settings,
however, it is feasible to actively select which points to label; the potential
gain from a careful choice is substantial, but the setting requires major
differences from typical active learning setups. We clarify the ways in which
active meta-learning can be used to label a context set, depending on which
parts of the meta-learning process use active learning. Within this framework,
we propose a natural algorithm based on fitting Gaussian mixtures for selecting
which points to label; though simple, the algorithm also has theoretical
motivation. The proposed algorithm outperforms state-of-the-art active learning
methods when used with various meta-learning algorithms across several
benchmark datasets.</description>
      <guid isPermaLink="false">2311.02879v3</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>VertiEncoder: Self-Supervised Kinodynamic Representation Learning on Vertically Challenging Terrain</title>
      <link>http://arxiv.org/abs/2409.11570v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages. Code: https://github.com/mhnazeri/VertiEncoder&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究目标&lt;/h4&gt;   - 提出VertiEncoder，一种自监督表示学习方法，旨在提高机器人在垂直挑战地形上的移动能力。&lt;br&gt;&lt;h4&gt;2. 多任务处理&lt;/h4&gt;   - VertiEncoder采用相同的预训练过程，可处理四种不同的下游任务：&lt;br&gt;     - 前向运动动力学学习&lt;br&gt;     - 逆向运动动力学学习&lt;br&gt;     - 行为克隆&lt;br&gt;     - 块重建&lt;br&gt;&lt;h4&gt;3. 技术框架&lt;/h4&gt;   - 使用TransformerEncoder，通过随机遮罩和下一个块重建学习周围环境的局部上下文。&lt;br&gt;&lt;h4&gt;4. 性能优势&lt;/h4&gt;   - VertiEncoder在四个不同任务上的表现优于专门的端到端模型，参数减少了77%。&lt;br&gt;&lt;h4&gt;5. 与现有方法比较&lt;/h4&gt;   - 在真实机器人部署中，VertiEncoder在运动动力学建模和规划方面与最先进的方法表现相当。&lt;br&gt;&lt;h4&gt;6. 研究意义&lt;/h4&gt;   - 结果表明，VertiEncoder有效缓解了过拟合问题，并促进了在不同环境上下文和下游车辆运动动力学任务中的更强鲁棒性和泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present VertiEncoder, a self-supervised representation learning approach
for robot mobility on vertically challenging terrain. Using the same
pre-training process, VertiEncoder can handle four different downstream tasks,
including forward kinodynamics learning, inverse kinodynamics learning,
behavior cloning, and patch reconstruction with a single representation.
VertiEncoder uses a TransformerEncoder to learn the local context of its
surroundings by random masking and next patch reconstruction. We show that
VertiEncoder achieves better performance across all four different tasks
compared to specialized End-to-End models with 77% fewer parameters. We also
show VertiEncoder's comparable performance against state-of-the-art kinodynamic
modeling and planning approaches in real-world robot deployment. These results
underscore the efficacy of VertiEncoder in mitigating overfitting and fostering
more robust generalization across diverse environmental contexts and downstream
vehicle kinodynamic tasks.</description>
      <guid isPermaLink="false">2409.11570v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Exploring ChatGPT-based Augmentation Strategies for Contrastive Aspect-based Sentiment Analysis</title>
      <link>http://arxiv.org/abs/2409.11218v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 3 figures&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究主题&lt;/h4&gt;   - 方面基础情感分析（ABSA）旨在识别句子中针对特定方面术语的情感，帮助揭示对产品、服务或话题的细致看法和态度。&lt;br&gt;&lt;h4&gt;2. 挑战&lt;/h4&gt;   - 标注数据的稀缺性对训练高质量模型构成显著挑战。&lt;br&gt;&lt;h4&gt;3. 解决方案&lt;/h4&gt;   - 探索使用ChatGPT这一表现良好的大型语言模型（LLM）进行数据增强，以提升对方面术语的情感分类性能。&lt;br&gt;&lt;h4&gt;4. 数据增强策略&lt;/h4&gt;   - 研究了三种基于ChatGPT的数据增强策略：&lt;br&gt;     - **上下文聚焦数据增强**：改变句子中上下文词的表达，而保持方面术语不变。&lt;br&gt;     - **方面聚焦数据增强**：改变方面术语，保持上下文词不变。&lt;br&gt;     - **上下文-方面数据增强**：结合上述两种增强策略，生成增强样本。&lt;br&gt;&lt;h4&gt;5. 对比学习&lt;/h4&gt;   - 在ABSA任务中引入对比学习，以进一步提升性能。&lt;br&gt;&lt;h4&gt;6. 实验结果&lt;/h4&gt;   - 大量实验表明，所有三种数据增强技术均提升了性能，其中上下文-方面数据增强策略表现最佳，超越了基线模型的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Aspect-based sentiment analysis (ABSA) involves identifying sentiment towards
specific aspect terms in a sentence and allows us to uncover nuanced
perspectives and attitudes on particular aspects of a product, service, or
topic. However, the scarcity of labeled data poses a significant challenge to
training high-quality models. To address this issue, we explore the potential
of data augmentation using ChatGPT, a well-performing large language model
(LLM), to enhance the sentiment classification performance towards aspect
terms. Specifically, we explore three data augmentation strategies based on
ChatGPT: context-focused, aspect-focused, and context-aspect data augmentation
techniques. Context-focused data augmentation focuses on changing the word
expression of context words in the sentence while keeping aspect terms
unchanged. In contrast, aspect-focused data augmentation aims to change aspect
terms but keep context words unchanged. Context-Aspect data augmentation
integrates the above two data augmentations to generate augmented samples.
Furthermore, we incorporate contrastive learning into the ABSA tasks to improve
performance. Extensive experiments show that all three data augmentation
techniques lead to performance improvements, with the context-aspect data
augmentation strategy performing best and surpassing the performance of the
baseline models.</description>
      <guid isPermaLink="false">2409.11218v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Edge-Based Graph Component Pooling</title>
      <link>http://arxiv.org/abs/2409.11856v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, presented at 21st International Workshop on Mining and
  Learning with Graphs, AstraZenica Bio &amp; Healthcare award Paper, ECML PKDD
  2024 Vilnius&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 图结构数据广泛存在于多个研究领域，如化学和社会学，包含的关系信息可用于通过几何深度学习统计建模图属性。&lt;br&gt;&lt;h4&gt;2. 图神经网络&lt;/h4&gt;   - 图神经网络采用消息传递层等技术，通过图传播局部特征。&lt;br&gt;&lt;h4&gt;3. 计算挑战&lt;/h4&gt;   - 在处理大型稀疏图时，消息传递层可能导致计算开销较大。&lt;br&gt;&lt;h4&gt;4. 图池化操作&lt;/h4&gt;   - 图池化操作可以通过移除或合并节点来降低计算成本，但移除节点会导致数据损失，而合并节点往往计算复杂。&lt;br&gt;&lt;h4&gt;5. 提出的解决方案&lt;/h4&gt;   - 提出了一个新的池化操作，旨在合并节点以避免数据损失，同时保持概念简单和计算高效。&lt;br&gt;&lt;h4&gt;6. 实验结果&lt;/h4&gt;   - 实证表明，所提池化操作在四个流行基准数据集上显著优于边池（edge pool），并将时间复杂度和可训练参数数量平均降低70.6%。&lt;br&gt;&lt;h4&gt;7. 与其他方法比较&lt;/h4&gt;   - 相较于另一种强大的方法——图同构网络（Graph Isomorphic Network），在两个流行基准数据集上表现更佳，同时将可学习参数数量平均降低60.9%。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph-structured data naturally occurs in many research fields, such as
chemistry and sociology. The relational information contained therein can be
leveraged to statistically model graph properties through geometrical deep
learning. Graph neural networks employ techniques, such as message-passing
layers, to propagate local features through a graph. However, message-passing
layers can be computationally expensive when dealing with large and sparse
graphs. Graph pooling operators offer the possibility of removing or merging
nodes in such graphs, thus lowering computational costs. However, pooling
operators that remove nodes cause data loss, and pooling operators that merge
nodes are often computationally expensive. We propose a pooling operator that
merges nodes so as not to cause data loss but is also conceptually simple and
computationally inexpensive. We empirically demonstrate that the proposed
pooling operator performs statistically significantly better than edge pool on
four popular benchmark datasets while reducing time complexity and the number
of trainable parameters by 70.6% on average. Compared to another maximally
powerful method named Graph Isomporhic Network, we show that we outperform them
on two popular benchmark datasets while reducing the number of learnable
parameters on average by 60.9%.</description>
      <guid isPermaLink="false">2409.11856v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>GBOT: Graph-Based 3D Object Tracking for Augmented Reality-Assisted Assembly Guidance</title>
      <link>http://arxiv.org/abs/2402.07677v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 组装零件的指导在增强现实（AR）领域具有前景，尤其在医疗和工业等时间敏感的环境中。&lt;br&gt;&lt;h4&gt;2. 需求&lt;/h4&gt;   - 增强现实的组装指导需要实时获取目标物体的6D位姿，确保无标记的连续跟踪。&lt;br&gt;&lt;h4&gt;3. 挑战&lt;/h4&gt;   - 用户手部或其他物体的遮挡，以及不同组装状态的复杂性，使得无标记的多物体跟踪变得困难。&lt;br&gt;&lt;h4&gt;4. 解决方案&lt;/h4&gt;   - 提出了基于图的物体跟踪（GBOT）方法，采用单视角RGB-D跟踪技术。&lt;br&gt;&lt;h4&gt;5. 跟踪过程&lt;/h4&gt;   - 通过6D位姿估计初始化实时无标记的多物体跟踪，并更新基于图的组装位姿。&lt;br&gt;&lt;h4&gt;6. 多状态组装图&lt;/h4&gt;   - 通过利用各个组装部件的相对位姿，更新多状态组装图，确保在不同组装状态下的有效跟踪。&lt;br&gt;&lt;h4&gt;7. 图的链接&lt;/h4&gt;   - 将个体对象连接到该图中，增强组装过程中的物体跟踪鲁棒性。&lt;br&gt;&lt;h4&gt;8. 评估方法&lt;/h4&gt;   - 引入了一种合成数据集，包含公开的可3D打印组装资产，作为未来工作的基准。&lt;br&gt;&lt;h4&gt;9. 实验结果&lt;/h4&gt;   - 在合成数据和真实测试数据中的定量实验和定性研究表明，GBOT在实现上下文感知的增强现实组装指导方面优于现有方法。&lt;br&gt;&lt;h4&gt;10. 数据和代码公开&lt;/h4&gt;    - 数据集和代码将公开发布，以供进一步研究使用。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-02-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/roth-hex-lab/gbot&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Guidance for assemblable parts is a promising field for augmented reality.
Augmented reality assembly guidance requires 6D object poses of target objects
in real time. Especially in time-critical medical or industrial settings,
continuous and markerless tracking of individual parts is essential to
visualize instructions superimposed on or next to the target object parts. In
this regard, occlusions by the user's hand or other objects and the complexity
of different assembly states complicate robust and real-time markerless
multi-object tracking. To address this problem, we present Graph-based Object
Tracking (GBOT), a novel graph-based single-view RGB-D tracking approach. The
real-time markerless multi-object tracking is initialized via 6D pose
estimation and updates the graph-based assembly poses. The tracking through
various assembly states is achieved by our novel multi-state assembly graph. We
update the multi-state assembly graph by utilizing the relative poses of the
individual assembly parts. Linking the individual objects in this graph enables
more robust object tracking during the assembly process. For evaluation, we
introduce a synthetic dataset of publicly available and 3D printable assembly
assets as a benchmark for future work. Quantitative experiments in synthetic
data and further qualitative study in real test data show that GBOT can
outperform existing work towards enabling context-aware augmented reality
assembly guidance. Dataset and code will be made publically available.</description>
      <guid isPermaLink="false">2402.07677v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Efficient and Scalable Point Cloud Generation with Sparse Point-Voxel Diffusion Models</title>
      <link>http://arxiv.org/abs/2408.06145v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究目标&lt;/h4&gt;   - 提出一种新型的点云U-Net扩散架构，用于3D生成建模，旨在生成高质量和多样化的3D形状，同时保持快速生成时间。&lt;br&gt;&lt;h4&gt;2. 网络架构&lt;/h4&gt;   - 采用双分支架构，结合高分辨率的点表示和稀疏体素的计算效率。&lt;br&gt;&lt;h4&gt;3. 性能对比&lt;/h4&gt;   - 最快的变体在无条件形状生成方面超越了所有非扩散生成方法，这是评估点云生成模型的主要基准。&lt;br&gt;   - 最大模型在扩散方法中达到了最新的 state-of-the-art 结果，运行时间约为之前最先进的PVD模型的70%。&lt;br&gt;&lt;h4&gt;4. 扩展评估&lt;/h4&gt;   - 除了无条件生成外，还进行了广泛的评估，包括在ShapeNet的所有类别上的条件生成，展示了模型在更大数据集上的可扩展性。&lt;br&gt;   - 实现了隐式生成，允许网络在更少的时间步长下产生高质量点云，进一步降低生成时间。&lt;br&gt;&lt;h4&gt;5. 额外任务评估&lt;/h4&gt;   - 在点云补全和超分辨率任务中的表现也进行了评估，模型在所有任务中表现优异。&lt;br&gt;&lt;h4&gt;6. 结论&lt;/h4&gt;   - 确立了该模型作为点云生成建模领域的先进扩散U-Net。&lt;br&gt;   - 代码可在[GitHub](https://github.com/JohnRomanelis/SPVD.git)上公开获取。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/JohnRomanelis/SPVD_Lightning&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a novel point cloud U-Net diffusion architecture for 3D generative
modeling capable of generating high-quality and diverse 3D shapes while
maintaining fast generation times. Our network employs a dual-branch
architecture, combining the high-resolution representations of points with the
computational efficiency of sparse voxels. Our fastest variant outperforms all
non-diffusion generative approaches on unconditional shape generation, the most
popular benchmark for evaluating point cloud generative models, while our
largest model achieves state-of-the-art results among diffusion methods, with a
runtime approximately 70% of the previously state-of-the-art PVD. Beyond
unconditional generation, we perform extensive evaluations, including
conditional generation on all categories of ShapeNet, demonstrating the
scalability of our model to larger datasets, and implicit generation which
allows our network to produce high quality point clouds on fewer timesteps,
further decreasing the generation time. Finally, we evaluate the architecture's
performance in point cloud completion and super-resolution. Our model excels in
all tasks, establishing it as a state-of-the-art diffusion U-Net for point
cloud generative modeling. The code is publicly available at
https://github.com/JohnRomanelis/SPVD.git.</description>
      <guid isPermaLink="false">2408.06145v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Geometry Aware Meta-Learning Neural Network for Joint Phase and Precoder Optimization in RIS</title>
      <link>http://arxiv.org/abs/2409.11270v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 在可重构智能表面（RIS）辅助系统中，基站的预编码矩阵和RIS元素的相位偏移的联合优化涉及复杂性较高。&lt;br&gt;&lt;h4&gt;2. 提出的方法&lt;/h4&gt;   - 本文提出了一种复值的、几何感知的元学习神经网络，旨在最大化多用户多输入单输出系统中的加权总速率。&lt;br&gt;&lt;h4&gt;3. 几何特性利用&lt;/h4&gt;   - 该方法利用复数圆几何进行相位偏移和球面几何进行预编码，从而在黎曼流形上进行优化，实现更快的收敛。&lt;br&gt;&lt;h4&gt;4. 网络架构&lt;/h4&gt;   - 使用复值神经网络来处理相位偏移，并采用欧拉启发式更新方法来更新预编码网络。&lt;br&gt;&lt;h4&gt;5. 性能优势&lt;/h4&gt;   - 本方法优于现有基于神经网络的算法，具体表现为：&lt;br&gt;     - 更高的加权总速率&lt;br&gt;     - 更低的能耗&lt;br&gt;     - 收敛速度显著更快&lt;br&gt;&lt;h4&gt;6. 具体改进&lt;/h4&gt;   - 相较于现有工作，收敛速度提高近100个迭代周期，加权总速率提升0.7 bps，功率增益达到1.8 dBm。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In reconfigurable intelligent surface (RIS) aided systems, the joint
optimization of the precoder matrix at the base station and the phase shifts of
the RIS elements involves significant complexity. In this paper, we propose a
complex-valued, geometry aware meta-learning neural network that maximizes the
weighted sum rate in a multi-user multiple input single output system. By
leveraging the complex circle geometry for phase shifts and spherical geometry
for the precoder, the optimization occurs on Riemannian manifolds, leading to
faster convergence. We use a complex-valued neural network for phase shifts and
an Euler inspired update for the precoder network. Our approach outperforms
existing neural network-based algorithms, offering higher weighted sum rates,
lower power consumption, and significantly faster convergence. Specifically, it
converges faster by nearly 100 epochs, with a 0.7 bps improvement in weighted
sum rate and a 1.8 dBm power gain when compared with existing work.</description>
      <guid isPermaLink="false">2409.11270v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>CLIP Adaptation by Intra-modal Overlap Reduction</title>
      <link>http://arxiv.org/abs/2409.11338v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  BMVC 2024, Oral&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 许多方法已被提出，以适应预训练的基础CLIP模型进行少量样本分类。由于CLIP在大规模语料库上训练，具有良好的泛化能力。&lt;br&gt;&lt;h4&gt;2. 研究重点&lt;/h4&gt;   - 本文分析了图像空间中的模态内部重叠，特别是嵌入表示方面的重叠情况。&lt;br&gt;&lt;h4&gt;3. 主要发现&lt;/h4&gt;   - 由于对比学习的影响，CLIP模型的嵌入在图像空间中，配对与未配对样本之间存在高余弦相似度分布重叠，这影响了依赖图像空间相似度进行预测的少量样本无训练分类方法的性能。&lt;br&gt;&lt;h4&gt;4. 问题解决方案&lt;/h4&gt;   - 为了解决模态内部重叠问题，提出在谷歌开放图像数据集上训练一个轻量级适配器，证明这能提高少量样本无训练分类的准确性。&lt;br&gt;&lt;h4&gt;5. 验证方法&lt;/h4&gt;   - 通过广泛的实证分析验证了所提出的方法。&lt;br&gt;&lt;h4&gt;6. 性能提升&lt;/h4&gt;   - 减少模态内部重叠带来了以下改进：&lt;br&gt;     - a) 在多个标准数据集上提升了性能。&lt;br&gt;     - b) 增强了对分布变化的鲁棒性。&lt;br&gt;     - c) 提高了特征方差，使得特征在下游任务中更具辨别力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Numerous methods have been proposed to adapt a pre-trained foundational CLIP
model for few-shot classification. As CLIP is trained on a large corpus, it
generalises well through adaptation to few-shot classification. In this work,
we analyse the intra-modal overlap in image space in terms of embedding
representation. Our analysis shows that, due to contrastive learning,
embeddings from CLIP model exhibit high cosine similarity distribution overlap
in the image space between paired and unpaired examples affecting the
performance of few-shot training-free classification methods which rely on
similarity in the image space for their predictions. To tackle intra-modal
overlap we propose to train a lightweight adapter on a generic set of samples
from the Google Open Images dataset demonstrating that this improves accuracy
for few-shot training-free classification. We validate our contribution through
extensive empirical analysis and demonstrate that reducing the intra-modal
overlap leads to a) improved performance on a number of standard datasets, b)
increased robustness to distribution shift and c) higher feature variance
rendering the features more discriminative for downstream tasks.</description>
      <guid isPermaLink="false">2409.11338v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Towards Modality-agnostic Label-efficient Segmentation with Entropy-Regularized Distribution Alignment</title>
      <link>http://arxiv.org/abs/2408.16520v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Extended version of arXiv:2305.15832; Code at
  https://github.com/LiyaoTang/ERDA&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 标签高效分割旨在使用稀疏和有限的真实标签进行有效的输入数据分割，尤其在3D点云分割中尤为重要，因为密集标注点云的难度较大。&lt;br&gt;&lt;h4&gt;2. 应用范围&lt;/h4&gt;   - 此方法同样适用于成本效益高的2D图像分割。&lt;br&gt;&lt;h4&gt;3. 伪标签的使用&lt;/h4&gt;   - 最近，伪标签被广泛应用于在有限真实标签下促进训练，并在2D和3D分割中取得了显著进展。&lt;br&gt;&lt;h4&gt;4. 现有问题&lt;/h4&gt;   - 现有的伪标签方法可能受到未标记数据中的噪声和变动的影响，导致生成的伪标签与当前模型预测之间存在显著差异。&lt;br&gt;&lt;h4&gt;5. 模型学习影响&lt;/h4&gt;   - 这种差异会进一步混淆并影响模型的学习过程，这在2D和3D模态的标签高效学习中是一个共同问题。&lt;br&gt;&lt;h4&gt;6. 新策略提出&lt;/h4&gt;   - 为了解决上述问题，提出了一种新颖的学习策略来规范化生成的伪标签，从而有效缩小伪标签与模型预测之间的差距。&lt;br&gt;&lt;h4&gt;7. 具体方法&lt;/h4&gt;   - 引入了熵正则化损失和分布对齐损失，形成ERDA学习策略。&lt;br&gt;&lt;h4&gt;8. 损失函数优化&lt;/h4&gt;   - 通过使用KL距离来制定分布对齐损失，ERDA简化为一个看似简单的交叉熵损失，能够同时优化伪标签生成模块和分割模型。&lt;br&gt;&lt;h4&gt;9. 创新点&lt;/h4&gt;   - 在伪标签生成方面进行了创新，使得ERDA在2D和3D数据模态下都能持续有效。&lt;br&gt;&lt;h4&gt;10. 性能表现&lt;/h4&gt;    - 该方法因其简单性和更具模态无关性的伪标签生成，已显示出在充分利用所有未标记数据进行训练方面的卓越性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/LiyaoTang/ERDA&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Label-efficient segmentation aims to perform effective segmentation on input
data using only sparse and limited ground-truth labels for training. This topic
is widely studied in 3D point cloud segmentation due to the difficulty of
annotating point clouds densely, while it is also essential for cost-effective
segmentation on 2D images. Until recently, pseudo-labels have been widely
employed to facilitate training with limited ground-truth labels, and promising
progress has been witnessed in both the 2D and 3D segmentation. However,
existing pseudo-labeling approaches could suffer heavily from the noises and
variations in unlabelled data, which would result in significant discrepancies
between generated pseudo-labels and current model predictions during training.
We analyze that this can further confuse and affect the model learning process,
which shows to be a shared problem in label-efficient learning across both 2D
and 3D modalities. To address this issue, we propose a novel learning strategy
to regularize the pseudo-labels generated for training, thus effectively
narrowing the gaps between pseudo-labels and model predictions. More
specifically, our method introduces an Entropy Regularization loss and a
Distribution Alignment loss for label-efficient learning, resulting in an ERDA
learning strategy. Interestingly, by using KL distance to formulate the
distribution alignment loss, ERDA reduces to a deceptively simple
cross-entropy-based loss which optimizes both the pseudo-label generation
module and the segmentation model simultaneously. In addition, we innovate in
the pseudo-label generation to make our ERDA consistently effective across both
2D and 3D data modalities for segmentation. Enjoying simplicity and more
modality-agnostic pseudo-label generation, our method has shown outstanding
performance in fully utilizing all unlabeled data points for training across
...</description>
      <guid isPermaLink="false">2408.16520v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>ChefFusion: Multimodal Foundation Model Integrating Recipe and Food Image Generation</title>
      <link>http://arxiv.org/abs/2409.12010v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 在食品计算领域已进行了大量研究，但大多数研究仅聚焦于单一任务，如：&lt;br&gt;     - t2t（根据食品标题和成分生成指令）&lt;br&gt;     - i2t（根据食品图像生成食谱）&lt;br&gt;     - t2i（根据食谱生成食品图像）&lt;br&gt;&lt;h4&gt;2. 研究缺口&lt;/h4&gt;   - 现有方法未能同时整合所有模态，存在多模态整合的空白。&lt;br&gt;&lt;h4&gt;3. 模型创新&lt;/h4&gt;   - 本文提出了一种新颖的食品计算基础模型，真正实现了多模态整合，支持多种任务，包括：&lt;br&gt;     - t2t&lt;br&gt;     - t2i&lt;br&gt;     - i2t&lt;br&gt;     - it2t&lt;br&gt;     - t2ti&lt;br&gt;&lt;h4&gt;4. 技术方法&lt;/h4&gt;   - 该模型利用大型语言模型（LLMs）以及预训练的图像编码器和解码器模型，能够执行多种与食品计算相关的任务，如：&lt;br&gt;     - 食品理解&lt;br&gt;     - 食品识别&lt;br&gt;     - 食谱生成&lt;br&gt;     - 食品图像生成&lt;br&gt;&lt;h4&gt;5. 性能优势&lt;/h4&gt;   - 与之前的模型相比，基础模型展示了显著更广泛的能力，特别是在食品图像生成和食谱生成任务中表现突出。&lt;br&gt;&lt;h4&gt;6. 开源资源&lt;/h4&gt;   - 项目ChefFusion已在GitHub上开源，供公众使用。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/peiyu-georgia-li/cheffusion-multimodal-foundation-model-integrating-recipe-and-food-image-generation&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Significant work has been conducted in the domain of food computing, yet
these studies typically focus on single tasks such as t2t (instruction
generation from food titles and ingredients), i2t (recipe generation from food
images), or t2i (food image generation from recipes). None of these approaches
integrate all modalities simultaneously. To address this gap, we introduce a
novel food computing foundation model that achieves true multimodality,
encompassing tasks such as t2t, t2i, i2t, it2t, and t2ti. By leveraging large
language models (LLMs) and pre-trained image encoder and decoder models, our
model can perform a diverse array of food computing-related tasks, including
food understanding, food recognition, recipe generation, and food image
generation. Compared to previous models, our foundation model demonstrates a
significantly broader range of capabilities and exhibits superior performance,
particularly in food image generation and recipe generation tasks. We
open-sourced ChefFusion at GitHub.</description>
      <guid isPermaLink="false">2409.12010v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Grid Graph Neural Networks with Self-Attention for Computational Mechanics</title>
      <link>http://arxiv.org/abs/2409.11899v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 有限元方法在多个学科中变得至关重要，尤其是在计算流体动力学（CFD）中，推动了对提高精度和效率的研究。&lt;br&gt;&lt;h4&gt;2. 技术转变&lt;/h4&gt;   - 尽管卷积神经网络（CNN）已成功地将网格映射为图像，但最近的关注点转向了利用图神经网络（GNN）进行直接网格处理。&lt;br&gt;&lt;h4&gt;3. 模型创新&lt;/h4&gt;   - 本文介绍了一种新模型，将自注意力机制与GNN中的消息传递相结合，实现了在著名的圆柱体流动基准测试中减少15%的均方根误差（RMSE）。&lt;br&gt;&lt;h4&gt;4. 动态网格修剪&lt;/h4&gt;   - 提出了基于自注意力机制的动态网格修剪技术，进一步推动了基于GNN的多网格方法，同样减少了15%的RMSE。&lt;br&gt;&lt;h4&gt;5. 自监督训练方法&lt;/h4&gt;   - 介绍了一种基于BERT的新自监督训练方法，导致RMSE减少25%。&lt;br&gt;&lt;h4&gt;6. 消融研究&lt;/h4&gt;   - 论文包含消融研究，表明所提模型在多个挑战性数据集上优于现有最先进模型，展示了与自然语言处理和图像处理领域相似的进展潜力。&lt;br&gt;&lt;h4&gt;7. 数据集贡献&lt;/h4&gt;   - 论文引入了一个新的数据集，其网格规模比现有数据集大至少一个数量级。&lt;br&gt;&lt;h4&gt;8. 资源获取&lt;/h4&gt;   - 代码和数据集将发布在[GitHub上](https://github.com/DonsetPG/multigrid-gnn)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/donsetpg/multigrid-gnn&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Advancement in finite element methods have become essential in various
disciplines, and in particular for Computational Fluid Dynamics (CFD), driving
research efforts for improved precision and efficiency. While Convolutional
Neural Networks (CNNs) have found success in CFD by mapping meshes into images,
recent attention has turned to leveraging Graph Neural Networks (GNNs) for
direct mesh processing. This paper introduces a novel model merging
Self-Attention with Message Passing in GNNs, achieving a 15\% reduction in RMSE
on the well known flow past a cylinder benchmark. Furthermore, a dynamic mesh
pruning technique based on Self-Attention is proposed, that leads to a robust
GNN-based multigrid approach, also reducing RMSE by 15\%. Additionally, a new
self-supervised training method based on BERT is presented, resulting in a 25\%
RMSE reduction. The paper includes an ablation study and outperforms
state-of-the-art models on several challenging datasets, promising advancements
similar to those recently achieved in natural language and image processing.
Finally, the paper introduces a dataset with meshes larger than existing ones
by at least an order of magnitude. Code and Datasets will be released at
https://github.com/DonsetPG/multigrid-gnn.</description>
      <guid isPermaLink="false">2409.11899v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>SceneTracker: Long-term Scene Flow Estimation Network</title>
      <link>http://arxiv.org/abs/2403.19924v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 本研究关注场景流估计在空间域的聚焦能力与3D物体跟踪在时间域的一致性之间的互补性。&lt;br&gt;&lt;h4&gt;2. 研究目标&lt;/h4&gt;   - 针对一个综合性的新任务：长时间场景流估计（LSFE），旨在在线捕捉细粒度和长期的3D运动。&lt;br&gt;&lt;h4&gt;3. 方法介绍&lt;/h4&gt;   - 引入了SceneTracker，一个基于学习的LSFE网络，采用迭代方法来逼近最佳轨迹。&lt;br&gt;&lt;h4&gt;4. 特征构建&lt;/h4&gt;   - 动态索引和构建外观与深度相关特征，同时利用Transformer探索和利用轨迹内外的长程连接。&lt;br&gt;&lt;h4&gt;5. 实验结果&lt;/h4&gt;   - 通过详细实验，SceneTracker在处理3D空间遮挡和深度噪声干扰方面表现出色，满足LSFE任务的需求。&lt;br&gt;&lt;h4&gt;6. 数据集构建&lt;/h4&gt;   - 构建了第一个真实世界评估数据集LSFDriving，进一步证明SceneTracker的良好泛化能力。&lt;br&gt;&lt;h4&gt;7. 资源获取&lt;/h4&gt;   - SceneTracker的代码和数据可在[GitHub上获取](https://github.com/wwsource/SceneTracker)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-03-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/wwsource/scenetracker&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Considering the complementarity of scene flow estimation in the spatial
domain's focusing capability and 3D object tracking in the temporal domain's
coherence, this study aims to address a comprehensive new task that can
simultaneously capture fine-grained and long-term 3D motion in an online
manner: long-term scene flow estimation (LSFE). We introduce SceneTracker, a
novel learning-based LSFE network that adopts an iterative approach to
approximate the optimal trajectory. Besides, it dynamically indexes and
constructs appearance and depth correlation features simultaneously and employs
the Transformer to explore and utilize long-range connections within and
between trajectories. With detailed experiments, SceneTracker shows superior
capabilities in handling 3D spatial occlusion and depth noise interference,
highly tailored to the LSFE task's needs. Finally, we build the first
real-world evaluation dataset, LSFDriving, further substantiating
SceneTracker's commendable generalization capacity. The code and data for
SceneTracker is available at https://github.com/wwsource/SceneTracker.</description>
      <guid isPermaLink="false">2403.19924v3</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Unleashing the Potential of Mamba: Boosting a LiDAR 3D Sparse Detector by Using Cross-Model Knowledge Distillation</title>
      <link>http://arxiv.org/abs/2409.11018v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - LiDAR基础的3D物体检测器在自动驾驶和机器人导航系统中实现实时感知至关重要，需要在准确性和速度之间取得平衡。&lt;br&gt;&lt;h4&gt;2. 现有问题&lt;/h4&gt;   - 许多现有的LiDAR检测模型依赖复杂的特征变换和提取过程，导致实时性能差和资源消耗高，这限制了它们的实际有效性。&lt;br&gt;&lt;h4&gt;3. 提出的新框架&lt;/h4&gt;   - 提出了一个名为FASD的快速LiDAR 3D物体检测框架，通过自适应统一的跨模型体素特征实现异构模型蒸馏。&lt;br&gt;&lt;h4&gt;4. 目标&lt;/h4&gt;   - 旨在将变压器的高性能序列建模能力蒸馏到低FLOPs的Mamba模型中，通过知识转移显著提高准确性。&lt;br&gt;&lt;h4&gt;5. 关键技术&lt;/h4&gt;   - **动态体素组**和**自适应注意力策略**集成到稀疏主干网络中，创建具有规模自适应注意力的强大教师模型，以有效建模全局视觉上下文。&lt;br&gt;&lt;h4&gt;6. 知识转移&lt;/h4&gt;   - 通过适配器进行特征对齐，利用潜在空间特征监督和跨度头蒸馏将知识从变压器转移到Mamba，提升性能并形成高效的学生模型。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 在Waymo和nuScenes数据集上评估该框架，实现了资源消耗降低4倍，并比当前最先进的方法提高了1-2%的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The LiDAR-based 3D object detector that strikes a balance between accuracy
and speed is crucial for achieving real-time perception in autonomous driving
and robotic navigation systems. To enhance the accuracy of point cloud
detection, integrating global context for visual understanding improves the
point clouds ability to grasp overall spatial information. However, many
existing LiDAR detection models depend on intricate feature transformation and
extraction processes, leading to poor real-time performance and high resource
consumption, which limits their practical effectiveness. In this work, we
propose a Faster LiDAR 3D object detection framework, called FASD, which
implements heterogeneous model distillation by adaptively uniform cross-model
voxel features. We aim to distill the transformer's capacity for
high-performance sequence modeling into Mamba models with low FLOPs, achieving
a significant improvement in accuracy through knowledge transfer. Specifically,
Dynamic Voxel Group and Adaptive Attention strategies are integrated into the
sparse backbone, creating a robust teacher model with scale-adaptive attention
for effective global visual context modeling. Following feature alignment with
the Adapter, we transfer knowledge from the Transformer to the Mamba through
latent space feature supervision and span-head distillation, resulting in
improved performance and an efficient student model. We evaluated the framework
on the Waymo and nuScenes datasets, achieving a 4x reduction in resource
consumption and a 1-2\% performance improvement over the current SoTA methods.</description>
      <guid isPermaLink="false">2409.11018v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>From Words to Poses: Enhancing Novel Object Pose Estimation with Vision Language Models</title>
      <link>http://arxiv.org/abs/2409.05413v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 机器人在现实场景中交互日益增多，需要不断适应新情况。&lt;br&gt;&lt;h4&gt;2. 零-shot姿态估计&lt;/h4&gt;   - 零-shot姿态估计器可以在没有先前知识的情况下确定物体姿态，以检测和抓取新物体。&lt;br&gt;&lt;h4&gt;3. 视觉语言模型的应用&lt;/h4&gt;   - 最近，视觉语言模型（VLMs）在机器人应用中取得显著进展，通过建立语言输入与图像输入之间的理解。&lt;br&gt;&lt;h4&gt;4. 研究贡献&lt;/h4&gt;   - 提出了一种新的框架，利用语言嵌入进行可提示的零-shot 6D物体姿态估计。&lt;br&gt;&lt;h4&gt;5. 方法概述&lt;/h4&gt;   - 通过语言嵌入的NeRF重建的相关性图来推导物体的粗略位置，然后使用点云配准方法计算姿态估计。&lt;br&gt;&lt;h4&gt;6. 分析与适用性&lt;/h4&gt;   - 提供了对LERF在开放集物体姿态估计中适用性的分析，考察相关性图的激活阈值等超参数。&lt;br&gt;&lt;h4&gt;7. 零-shot能力的研究&lt;/h4&gt;   - 调查在实例级和类别级上的零-shot能力。&lt;br&gt;&lt;h4&gt;8. 未来计划&lt;/h4&gt;   - 计划在真实环境中进行机器人抓取实验。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robots are increasingly envisioned to interact in real-world scenarios, where
they must continuously adapt to new situations. To detect and grasp novel
objects, zero-shot pose estimators determine poses without prior knowledge.
Recently, vision language models (VLMs) have shown considerable advances in
robotics applications by establishing an understanding between language input
and image input. In our work, we take advantage of VLMs zero-shot capabilities
and translate this ability to 6D object pose estimation. We propose a novel
framework for promptable zero-shot 6D object pose estimation using language
embeddings. The idea is to derive a coarse location of an object based on the
relevancy map of a language-embedded NeRF reconstruction and to compute the
pose estimate with a point cloud registration method. Additionally, we provide
an analysis of LERF's suitability for open-set object pose estimation. We
examine hyperparameters, such as activation thresholds for relevancy maps and
investigate the zero-shot capabilities on an instance- and category-level.
Furthermore, we plan to conduct robotic grasping experiments in a real-world
setting.</description>
      <guid isPermaLink="false">2409.05413v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>GeoFormer: Learning Point Cloud Completion with Tri-Plane Integrated Transformer</title>
      <link>http://arxiv.org/abs/2408.06596v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  accepted by the 32nd ACM International Conference on Multimedia
  (MM'24)&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究目标&lt;/h4&gt;   - 点云补全旨在从部分点云中恢复准确的全局几何结构并保留细致的局部细节。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限&lt;/h4&gt;   - 传统方法通常直接从三维点云坐标预测未见点，或使用自投影的多视图深度图简化任务。&lt;br&gt;   - 然而，这些灰度深度图无法实现多视图一致性，从而限制了性能。&lt;br&gt;&lt;h4&gt;3. 提出的解决方案&lt;/h4&gt;   - 引入GeoFormer模型，同时增强点的全局几何结构和局部细节。&lt;br&gt;&lt;h4&gt;4. 主要组件&lt;/h4&gt;   - **CCM特征增强点生成器**：集成来自多视图一致的典型坐标图（CCMs）的图像特征，与纯点特征对齐，从而增强全局几何特征。&lt;br&gt;   - **多尺度几何感知上采样模块**：逐步增强局部细节，通过跨注意力机制在从部分输入提取的多尺度特征和之前估计的点的特征之间进行交互。&lt;br&gt;&lt;h4&gt;5. 实验验证&lt;/h4&gt;   - 在PCN、ShapeNet-55/34和KITTI基准测试上的广泛实验表明，GeoFormer在性能上超越了近期方法，实现了最先进的表现。&lt;br&gt;&lt;h4&gt;6. 代码可用性&lt;/h4&gt;   - 相关代码可在GitHub上获取：[GeoFormer GitHub](https://github.com/Jinpeng-Yu/GeoFormer)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3664647.3680842&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/jinpeng-yu/geoformer&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud completion aims to recover accurate global geometry and preserve
fine-grained local details from partial point clouds. Conventional methods
typically predict unseen points directly from 3D point cloud coordinates or use
self-projected multi-view depth maps to ease this task. However, these
gray-scale depth maps cannot reach multi-view consistency, consequently
restricting the performance. In this paper, we introduce a GeoFormer that
simultaneously enhances the global geometric structure of the points and
improves the local details. Specifically, we design a CCM Feature Enhanced
Point Generator to integrate image features from multi-view consistent
canonical coordinate maps (CCMs) and align them with pure point features,
thereby enhancing the global geometry feature. Additionally, we employ the
Multi-scale Geometry-aware Upsampler module to progressively enhance local
details. This is achieved through cross attention between the multi-scale
features extracted from the partial input and the features derived from
previously estimated points. Extensive experiments on the PCN, ShapeNet-55/34,
and KITTI benchmarks demonstrate that our GeoFormer outperforms recent methods,
achieving the state-of-the-art performance. Our code is available at
\href{https://github.com/Jinpeng-Yu/GeoFormer}{https://github.com/Jinpeng-Yu/GeoFormer}.</description>
      <guid isPermaLink="false">2408.06596v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Few-Shot Classification without Forgetting through Multi-Level Contrastive Constraints</title>
      <link>http://arxiv.org/abs/2409.11286v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 大多数近期的少样本学习方法基于元学习和情景训练。&lt;br&gt;&lt;h4&gt;2. 存在的问题&lt;/h4&gt;   - (1) **归纳偏差**：现有方法可能导致模型对特定任务或数据的过度适应。&lt;br&gt;   - (2) **灾难性遗忘**：模型在学习新任务时可能会遗忘先前学到的知识。&lt;br&gt;&lt;h4&gt;3. 提出的框架&lt;/h4&gt;   - 提出了一个新颖的多层次对比约束（MLCC）框架，旨在将“情景内学习”和“情景间学习”整合为统一的交互学习范式。&lt;br&gt;&lt;h4&gt;4. 空间感知交互建模&lt;/h4&gt;   - 采用空间感知交互建模方案，探索每个类别在情景内相似性/不相似性分布之间的正确归纳范式。&lt;br&gt;&lt;h4&gt;5. 跨阶段分布适配策略&lt;/h4&gt;   - 设计了跨阶段分布适配策略，以对齐来自不同时间阶段的情景间分布，减少现有预测分布与过去预测分布之间的语义差距。&lt;br&gt;&lt;h4&gt;6. 实验验证&lt;/h4&gt;   - 在多个少样本数据集上的广泛实验表明，MLCC方法在性能上始终优于现有最先进的基线模型。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Most recent few-shot learning approaches are based on meta-learning with
episodic training. However, prior studies encounter two crucial problems: (1)
\textit{the presence of inductive bias}, and (2) \textit{the occurrence of
catastrophic forgetting}. In this paper, we propose a novel Multi-Level
Contrastive Constraints (MLCC) framework, that jointly integrates
within-episode learning and across-episode learning into a unified interactive
learning paradigm to solve these issues. Specifically, we employ a space-aware
interaction modeling scheme to explore the correct inductive paradigms for each
class between within-episode similarity/dis-similarity distributions.
Additionally, with the aim of better utilizing former prior knowledge, a
cross-stage distribution adaption strategy is designed to align the
across-episode distributions from different time stages, thus reducing the
semantic gap between existing and past prediction distribution. Extensive
experiments on multiple few-shot datasets demonstrate the consistent
superiority of MLCC approach over the existing state-of-the-art baselines.</description>
      <guid isPermaLink="false">2409.11286v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Learning Spatially-Aware Language and Audio Embedding</title>
      <link>http://arxiv.org/abs/2409.11369v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  25 pages, 7 figures&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 人类能够根据不精确的自然语言描述想象声音场景，例如“狮子吼声来自我身后”。&lt;br&gt;&lt;h4&gt;2. 机器理解的挑战&lt;/h4&gt;   - 机器要具备相同的理解能力，必须了解“狮子”的语义属性、空间概念“身后”，以及声音的语义和空间属性如何结合。&lt;br&gt;&lt;h4&gt;3. 现有模型的局限&lt;/h4&gt;   - 现有的音频基础模型主要基于非空间音频和文本对进行训练，缺乏空间感知能力。&lt;br&gt;   - 声音事件定位和检测模型仅能识别固定类别的声音，并将声音源定位到绝对位置，而非自然语言描述的位置。&lt;br&gt;&lt;h4&gt;4. 研究贡献&lt;/h4&gt;   - 提出ELSA（Spatially Aware Audio and Text Embedding），一个通过多模态对比学习训练的空间感知音频和文本嵌入模型。&lt;br&gt;&lt;h4&gt;5. ELSA功能&lt;/h4&gt;   - 支持非空间音频、空间音频和开放词汇的文本描述，涵盖声音的空间和语义成分。&lt;br&gt;&lt;h4&gt;6. 训练方法&lt;/h4&gt;   - (a) 对三个开放源音频数据集进行空间增强，总计4,738小时音频和文本。&lt;br&gt;   - (b) 设计编码器以捕捉非空间音频的语义，以及空间音频的语义和空间属性。&lt;br&gt;&lt;h4&gt;7. 性能评估&lt;/h4&gt;   - ELSA在语义检索和3D源定位方面与现有最先进技术具有竞争力。&lt;br&gt;   - 在音频到文本和文本到音频的检索率上，ELSA比基线提高了2.8%。&lt;br&gt;   - 在3D源定位的平均绝对误差上，ELSA比基线减少了11.6度。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humans can picture a sound scene given an imprecise natural language
description. For example, it is easy to imagine an acoustic environment given a
phrase like "the lion roar came from right behind me!". For a machine to have
the same degree of comprehension, the machine must know what a lion is
(semantic attribute), what the concept of "behind" is (spatial attribute) and
how these pieces of linguistic information align with the semantic and spatial
attributes of the sound (what a roar sounds like when its coming from behind).
State-of-the-art audio foundation models which learn to map between audio
scenes and natural textual descriptions, are trained on non-spatial audio and
text pairs, and hence lack spatial awareness. In contrast, sound event
localization and detection models are limited to recognizing sounds from a
fixed number of classes, and they localize the source to absolute position
(e.g., 0.2m) rather than a position described using natural language (e.g.,
"next to me"). To address these gaps, we present ELSA a spatially aware-audio
and text embedding model trained using multimodal contrastive learning. ELSA
supports non-spatial audio, spatial audio, and open vocabulary text captions
describing both the spatial and semantic components of sound. To train ELSA:
(a) we spatially augment the audio and captions of three open-source audio
datasets totaling 4,738 hours of audio, and (b) we design an encoder to capture
the semantics of non-spatial audio, and the semantics and spatial attributes of
spatial audio using contrastive learning. ELSA is competitive with
state-of-the-art for both semantic retrieval and 3D source localization. In
particular, ELSA achieves +2.8% mean audio-to-text and text-to-audio R@1 above
the baseline, and outperforms by -11.6{\deg} mean-absolute-error in 3D source
localization over the baseline.</description>
      <guid isPermaLink="false">2409.11369v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Fusion with LLMs for Engagement Prediction in Natural Conversation</title>
      <link>http://arxiv.org/abs/2409.09135v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages, first three authors equal contribution&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 背景与发展&lt;/h4&gt;   - 智能眼镜在传感器技术、设计和处理能力方面经历了显著进步，带来了高密度人类行为数据的新机遇。&lt;br&gt;&lt;h4&gt;2. 研究重点&lt;/h4&gt;   - 研究目标是通过分析言语和非言语线索来预测双边互动中的参与度，特别是识别不感兴趣或困惑的迹象。&lt;br&gt;&lt;h4&gt;3. 影响与应用&lt;/h4&gt;   - 该研究有望革新人类沟通的理解，提升专业环境中的协作效果，改善心理健康支持，以及增强对沟通障碍人士的可及性。&lt;br&gt;&lt;h4&gt;4. 数据集收集&lt;/h4&gt;   - 收集了34名参与者在轻松双边对话中的数据，每位参与者在对话结束时提供自我报告的参与度评分。&lt;br&gt;&lt;h4&gt;5. 创新方法&lt;/h4&gt;   - 引入了一种新颖的融合策略，利用大型语言模型（LLMs）将多种行为模态整合为“多模态转录”，以便进行行为推理任务。&lt;br&gt;&lt;h4&gt;6. 初步结果&lt;/h4&gt;   - 该方法在初步实施中表现出与已有融合技术相当的性能，显示出进一步研究和优化的强大潜力。&lt;br&gt;&lt;h4&gt;7. 研究的独特性&lt;/h4&gt;   - 这是首次通过语言模型“推理”现实世界人类行为的方法之一。&lt;br&gt;&lt;h4&gt;8. 数据收集的优势&lt;/h4&gt;   - 智能眼镜能够无干扰地收集人类行为的高密度多模态数据，为理解和改善人类沟通开辟了新方法，具有重要的社会利益潜力。&lt;br&gt;&lt;h4&gt;9. 数据共享&lt;/h4&gt;   - 研究中收集的特征和数据将公开，以促进进一步研究。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Over the past decade, wearable computing devices (``smart glasses'') have
undergone remarkable advancements in sensor technology, design, and processing
power, ushering in a new era of opportunity for high-density human behavior
data. Equipped with wearable cameras, these glasses offer a unique opportunity
to analyze non-verbal behavior in natural settings as individuals interact. Our
focus lies in predicting engagement in dyadic interactions by scrutinizing
verbal and non-verbal cues, aiming to detect signs of disinterest or confusion.
Leveraging such analyses may revolutionize our understanding of human
communication, foster more effective collaboration in professional
environments, provide better mental health support through empathetic virtual
interactions, and enhance accessibility for those with communication barriers.
  In this work, we collect a dataset featuring 34 participants engaged in
casual dyadic conversations, each providing self-reported engagement ratings at
the end of each conversation. We introduce a novel fusion strategy using Large
Language Models (LLMs) to integrate multiple behavior modalities into a
``multimodal transcript'' that can be processed by an LLM for behavioral
reasoning tasks. Remarkably, this method achieves performance comparable to
established fusion techniques even in its preliminary implementation,
indicating strong potential for further research and optimization. This fusion
method is one of the first to approach ``reasoning'' about real-world human
behavior through a language model. Smart glasses provide us the ability to
unobtrusively gather high-density multimodal data on human behavior, paving the
way for new approaches to understanding and improving human communication with
the potential for important societal benefits. The features and data collected
during the studies will be made publicly available to promote further research.</description>
      <guid isPermaLink="false">2409.09135v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>DGD: Dynamic 3D Gaussians Distillation</title>
      <link>http://arxiv.org/abs/2405.19321v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究目标&lt;/h4&gt;   - 本文旨在学习动态3D语义辐射场，输入为单一的单目视频。&lt;br&gt;&lt;h4&gt;2. 语义辐射场特性&lt;/h4&gt;   - 学习到的语义辐射场能够捕捉每个点的语义信息、颜色和几何属性，适用于动态3D场景。&lt;br&gt;&lt;h4&gt;3. 功能实现&lt;/h4&gt;   - 该方法支持生成新的视角及其相应的语义信息，实现对多种3D语义实体的分割和跟踪。&lt;br&gt;&lt;h4&gt;4. 用户交互&lt;/h4&gt;   - 提供简单直观的接口，通过用户点击或文本提示来指定要跟踪的对象。&lt;br&gt;&lt;h4&gt;5. 方法介绍&lt;/h4&gt;   - 提出DGD，作为动态3D场景外观和语义的统一3D表示，基于新近提出的动态3D高斯表示。&lt;br&gt;&lt;h4&gt;6. 优化策略&lt;/h4&gt;   - 该表示随着时间的推移进行优化，同时考虑颜色和语义信息。&lt;br&gt;&lt;h4&gt;7. 联合优化&lt;/h4&gt;   - 关键在于外观和语义属性的联合优化，这共同影响场景的几何特性。&lt;br&gt;&lt;h4&gt;8. 评估结果&lt;/h4&gt;   - 评估表明，该方法能够实现高密度的语义3D对象跟踪，且渲染速度快，适用于多样化的场景。&lt;br&gt;&lt;h4&gt;9. 项目链接&lt;/h4&gt;   - 项目网页可访问 [DGD-Website](https://isaaclabe.github.io/DGD-Website/)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-05-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We tackle the task of learning dynamic 3D semantic radiance fields given a
single monocular video as input. Our learned semantic radiance field captures
per-point semantics as well as color and geometric properties for a dynamic 3D
scene, enabling the generation of novel views and their corresponding
semantics. This enables the segmentation and tracking of a diverse set of 3D
semantic entities, specified using a simple and intuitive interface that
includes a user click or a text prompt. To this end, we present DGD, a unified
3D representation for both the appearance and semantics of a dynamic 3D scene,
building upon the recently proposed dynamic 3D Gaussians representation. Our
representation is optimized over time with both color and semantic information.
Key to our method is the joint optimization of the appearance and semantic
attributes, which jointly affect the geometric properties of the scene. We
evaluate our approach in its ability to enable dense semantic 3D object
tracking and demonstrate high-quality results that are fast to render, for a
diverse set of scenes. Our project webpage is available on
https://isaaclabe.github.io/DGD-Website/</description>
      <guid isPermaLink="false">2405.19321v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Mahalanobis k-NN: A Statistical Lens for Robust Point-Cloud Registrations</title>
      <link>http://arxiv.org/abs/2409.06267v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究主题&lt;/h4&gt;   - 本文讨论了Mahalanobis k-NN，旨在解决在学习型点云配准中，面对任意密度的源点云或目标点云时的特征匹配挑战。&lt;br&gt;&lt;h4&gt;2. 方法特点&lt;/h4&gt;   - 采用Mahalanobis k-NN的固有特性，捕捉局部邻域的分布和表面几何特征。&lt;br&gt;&lt;h4&gt;3. 方法集成性&lt;/h4&gt;   - 本方法可无缝集成到任何基于局部图的方法中进行点云分析。&lt;br&gt;&lt;h4&gt;4. 研究方法&lt;/h4&gt;   - 重点关注两种不同的方法论：&lt;br&gt;     - 深度最近点（Deep Closest Point, DCP）&lt;br&gt;     - 深度通用流形嵌入（Deep Universal Manifold Embedding, DeepUME）&lt;br&gt;&lt;h4&gt;5. 实验结果&lt;/h4&gt;   - 在ModelNet40和Faust数据集上的广泛基准测试显示了所提方法在点云配准任务中的有效性。&lt;br&gt;&lt;h4&gt;6. 特征能力发现&lt;/h4&gt;   - 首次证明，通过点云配准获得的特征本质上具备区分能力。&lt;br&gt;&lt;h4&gt;7. 性能提升&lt;/h4&gt;   - 在ModelNet40和ScanObjectNN上的点云少样本分类任务中，平均准确率提升约20%。&lt;br&gt;&lt;h4&gt;8. 代码获取&lt;/h4&gt;   - 相关代码已公开，链接为 [Mahalanobis-k-NN](https://github.com/TejasAnvekar/Mahalanobis-k-NN)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/TejasAnvekar/Mahalanobis-k-NN&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we discuss Mahalanobis k-NN: a statistical lens designed to
address the challenges of feature matching in learning-based point cloud
registration when confronted with an arbitrary density of point clouds, either
in the source or target point cloud. We tackle this by adopting Mahalanobis
k-NN's inherent property to capture the distribution of the local neighborhood
and surficial geometry. Our method can be seamlessly integrated into any
local-graph-based point cloud analysis method. In this paper, we focus on two
distinct methodologies: Deep Closest Point (DCP) and Deep Universal Manifold
Embedding (DeepUME). Our extensive benchmarking on the ModelNet40 and Faust
datasets highlights the efficacy of the proposed method in point cloud
registration tasks. Moreover, we establish for the first time that the features
acquired through point cloud registration inherently can possess discriminative
capabilities. This is evident by a substantial improvement of about 20\% in the
average accuracy observed in the point cloud few-shot classification task
benchmarked on ModelNet40 and ScanObjectNN. The code is publicly available at
https://github.com/TejasAnvekar/Mahalanobis-k-NN</description>
      <guid isPermaLink="false">2409.06267v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Diffusion-Occ: 3D Point Cloud Completion via Occupancy Diffusion</title>
      <link>http://arxiv.org/abs/2408.14846v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  After a closer examination of our work, we've determined that our
  experiments are not thorough and robust enough, possibly impacting the
  accuracy of our conclusions. Hence, we've decided to withdraw our article
  and, after refining our experiments, intend to resubmit the paper once
  significant improvements have been made&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 点云对于捕捉三维数据至关重要，但常因分辨率和遮挡等限制而导致不完整。&lt;br&gt;&lt;h4&gt;2. 传统方法的局限性&lt;/h4&gt;   - 传统方法通常依赖于判别框架中的点基方法进行点云补全，效果有限。&lt;br&gt;&lt;h4&gt;3. 新框架介绍&lt;/h4&gt;   - 本文提出了**Diffusion-Occ**，一种新颖的扩散点云补全框架。&lt;br&gt;&lt;h4&gt;4. 两阶段方法&lt;/h4&gt;   - 采用两阶段的粗到细策略：&lt;br&gt;     - **第一阶段**：粗密度体素预测网络（CDNet）处理部分点，预测粗体素密度，通过体素分类简化全局特征提取，而非之前的回归方法。&lt;br&gt;     - **第二阶段**：引入占据生成网络（OccGen），基于变换器架构的条件占据扩散模型，利用我们的点-体素融合（PVF）块增强模型，结合粗体素密度和部分点以利用全局和局部特征。&lt;br&gt;&lt;h4&gt;5. 点云补全过程&lt;/h4&gt;   - 通过对占据场进行阈值化，将其转换为完整的点云。&lt;br&gt;&lt;h4&gt;6. 训练与推理效率&lt;/h4&gt;   - 方法采用多样化的训练组合和高效的扩散参数化，使得在训练和推理过程中实现有效的一步采样。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 实验结果表明，Diffusion-Occ在性能上超越了现有的判别和生成方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point clouds are crucial for capturing three-dimensional data but often
suffer from incompleteness due to limitations such as resolution and occlusion.
Traditional methods typically rely on point-based approaches within
discriminative frameworks for point cloud completion. In this paper, we
introduce \textbf{Diffusion-Occ}, a novel framework for Diffusion Point Cloud
Completion. Diffusion-Occ utilizes a two-stage coarse-to-fine approach. In the
first stage, the Coarse Density Voxel Prediction Network (CDNet) processes
partial points to predict coarse density voxels, streamlining global feature
extraction through voxel classification, as opposed to previous
regression-based methods. In the second stage, we introduce the Occupancy
Generation Network (OccGen), a conditional occupancy diffusion model based on a
transformer architecture and enhanced by our Point-Voxel Fuse (PVF) block. This
block integrates coarse density voxels with partial points to leverage both
global and local features for comprehensive completion. By thresholding the
occupancy field, we convert it into a complete point cloud. Additionally, our
method employs diverse training mixtures and efficient diffusion
parameterization to enable effective one-step sampling during both training and
inference. Experimental results demonstrate that Diffusion-Occ outperforms
existing discriminative and generative methods.</description>
      <guid isPermaLink="false">2408.14846v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Can Transfer Learning be Used to Identify Tropical State-Dependent Bias Relevant to Midlatitude Subseasonal Predictability?</title>
      <link>http://arxiv.org/abs/2409.10755v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work has been submitted for publication in Artificial
  Intelligence for the Earth Systems (AIES). Copyright in this work may be
  transferred without further notice&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 先前研究表明，气候系统的特定状态可以增强亚季节预测能力，即状态依赖的可预测性。&lt;br&gt;&lt;h4&gt;2. 模型偏差问题&lt;/h4&gt;   - 地球系统模型中的偏差可能影响这些状态的表现及其后续演变，从而影响预测效果。&lt;br&gt;&lt;h4&gt;3. 研究目的&lt;/h4&gt;   - 本文提出一种机器学习框架，用于识别地球系统模型中的状态依赖偏差。&lt;br&gt;&lt;h4&gt;4. 研究方法&lt;/h4&gt;   - 特别关注使用可解释的神经网络进行迁移学习，以识别与亚季节可预测性相关的热带状态偏差，使用的是能源超级地球系统模型版本2（E3SMv2）的历史模拟数据。&lt;br&gt;&lt;h4&gt;5. 数据需求发现&lt;/h4&gt;   - 通过完美模型框架的研究发现，迁移学习可能需要比现有再分析数据集提供的更多的数据来更新神经网络权重。&lt;br&gt;&lt;h4&gt;6. 研究启示&lt;/h4&gt;   - 研究结果提出了对未来聚焦于亚季节变异模式的迁移学习方法的警示，强调了数据充足性的重要性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Previous research has demonstrated that specific states of the climate system
can lead to enhanced subseasonal predictability (i.e., state-dependent
predictability). However, biases in Earth system models can affect the
representation of these states and their subsequent evolution. Here, we present
a machine learning framework to identify state-dependent biases in Earth system
models. In particular, we investigate the utility of transfer learning with
explainable neural networks to identify tropical state-dependent biases in
historical simulations of the Energy Exascale Earth System Model version 2
(E3SMv2) relevant for midlatitude subseasonal predictability. Using a perfect
model framework, we find transfer learning may require substantially more data
than provided by present-day reanalysis datasets to update neural network
weights, imparting a cautionary tale for future transfer learning approaches
focused on subseasonal modes of variability.</description>
      <guid isPermaLink="false">2409.10755v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Learnable Chamfer Distance for Point Cloud Reconstruction</title>
      <link>http://arxiv.org/abs/2312.16582v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by Pattern Recognition Letters&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 点云是具有排列不变性的3D信号，现有的重建网络通常通过预定义规则测量点云之间的形状差异。&lt;br&gt;&lt;h4&gt;2. 静态匹配规则的局限性&lt;/h4&gt;   - 静态匹配规则可能与实际形状差异存在偏差，影响重建效果。&lt;br&gt;&lt;h4&gt;3. 动态学习结构的挑战&lt;/h4&gt;   - 尽管一些研究提出了动态更新的可学习结构来替代匹配规则，但它们通常需要更多迭代才能良好收敛。&lt;br&gt;&lt;h4&gt;4. 提出的解决方案&lt;/h4&gt;   - 本文提出了一种简单而有效的重建损失函数，称为可学习的Chamfer距离（LCD），通过动态关注不同权重分布的匹配距离来优化重建。&lt;br&gt;&lt;h4&gt;5. 对抗训练策略&lt;/h4&gt;   - 通过对抗训练策略，LCD能够识别重建结果中的缺陷，克服静态匹配规则的不足，同时在低迭代情况下也能保证性能。&lt;br&gt;&lt;h4&gt;6. 实验验证&lt;/h4&gt;   - 在多个重建网络上的实验表明，LCD能够实现更好的重建性能，并提取更具代表性的表示，同时具有更快的收敛速度和相当的训练效率。&lt;br&gt;&lt;h4&gt;7. 代码获取&lt;/h4&gt;   - 相关源代码可在GitHub上获取，链接为 [LCDNet](https://github.com/Tianxinhuang/LCDNet.git)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2023-12-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/tianxinhuang/lcdnet&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As point clouds are 3D signals with permutation invariance, most existing
works train their reconstruction networks by measuring shape differences with
the average point-to-point distance between point clouds matched with
predefined rules. However, the static matching rules may deviate from actual
shape differences. Although some works propose dynamically-updated learnable
structures to replace matching rules, they need more iterations to converge
well. In this work, we propose a simple but effective reconstruction loss,
named Learnable Chamfer Distance (LCD) by dynamically paying attention to
matching distances with different weight distributions controlled with a group
of learnable networks. By training with adversarial strategy, LCD learns to
search defects in reconstructed results and overcomes the weaknesses of static
matching rules, while the performances at low iterations can also be guaranteed
by the basic matching algorithm. Experiments on multiple reconstruction
networks confirm that LCD can help achieve better reconstruction performances
and extract more representative representations with faster convergence and
comparable training efficiency. The source codes are provided in
https://github.com/Tianxinhuang/LCDNet.git.</description>
      <guid isPermaLink="false">2312.16582v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Self-supervised Multimodal Speech Representations for the Assessment of Schizophrenia Symptoms</title>
      <link>http://arxiv.org/abs/2409.09733v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to ICASSP 2025&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 多模态精神分裂症评估系统近年来受到关注，旨在提高对精神分裂症的理解和诊断。&lt;br&gt;&lt;h4&gt;2. 系统介绍&lt;/h4&gt;   - 本文介绍了一种精神分裂症评估系统，旨在区分精神分裂症的主要症状类别，并预测整体严重性评分。&lt;br&gt;&lt;h4&gt;3. 模型开发&lt;/h4&gt;   - 开发了一种基于向量量化变分自编码器（VQ-VAE）的多模态表征学习（MRL）模型，用于生成与任务无关的语音表征，这些表征来源于声道变量（TVs）和面部动作单元（FAUs）。&lt;br&gt;&lt;h4&gt;4. 多任务学习&lt;/h4&gt;   - 这些表征被用于基于多任务学习（MTL）的下游预测模型，以获得类别标签和整体严重性评分。&lt;br&gt;&lt;h4&gt;5. 性能提升&lt;/h4&gt;   - 提出的框架在多类分类任务中超越了之前的研究，所有评估指标（加权F1分数、AUC-ROC分数和加权准确率）均表现优越。&lt;br&gt;&lt;h4&gt;6. 新任务的解决&lt;/h4&gt;   - 此外，该系统能够估计精神分裂症的严重性评分，这是早期方法未能解决的任务。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal schizophrenia assessment systems have gained traction over the
last few years. This work introduces a schizophrenia assessment system to
discern between prominent symptom classes of schizophrenia and predict an
overall schizophrenia severity score. We develop a Vector Quantized Variational
Auto-Encoder (VQ-VAE) based Multimodal Representation Learning (MRL) model to
produce task-agnostic speech representations from vocal Tract Variables (TVs)
and Facial Action Units (FAUs). These representations are then used in a
Multi-Task Learning (MTL) based downstream prediction model to obtain class
labels and an overall severity score. The proposed framework outperforms the
previous works on the multi-class classification task across all evaluation
metrics (Weighted F1 score, AUC-ROC score, and Weighted Accuracy).
Additionally, it estimates the schizophrenia severity score, a task not
addressed by earlier approaches.</description>
      <guid isPermaLink="false">2409.09733v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Multi-frequency Electrical Impedance Tomography Reconstruction with Multi-Branch Attention Image Prior</title>
      <link>http://arxiv.org/abs/2409.10794v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 10 figures, journal&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 多频率电阻抗成像（mfEIT）是一种有前景的生物医学成像技术，旨在估计不同频率下的组织导电性。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 当前的先进算法依赖于监督学习和多测量向量（MMV），需要大量训练数据，这使得训练过程耗时、成本高，并且不够实用。&lt;br&gt;&lt;h4&gt;3. 训练数据的依赖性问题&lt;/h4&gt;   - 监督MMV方法对训练数据的依赖可能导致不同频率下的导电性对比出现错误，这在生物医学应用中引发重大担忧。&lt;br&gt;&lt;h4&gt;4. 提出的新方法&lt;/h4&gt;   - 本文提出了一种基于多分支注意力图像先验（MAIP）的新型无监督学习方法，用于mfEIT重建。&lt;br&gt;&lt;h4&gt;5. 多分支注意力网络（MBA-Net）&lt;/h4&gt;   - 该方法利用精心设计的MBA-Net来表示多种频率依赖的导电性图像，并通过迭代更新参数同时重建mfEIT图像。&lt;br&gt;&lt;h4&gt;6. 隐式正则化能力&lt;/h4&gt;   - MBA-Net的隐式正则化能力使算法能够捕捉显著的频率间和频率内的相关性，从而实现稳健的mfEIT重建，无需训练数据。&lt;br&gt;&lt;h4&gt;7. 实验验证&lt;/h4&gt;   - 通过模拟实验和真实世界实验，所提方法的性能与现有先进算法相当，甚至更优，并展现出更强的泛化能力。&lt;br&gt;&lt;h4&gt;8. 应用潜力&lt;/h4&gt;   - 这些结果表明，基于MAIP的方法可以提高mfEIT在各种环境中的可靠性和适用性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-frequency Electrical Impedance Tomography (mfEIT) is a promising
biomedical imaging technique that estimates tissue conductivities across
different frequencies. Current state-of-the-art (SOTA) algorithms, which rely
on supervised learning and Multiple Measurement Vectors (MMV), require
extensive training data, making them time-consuming, costly, and less practical
for widespread applications. Moreover, the dependency on training data in
supervised MMV methods can introduce erroneous conductivity contrasts across
frequencies, posing significant concerns in biomedical applications. To address
these challenges, we propose a novel unsupervised learning approach based on
Multi-Branch Attention Image Prior (MAIP) for mfEIT reconstruction. Our method
employs a carefully designed Multi-Branch Attention Network (MBA-Net) to
represent multiple frequency-dependent conductivity images and simultaneously
reconstructs mfEIT images by iteratively updating its parameters. By leveraging
the implicit regularization capability of the MBA-Net, our algorithm can
capture significant inter- and intra-frequency correlations, enabling robust
mfEIT reconstruction without the need for training data. Through simulation and
real-world experiments, our approach demonstrates performance comparable to, or
better than, SOTA algorithms while exhibiting superior generalization
capability. These results suggest that the MAIP-based method can be used to
improve the reliability and applicability of mfEIT in various settings.</description>
      <guid isPermaLink="false">2409.10794v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>U2UData: A Large-scale Cooperative Perception Dataset for Swarm UAVs Autonomous Flight</title>
      <link>http://arxiv.org/abs/2408.00606v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ACM MM24&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 现代自主飞行感知系统对遮挡敏感，且远程能力有限，这是提高低空经济任务表现的关键瓶颈。&lt;br&gt;&lt;h4&gt;2. U2U合作感知系统的潜力&lt;/h4&gt;   - UAV到UAV（U2U）合作感知系统被认为有潜力革新自主飞行行业。&lt;br&gt;&lt;h4&gt;3. 数据集缺乏的问题&lt;/h4&gt;   - 目前缺乏大规模数据集，阻碍了这一领域的进展。&lt;br&gt;&lt;h4&gt;4. U2UData的介绍&lt;/h4&gt;   - 本文提出了U2UData，这是第一个大规模的群体UAV自主飞行合作感知数据集。&lt;br&gt;&lt;h4&gt;5. 数据集收集方式&lt;/h4&gt;   - 数据集由三架UAV在U2USim环境中自主飞行收集，覆盖9 km²的飞行区域。&lt;br&gt;&lt;h4&gt;6. 数据集内容&lt;/h4&gt;   - 包含315K个LiDAR帧、945K个RGB和深度帧，以及241万个注释的3D边界框，涵盖3个类别。&lt;br&gt;   - 还记录了亮度、温度、湿度、烟雾和气流值，涵盖所有飞行路线。&lt;br&gt;&lt;h4&gt;7. U2USim环境&lt;/h4&gt;   - U2USim是第一个真实世界映射的群体UAV仿真环境，以云南省为原型，包含4种地形、7种天气条件和8种传感器类型。&lt;br&gt;&lt;h4&gt;8. 感知任务的定义&lt;/h4&gt;   - U2UData引入了两个感知任务：合作3D物体检测和合作3D物体跟踪。&lt;br&gt;&lt;h4&gt;9. 基准测试&lt;/h4&gt;   - 本文提供了对这些任务的近期合作感知算法的全面基准测试。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3664647.3681151&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/fengtt42/u2udata&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern perception systems for autonomous flight are sensitive to occlusion
and have limited long-range capability, which is a key bottleneck in improving
low-altitude economic task performance. Recent research has shown that the
UAV-to-UAV (U2U) cooperative perception system has great potential to
revolutionize the autonomous flight industry. However, the lack of a
large-scale dataset is hindering progress in this area. This paper presents
U2UData, the first large-scale cooperative perception dataset for swarm UAVs
autonomous flight. The dataset was collected by three UAVs flying autonomously
in the U2USim, covering a 9 km$^2$ flight area. It comprises 315K LiDAR frames,
945K RGB and depth frames, and 2.41M annotated 3D bounding boxes for 3 classes.
It also includes brightness, temperature, humidity, smoke, and airflow values
covering all flight routes. U2USim is the first real-world mapping swarm UAVs
simulation environment. It takes Yunnan Province as the prototype and includes
4 terrains, 7 weather conditions, and 8 sensor types. U2UData introduces two
perception tasks: cooperative 3D object detection and cooperative 3D object
tracking. This paper provides comprehensive benchmarks of recent cooperative
perception algorithms on these tasks.</description>
      <guid isPermaLink="false">2408.00606v3</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Point Cloud Registration with Self-Distillation</title>
      <link>http://arxiv.org/abs/2409.07558v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Oral at BMVC 2024&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 刚性点云注册是机器人技术和自动驾驶领域中的一个基本问题。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 目前深度学习方法可以训练模型来匹配一对点云，但收集真实变换的成本高，导致训练不具可扩展性。&lt;br&gt;&lt;h4&gt;3. 自蒸馏方法的提出&lt;/h4&gt;   - 提出了一个自蒸馏的方法，以无监督的方式学习点云注册。&lt;br&gt;&lt;h4&gt;4. 教师网络和学生网络&lt;/h4&gt;   - 每个样本传递给教师网络，同时增强视图传递给学生网络。教师网络包含可训练的特征提取器和无学习的鲁棒求解器（如RANSAC）。&lt;br&gt;&lt;h4&gt;5. 一致性强制&lt;/h4&gt;   - 求解器强制保证对应关系的一致性，并优化无监督的内点比率，消除了对真实标签的需求。&lt;br&gt;&lt;h4&gt;6. 简化训练过程&lt;/h4&gt;   - 通过去除对初始手工特征或连续点云帧的需求，简化了训练过程。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 方法在RGB-D基准数据集3DMatch上超越了相关方法，并且在汽车雷达数据上具有良好的泛化能力，而经典特征在该领域的表现较差。&lt;br&gt;&lt;h4&gt;8. 代码可用性&lt;/h4&gt;   - 相关代码可在GitHub上获取，链接为 [https://github.com/boschresearch/direg](https://github.com/boschresearch/direg)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/boschresearch/direg&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Rigid point cloud registration is a fundamental problem and highly relevant
in robotics and autonomous driving. Nowadays deep learning methods can be
trained to match a pair of point clouds, given the transformation between them.
However, this training is often not scalable due to the high cost of collecting
ground truth poses. Therefore, we present a self-distillation approach to learn
point cloud registration in an unsupervised fashion. Here, each sample is
passed to a teacher network and an augmented view is passed to a student
network. The teacher includes a trainable feature extractor and a learning-free
robust solver such as RANSAC. The solver forces consistency among
correspondences and optimizes for the unsupervised inlier ratio, eliminating
the need for ground truth labels. Our approach simplifies the training
procedure by removing the need for initial hand-crafted features or consecutive
point cloud frames as seen in related methods. We show that our method not only
surpasses them on the RGB-D benchmark 3DMatch but also generalizes well to
automotive radar, where classical features adopted by others fail. The code is
available at https://github.com/boschresearch/direg .</description>
      <guid isPermaLink="false">2409.07558v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Physically-Based Photometric Bundle Adjustment in Non-Lambertian Environments</title>
      <link>http://arxiv.org/abs/2409.11854v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to 2024 IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS 2024)&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 光度束调整（PBA）广泛用于通过假设一个朗伯世界来估计相机姿态和3D几何形状。&lt;br&gt;&lt;h4&gt;2. 问题陈述&lt;/h4&gt;   - 由于真实环境中常见的非漫反射，光度一致性的假设常常被违反。&lt;br&gt;&lt;h4&gt;3. 影响&lt;/h4&gt;   - 光度不一致性显著影响现有PBA方法的可靠性。&lt;br&gt;&lt;h4&gt;4. 新方法的提出&lt;/h4&gt;   - 提出了一个新颖的基于物理的PBA方法，以解决光度不一致性问题。&lt;br&gt;&lt;h4&gt;5. 物理基础权重&lt;/h4&gt;   - 引入与材料、照明和光路径相关的物理基础权重，以区分具有不同光度不一致性水平的像素对。&lt;br&gt;&lt;h4&gt;6. 模型设计&lt;/h4&gt;   - 设计了基于序列图像的材料估计模型和基于点云的照明估计模型。&lt;br&gt;&lt;h4&gt;7. 数据集建立&lt;/h4&gt;   - 建立了第一个与SLAM相关的非朗伯场景数据集，并提供了完整的照明和材料的真实值。&lt;br&gt;&lt;h4&gt;8. 实验验证&lt;/h4&gt;   - 大量实验表明，所提出的PBA方法在准确性上优于现有方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Photometric bundle adjustment (PBA) is widely used in estimating the camera
pose and 3D geometry by assuming a Lambertian world. However, the assumption of
photometric consistency is often violated since the non-diffuse reflection is
common in real-world environments. The photometric inconsistency significantly
affects the reliability of existing PBA methods. To solve this problem, we
propose a novel physically-based PBA method. Specifically, we introduce the
physically-based weights regarding material, illumination, and light path.
These weights distinguish the pixel pairs with different levels of photometric
inconsistency. We also design corresponding models for material estimation
based on sequential images and illumination estimation based on point clouds.
In addition, we establish the first SLAM-related dataset of non-Lambertian
scenes with complete ground truth of illumination and material. Extensive
experiments demonstrated that our PBA method outperforms existing approaches in
accuracy.</description>
      <guid isPermaLink="false">2409.11854v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Coupled Laplacian Eigenmaps for Locally-Aware 3D Rigid Point Cloud Matching</title>
      <link>http://arxiv.org/abs/2402.17372v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted at Computer Vision and Patter
  Recognition (CVPR) 2024&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 点云匹配在计算机视觉、医学和机器人领域至关重要，主要用于寻找一对点云或体素之间的对应关系。&lt;br&gt;&lt;h4&gt;2. 局部差异的重要性&lt;/h4&gt;   - 在某些实际场景中，强调局部差异对于准确识别正确匹配至关重要，从而提高匹配过程的整体鲁棒性和可靠性。&lt;br&gt;&lt;h4&gt;3. 现有方法的局限性&lt;/h4&gt;   - 常用的形状描述符存在多个局限，通常无法提供关于配对几何体的有意义的局部信息。&lt;br&gt;&lt;h4&gt;4. 新方法的提出&lt;/h4&gt;   - 本研究提出了一种基于图拉普拉斯特征映射的新技术，通过考虑细致的局部结构来匹配点云。&lt;br&gt;&lt;h4&gt;5. Coupled Laplacian的引入&lt;/h4&gt;   - 为了解决拉普拉斯特征映射的顺序和符号模糊性，提出了一种新的算子——Coupled Laplacian，能够轻松生成多个已注册几何体的对齐特征空间。&lt;br&gt;&lt;h4&gt;6. 相似性度量&lt;/h4&gt;   - 显示对齐的高维空间之间的相似性提供了一个局部有意义的分数，用于形状匹配。&lt;br&gt;&lt;h4&gt;7. 性能评估&lt;/h4&gt;   - 首先在MVTec 3D-AD数据集上进行点对点的性能评估，专注于对象异常定位任务。&lt;br&gt;&lt;h4&gt;8. 新医疗任务的定义&lt;/h4&gt;   - 定义了一项新的医疗任务——自动骨侧估计（BSE），通过来源于耦合特征空间的全局相似性分数来解决。&lt;br&gt;&lt;h4&gt;9. 基准测试的提出&lt;/h4&gt;   - 提出了一个基准测试，收集来自各种公共数据集的骨表面结构。&lt;br&gt;&lt;h4&gt;10. 结果表现&lt;/h4&gt;    - 基于Coupled Laplacian的匹配技术在这两个任务上表现优于其他方法，达到令人印象深刻的准确率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/matteo-bastico/couplap&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud matching, a crucial technique in computer vision, medical and
robotics fields, is primarily concerned with finding correspondences between
pairs of point clouds or voxels. In some practical scenarios, emphasizing local
differences is crucial for accurately identifying a correct match, thereby
enhancing the overall robustness and reliability of the matching process.
Commonly used shape descriptors have several limitations and often fail to
provide meaningful local insights about the paired geometries. In this work, we
propose a new technique, based on graph Laplacian eigenmaps, to match point
clouds by taking into account fine local structures. To deal with the order and
sign ambiguity of Laplacian eigenmaps, we introduce a new operator, called
Coupled Laplacian (https://github.com/matteo-bastico/CoupLap), that allows to
easily generate aligned eigenspaces for multiple registered geometries. We show
that the similarity between those aligned high-dimensional spaces provides a
locally meaningful score to match shapes. We firstly evaluate the performance
of the proposed technique in a point-wise manner, focusing on the task of
object anomaly localization on the MVTec 3D-AD dataset. Additionally, we define
a new medical task, called automatic Bone Side Estimation (BSE), which we
address through a global similarity score derived from coupled eigenspaces. In
order to test it, we propose a benchmark collecting bone surface structures
from various public datasets. Our matching technique, based on Coupled
Laplacian, outperforms other methods by reaching an impressive accuracy on both
tasks.</description>
      <guid isPermaLink="false">2402.17372v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Loss Distillation via Gradient Matching for Point Cloud Completion with Weighted Chamfer Distance</title>
      <link>http://arxiv.org/abs/2409.06171v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 7 figures, 7 tables, this paper was accepted to IEEE/RSJ
  International Conference on Intelligent Robots and Systems (IROS) 2024&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 3D点云提升了机器人感知环境几何信息的能力，使得抓取姿态检测和场景理解等下游任务成为可能。&lt;br&gt;&lt;h4&gt;2. 问题陈述&lt;/h4&gt;   - 这些任务的性能严重依赖于输入数据的质量，不完整的数据会导致较差的结果和失败案例。&lt;br&gt;&lt;h4&gt;3. 现有方法&lt;/h4&gt;   - 近年来，针对深度学习基础的点云补全设计的训练损失函数（如Chamfer距离及其变种HyperCD）表明良好的梯度加权方案可以显著提升性能。&lt;br&gt;&lt;h4&gt;4. 挑战&lt;/h4&gt;   - CD类损失函数通常需要数据相关的参数调优，这对于数据量大的任务来说是耗时的。&lt;br&gt;&lt;h4&gt;5. 研究目标&lt;/h4&gt;   - 本研究旨在找到一系列无需参数调优的加权训练损失（称为weighted CD）。&lt;br&gt;&lt;h4&gt;6. 方法论&lt;/h4&gt;   - 提出了一种搜索方案“通过梯度匹配的损失蒸馏”，以模拟HyperCD和weighted CD之间的反向传播学习行为，从而找到合适的候选损失函数。&lt;br&gt;&lt;h4&gt;7. 优化策略&lt;/h4&gt;   - 一旦找到合适的损失函数，提出了一种新颖的双层优化公式，以基于weighted CD损失训练主干网络。&lt;br&gt;&lt;h4&gt;8. 实验结果&lt;/h4&gt;   - 观察到： &lt;br&gt;     - 使用适当的加权函数，weighted CD的性能可以与HyperCD相媲美。&lt;br&gt;     - Landau加权CD（称为Landau CD）在点云补全任务中表现优于HyperCD，并在多个基准数据集上取得新状态的结果。&lt;br&gt;&lt;h4&gt;9. 代码可用性&lt;/h4&gt;   - 提供了演示代码的链接，方便其他研究人员使用。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/zhang-vislab/iros2024-lossdistillationweightedcd&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D point clouds enhanced the robot's ability to perceive the geometrical
information of the environments, making it possible for many downstream tasks
such as grasp pose detection and scene understanding. The performance of these
tasks, though, heavily relies on the quality of data input, as incomplete can
lead to poor results and failure cases. Recent training loss functions designed
for deep learning-based point cloud completion, such as Chamfer distance (CD)
and its variants (\eg HyperCD ), imply a good gradient weighting scheme can
significantly boost performance. However, these CD-based loss functions usually
require data-related parameter tuning, which can be time-consuming for
data-extensive tasks. To address this issue, we aim to find a family of
weighted training losses ({\em weighted CD}) that requires no parameter tuning.
To this end, we propose a search scheme, {\em Loss Distillation via Gradient
Matching}, to find good candidate loss functions by mimicking the learning
behavior in backpropagation between HyperCD and weighted CD. Once this is done,
we propose a novel bilevel optimization formula to train the backbone network
based on the weighted CD loss. We observe that: (1) with proper weighted
functions, the weighted CD can always achieve similar performance to HyperCD,
and (2) the Landau weighted CD, namely {\em Landau CD}, can outperform HyperCD
for point cloud completion and lead to new state-of-the-art results on several
benchmark datasets. {\it Our demo code is available at
\url{https://github.com/Zhang-VISLab/IROS2024-LossDistillationWeightedCD}.}</description>
      <guid isPermaLink="false">2409.06171v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Reviewer Experience in Code Review Comment Generation</title>
      <link>http://arxiv.org/abs/2409.10959v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 现代代码审查是一种普遍的软件质量保证过程，旨在识别新编写代码中的潜在问题。&lt;br&gt;&lt;h4&gt;2. 挑战&lt;/h4&gt;   - 尽管代码审查有效，但对人类审查员来说，过程消耗大量精力。&lt;br&gt;&lt;h4&gt;3. 研究目的&lt;/h4&gt;   - 为减轻审查员的工作负担，研究人员训练深度学习模型模拟人类审查员生成自然语言代码审查评论，称为代码审查评论生成。&lt;br&gt;&lt;h4&gt;4. 现有方法&lt;/h4&gt;   - 先前的研究利用机器学习技术和神经模型（如迁移学习和变换器架构）在该任务上取得了改进。&lt;br&gt;&lt;h4&gt;5. 问题&lt;/h4&gt;   - 生成的评论质量仍然不理想，原因在于用于模型训练的开源代码审查数据质量较低，尤其是来自具有不同软件开发经验的审查员的反馈。&lt;br&gt;&lt;h4&gt;6. 提出的新方法&lt;/h4&gt;   - 为了适应这种经验差异，提出了一套经验感知训练方法，利用审查员的过往创作和审查经验作为评论质量的信号。&lt;br&gt;&lt;h4&gt;7. 经验感知损失函数（ELF）&lt;/h4&gt;   - 具体而言，提出了经验感知损失函数（ELF），将审查员在项目中的创作和审查权重作为模型损失函数的权重，从而增强经验丰富的审查员对模型行为的影响。&lt;br&gt;&lt;h4&gt;8. 实验结果&lt;/h4&gt;   - 与现有最先进模型相比，ELF能够生成更高质量的评论，在准确性、信息量和评论类型生成方面均有所提升。&lt;br&gt;&lt;h4&gt;9. 主要贡献&lt;/h4&gt;   - 本研究的关键贡献在于展示如何将传统软件工程概念（如审查员经验）集成到基于AI的自动化代码审查模型设计中。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern code review is a ubiquitous software quality assurance process aimed
at identifying potential issues within newly written code. Despite its
effectiveness, the process demands large amounts of effort from the human
reviewers involved. To help alleviate this workload, researchers have trained
deep learning models to imitate human reviewers in providing natural language
code reviews. Formally, this task is known as code review comment generation.
Prior work has demonstrated improvements in this task by leveraging machine
learning techniques and neural models, such as transfer learning and the
transformer architecture. However, the quality of the model generated reviews
remain sub-optimal due to the quality of the open-source code review data used
in model training. This is in part due to the data obtained from open-source
projects where code reviews are conducted in a public forum, and reviewers
possess varying levels of software development experience, potentially
affecting the quality of their feedback. To accommodate for this variation, we
propose a suite of experience-aware training methods that utilise the
reviewers' past authoring and reviewing experiences as signals for review
quality. Specifically, we propose experience-aware loss functions (ELF), which
use the reviewers' authoring and reviewing ownership of a project as weights in
the model's loss function. Through this method, experienced reviewers' code
reviews yield larger influence over the model's behaviour. Compared to the SOTA
model, ELF was able to generate higher quality reviews in terms of accuracy,
informativeness, and comment types generated. The key contribution of this work
is the demonstration of how traditional software engineering concepts such as
reviewer experience can be integrated into the design of AI-based automated
code review models.</description>
      <guid isPermaLink="false">2409.10959v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>DETECLAP: Enhancing Audio-Visual Representation Learning with Object Information</title>
      <link>http://arxiv.org/abs/2409.11729v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  under review&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 当前的音频-视觉表示学习能够捕捉粗略的物体类别（如“动物”和“乐器”），但缺乏识别细粒度细节的能力（如“狗”和“长笛”）。&lt;br&gt;&lt;h4&gt;2. 提出的方法&lt;/h4&gt;   - 引入DETECLAP方法，旨在增强音频-视觉表示学习的物体信息感知能力。&lt;br&gt;&lt;h4&gt;3. 核心思想&lt;/h4&gt;   - 将音频-视觉标签预测损失引入现有的对比音频-视觉掩蔽自编码器（Contrastive Audio-Visual Masked AutoEncoder），以提升模型的物体意识。&lt;br&gt;&lt;h4&gt;4. 标签准备&lt;/h4&gt;   - 为避免高成本的手动注释，利用最先进的语言-音频模型和物体检测器从音频和视觉输入中准备物体标签。&lt;br&gt;&lt;h4&gt;5. 评估方法&lt;/h4&gt;   - 使用VGGSound和AudioSet20K数据集评估音频-视觉检索和分类的方法。&lt;br&gt;&lt;h4&gt;6. 实验结果&lt;/h4&gt;   - 在音频到视觉检索中，召回率提升1.5%；在视觉到音频检索中，召回率提升1.2%；音频-视觉分类准确率提升0.6%。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current audio-visual representation learning can capture rough object
categories (e.g., ``animals'' and ``instruments''), but it lacks the ability to
recognize fine-grained details, such as specific categories like ``dogs'' and
``flutes'' within animals and instruments. To address this issue, we introduce
DETECLAP, a method to enhance audio-visual representation learning with object
information. Our key idea is to introduce an audio-visual label prediction loss
to the existing Contrastive Audio-Visual Masked AutoEncoder to enhance its
object awareness. To avoid costly manual annotations, we prepare object labels
from both audio and visual inputs using state-of-the-art language-audio models
and object detectors. We evaluate the method of audio-visual retrieval and
classification using the VGGSound and AudioSet20K datasets. Our method achieves
improvements in recall@10 of +1.5% and +1.2% for audio-to-visual and
visual-to-audio retrieval, respectively, and an improvement in accuracy of
+0.6% for audio-visual classification.</description>
      <guid isPermaLink="false">2409.11729v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Hybrid-Task Meta-Learning: A Graph Neural Network Approach for Scalable and Transferable Bandwidth Allocation</title>
      <link>http://arxiv.org/abs/2401.10253v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究目标&lt;/h4&gt;   - 开发一个基于深度学习的带宽分配策略，具备以下特性：&lt;br&gt;     - 1) 可扩展性：能够适应用户数量的变化。&lt;br&gt;     - 2) 可迁移性：适用于不同通信场景，如非平稳无线信道、不同的服务质量（QoS）要求以及动态可用资源。&lt;br&gt;&lt;h4&gt;2. 可扩展性实现&lt;/h4&gt;   - 带宽分配策略通过图神经网络（GNN）表示，训练参数的数量不随用户数量变化，确保了可扩展性。&lt;br&gt;&lt;h4&gt;3. 通用性增强&lt;/h4&gt;   - 为了提升GNN的泛化能力，提出了一种混合任务元学习（HML）算法：&lt;br&gt;     - 在元训练阶段，利用不同通信场景训练GNN的初始参数。&lt;br&gt;     - 在元测试阶段，使用少量样本对GNN进行微调，以适应未见过的通信场景。&lt;br&gt;&lt;h4&gt;4. 实验结果&lt;/h4&gt;   - 仿真结果表明，HML方法相较于现有基准可提升初始性能8.79%，并提高采样效率73%。&lt;br&gt;&lt;h4&gt;5. 微调效果&lt;/h4&gt;   - 在微调后，基于GNN的近优政策在推理复杂度显著降低的情况下，能够实现接近于最佳策略的相似奖励，这一最佳策略是通过迭代优化获得的。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2023-12-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we develop a deep learning-based bandwidth allocation policy
that is: 1) scalable with the number of users and 2) transferable to different
communication scenarios, such as non-stationary wireless channels, different
quality-of-service (QoS) requirements, and dynamically available resources. To
support scalability, the bandwidth allocation policy is represented by a graph
neural network (GNN), with which the number of training parameters does not
change with the number of users. To enable the generalization of the GNN, we
develop a hybrid-task meta-learning (HML) algorithm that trains the initial
parameters of the GNN with different communication scenarios during
meta-training. Next, during meta-testing, a few samples are used to fine-tune
the GNN with unseen communication scenarios. Simulation results demonstrate
that our HML approach can improve the initial performance by $8.79\%$, and
sampling efficiency by $73\%$, compared with existing benchmarks. After
fine-tuning, our near-optimal GNN-based policy can achieve close to the same
reward with much lower inference complexity compared to the optimal policy
obtained using iterative optimization.</description>
      <guid isPermaLink="false">2401.10253v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Attention-Enhanced Feature Fusion-based Weekly Supervised Anomaly Violence Detection</title>
      <link>http://arxiv.org/abs/2409.11223v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 弱监督视频异常检测（WS-VAD）是计算机视觉中的关键领域，旨在开发智能监控系统。&lt;br&gt;&lt;h4&gt;2. 特征流&lt;/h4&gt;   - 系统使用三种特征流：RGB视频、光流和音频信号，每个流提取互补的空间和时间特征，以提高检测准确性和鲁棒性。&lt;br&gt;&lt;h4&gt;3. RGB视频流处理&lt;/h4&gt;   - 第一个流采用基于注意力的多阶段特征增强方法：&lt;br&gt;     - **第一阶段**：使用基于视觉变换器（ViT）的CLIP模块，结合I3D和时间上下文聚合（TCA）的丰富时空特征。&lt;br&gt;     - **第二阶段**：通过不确定性调节的双重记忆单元（UR-DMU）模型有效捕捉时间依赖性，同时学习正常和异常数据的表示。&lt;br&gt;     - **第三阶段**：选择最相关的时空特征。&lt;br&gt;&lt;h4&gt;4. 光流特征提取&lt;/h4&gt;   - 第二个流通过深度学习和注意力模块的整合，从光流数据中提取增强的时空特征。&lt;br&gt;&lt;h4&gt;5. 音频特征处理&lt;/h4&gt;   - 音频流使用与VGGish模型集成的注意力模块捕捉听觉线索，旨在基于声音模式检测异常。&lt;br&gt;&lt;h4&gt;6. 多模态融合&lt;/h4&gt;   - 通过融合不同模态的特征，模型整合运动和音频信号，这些信号通常指示无法通过视觉分析单独检测的异常事件。&lt;br&gt;&lt;h4&gt;7. 性能提升&lt;/h4&gt;   - 多模态融合的特征集显著提高了异常检测的准确性和鲁棒性，在三个数据集上表现出色。&lt;br&gt;&lt;h4&gt;8. 实验结果&lt;/h4&gt;   - 大规模实验表明，所提系统在三个基准数据集上的性能优于现有最先进的系统，证明了该方法的有效性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Weakly supervised video anomaly detection (WS-VAD) is a crucial area in
computer vision for developing intelligent surveillance systems. This system
uses three feature streams: RGB video, optical flow, and audio signals, where
each stream extracts complementary spatial and temporal features using an
enhanced attention module to improve detection accuracy and robustness. In the
first stream, we employed an attention-based, multi-stage feature enhancement
approach to improve spatial and temporal features from the RGB video where the
first stage consists of a ViT-based CLIP module, with top-k features
concatenated in parallel with I3D and Temporal Contextual Aggregation (TCA)
based rich spatiotemporal features. The second stage effectively captures
temporal dependencies using the Uncertainty-Regulated Dual Memory Units
(UR-DMU) model, which learns representations of normal and abnormal data
simultaneously, and the third stage is employed to select the most relevant
spatiotemporal features. The second stream extracted enhanced attention-based
spatiotemporal features from the flow data modality-based feature by taking
advantage of the integration of the deep learning and attention module. The
audio stream captures auditory cues using an attention module integrated with
the VGGish model, aiming to detect anomalies based on sound patterns. These
streams enrich the model by incorporating motion and audio signals often
indicative of abnormal events undetectable through visual analysis alone. The
concatenation of the multimodal fusion leverages the strengths of each
modality, resulting in a comprehensive feature set that significantly improves
anomaly detection accuracy and robustness across three datasets. The extensive
experiment and high performance with the three benchmark datasets proved the
effectiveness of the proposed system over the existing state-of-the-art system.</description>
      <guid isPermaLink="false">2409.11223v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised state learning from pairs of states</title>
      <link>http://arxiv.org/abs/2409.11120v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 收到一系列量子比特（qubits），每个量子比特保证处于两种纯态之一，但具体状态未知。&lt;br&gt;&lt;h4&gt;2. 任务目标&lt;/h4&gt;   - 任务是确定这些状态或构建一个正算子值测量（POVM）来区分这两种状态，类似于无监督学习的量子版本。&lt;br&gt;&lt;h4&gt;3. 问题挑战&lt;/h4&gt;   - 在没有更多信息的情况下，仅能确定序列的密度矩阵，而密度矩阵通常可以以多种方式分解成纯态。&lt;br&gt;&lt;h4&gt;4. 额外信息的需求&lt;/h4&gt;   - 为了解决此问题，需要额外的信息，可能是经典信息或量子信息。&lt;br&gt;&lt;h4&gt;5. 解决方案&lt;/h4&gt;   - 如果提供每个量子比特的额外副本，即接收到配对的量子比特（两者处于相同状态），则可以完成任务。&lt;br&gt;&lt;h4&gt;6. 模拟实验&lt;/h4&gt;   - 进行了数值模拟，测量了一系列量子比特对，结果显示可以高精度地找到未知状态及其出现的概率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Suppose you receive a sequence of qubits where each qubit is guaranteed to be
in one of two pure states, but you do not know what those states are. Your task
is to either determine the states or to construct a POVM (Positive Operator
Valued Measure) that will discriminate them. This can be viewed as a quantum
analog of unsupervised learning. A problem is that without more information,
all that can be determined is the density matrix of the sequence, and, in
general, density matrices can be decomposed into pure states in many different
ways. To solve the problem additional information, either classical or quantum,
is required. We show that if an additional copy of each qubit is supplied, that
is, one receives pairs of qubits, both in the same state, rather than single
qubits, the task can be accomplished. We then simulate numerically the
measurement of a sequence of qubit pairs and show that the unknown states and
their respective probabilities of occurrence can be found with high accuracy.</description>
      <guid isPermaLink="false">2409.11120v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Efficiently Expanding Receptive Fields: Local Split Attention and Parallel Aggregation for Enhanced Large-scale Point Cloud Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2409.01662v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 扩大深度学习模型的感受野对于大规模3D点云分割是一种有效技术，能捕获丰富的上下文信息，增强网络学习有意义特征的能力。&lt;br&gt;&lt;h4&gt;2. 挑战&lt;/h4&gt;   - 扩大感受野常导致计算复杂性增加和过拟合风险，影响学习效率和效果。&lt;br&gt;&lt;h4&gt;3. 提出的解决方案&lt;/h4&gt;   - 提出了局部分割注意力池化（LSAP）机制，通过一系列局部分割操作有效扩大感受野，便于获取更广泛的上下文知识。&lt;br&gt;&lt;h4&gt;4. 优化计算负载&lt;/h4&gt;   - 同时优化与注意力池化层相关的计算负载，以确保更流畅的处理流程。&lt;br&gt;&lt;h4&gt;5. 引入PAE模块&lt;/h4&gt;   - 基于LSAP，提出了并行聚合增强（PAE）模块，使数据的2D和3D邻域信息能够并行处理，进一步增强网络中的上下文表示。&lt;br&gt;&lt;h4&gt;6. 新框架LSNet&lt;/h4&gt;   - 提出了一个新框架，称为LSNet，用于大规模点云语义分割。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 广泛的评估表明，PAE模块的无缝集成显著改善了现有框架的平均交并比（mIoU）指标，提升幅度最高可达11%。&lt;br&gt;&lt;h4&gt;8. 性能比较&lt;/h4&gt;   - LSNet在三个基准数据集（S3DIS、Toronto3D和SensatUrban）上表现优于最先进的语义分割网络。&lt;br&gt;&lt;h4&gt;9. 计算效率&lt;/h4&gt;   - 相较于使用相似感受野的其他方法，LSNet实现了约38.8%的显著加速，突显了其计算效率和在真实大规模场景中的实用性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Expanding the receptive field in a deep learning model for large-scale 3D
point cloud segmentation is an effective technique for capturing rich
contextual information, which consequently enhances the network's ability to
learn meaningful features. However, this often leads to increased computational
complexity and risk of overfitting, challenging the efficiency and
effectiveness of the learning paradigm. To address these limitations, we
propose the Local Split Attention Pooling (LSAP) mechanism to effectively
expand the receptive field through a series of local split operations, thus
facilitating the acquisition of broader contextual knowledge. Concurrently, it
optimizes the computational workload associated with attention-pooling layers
to ensure a more streamlined processing workflow. Based on LSAP, a Parallel
Aggregation Enhancement (PAE) module is introduced to enable parallel
processing of data using both 2D and 3D neighboring information to further
enhance contextual representations within the network. In light of the
aforementioned designs, we put forth a novel framework, designated as LSNet,
for large-scale point cloud semantic segmentation. Extensive evaluations
demonstrated the efficacy of seamlessly integrating the proposed PAE module
into existing frameworks, yielding significant improvements in mean
intersection over union (mIoU) metrics, with a notable increase of up to 11%.
Furthermore, LSNet demonstrated superior performance compared to
state-of-the-art semantic segmentation networks on three benchmark datasets,
including S3DIS, Toronto3D, and SensatUrban. It is noteworthy that our method
achieved a substantial speedup of approximately 38.8% compared to those
employing similar-sized receptive fields, which serves to highlight both its
computational efficiency and practical utility in real-world large-scale
scenes.</description>
      <guid isPermaLink="false">2409.01662v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Metric-Semantic Factor Graph Generation based on Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2409.11972v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to ICRA 2025&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 理解几何结构与语义概念之间的关系对于构建复杂环境的准确模型至关重要。&lt;br&gt;&lt;h4&gt;2. 室内空间的特点&lt;/h4&gt;   - 在室内环境中，某些空间约束（如平面之间的相对位置）在布局变化中保持一致。&lt;br&gt;&lt;h4&gt;3. 研究目标&lt;/h4&gt;   - 探索如何在图形SLAM框架中捕捉这些不变关系，通过优化因子图将高层概念（如房间和墙壁）与几何元素（如平面）连接。&lt;br&gt;&lt;h4&gt;4. 现有方法的局限&lt;/h4&gt;   - 之前的研究主要采用针对每个概念生成的临时解决方案，并使用手动定义的因子。&lt;br&gt;&lt;h4&gt;5. 新方法的提出&lt;/h4&gt;   - 本文提出了一种新颖的度量-语义因子图生成方法，包括定义语义场景图、整合几何信息和学习相互连接的因子，全部基于图神经网络（GNNs）。&lt;br&gt;&lt;h4&gt;6. 边缘分类网络&lt;/h4&gt;   - 边缘分类网络（G-GNN）将平面之间的边缘分类为“同一房间”、“同一墙壁”或“无”类型。&lt;br&gt;&lt;h4&gt;7. 聚类生成&lt;/h4&gt;   - 将生成的关系进行聚类，为每个聚类生成一个房间或墙壁。&lt;br&gt;&lt;h4&gt;8. 几何起源推断&lt;/h4&gt;   - 第二类网络（F-GNN）推断新节点的几何起源。&lt;br&gt;&lt;h4&gt;9. 因子的定义&lt;/h4&gt;   - 因子的定义使用与生成节点的度量属性相同的F-GNN。&lt;br&gt;&lt;h4&gt;10. 与S-Graphs+算法的共享&lt;/h4&gt;    - 将新的因子图与S-Graphs+算法共享，扩展其图的表现力和场景表示，最终目标是提高SLAM性能。&lt;br&gt;&lt;h4&gt;11. 环境复杂性&lt;/h4&gt;    - 通过在L形房间上训练网络，将环境复杂性增加到N平面房间。&lt;br&gt;&lt;h4&gt;12. 评估方法&lt;/h4&gt;    - 在合成和模拟场景中进行评估，因为缺乏所需复杂布局的真实数据集。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding the relationships between geometric structures and semantic
concepts is crucial for building accurate models of complex environments. In
indoors, certain spatial constraints, such as the relative positioning of
planes, remain consistent despite variations in layout. This paper explores how
these invariant relationships can be captured in a graph SLAM framework by
representing high-level concepts like rooms and walls, linking them to
geometric elements like planes through an optimizable factor graph. Several
efforts have tackled this issue with add-hoc solutions for each concept
generation and with manually-defined factors.
  This paper proposes a novel method for metric-semantic factor graph
generation which includes defining a semantic scene graph, integrating
geometric information, and learning the interconnecting factors, all based on
Graph Neural Networks (GNNs). An edge classification network (G-GNN) sorts the
edges between planes into same room, same wall or none types. The resulting
relations are clustered, generating a room or wall for each cluster. A second
family of networks (F-GNN) infers the geometrical origin of the new nodes. The
definition of the factors employs the same F-GNN used for the metric attribute
of the generated nodes. Furthermore, share the new factor graph with the
S-Graphs+ algorithm, extending its graph expressiveness and scene
representation with the ultimate goal of improving the SLAM performance. The
complexity of the environments is increased to N-plane rooms by training the
networks on L-shaped rooms. The framework is evaluated in synthetic and
simulated scenarios as no real datasets of the required complex layouts are
available.</description>
      <guid isPermaLink="false">2409.11972v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>SpheriGait: Enriching Spatial Representation via Spherical Projection for LiDAR-based Gait Recognition</title>
      <link>http://arxiv.org/abs/2409.11869v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 步态识别是一种快速发展的远程识别个体的技术。&lt;br&gt;&lt;h4&gt;2. 现有研究局限&lt;/h4&gt;   - 之前的研究主要使用2D传感器收集步态数据，虽然取得了显著进展，但忽视了3D动态特征对识别的影响。&lt;br&gt;&lt;h4&gt;3. LiDAR的优势&lt;/h4&gt;   - 使用LiDAR 3D点云进行步态识别不仅能直接捕捉3D空间特征，还能减少光照条件的影响，同时保护隐私。&lt;br&gt;&lt;h4&gt;4. 研究问题&lt;/h4&gt;   - 关键在于如何有效提取点云中的区分性3D动态表示。&lt;br&gt;&lt;h4&gt;5. 提出的方法&lt;/h4&gt;   - 本文提出了一种名为SpheriGait的方法，用于从点云中提取和增强动态特征，以实现基于LiDAR的步态识别。&lt;br&gt;&lt;h4&gt;6. 方法细节&lt;/h4&gt;   - SpheriGait用球面投影替代传统的点云平面投影方法，以增强动态特征的感知。&lt;br&gt;&lt;h4&gt;7. 网络模块&lt;/h4&gt;   - 提出了一个名为DAM-L的网络模块，用于从投影后的点云数据中提取步态线索。&lt;br&gt;&lt;h4&gt;8. 实验结果&lt;/h4&gt;   - 进行了广泛实验，结果表明SpheriGait在SUSTech1K数据集上达到了最先进的性能。&lt;br&gt;   - 验证了球面投影方法可作为通用数据预处理技术，增强其他基于LiDAR的步态识别方法的性能，展现出卓越的灵活性和实用性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Gait recognition is a rapidly progressing technique for the remote
identification of individuals. Prior research predominantly employing 2D
sensors to gather gait data has achieved notable advancements; nonetheless,
they have unavoidably neglected the influence of 3D dynamic characteristics on
recognition. Gait recognition utilizing LiDAR 3D point clouds not only directly
captures 3D spatial features but also diminishes the impact of lighting
conditions while ensuring privacy protection.The essence of the problem lies in
how to effectively extract discriminative 3D dynamic representation from point
clouds.In this paper, we proposes a method named SpheriGait for extracting and
enhancing dynamic features from point clouds for Lidar-based gait recognition.
Specifically, it substitutes the conventional point cloud plane projection
method with spherical projection to augment the perception of dynamic
feature.Additionally, a network block named DAM-L is proposed to extract gait
cues from the projected point cloud data. We conducted extensive experiments
and the results demonstrated the SpheriGait achieved state-of-the-art
performance on the SUSTech1K dataset, and verified that the spherical
projection method can serve as a universal data preprocessing technique to
enhance the performance of other LiDAR-based gait recognition methods,
exhibiting exceptional flexibility and practicality.</description>
      <guid isPermaLink="false">2409.11869v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>IMRL: Integrating Visual, Physical, Temporal, and Geometric Representations for Enhanced Food Acquisition</title>
      <link>http://arxiv.org/abs/2409.12092v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 机器人辅助进食对改善有进食障碍个体的生活质量具有重要潜力。&lt;br&gt;&lt;h4&gt;2. 研究挑战&lt;/h4&gt;   - 在不同条件下获取多样化的食品并泛化到未见过的食品面临独特挑战。&lt;br&gt;   - 现有方法依赖于表面几何信息（如边界框和姿态），这些信息基于视觉线索（如颜色、形状和纹理），但适应性和鲁棒性不足。&lt;br&gt;&lt;h4&gt;3. 问题原因&lt;/h4&gt;   - 食品在物理属性相似但视觉外观不同的情况下，现有方法往往表现较差。&lt;br&gt;&lt;h4&gt;4. 方法介绍&lt;/h4&gt;   - 本文采用模仿学习（IL）来学习食品获取策略。&lt;br&gt;   - 现有方法使用IL或强化学习（RL）基于现成的图像编码器（如ResNet-50），但这些表示不够鲁棒，难以在多样化的获取场景中泛化。&lt;br&gt;&lt;h4&gt;5. 新方法的提出&lt;/h4&gt;   - 提出了IMRL（集成多维表示学习），该方法集成视觉、物理、时间和几何表示，以增强IL在食品获取中的鲁棒性和泛化能力。&lt;br&gt;&lt;h4&gt;6. 方法特点&lt;/h4&gt;   - IMRL能够捕捉食品类型和物理属性（如固体、半固体、颗粒状、液体和混合物），建模获取动作的时间动态，并引入几何信息来确定最佳舀取点和评估碗的饱和度。&lt;br&gt;&lt;h4&gt;7. 适应性&lt;/h4&gt;   - IMRL使IL能够根据上下文自适应调整舀取策略，从而提高机器人处理多样食品获取场景的能力。&lt;br&gt;&lt;h4&gt;8. 实验结果&lt;/h4&gt;   - 在真实机器人上的实验表明，所提方法在各种食品和碗配置下表现出鲁棒性和适应性，包括对未见设置的零-shot泛化。&lt;br&gt;   - 与最佳基线相比，成功率提高了最多35%。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robotic assistive feeding holds significant promise for improving the quality
of life for individuals with eating disabilities. However, acquiring diverse
food items under varying conditions and generalizing to unseen food presents
unique challenges. Existing methods that rely on surface-level geometric
information (e.g., bounding box and pose) derived from visual cues (e.g.,
color, shape, and texture) often lacks adaptability and robustness, especially
when foods share similar physical properties but differ in visual appearance.
We employ imitation learning (IL) to learn a policy for food acquisition.
Existing methods employ IL or Reinforcement Learning (RL) to learn a policy
based on off-the-shelf image encoders such as ResNet-50. However, such
representations are not robust and struggle to generalize across diverse
acquisition scenarios. To address these limitations, we propose a novel
approach, IMRL (Integrated Multi-Dimensional Representation Learning), which
integrates visual, physical, temporal, and geometric representations to enhance
the robustness and generalizability of IL for food acquisition. Our approach
captures food types and physical properties (e.g., solid, semi-solid, granular,
liquid, and mixture), models temporal dynamics of acquisition actions, and
introduces geometric information to determine optimal scooping points and
assess bowl fullness. IMRL enables IL to adaptively adjust scooping strategies
based on context, improving the robot's capability to handle diverse food
acquisition scenarios. Experiments on a real robot demonstrate our approach's
robustness and adaptability across various foods and bowl configurations,
including zero-shot generalization to unseen settings. Our approach achieves
improvement up to $35\%$ in success rate compared with the best-performing
baseline.</description>
      <guid isPermaLink="false">2409.12092v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>GLIM: 3D Range-Inertial Localization and Mapping with GPU-Accelerated Scan Matching Factors</title>
      <link>http://arxiv.org/abs/2407.10344v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Robotics and Autonomous Systems&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究主题&lt;/h4&gt;   - 本文介绍了GLIM，一个结合GPU加速扫描匹配因子的3D范围惯性定位与地图构建框架。&lt;br&gt;&lt;h4&gt;2. 里程计估计模块&lt;/h4&gt;   - GLIM的里程计估计模块结合了固定滞后平滑和基于关键帧的点云匹配，可以处理几秒钟内完全退化的范围数据，同时有效减少轨迹估计漂移。&lt;br&gt;&lt;h4&gt;3. 多摄像头视觉特征约束&lt;/h4&gt;   - 模块以紧密耦合的方式整合多摄像头视觉特征约束，进一步提升稳定性和准确性。&lt;br&gt;&lt;h4&gt;4. 全局轨迹优化模块&lt;/h4&gt;   - 全局轨迹优化模块直接最小化整个地图上子图之间的配准误差，使得可以准确约束子图之间相对姿态，即使重叠部分较小。&lt;br&gt;&lt;h4&gt;5. 实时性能&lt;/h4&gt;   - 尽管里程计估计和全局轨迹优化算法的计算需求远高于现有方法，但由于对配准误差评估算法和整个系统的精心设计，使得它们能够实时运行，充分利用GPU并行处理能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-07-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1016/j.robot.2024.104750&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/koide3/glim&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This article presents GLIM, a 3D range-inertial localization and mapping
framework with GPU-accelerated scan matching factors. The odometry estimation
module of GLIM employs a combination of fixed-lag smoothing and keyframe-based
point cloud matching that makes it possible to deal with a few seconds of
completely degenerated range data while efficiently reducing trajectory
estimation drift. It also incorporates multi-camera visual feature constraints
in a tightly coupled way to further improve the stability and accuracy. The
global trajectory optimization module directly minimizes the registration
errors between submaps over the entire map. This approach enables us to
accurately constrain the relative pose between submaps with a small overlap.
Although both the odometry estimation and global trajectory optimization
algorithms require much more computation than existing methods, we show that
they can be run in real-time due to the careful design of the registration
error evaluation algorithm and the entire system to fully leverage GPU parallel
processing.</description>
      <guid isPermaLink="false">2407.10344v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>CF-PRNet: Coarse-to-Fine Prototype Refining Network for Point Cloud Completion and Reconstruction</title>
      <link>http://arxiv.org/abs/2409.08443v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Technical Report of the 1st place solution to CVPPA@ECCV2024: Shape
  Completion and Reconstruction of Sweet Peppers Challenge&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 在现代农业中，精确监测植物和水果对于高通量表型分析和自动化收获至关重要。&lt;br&gt;&lt;h4&gt;2. 研究问题&lt;/h4&gt;   - 本文解决了从部分视角重建水果准确3D形状的挑战，这在农业环境中很常见。&lt;br&gt;&lt;h4&gt;3. 方法介绍&lt;/h4&gt;   - 提出了CF-PRNet，一种粗细网格原型精炼网络，训练阶段利用高分辨率3D数据，但在实时推理时仅需单张RGB-D图像。&lt;br&gt;&lt;h4&gt;4. 方法流程&lt;/h4&gt;   - 从水果的部分视角提取不完整的点云数据，通过一系列卷积块进行处理。&lt;br&gt;   - 提取的特征用于生成缩放向量，以精炼两个逐步构建的3D网格原型：一个粗略的和一个细致的。&lt;br&gt;&lt;h4&gt;5. 精细化过程&lt;/h4&gt;   - 这种渐进式精炼有助于详细完成最终点云，实现精确的重建。&lt;br&gt;&lt;h4&gt;6. 性能评估&lt;/h4&gt;   - CF-PRNet在性能指标上表现优异，Chamfer距离为3.78，F1得分为66.76%，精度为56.56%，召回率为85.31%。&lt;br&gt;   - 在“甜椒形状补全与重建挑战”中获得第一名。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/uqzhichen/CF-PRNet&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In modern agriculture, precise monitoring of plants and fruits is crucial for
tasks such as high-throughput phenotyping and automated harvesting. This paper
addresses the challenge of reconstructing accurate 3D shapes of fruits from
partial views, which is common in agricultural settings. We introduce CF-PRNet,
a coarse-to-fine prototype refining network, leverages high-resolution 3D data
during the training phase but requires only a single RGB-D image for real-time
inference. Our approach begins by extracting the incomplete point cloud data
that constructed from a partial view of a fruit with a series of convolutional
blocks. The extracted features inform the generation of scaling vectors that
refine two sequentially constructed 3D mesh prototypes - one coarse and one
fine-grained. This progressive refinement facilitates the detailed completion
of the final point clouds, achieving detailed and accurate reconstructions.
CF-PRNet demonstrates excellent performance metrics with a Chamfer Distance of
3.78, an F1 Score of 66.76%, a Precision of 56.56%, and a Recall of 85.31%, and
win the first place in the Shape Completion and Reconstruction of Sweet Peppers
Challenge.</description>
      <guid isPermaLink="false">2409.08443v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>UltimateDO: An Efficient Framework to Marry Occupancy Prediction with 3D Object Detection via Channel2height</title>
      <link>http://arxiv.org/abs/2409.11160v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 占用检测和3D物体检测是现代自动驾驶系统中的两个标准任务。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 目前的做法是为每个任务部署独立模型，或设计具有独立头部的多任务范式，但存在部署困难（如3D卷积、变换器等）和任务协调不足的问题。&lt;br&gt;&lt;h4&gt;3. 框架设计的目标&lt;/h4&gt;   - 提出应设计一个有利的框架，以实现对多种边缘芯片的便捷部署，同时保持高精度和较少的时间消耗。&lt;br&gt;&lt;h4&gt;4. 新范式的提出&lt;/h4&gt;   - 重新审视3D物体检测与占用预测之间的交互范式，采用2D卷积对模型进行重新构建，并优先考虑每个任务对其他任务的贡献。&lt;br&gt;&lt;h4&gt;5. 提出的方法&lt;/h4&gt;   - 提出了一种快速3D物体检测和占用预测的方法（UltimateDO），将轻量级占用预测头（FlashOcc）与3D物体检测网络结合，实现了仅增加1.1毫秒的额外时间消耗，同时促进了相互支持。&lt;br&gt;&lt;h4&gt;6. 实验验证&lt;/h4&gt;   - 在具有挑战性的nuScenes系列基准上实现UltimateDO，验证其效果。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Occupancy and 3D object detection are characterized as two standard tasks in
modern autonomous driving system. In order to deploy them on a series of edge
chips with better precision and time-consuming trade-off, contemporary
approaches either deploy standalone models for individual tasks, or design a
multi-task paradigm with separate heads. However, they might suffer from
deployment difficulties (i.e., 3D convolution, transformer and so on) or
deficiencies in task coordination. Instead, we argue that a favorable framework
should be devised in pursuit of ease deployment on diverse chips and high
precision with little time-consuming. Oriented at this, we revisit the paradigm
for interaction between 3D object detection and occupancy prediction,
reformulate the model with 2D convolution and prioritize the tasks such that
each contributes to other. Thus, we propose a method to achieve fast 3D object
detection and occupancy prediction (UltimateDO), wherein the light occupancy
prediction head in FlashOcc is married to 3D object detection network, with
negligible additional timeconsuming of only 1.1ms while facilitating each
other. We instantiate UltimateDO on the challenging nuScenes-series benchmarks.</description>
      <guid isPermaLink="false">2409.11160v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Domain Generalization through Meta-Learning: A Survey</title>
      <link>http://arxiv.org/abs/2404.02785v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 深度神经网络（DNNs）在人工智能领域取得了革命性进展，但在处理分布外（OOD）数据时常表现不佳。&lt;br&gt;   - OOD数据是由于现实应用中不可避免的领域变化导致的常见情况。&lt;br&gt;&lt;h4&gt;2. 问题描述&lt;/h4&gt;   - DNNs通常假设训练和测试数据具有相同的分布，但这一假设在实践中常常被违反。&lt;br&gt;   - 尽管DNNs在大数据和计算能力上表现出色，但在分布变化和有限标记数据的情况下，容易导致过拟合和在不同任务和领域中的泛化能力较差。&lt;br&gt;&lt;h4&gt;3. 元学习的潜力&lt;/h4&gt;   - 元学习是一种有前景的方法，通过使用算法获取不同任务之间的可转移知识，实现快速适应，避免每个任务从头学习的需求。&lt;br&gt;&lt;h4&gt;4. 论文重点&lt;/h4&gt;   - 本文重点探讨元学习在领域泛化中的贡献，首先阐明元学习的概念，并基于特征提取策略和分类器学习方法引入一种新分类法，为方法提供更细致的视角。&lt;br&gt;&lt;h4&gt;5. 决策图的设计&lt;/h4&gt;   - 提出了一个决策图，帮助读者根据数据可用性和领域变化导航分类法，从而选择和开发适合其特定问题需求的模型。&lt;br&gt;&lt;h4&gt;6. 文献综述&lt;/h4&gt;   - 通过对现有方法及其理论的全面回顾，阐明该领域的基础知识。&lt;br&gt;&lt;h4&gt;7. 研究方向&lt;/h4&gt;   - 本文提供了实用的见解，并对有前景的研究方向进行了深入讨论。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-04-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep neural networks (DNNs) have revolutionized artificial intelligence but
often lack performance when faced with out-of-distribution (OOD) data, a common
scenario due to the inevitable domain shifts in real-world applications. This
limitation stems from the common assumption that training and testing data
share the same distribution--an assumption frequently violated in practice.
Despite their effectiveness with large amounts of data and computational power,
DNNs struggle with distributional shifts and limited labeled data, leading to
overfitting and poor generalization across various tasks and domains.
Meta-learning presents a promising approach by employing algorithms that
acquire transferable knowledge across various tasks for fast adaptation,
eliminating the need to learn each task from scratch. This survey paper delves
into the realm of meta-learning with a focus on its contribution to domain
generalization. We first clarify the concept of meta-learning for domain
generalization and introduce a novel taxonomy based on the feature extraction
strategy and the classifier learning methodology, offering a granular view of
methodologies. Additionally, we present a decision graph to assist readers in
navigating the taxonomy based on data availability and domain shifts, enabling
them to select and develop a proper model tailored to their specific problem
requirements. Through an exhaustive review of existing methods and underlying
theories, we map out the fundamentals of the field. Our survey provides
practical insights and an informed discussion on promising research directions.</description>
      <guid isPermaLink="false">2404.02785v3</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Learning a Terrain- and Robot-Aware Dynamics Model for Autonomous Mobile Robot Navigation</title>
      <link>http://arxiv.org/abs/2409.11452v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to Robotics and Autonomous Systems. arXiv admin note:
  substantial text overlap with arXiv:2307.09206&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 移动机器人需要能够规划成本有效的路径以实现自主导航。&lt;br&gt;   - 机器人和地形的属性可能会有所变化，例如地形的摩擦力可能在不同位置不同。&lt;br&gt;&lt;h4&gt;2. 挑战&lt;/h4&gt;   - 机器人属性（如负载或磨损）也会变化，这可能导致执行器增益或关节摩擦的变化。&lt;br&gt;   - 自主导航方法需要能够适应这些变化。&lt;br&gt;&lt;h4&gt;3. 提出的方法&lt;/h4&gt;   - 本文提出了一种新颖的学习方法，构建一个概率的、考虑地形和机器人属性的前向动力学模型（TRADYN）。&lt;br&gt;   - TRADYN能够适应这些变化，并展示其在导航中的应用。&lt;br&gt;&lt;h4&gt;4. 学习方法的创新&lt;/h4&gt;   - 该学习方法基于近年在移动机器人导航中利用神经过程的元学习的进展。&lt;br&gt;&lt;h4&gt;5. 实验评估&lt;/h4&gt;   - 在模拟环境中评估该方法，针对具有单轮动力学的机器人进行2D导航，地形具有空间变化的摩擦系数。&lt;br&gt;&lt;h4&gt;6. 性能对比&lt;/h4&gt;   - 实验结果表明，TRADYN在长时间预测中相比于未适应机器人或地形变化的模型消融版本具有更低的预测误差。&lt;br&gt;&lt;h4&gt;7. 规划控制的应用&lt;/h4&gt;   - 在模型预测控制框架下对导航规划进行评估，考虑不同噪声来源。&lt;br&gt;   - 结果显示，该方法在规划控制有效路径时，通过考虑机器人和地形属性，提升了性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mobile robots should be capable of planning cost-efficient paths for
autonomous navigation. Typically, the terrain and robot properties are subject
to variations. For instance, properties of the terrain such as friction may
vary across different locations. Also, properties of the robot may change such
as payloads or wear and tear, e.g., causing changing actuator gains or joint
friction. Autonomous navigation approaches should thus be able to adapt to such
variations. In this article, we propose a novel approach for learning a
probabilistic, terrain- and robot-aware forward dynamics model (TRADYN) which
can adapt to such variations and demonstrate its use for navigation. Our
learning approach extends recent advances in meta-learning forward dynamics
models based on Neural Processes for mobile robot navigation. We evaluate our
method in simulation for 2D navigation of a robot with uni-cycle dynamics with
varying properties on terrain with spatially varying friction coefficients. In
our experiments, we demonstrate that TRADYN has lower prediction error over
long time horizons than model ablations which do not adapt to robot or terrain
variations. We also evaluate our model for navigation planning in a
model-predictive control framework and under various sources of noise. We
demonstrate that our approach yields improved performance in planning
control-efficient paths by taking robot and terrain properties into account.</description>
      <guid isPermaLink="false">2409.11452v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Generalized Category Discovery</title>
      <link>http://arxiv.org/abs/2409.11624v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究目标&lt;/h4&gt;   - **通用类别发现（GCD）**：旨在将输入分类为已知类别和新类别，这对开放世界的科学发现至关重要。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 当前的GCD方法仅限于单模态数据，未能考虑大多数真实世界数据的多模态特性。&lt;br&gt;&lt;h4&gt;3. 研究创新&lt;/h4&gt;   - 本文将GCD扩展到多模态设置，不同模态的输入提供更丰富和互补的信息。&lt;br&gt;&lt;h4&gt;4. 关键挑战&lt;/h4&gt;   - 理论分析和实证验证表明，多模态GCD的主要挑战在于有效对齐不同模态之间的异质信息。&lt;br&gt;&lt;h4&gt;5. 提出的新框架&lt;/h4&gt;   - 提出了MM-GCD，一个新颖的框架，通过对比学习和蒸馏技术对齐不同模态的特征和输出空间。&lt;br&gt;&lt;h4&gt;6. 性能表现&lt;/h4&gt;   - MM-GCD在UPMC-Food101和N24News数据集上达到了新的最先进性能，分别超越了之前的方法11.5%和4.7%。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generalized Category Discovery (GCD) aims to classify inputs into both known
and novel categories, a task crucial for open-world scientific discoveries.
However, current GCD methods are limited to unimodal data, overlooking the
inherently multimodal nature of most real-world data. In this work, we extend
GCD to a multimodal setting, where inputs from different modalities provide
richer and complementary information. Through theoretical analysis and
empirical validation, we identify that the key challenge in multimodal GCD lies
in effectively aligning heterogeneous information across modalities. To address
this, we propose MM-GCD, a novel framework that aligns both the feature and
output spaces of different modalities using contrastive learning and
distillation techniques. MM-GCD achieves new state-of-the-art performance on
the UPMC-Food101 and N24News datasets, surpassing previous methods by 11.5\%
and 4.7\%, respectively.</description>
      <guid isPermaLink="false">2409.11624v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>SiamMo: Siamese Motion-Centric 3D Object Tracking</title>
      <link>http://arxiv.org/abs/2408.01688v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 当前的3D单目标跟踪方法主要依赖于Siamese匹配基础的范式，但在处理无纹理和不完整的LiDAR点云时存在困难。&lt;br&gt;&lt;h4&gt;2. 新方法的提出&lt;/h4&gt;   - 本文介绍了SiamMo，一种新颖且简单的Siamese运动中心跟踪方法。 &lt;br&gt;&lt;h4&gt;3. 创新点&lt;/h4&gt;   - 与传统的单流架构不同，SiamMo采用Siamese特征提取，使特征提取与时间融合解耦，从而显著提升跟踪性能。&lt;br&gt;&lt;h4&gt;4. 模块设计&lt;/h4&gt;   - 设计了一个时空特征聚合模块，以多尺度整合Siamese特征，有效捕捉运动信息。&lt;br&gt;   - 引入了一个基于框的信息编码模块，将物体大小先验编码到运动估计中。&lt;br&gt;&lt;h4&gt;5. 简化流程&lt;/h4&gt;   - SiamMo是一个纯粹的运动中心跟踪器，省去了额外的步骤，如分割和框优化。&lt;br&gt;&lt;h4&gt;6. 性能表现&lt;/h4&gt;   - SiamMo在多个基准测试中超越了最先进的方法，并在复杂场景中表现出色。&lt;br&gt;   - 在KITTI跟踪基准上，SiamMo以90.1%的精度创下新纪录，同时保持108 FPS的高推理速度。&lt;br&gt;&lt;h4&gt;7. 代码可获取性&lt;/h4&gt;   - 代码将在 https://github.com/HDU-VRLab/SiamMo 发布。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/hdu-vrlab/siammo&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current 3D single object tracking methods primarily rely on the Siamese
matching-based paradigm, which struggles with textureless and incomplete LiDAR
point clouds. Conversely, the motion-centric paradigm avoids appearance
matching, thus overcoming these issues. However, its complex multi-stage
pipeline and the limited temporal modeling capability of a single-stream
architecture constrain its potential. In this paper, we introduce SiamMo, a
novel and simple Siamese motion-centric tracking approach. Unlike the
traditional single-stream architecture, we employ Siamese feature extraction
for motion-centric tracking. This decouples feature extraction from temporal
fusion, significantly enhancing tracking performance. Additionally, we design a
Spatio-Temporal Feature Aggregation module to integrate Siamese features at
multiple scales, capturing motion information effectively. We also introduce a
Box-aware Feature Encoding module to encode object size priors into motion
estimation. SiamMo is a purely motion-centric tracker that eliminates the need
for additional processes like segmentation and box refinement. Without whistles
and bells, SiamMo not only surpasses state-of-the-art methods across multiple
benchmarks but also demonstrates exceptional robustness in challenging
scenarios. SiamMo sets a new record on the KITTI tracking benchmark with 90.1\%
precision while maintaining a high inference speed of 108 FPS. The code will be
released at https://github.com/HDU-VRLab/SiamMo.</description>
      <guid isPermaLink="false">2408.01688v2</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Bridging Domain Gap for Flight-Ready Spaceborne Vision</title>
      <link>http://arxiv.org/abs/2409.11661v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to Journal of Spacecraft and Rockets; Appeared as Chapter 4
  of Tae Ha Park's PhD thesis&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究目标&lt;/h4&gt;   - 本文介绍了航天器姿态网络第3版（SPNv3），一种用于已知非合作目标航天器的单目姿态估计的神经网络（NN）。&lt;br&gt;&lt;h4&gt;2. 设计特点&lt;/h4&gt;   - 与现有文献不同，SPNv3旨在高效计算的同时，对在地面离线训练和验证中未观察到的太空图像具有鲁棒性。&lt;br&gt;&lt;h4&gt;3. 重要性&lt;/h4&gt;   - 这些特性对于在空间级边缘设备上部署神经网络至关重要。&lt;br&gt;&lt;h4&gt;4. 关键设计选择&lt;/h4&gt;   - 通过细致的神经网络设计选择实现这些特性，进行的广泛权衡分析揭示了数据增强、迁移学习和视觉变换器架构等特性，这些特性有助于同时最大化鲁棒性和最小化计算开销。&lt;br&gt;&lt;h4&gt;5. 实验结果&lt;/h4&gt;   - 实验表明，最终的SPNv3在来自机器人测试平台的硬件在环图像上能够实现最先进的姿态精度，且训练完全基于计算机生成的合成图像，这有效地弥合了合成图像与真实图像之间的领域差距。&lt;br&gt;&lt;h4&gt;6. 性能表现&lt;/h4&gt;   - SPNv3在代表性的图形处理单元系统上运行频率远高于现代卫星导航滤波器的更新频率，并具有飞行遗产。&lt;br&gt;&lt;h4&gt;7. 应用范围&lt;/h4&gt;   - 总体而言，SPNv3是一个高效、适合飞行的神经网络模型，适用于多种近距离会合和与目标在轨物体的接近操作。&lt;br&gt;&lt;h4&gt;8. 代码可获取性&lt;/h4&gt;   - SPNv3的代码实现将公开提供。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work presents Spacecraft Pose Network v3 (SPNv3), a Neural Network (NN)
for monocular pose estimation of a known, non-cooperative target spacecraft. As
opposed to existing literature, SPNv3 is designed and trained to be
computationally efficient while providing robustness to spaceborne images that
have not been observed during offline training and validation on the ground.
These characteristics are essential to deploying NNs on space-grade edge
devices. They are achieved through careful NN design choices, and an extensive
trade-off analysis reveals features such as data augmentation, transfer
learning and vision transformer architecture as a few of those that contribute
to simultaneously maximizing robustness and minimizing computational overhead.
Experiments demonstrate that the final SPNv3 can achieve state-of-the-art pose
accuracy on hardware-in-the-loop images from a robotic testbed while having
trained exclusively on computer-generated synthetic images, effectively
bridging the domain gap between synthetic and real imagery. At the same time,
SPNv3 runs well above the update frequency of modern satellite navigation
filters when tested on a representative graphical processing unit system with
flight heritage. Overall, SPNv3 is an efficient, flight-ready NN model readily
applicable to a wide range of close-range rendezvous and proximity operations
with target resident space objects. The code implementation of SPNv3 will be
made publicly available.</description>
      <guid isPermaLink="false">2409.11661v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Registration between Point Cloud Streams and Sequential Bounding Boxes via Gradient Descent</title>
      <link>http://arxiv.org/abs/2409.09312v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究目的&lt;/h4&gt;   - 本文提出了一种算法，用于将序列边界框与点云流进行配准。&lt;br&gt;&lt;h4&gt;2. 方法背景&lt;/h4&gt;   - 不同于常见的点云配准技术，该方法利用边界框的特性（如大小、形状和时间信息）来辅助对齐，从而提供显著的支持和性能提升。&lt;br&gt;&lt;h4&gt;3. 新方法提出&lt;/h4&gt;   - 提出了一个新的方法来解决该问题，具体通过整体目标函数来建模配准过程，该函数包含最终目标和所有约束。&lt;br&gt;&lt;h4&gt;4. 优化方法&lt;/h4&gt;   - 使用梯度下降法对目标函数进行优化。&lt;br&gt;&lt;h4&gt;5. 实验结果&lt;/h4&gt;   - 实验结果显示，所提方法在IoU（交并比）方面有40%的改善，并且在点云流与序列边界框之间的配准上表现出更强的鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we propose an algorithm for registering sequential bounding
boxes with point cloud streams. Unlike popular point cloud registration
techniques, the alignment of the point cloud and the bounding box can rely on
the properties of the bounding box, such as size, shape, and temporal
information, which provides substantial support and performance gains.
Motivated by this, we propose a new approach to tackle this problem.
Specifically, we model the registration process through an overall objective
function that includes the final goal and all constraints. We then optimize the
function using gradient descent. Our experiments show that the proposed method
performs remarkably well with a 40\% improvement in IoU and demonstrates more
robust registration between point cloud streams and sequential bounding boxes</description>
      <guid isPermaLink="false">2409.09312v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>RUIE: Retrieval-based Unified Information Extraction using Large Language Model</title>
      <link>http://arxiv.org/abs/2409.11673v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 3 figures&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 统一信息提取（UIE）旨在通过单一模型或框架完成所有信息提取任务。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 之前的研究主要集中在使用构建数据集对大语言模型（LLMs）进行指令调优，这些方法需要大量计算资源，且在未见任务上泛化能力较差。&lt;br&gt;&lt;h4&gt;3. 新方法提出&lt;/h4&gt;   - 本文提出了RUIE（基于检索的统一信息提取）框架，通过利用上下文学习来实现快速泛化，同时降低计算成本。&lt;br&gt;&lt;h4&gt;4. 关键挑战&lt;/h4&gt;   - RUIE的关键挑战在于选择最有利于LLMs有效处理多样化信息提取任务的示例。&lt;br&gt;&lt;h4&gt;5. 解决方案&lt;/h4&gt;   - 通过整合LLMs对候选示例的偏好进行排名，并设计一个增强关键词的奖励模型，以捕捉查询与示例之间的细粒度关系。&lt;br&gt;&lt;h4&gt;6. 训练方法&lt;/h4&gt;   - 通过对比学习和知识蒸馏训练用于UIE的双编码器检索器。&lt;br&gt;&lt;h4&gt;7. 创新点&lt;/h4&gt;   - 据我们所知，RUIE是首个可训练的检索框架，用于统一信息提取。&lt;br&gt;&lt;h4&gt;8. 实验结果&lt;/h4&gt;   - 在8个保留数据集上的实验结果表明，RUIE在泛化到未见任务方面表现有效，平均F1分数分别比指令调优方法提高了19.22，比其他检索器提高了3.13。&lt;br&gt;&lt;h4&gt;9. 进一步分析&lt;/h4&gt;   - 进一步的分析确认了RUIE对不同规模的LLMs的适应性以及其关键组件的重要性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unified information extraction (UIE) aims to complete all information
extraction tasks using a single model or framework. While previous work has
primarily focused on instruction-tuning large language models (LLMs) with
constructed datasets, these methods require significant computational resources
and struggle to generalize to unseen tasks. To address these limitations, we
propose RUIE (Retrieval-based Unified Information Extraction), a framework that
leverages in-context learning to enable rapid generalization while reducing
computational costs. The key challenge in RUIE is selecting the most beneficial
demonstrations for LLMs to effectively handle diverse IE tasks. To achieve
this, we integrate LLM preferences for ranking candidate demonstrations and
design a keyword-enhanced reward model to capture fine-grained relationships
between queries and demonstrations. We then train a bi-encoder retriever for
UIE through contrastive learning and knowledge distillation. To the best of our
knowledge, RUIE is the first trainable retrieval framework for UIE.
Experimental results on 8 held-out datasets demonstrate RUIE's effectiveness in
generalizing to unseen tasks, with average F1-score improvements of 19.22 and
3.13 compared to instruction-tuning methods and other retrievers, respectively.
Further analysis confirms RUIE's adaptability to LLMs of varying sizes and the
importance of its key components.</description>
      <guid isPermaLink="false">2409.11673v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Fusion in Context: A Multimodal Approach to Affective State Recognition</title>
      <link>http://arxiv.org/abs/2409.11906v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 准确识别人类情感是情感计算和人机交互（HRI）中的关键挑战。&lt;br&gt;&lt;h4&gt;2. 情感的重要性&lt;/h4&gt;   - 情感状态在塑造行为、决策和社会交往中起着重要作用。&lt;br&gt;&lt;h4&gt;3. 上下文影响&lt;/h4&gt;   - 情感表达受上下文因素影响，如果不考虑这些因素，可能会导致误解。&lt;br&gt;&lt;h4&gt;4. 多模态融合的前景&lt;/h4&gt;   - 结合面部表情、语音和生理信号等多种模态的多模态融合，已显示出在提高情感识别方面的潜力。&lt;br&gt;&lt;h4&gt;5. 提出的新方法&lt;/h4&gt;   - 本文提出了一种基于变压器的多模态融合方法，利用面部热数据、面部动作单元和文本上下文信息进行上下文感知的情感识别。&lt;br&gt;&lt;h4&gt;6. 模态特定编码器&lt;/h4&gt;   - 探索针对特定模态的编码器，以学习定制的表示。&lt;br&gt;&lt;h4&gt;7. 融合与处理&lt;/h4&gt;   - 采用加法融合将不同模态的表示结合，并通过共享的变压器编码器处理，以捕捉时间依赖性和交互作用。&lt;br&gt;&lt;h4&gt;8. 实验设计&lt;/h4&gt;   - 提出的方法在一个设计用于诱发各种情感状态的具体桌面“吃豆人”游戏中进行评估。&lt;br&gt;&lt;h4&gt;9. 实验结果&lt;/h4&gt;   - 结果证明了结合上下文信息和多模态融合在情感状态识别中的有效性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate recognition of human emotions is a crucial challenge in affective
computing and human-robot interaction (HRI). Emotional states play a vital role
in shaping behaviors, decisions, and social interactions. However, emotional
expressions can be influenced by contextual factors, leading to
misinterpretations if context is not considered. Multimodal fusion, combining
modalities like facial expressions, speech, and physiological signals, has
shown promise in improving affect recognition. This paper proposes a
transformer-based multimodal fusion approach that leverages facial thermal
data, facial action units, and textual context information for context-aware
emotion recognition. We explore modality-specific encoders to learn tailored
representations, which are then fused using additive fusion and processed by a
shared transformer encoder to capture temporal dependencies and interactions.
The proposed method is evaluated on a dataset collected from participants
engaged in a tangible tabletop Pacman game designed to induce various affective
states. Our results demonstrate the effectiveness of incorporating contextual
information and multimodal fusion for affective state recognition.</description>
      <guid isPermaLink="false">2409.11906v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Topological Deep Learning with State-Space Models: A Mamba Approach for Simplicial Complexes</title>
      <link>http://arxiv.org/abs/2409.12033v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 基于消息传递（MP）机制的图神经网络是处理图结构数据的主要方法。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 这些网络仅能建模成对交互，难以明确捕捉具有$n$-体关系的系统复杂性。&lt;br&gt;&lt;h4&gt;3. 新兴领域&lt;/h4&gt;   - 拓扑深度学习作为一个有前景的领域，专注于使用各种拓扑领域（如单纯形和细胞复合体）研究和建模高阶交互。&lt;br&gt;&lt;h4&gt;4. 新挑战&lt;/h4&gt;   - 尽管新领域提供了强大的表示能力，但也带来了新的挑战，如如何通过高阶消息传递有效建模高阶结构间的交互。&lt;br&gt;&lt;h4&gt;5. 序列建模的有效性&lt;/h4&gt;   - 结构状态空间序列模型在序列建模中表现出色，最近通过将节点的邻域编码为序列，适应于图数据，避免了消息传递机制。&lt;br&gt;&lt;h4&gt;6. 新方法提出&lt;/h4&gt;   - 本文提出了一种新颖的架构，旨在与单纯形复合体共同工作，利用Mamba状态空间模型作为基础。&lt;br&gt;&lt;h4&gt;7. 节点序列生成&lt;/h4&gt;   - 我们的方法基于邻近单元生成节点的序列，允许所有高阶结构之间的直接通信，无论其等级如何。&lt;br&gt;&lt;h4&gt;8. 模型验证&lt;/h4&gt;   - 我们对模型进行了广泛验证，结果表明其在单纯形复合体上的性能与最先进的模型相当。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks based on the message-passing (MP) mechanism are a
dominant approach for handling graph-structured data. However, they are
inherently limited to modeling only pairwise interactions, making it difficult
to explicitly capture the complexity of systems with $n$-body relations. To
address this, topological deep learning has emerged as a promising field for
studying and modeling higher-order interactions using various topological
domains, such as simplicial and cellular complexes. While these new domains
provide powerful representations, they introduce new challenges, such as
effectively modeling the interactions among higher-order structures through
higher-order MP. Meanwhile, structured state-space sequence models have proven
to be effective for sequence modeling and have recently been adapted for graph
data by encoding the neighborhood of a node as a sequence, thereby avoiding the
MP mechanism. In this work, we propose a novel architecture designed to operate
with simplicial complexes, utilizing the Mamba state-space model as its
backbone. Our approach generates sequences for the nodes based on the
neighboring cells, enabling direct communication between all higher-order
structures, regardless of their rank. We extensively validate our model,
demonstrating that it achieves competitive performance compared to
state-of-the-art models developed for simplicial complexes.</description>
      <guid isPermaLink="false">2409.12033v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>When 3D Partial Points Meets SAM: Tooth Point Cloud Segmentation with Sparse Labels</title>
      <link>http://arxiv.org/abs/2409.01691v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To appear at MICCAI24&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 牙齿点云分割是许多正畸应用中的基础任务。&lt;br&gt;&lt;h4&gt;2. 现有方法问题&lt;/h4&gt;   - 当前研究主要集中在完全监督学习上，这需要昂贵且繁琐的手动逐点标注。&lt;br&gt;&lt;h4&gt;3. 弱监督学习的局限&lt;/h4&gt;   - 虽然最近提出了一些弱监督替代方法，利用弱标签进行三维分割并取得了一定成果，但在标签极其稀疏时，它们的表现往往不佳。&lt;br&gt;&lt;h4&gt;4. 新方法提出&lt;/h4&gt;   - 基于Segment Anything Model (SAM) 的强大提示分割能力，提出了名为SAMTooth的框架，以补充极其稀疏的监督。&lt;br&gt;&lt;h4&gt;5. 提示生成策略&lt;/h4&gt;   - 引入了一种新颖的“基于置信度的提示生成”策略，通过粗略类别预测与置信度过滤相结合，自动生成适当的点提示。&lt;br&gt;&lt;h4&gt;6. 特征学习的优化&lt;/h4&gt;   - 为充分利用SAM输出中的结构和形状线索，提出了一种“掩膜引导表示学习”，将生成的牙齿掩膜重投影到三维空间，并约束不同牙齿的点具有独特的表示。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 在公共数据集上进行实验，发现仅使用0.1%的标注（每颗牙齿一个点），该方法的性能显著优于最近的弱监督方法，并且与最新的完全监督方法相当。&lt;br&gt;&lt;h4&gt;8. 应用潜力&lt;/h4&gt;   - 展示了在稀疏标签下应用SAM于三维感知任务的显著潜力。&lt;br&gt;&lt;h4&gt;9. 代码可获取性&lt;/h4&gt;   - 代码可在GitHub上获得，链接为 https://github.com/CUHK-AIM-Group/SAMTooth。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tooth point cloud segmentation is a fundamental task in many orthodontic
applications. Current research mainly focuses on fully supervised learning
which demands expensive and tedious manual point-wise annotation. Although
recent weakly-supervised alternatives are proposed to use weak labels for 3D
segmentation and achieve promising results, they tend to fail when the labels
are extremely sparse. Inspired by the powerful promptable segmentation
capability of the Segment Anything Model (SAM), we propose a framework named
SAMTooth that leverages such capacity to complement the extremely sparse
supervision. To automatically generate appropriate point prompts for SAM, we
propose a novel Confidence-aware Prompt Generation strategy, where coarse
category predictions are aggregated with confidence-aware filtering.
Furthermore, to fully exploit the structural and shape clues in SAM's outputs
for assisting the 3D feature learning, we advance a Mask-guided Representation
Learning that re-projects the generated tooth masks of SAM into 3D space and
constrains these points of different teeth to possess distinguished
representations. To demonstrate the effectiveness of the framework, we conduct
experiments on the public dataset and surprisingly find with only 0.1\%
annotations (one point per tooth), our method can surpass recent weakly
supervised methods by a large margin, and the performance is even comparable to
the recent fully-supervised methods, showcasing the significant potential of
applying SAM to 3D perception tasks with sparse labels. Code is available at
https://github.com/CUHK-AIM-Group/SAMTooth.</description>
      <guid isPermaLink="false">2409.01691v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Differentiable Collision-Supervised Tooth Arrangement Network with a Decoupling Perspective</title>
      <link>http://arxiv.org/abs/2409.11937v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 13 figures&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 牙齿排列是数字正畸规划过程中的重要步骤。&lt;br&gt;&lt;h4&gt;2. 现有方法问题&lt;/h4&gt;   - 现有基于学习的方法直接回归牙齿运动，使用隐藏牙齿特征，这样会导致目标姿态感知和运动回归的耦合，可能导致三维变换的感知不佳。&lt;br&gt;&lt;h4&gt;3. 忽视问题&lt;/h4&gt;   - 现有方法忽略了预测牙齿排列中的重叠或间隙，这在实际应用中通常是不可接受的。&lt;br&gt;&lt;h4&gt;4. 新方法提出&lt;/h4&gt;   - 提出了DTAN（可微分碰撞监督牙齿排列网络），通过解耦预测任务和特征建模来解决这些问题。&lt;br&gt;&lt;h4&gt;5. 任务解耦&lt;/h4&gt;   - DTAN首先预测最终牙齿姿态的隐藏特征，然后利用这些特征辅助回归起始和目标牙齿之间的运动，从而解耦牙齿排列任务。&lt;br&gt;&lt;h4&gt;6. 特征学习&lt;/h4&gt;   - DTAN将隐藏特征进一步解耦为几何特征和位置特征，并通过特征一致性约束进行监督，以提高特征学习效果。&lt;br&gt;&lt;h4&gt;7. 新损失函数&lt;/h4&gt;   - 提出了新颖的可微分碰撞损失函数，用于点云数据，约束牙齿之间的相关姿态，这一方法可扩展至其他三维点云任务。&lt;br&gt;&lt;h4&gt;8. 可控性增强&lt;/h4&gt;   - 提出了基于弓宽的牙齿排列网络C-DTAN，以增强结果的可控性。&lt;br&gt;&lt;h4&gt;9. 性能提升&lt;/h4&gt;   - 构建了三个不同的牙齿排列数据集，并与现有方法相比，在准确性和速度上实现了显著提升。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tooth arrangement is an essential step in the digital orthodontic planning
process. Existing learning-based methods use hidden teeth features to directly
regress teeth motions, which couples target pose perception and motion
regression. It could lead to poor perceptions of three-dimensional
transformation. They also ignore the possible overlaps or gaps between teeth of
predicted dentition, which is generally unacceptable. Therefore, we propose
DTAN, a differentiable collision-supervised tooth arrangement network,
decoupling predicting tasks and feature modeling. DTAN decouples the tooth
arrangement task by first predicting the hidden features of the final teeth
poses and then using them to assist in regressing the motions between the
beginning and target teeth. To learn the hidden features better, DTAN also
decouples the teeth-hidden features into geometric and positional features,
which are further supervised by feature consistency constraints. Furthermore,
we propose a novel differentiable collision loss function for point cloud data
to constrain the related gestures between teeth, which can be easily extended
to other 3D point cloud tasks. We propose an arch-width guided tooth
arrangement network, named C-DTAN, to make the results controllable. We
construct three different tooth arrangement datasets and achieve drastically
improved performance on accuracy and speed compared with existing methods.</description>
      <guid isPermaLink="false">2409.11937v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Self-Contrastive Forward-Forward Algorithm</title>
      <link>http://arxiv.org/abs/2409.11593v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - Forward-Forward (FF) 算法是一种新颖的前向模式学习方法，能够逐层和局部地更新权重，支持监督和无监督学习。&lt;br&gt;&lt;h4&gt;2. 应用领域&lt;/h4&gt;   - FF算法适用于脑启发学习、低功耗硬件神经网络和大规模模型的分布式学习等应用。&lt;br&gt;&lt;h4&gt;3. 现存挑战&lt;/h4&gt;   - 虽然FF在手写数字识别任务中表现良好，但在自然图像和时间序列任务中的性能仍然存在挑战。&lt;br&gt;&lt;h4&gt;4. 主要限制&lt;/h4&gt;   - 一个关键限制是需要生成高质量的负样本以用于对比学习，尤其是在无监督任务中，目前缺乏多样化的解决方案。&lt;br&gt;&lt;h4&gt;5. 新方法提出&lt;/h4&gt;   - 提出了自对比前向-前向（Self-Contrastive Forward-Forward, SCFF）方法，受自监督对比学习的启发。&lt;br&gt;&lt;h4&gt;6. 样本生成能力&lt;/h4&gt;   - SCFF能够生成适用于不同数据集的正负样本，超越现有的局部前向算法，在无监督分类准确性上取得更好效果。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 在MNIST（MLP: 98.7%）、CIFAR-10（CNN: 80.75%）和STL-10（CNN: 77.3%）上，SCFF的表现超过了现有算法。&lt;br&gt;&lt;h4&gt;8. 新应用领域&lt;/h4&gt;   - SCFF是首个支持FF训练递归神经网络的方法，为更复杂的任务以及连续时间的视频和文本处理打开了新的可能性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Forward-Forward (FF) algorithm is a recent, purely forward-mode learning
method, that updates weights locally and layer-wise and supports supervised as
well as unsupervised learning. These features make it ideal for applications
such as brain-inspired learning, low-power hardware neural networks, and
distributed learning in large models. However, while FF has shown promise on
written digit recognition tasks, its performance on natural images and
time-series remains a challenge. A key limitation is the need to generate
high-quality negative examples for contrastive learning, especially in
unsupervised tasks, where versatile solutions are currently lacking. To address
this, we introduce the Self-Contrastive Forward-Forward (SCFF) method, inspired
by self-supervised contrastive learning. SCFF generates positive and negative
examples applicable across different datasets, surpassing existing local
forward algorithms for unsupervised classification accuracy on MNIST (MLP:
98.7%), CIFAR-10 (CNN: 80.75%), and STL-10 (CNN: 77.3%). Additionally, SCFF is
the first to enable FF training of recurrent neural networks, opening the door
to more complex tasks and continuous-time video and text processing.</description>
      <guid isPermaLink="false">2409.11593v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Location based Probabilistic Load Forecasting of EV Charging Sites: Deep Transfer Learning with Multi-Quantile Temporal Convolutional Network</title>
      <link>http://arxiv.org/abs/2409.11862v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 10 figures&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 电动汽车（EVs）的电气化是减少化石燃料使用和环境污染的潜在方法。&lt;br&gt;&lt;h4&gt;2. EV类型和用户群体&lt;/h4&gt;   - 各种类型的电动汽车适用于不同的运输模式（包括空中、水上和陆地），并且不同的用户群体（通勤者、商业用户、家庭用户、驾驶员）使用不同的充电基础设施（公共、私人、家庭和工作场所）。&lt;br&gt;&lt;h4&gt;3. 使用模式和能量需求&lt;/h4&gt;   - EV的使用模式和能量需求具有很大的随机性，因此需要对充电需求进行特征化和预测，以防止电力短缺。&lt;br&gt;&lt;h4&gt;4. 现有模型限制&lt;/h4&gt;   - 之前开发的数据驱动负载模型仅限于特定用例和地点，缺乏足够的适应性，无法在多样化位置的充电站之间有效进行日预测知识转移。&lt;br&gt;&lt;h4&gt;5. 新方法提出&lt;/h4&gt;   - 本文提出了一种基于位置的EV充电站负载预测方法，采用深度多分位时间卷积网络（MQ-TCN），以克服早期模型的限制。&lt;br&gt;&lt;h4&gt;6. 实验数据来源&lt;/h4&gt;   - 实验数据来自四个充电站：Caltech、JPL、Office-1和NREL，这些站点有不同的EV用户类型，包括学生、全职和兼职员工、随机访客等。&lt;br&gt;&lt;h4&gt;7. 模型性能&lt;/h4&gt;   - 在JPL充电站，所提出的深度MQ-TCN模型在日负载预测中的预测区间覆盖概率（PICP）得分为93.62%，比XGBoost模型提高了28.93%。&lt;br&gt;&lt;h4&gt;8. 知识转移能力&lt;/h4&gt;   - 通过采用归纳转移学习（TL）方法，MQ-TCN模型在NREL站点的负载预测任务中仅使用两周数据就达到了96.88%的PICP得分。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electrification of vehicles is a potential way of reducing fossil fuel usage
and thus lessening environmental pollution. Electric Vehicles (EVs) of various
types for different transport modes (including air, water, and land) are
evolving. Moreover, different EV user groups (commuters, commercial or domestic
users, drivers) may use different charging infrastructures (public, private,
home, and workplace) at various times. Therefore, usage patterns and energy
demand are very stochastic. Characterizing and forecasting the charging demand
of these diverse EV usage profiles is essential in preventing power outages.
Previously developed data-driven load models are limited to specific use cases
and locations. None of these models are simultaneously adaptive enough to
transfer knowledge of day-ahead forecasting among EV charging sites of diverse
locations, trained with limited data, and cost-effective. This article presents
a location-based load forecasting of EV charging sites using a deep
Multi-Quantile Temporal Convolutional Network (MQ-TCN) to overcome the
limitations of earlier models. We conducted our experiments on data from four
charging sites, namely Caltech, JPL, Office-1, and NREL, which have diverse EV
user types like students, full-time and part-time employees, random visitors,
etc. With a Prediction Interval Coverage Probability (PICP) score of 93.62\%,
our proposed deep MQ-TCN model exhibited a remarkable 28.93\% improvement over
the XGBoost model for a day-ahead load forecasting at the JPL charging site. By
transferring knowledge with the inductive Transfer Learning (TL) approach, the
MQ-TCN model achieved a 96.88\% PICP score for the load forecasting task at the
NREL site using only two weeks of data.</description>
      <guid isPermaLink="false">2409.11862v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>RealDiff: Real-world 3D Shape Completion using Self-Supervised Diffusion Models</title>
      <link>http://arxiv.org/abs/2409.10180v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 点云补全旨在从部分观察中恢复物体的完整3D形状。&lt;br&gt;&lt;h4&gt;2. 现有方法限制&lt;/h4&gt;   - 尽管依赖于合成形状先验的方法在该领域取得了良好效果，但其在真实数据中的适用性和泛化能力仍然有限。&lt;br&gt;&lt;h4&gt;3. 新方法提出&lt;/h4&gt;   - 提出了一个自监督框架，称为RealDiff，将点云补全问题公式化为直接基于真实测量的条件生成问题。&lt;br&gt;&lt;h4&gt;4. 处理噪声观察&lt;/h4&gt;   - 为了更好地处理噪声观察，RealDiff利用额外的几何线索，而不依赖于合成数据进行训练。&lt;br&gt;&lt;h4&gt;5. 扩散过程模拟&lt;/h4&gt;   - RealDiff在缺失的物体部分模拟扩散过程，同时根据部分输入条件生成，以应对任务的多模态特性。&lt;br&gt;&lt;h4&gt;6. 训练正则化&lt;/h4&gt;   - 通过将我们方法预测的物体轮廓和深度图与外部估计的结果进行匹配，进一步正则化训练过程。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 实验结果表明，我们的方法在真实世界点云补全任务中始终优于最先进的方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud completion aims to recover the complete 3D shape of an object
from partial observations. While approaches relying on synthetic shape priors
achieved promising results in this domain, their applicability and
generalizability to real-world data are still limited. To tackle this problem,
we propose a self-supervised framework, namely RealDiff, that formulates point
cloud completion as a conditional generation problem directly on real-world
measurements. To better deal with noisy observations without resorting to
training on synthetic data, we leverage additional geometric cues.
Specifically, RealDiff simulates a diffusion process at the missing object
parts while conditioning the generation on the partial input to address the
multimodal nature of the task. We further regularize the training by matching
object silhouettes and depth maps, predicted by our method, with the externally
estimated ones. Experimental results show that our method consistently
outperforms state-of-the-art methods in real-world point cloud completion.</description>
      <guid isPermaLink="false">2409.10180v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>3D Geometric Shape Assembly via Efficient Point Cloud Matching</title>
      <link>http://arxiv.org/abs/2407.10542v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICML 2024&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 组装几何形状成为更大目标结构的能力在多个实际应用中至关重要。&lt;br&gt;&lt;h4&gt;2. 研究目标&lt;/h4&gt;   - 本文旨在通过在部分形状的点云之间建立局部对应关系来解决这一问题，分为粗略和精细两个层面。&lt;br&gt;&lt;h4&gt;3. 新方法介绍&lt;/h4&gt;   - 引入了Proxy Match Transform (PMT)，这是一种近似的高阶特征变换层，能够在低内存和计算成本下实现部件 mating surfaces 的可靠匹配。&lt;br&gt;&lt;h4&gt;4. 框架构建&lt;/h4&gt;   - 基于PMT，提出了一种新框架，称为Proxy Match TransformeR (PMTR)，专门用于几何组装任务。&lt;br&gt;&lt;h4&gt;5. 实验评估&lt;/h4&gt;   - 在大规模3D几何形状组装基准数据集“Breaking Bad”上评估PMTR，结果显示其在性能和效率上优于现有的最先进方法。&lt;br&gt;&lt;h4&gt;6. 项目资源&lt;/h4&gt;   - 项目页面可访问：[PMTR](https://nahyuklee.github.io/pmtr)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-07-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning to assemble geometric shapes into a larger target structure is a
pivotal task in various practical applications. In this work, we tackle this
problem by establishing local correspondences between point clouds of part
shapes in both coarse- and fine-levels. To this end, we introduce Proxy Match
Transform (PMT), an approximate high-order feature transform layer that enables
reliable matching between mating surfaces of parts while incurring low costs in
memory and computation. Building upon PMT, we introduce a new framework, dubbed
Proxy Match TransformeR (PMTR), for the geometric assembly task. We evaluate
the proposed PMTR on the large-scale 3D geometric shape assembly benchmark
dataset of Breaking Bad and demonstrate its superior performance and efficiency
compared to state-of-the-art methods. Project page:
https://nahyuklee.github.io/pmtr.</description>
      <guid isPermaLink="false">2407.10542v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>MoRAG -- Multi-Fusion Retrieval Augmented Generation for Human Motion</title>
      <link>http://arxiv.org/abs/2409.12140v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究介绍&lt;/h4&gt;   - 本文提出MoRAG，一种基于多部分融合的检索增强生成策略，用于文本基础的人类动作生成。&lt;br&gt;&lt;h4&gt;2. 方法优势&lt;/h4&gt;   - 该方法通过改进的动作检索过程，增强了动作扩散模型的性能。&lt;br&gt;&lt;h4&gt;3. 大语言模型应用&lt;/h4&gt;   - 通过有效提示大语言模型（LLMs），解决了动作检索中的拼写错误和重述问题。&lt;br&gt;&lt;h4&gt;4. 多部分检索策略&lt;/h4&gt;   - 利用多部分检索策略，提高了在语言空间中的动作检索的普适性。&lt;br&gt;&lt;h4&gt;5. 样本生成&lt;/h4&gt;   - 通过空间组合检索到的动作，创造多样的动作样本。&lt;br&gt;&lt;h4&gt;6. 低级运动信息&lt;/h4&gt;   - 通过利用低级、部件特定的运动信息，可以为未见的文本描述构建动作样本。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 实验表明，该框架可以作为即插即用模块，提升动作扩散模型的性能。&lt;br&gt;&lt;h4&gt;8. 资源共享&lt;/h4&gt;   - 代码、预训练模型和示例视频将会在网站上提供：[MoRAG](https://motion-rag.github.io/)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce MoRAG, a novel multi-part fusion based retrieval-augmented
generation strategy for text-based human motion generation. The method enhances
motion diffusion models by leveraging additional knowledge obtained through an
improved motion retrieval process. By effectively prompting large language
models (LLMs), we address spelling errors and rephrasing issues in motion
retrieval. Our approach utilizes a multi-part retrieval strategy to improve the
generalizability of motion retrieval across the language space. We create
diverse samples through the spatial composition of the retrieved motions.
Furthermore, by utilizing low-level, part-specific motion information, we can
construct motion samples for unseen text descriptions. Our experiments
demonstrate that our framework can serve as a plug-and-play module, improving
the performance of motion diffusion models. Code, pretrained models and sample
videos will be made available at: https://motion-rag.github.io/</description>
      <guid isPermaLink="false">2409.12140v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing the Reliability of LiDAR Point Cloud Sampling: A Colorization and Super-Resolution Approach Based on LiDAR-Generated Images</title>
      <link>http://arxiv.org/abs/2409.11532v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - LiDAR（激光雷达）技术在机器人和自主系统中是关键传感器，近年来取得显著进展。&lt;br&gt;&lt;h4&gt;2. 技术改进&lt;/h4&gt;   - LiDAR的改进包括点云分辨率提升和提供360度低分辨率图像的能力。&lt;br&gt;   - 这些图像编码了深度、反射率和近红外光等多种数据。&lt;br&gt;&lt;h4&gt;3. 问题陈述&lt;/h4&gt;   - 点密度过高及传统点云采样可能导致问题，尤其在LiDAR里程计等应用中，误导性点和几何信息退化会引起漂移错误。&lt;br&gt;&lt;h4&gt;4. 研究方向&lt;/h4&gt;   - 当前研究重点在利用LiDAR生成的图像提升情境感知。&lt;br&gt;&lt;h4&gt;5. 文献回顾&lt;/h4&gt;   - 本文回顾了当前深度学习技术，包括上色和超分辨率，这些技术通常用于传统计算机视觉任务。&lt;br&gt;&lt;h4&gt;6. 方法应用&lt;/h4&gt;   - 将这些技术应用于LiDAR生成的图像，并进行定性分析。&lt;br&gt;&lt;h4&gt;7. 新方法提出&lt;/h4&gt;   - 基于分析，开发了一种新方法，选择性地整合最适合的上色和超分辨率方法与LiDAR图像，以从LiDAR点云中采样可靠点。&lt;br&gt;&lt;h4&gt;8. 目标&lt;/h4&gt;   - 该方法旨在提高点云配准的准确性，避免因缺乏几何信息导致的匹配错误，从而增强LiDAR系统在实际应用中的效用和精度。&lt;br&gt;&lt;h4&gt;9. 实验评估&lt;/h4&gt;   - 评估结果表明，所提方法相比于之前的工作表现优越，减少了平移和旋转误差，并使用了更少的点。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, Light Detection and Ranging (LiDAR) technology, a critical
sensor in robotics and autonomous systems, has seen significant advancements.
These improvements include enhanced resolution of point clouds and the
capability to provide 360{\deg} low-resolution images. These images encode
various data such as depth, reflectivity, and near-infrared light within the
pixels. However, an excessive density of points and conventional point cloud
sampling can be counterproductive, particularly in applications such as LiDAR
odometry, where misleading points and degraded geometry information may induce
drift errors. Currently, extensive research efforts are being directed towards
leveraging LiDAR-generated images to improve situational awareness. This paper
presents a comprehensive review of current deep learning (DL) techniques,
including colorization and super-resolution, which are traditionally utilized
in conventional computer vision tasks. These techniques are applied to
LiDAR-generated images and are analyzed qualitatively. Based on this analysis,
we have developed a novel approach that selectively integrates the most suited
colorization and super-resolution methods with LiDAR imagery to sample reliable
points from the LiDAR point cloud. This approach aims to not only improve the
accuracy of point cloud registration but also avoid mismatching caused by
lacking geometry information, thereby augmenting the utility and precision of
LiDAR systems in practical applications. In our evaluation, the proposed
approach demonstrates superior performance compared to our previous work,
achieving lower translation and rotation errors with a reduced number of
points.</description>
      <guid isPermaLink="false">2409.11532v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>DynaMo: In-Domain Dynamics Pretraining for Visuo-Motor Control</title>
      <link>http://arxiv.org/abs/2409.12192v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 模仿学习被证明是训练复杂的视动政策（visuomotor policies）的有效工具。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 当前方法通常需要数百到数千个专家示例来处理高维视觉观测。&lt;br&gt;   - 数据效率低的主要原因是视觉表示主要是在域外数据上预训练或通过行为克隆目标直接训练。&lt;br&gt;&lt;h4&gt;3. 新方法提出&lt;/h4&gt;   - 本文提出DynaMo，一种新的域内自监督学习方法，用于学习视觉表示。&lt;br&gt;&lt;h4&gt;4. 方法概述&lt;/h4&gt;   - 在一组专家示例的基础上，联合学习潜在逆动态模型和前向动态模型，预测潜在空间中的下一个帧。&lt;br&gt;   - 该方法不依赖于增强、对比采样或真实动作的访问。&lt;br&gt;&lt;h4&gt;5. 数据需求&lt;/h4&gt;   - DynaMo不需要任何域外数据，如互联网数据集或跨实体数据集。&lt;br&gt;&lt;h4&gt;6. 实验验证&lt;/h4&gt;   - 在六个模拟和真实环境中，使用DynaMo学习的表示在下游模仿学习性能上显著优于先前的自监督学习目标和预训练表示。&lt;br&gt;&lt;h4&gt;7. 适用性&lt;/h4&gt;   - DynaMo的性能提升适用于多种策略类别，如行为变换器（Behavior Transformer）、扩散策略（Diffusion Policy）、多层感知器（MLP）和最近邻（nearest neighbors）。&lt;br&gt;&lt;h4&gt;8. 消融研究&lt;/h4&gt;   - 最后，论文对DynaMo的关键组件进行了消融研究，并测量其对下游策略性能的影响。&lt;br&gt;&lt;h4&gt;9. 附加资源&lt;/h4&gt;   - 机器人视频可在指定网站上观看：[DynaMo](https://dynamo-ssl.github.io)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Imitation learning has proven to be a powerful tool for training complex
visuomotor policies. However, current methods often require hundreds to
thousands of expert demonstrations to handle high-dimensional visual
observations. A key reason for this poor data efficiency is that visual
representations are predominantly either pretrained on out-of-domain data or
trained directly through a behavior cloning objective. In this work, we present
DynaMo, a new in-domain, self-supervised method for learning visual
representations. Given a set of expert demonstrations, we jointly learn a
latent inverse dynamics model and a forward dynamics model over a sequence of
image embeddings, predicting the next frame in latent space, without
augmentations, contrastive sampling, or access to ground truth actions.
Importantly, DynaMo does not require any out-of-domain data such as Internet
datasets or cross-embodied datasets. On a suite of six simulated and real
environments, we show that representations learned with DynaMo significantly
improve downstream imitation learning performance over prior self-supervised
learning objectives, and pretrained representations. Gains from using DynaMo
hold across policy classes such as Behavior Transformer, Diffusion Policy, MLP,
and nearest neighbors. Finally, we ablate over key components of DynaMo and
measure its impact on downstream policy performance. Robot videos are best
viewed at https://dynamo-ssl.github.io</description>
      <guid isPermaLink="false">2409.12192v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>RockTrack: A 3D Robust Multi-Camera-Ken Multi-Object Tracking Framework</title>
      <link>http://arxiv.org/abs/2409.11749v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  RockTrack establishes a new state-of-the-art with 59.1% AMOTA on the
  nuScenes vision-only test leaderboard with ResNet50-level backbone&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 3D多目标跟踪（MOT）在3D物体检测的快速进展，尤其是在成本效益高的多摄像头设置下，取得了显著的性能提升。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 目前的端到端训练方法导致多摄像头跟踪器变得特定于检测器，限制了其通用性。&lt;br&gt;   - 现有通用跟踪器忽视了多摄像头检测器的独特特征，如运动观测的不可靠性和视觉信息的可行性。&lt;br&gt;&lt;h4&gt;3. 新方法提出&lt;/h4&gt;   - 本文提出RockTrack，一种针对多摄像头检测器的3D MOT方法。&lt;br&gt;&lt;h4&gt;4. 方法框架&lt;/h4&gt;   - RockTrack遵循检测跟踪（Tracking-By-Detection）框架，兼容多种现成的检测器。&lt;br&gt;&lt;h4&gt;5. 预处理模块&lt;/h4&gt;   - 引入信心引导的预处理模块，从单个检测器中提取可靠的运动和图像观测，利用不同的表示空间。&lt;br&gt;&lt;h4&gt;6. 观测融合&lt;/h4&gt;   - 这些观测在关联模块中融合，利用几何和外观线索最小化不匹配。&lt;br&gt;&lt;h4&gt;7. 匹配传播&lt;/h4&gt;   - 生成的匹配通过分阶段估计过程传播，为启发式噪声建模奠定基础。&lt;br&gt;&lt;h4&gt;8. 相似度度量&lt;/h4&gt;   - 提出了一种新颖的外观相似度度量，用于在多摄像头环境中明确表征对象的亲和力。&lt;br&gt;&lt;h4&gt;9. 实验结果&lt;/h4&gt;   - RockTrack在nuScenes视觉跟踪排行榜上实现了59.1%的平均多目标跟踪准确率（AMOTA），表现出卓越的计算效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Multi-Object Tracking (MOT) obtains significant performance improvements
with the rapid advancements in 3D object detection, particularly in
cost-effective multi-camera setups. However, the prevalent end-to-end training
approach for multi-camera trackers results in detector-specific models,
limiting their versatility. Moreover, current generic trackers overlook the
unique features of multi-camera detectors, i.e., the unreliability of motion
observations and the feasibility of visual information. To address these
challenges, we propose RockTrack, a 3D MOT method for multi-camera detectors.
Following the Tracking-By-Detection framework, RockTrack is compatible with
various off-the-shelf detectors. RockTrack incorporates a confidence-guided
preprocessing module to extract reliable motion and image observations from
distinct representation spaces from a single detector. These observations are
then fused in an association module that leverages geometric and appearance
cues to minimize mismatches. The resulting matches are propagated through a
staged estimation process, forming the basis for heuristic noise modeling.
Additionally, we introduce a novel appearance similarity metric for explicitly
characterizing object affinities in multi-camera settings. RockTrack achieves
state-of-the-art performance on the nuScenes vision-only tracking leaderboard
with 59.1% AMOTA while demonstrating impressive computational efficiency.</description>
      <guid isPermaLink="false">2409.11749v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Privacy Challenges in Meta-Learning: An Investigation on Model-Agnostic Meta-Learning</title>
      <link>http://arxiv.org/abs/2406.00249v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究主题&lt;/h4&gt;   - 论文讨论元学习（meta-learning），涉及多个学习者在数据受限环境中协作。&lt;br&gt;&lt;h4&gt;2. 当前方法概述&lt;/h4&gt;   - 当前的元学习方法中，任务学习者从敏感数据（称为支持集）中局部学习模型。&lt;br&gt;&lt;h4&gt;3. 信息共享机制&lt;/h4&gt;   - 任务学习者使用另一部分数据（称为查询集）计算模型相关信息（如梯度或损失值），并与元学习者共享这些信息。&lt;br&gt;&lt;h4&gt;4. 元学习者的作用&lt;/h4&gt;   - 元学习者利用共享的信息更新其元知识。&lt;br&gt;&lt;h4&gt;5. 隐私问题&lt;/h4&gt;   - 尽管没有显式的数据共享，隐私问题依然存在。&lt;br&gt;&lt;h4&gt;6. 研究重点&lt;/h4&gt;   - 本文重点检视一个重要的元学习算法——模型无关元学习（MAML）中的潜在数据泄漏。&lt;br&gt;&lt;h4&gt;7. 梯度共享分析&lt;/h4&gt;   - 在MAML中，梯度在元学习者与任务学习者之间共享，研究的主要目标是分析梯度及其包含的任务数据集信息。&lt;br&gt;&lt;h4&gt;8. 会员推断攻击&lt;/h4&gt;   - 提出针对包含支持集和查询集的任务数据集的会员推断攻击。&lt;br&gt;&lt;h4&gt;9. 噪声注入方法&lt;/h4&gt;   - 探索多种噪声注入方法，以保护任务数据的隐私并抵御潜在攻击。&lt;br&gt;&lt;h4&gt;10. 实验结果&lt;/h4&gt;    - 实验结果表明，对MAML的攻击是有效的，而适当的噪声注入方法在抵御这些攻击方面也有效。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-06-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Meta-learning involves multiple learners, each dedicated to specific tasks,
collaborating in a data-constrained setting. In current meta-learning methods,
task learners locally learn models from sensitive data, termed support sets.
These task learners subsequently share model-related information, such as
gradients or loss values, which is computed using another part of the data
termed query set, with a meta-learner. The meta-learner employs this
information to update its meta-knowledge. Despite the absence of explicit data
sharing, privacy concerns persist. This paper examines potential data leakage
in a prominent metalearning algorithm, specifically Model-Agnostic
Meta-Learning (MAML). In MAML, gradients are shared between the metalearner and
task-learners. The primary objective is to scrutinize the gradient and the
information it encompasses about the task dataset. Subsequently, we endeavor to
propose membership inference attacks targeting the task dataset containing
support and query sets. Finally, we explore various noise injection methods
designed to safeguard the privacy of task data and thwart potential attacks.
Experimental results demonstrate the effectiveness of these attacks on MAML and
the efficacy of proper noise injection methods in countering them.</description>
      <guid isPermaLink="false">2406.00249v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>RoboMorph: In-Context Meta-Learning for Robot Dynamics Modeling</title>
      <link>http://arxiv.org/abs/2409.11815v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 深度学习领域经历了重大变革，Transformer架构的广泛应用，特别是在自然语言处理（NLP）领域。&lt;br&gt;&lt;h4&gt;2. 应用现状&lt;/h4&gt;   - 物理应用（如求解偏微分方程和图像视觉）有了新的探索，但在机器人领域，尤其是面临高非线性挑战时，Transformer的应用仍然稀少。&lt;br&gt;&lt;h4&gt;3. 现有局限&lt;/h4&gt;   - 现有研究多集中于利用Transformers为机器人提供高层次任务知识，较少关注系统识别。&lt;br&gt;&lt;h4&gt;4. 新方法提出&lt;/h4&gt;   - 本文提出了一种新方法，使用Transformer架构学习高维物理系统（如Franka机器人臂）的元动态模型，无需先验的物理参数知识。&lt;br&gt;&lt;h4&gt;5. 研究目标&lt;/h4&gt;   - 目标是根据每个关节的扭矩信号预测关心的量（如末端执行器位姿和关节位置）。&lt;br&gt;&lt;h4&gt;6. 应用价值&lt;/h4&gt;   - 这种预测可作为深度模型预测控制框架中的一个组成部分。&lt;br&gt;&lt;h4&gt;7. 模型功能&lt;/h4&gt;   - 元模型建立了扭矩与位置之间的关联，并预测完整轨迹的输出。&lt;br&gt;&lt;h4&gt;8. 实证证据&lt;/h4&gt;   - 该研究提供了在上下文学习范式有效性的实证证据，暗示未来在没有显式物理参数知识的情况下学习机器人系统动态的改进。&lt;br&gt;&lt;h4&gt;9. 附加资源&lt;/h4&gt;   - 代码、视频和补充材料可在项目网站找到：[RoboMorph](https://sites.google.com/view/robomorph/)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The landscape of Deep Learning has experienced a major shift with the
pervasive adoption of Transformer-based architectures, particularly in Natural
Language Processing (NLP). Novel avenues for physical applications, such as
solving Partial Differential Equations and Image Vision, have been explored.
However, in challenging domains like robotics, where high non-linearity poses
significant challenges, Transformer-based applications are scarce. While
Transformers have been used to provide robots with knowledge about high-level
tasks, few efforts have been made to perform system identification. This paper
proposes a novel methodology to learn a meta-dynamical model of a
high-dimensional physical system, such as the Franka robotic arm, using a
Transformer-based architecture without prior knowledge of the system's physical
parameters. The objective is to predict quantities of interest (end-effector
pose and joint positions) given the torque signals for each joint. This
prediction can be useful as a component for Deep Model Predictive Control
frameworks in robotics. The meta-model establishes the correlation between
torques and positions and predicts the output for the complete trajectory. This
work provides empirical evidence of the efficacy of the in-context learning
paradigm, suggesting future improvements in learning the dynamics of robotic
systems without explicit knowledge of physical parameters. Code, videos, and
supplementary materials can be found at project website. See
https://sites.google.com/view/robomorph/</description>
      <guid isPermaLink="false">2409.11815v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>VoxelTrack: Exploring Voxel Representation for 3D Point Cloud Object Tracking</title>
      <link>http://arxiv.org/abs/2408.02263v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 当前基于LiDAR点云的3D单目标跟踪（SOT）方法通常依赖于点基表示网络。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 这些网络存在一些基本问题：&lt;br&gt;     1. **池化操作问题**：池化操作用于处理无序点云，限制了对有助于跟踪的3D空间信息的捕捉。&lt;br&gt;     2. **集合抽象操作问题**：采用的集合抽象操作难以处理密度不一致的点云，阻碍了3D空间信息的建模。&lt;br&gt;&lt;h4&gt;3. 新方法提出&lt;/h4&gt;   - 提出了一个新颖的跟踪框架，称为VoxelTrack。&lt;br&gt;&lt;h4&gt;4. 方法概述&lt;/h4&gt;   - VoxelTrack通过将无序点云体素化为3D体素，并通过稀疏卷积块提取特征，有效建模精确且稳健的3D空间信息，从而指导被跟踪物体的准确位置预测。&lt;br&gt;&lt;h4&gt;5. 双流编码器&lt;/h4&gt;   - VoxelTrack结合了双流编码器和交叉迭代特征融合模块，以进一步探索细粒度的3D空间信息进行跟踪。&lt;br&gt;&lt;h4&gt;6. 模型简化&lt;/h4&gt;   - 由于准确的3D空间信息建模，VoxelTrack简化了跟踪管道，仅使用单一回归损失。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 在KITTI、NuScenes和Waymo Open Dataset三个广泛采用的数据集上进行了广泛实验。&lt;br&gt;   - 实验结果表明，VoxelTrack在这三个数据集上分别达到了88.3%、71.4%和63.6%的平均精度，表现出最先进的性能。&lt;br&gt;&lt;h4&gt;8. 实时性能&lt;/h4&gt;   - 在单个TITAN RTX GPU上以36帧每秒的速度超越了现有跟踪器。&lt;br&gt;&lt;h4&gt;9. 开放获取&lt;/h4&gt;   - 源代码和模型将会发布。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current LiDAR point cloud-based 3D single object tracking (SOT) methods
typically rely on point-based representation network. Despite demonstrated
success, such networks suffer from some fundamental problems: 1) It contains
pooling operation to cope with inherently disordered point clouds, hindering
the capture of 3D spatial information that is useful for tracking, a regression
task. 2) The adopted set abstraction operation hardly handles
density-inconsistent point clouds, also preventing 3D spatial information from
being modeled. To solve these problems, we introduce a novel tracking
framework, termed VoxelTrack. By voxelizing inherently disordered point clouds
into 3D voxels and extracting their features via sparse convolution blocks,
VoxelTrack effectively models precise and robust 3D spatial information,
thereby guiding accurate position prediction for tracked objects. Moreover,
VoxelTrack incorporates a dual-stream encoder with cross-iterative feature
fusion module to further explore fine-grained 3D spatial information for
tracking. Benefiting from accurate 3D spatial information being modeled, our
VoxelTrack simplifies tracking pipeline with a single regression loss.
Extensive experiments are conducted on three widely-adopted datasets including
KITTI, NuScenes and Waymo Open Dataset. The experimental results confirm that
VoxelTrack achieves state-of-the-art performance (88.3%, 71.4% and 63.6% mean
precision on the three datasets, respectively), and outperforms the existing
trackers with a real-time speed of 36 Fps on a single TITAN RTX GPU. The source
code and model will be released.</description>
      <guid isPermaLink="false">2408.02263v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>JEAN: Joint Expression and Audio-guided NeRF-based Talking Face Generation</title>
      <link>http://arxiv.org/abs/2409.12156v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by BMVC 2024. Project Page:
  https://starc52.github.io/publications/2024-07-19-JEAN&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究目的&lt;/h4&gt;   - 提出了一种新方法，用于联合生成表情和音频引导的说话人脸。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 近期的方法要么难以保持说话者的身份，要么无法生成真实的面部表情。&lt;br&gt;&lt;h4&gt;3. 新方法概述&lt;/h4&gt;   - 本文提出了一种基于NeRF的网络，以应对上述挑战。&lt;br&gt;&lt;h4&gt;4. 训练数据与表示学习&lt;/h4&gt;   - 网络在没有任何真实标签的单目视频上训练，因此必须学习音频和表情的解耦表示。&lt;br&gt;&lt;h4&gt;5. 音频特征学习&lt;/h4&gt;   - 首先以自监督方式学习音频特征，使用来自多个主体的语音片段。&lt;br&gt;   - 通过对比学习技术，确保学习到的音频特征与唇部运动对齐，并与面部其余部分的肌肉运动解耦。&lt;br&gt;&lt;h4&gt;6. 表情特征学习&lt;/h4&gt;   - 设计了一种基于变换器的架构，学习表情特征，捕捉长距离面部表情，并将其与特定于语言的口部动作解耦。&lt;br&gt;&lt;h4&gt;7. 评估结果&lt;/h4&gt;   - 通过定量和定性评估，证明了该方法能够合成高保真度的说话人脸视频，实现了最先进的面部表情转移和与未见音频的唇部同步。&lt;br&gt;&lt;h4&gt;8. 成果意义&lt;/h4&gt;   - 方法在生成自然的说话人脸视频方面表现出色，具有重要的应用潜力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce a novel method for joint expression and audio-guided talking
face generation. Recent approaches either struggle to preserve the speaker
identity or fail to produce faithful facial expressions. To address these
challenges, we propose a NeRF-based network. Since we train our network on
monocular videos without any ground truth, it is essential to learn
disentangled representations for audio and expression. We first learn audio
features in a self-supervised manner, given utterances from multiple subjects.
By incorporating a contrastive learning technique, we ensure that the learned
audio features are aligned to the lip motion and disentangled from the muscle
motion of the rest of the face. We then devise a transformer-based architecture
that learns expression features, capturing long-range facial expressions and
disentangling them from the speech-specific mouth movements. Through
quantitative and qualitative evaluation, we demonstrate that our method can
synthesize high-fidelity talking face videos, achieving state-of-the-art facial
expression transfer along with lip synchronization to unseen audio.</description>
      <guid isPermaLink="false">2409.12156v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Panoptic-Depth Forecasting</title>
      <link>http://arxiv.org/abs/2409.12008v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 预测场景的语义和三维结构对机器人安全导航和行动规划至关重要。&lt;br&gt;&lt;h4&gt;2. 现有方法的局限性&lt;/h4&gt;   - 最近的方法探讨了语义和全景场景预测，但忽略了场景的几何信息。&lt;br&gt;&lt;h4&gt;3. 新任务提出&lt;/h4&gt;   - 本文提出了“全景深度预测”任务，旨在从单目相机图像中联合预测未观察到的未来帧的全景分割和深度图。&lt;br&gt;&lt;h4&gt;4. 数据集扩展&lt;/h4&gt;   - 扩展了流行的KITTI-360和Cityscapes基准，利用LiDAR点云计算深度图，并利用顺序标记数据。&lt;br&gt;&lt;h4&gt;5. 评估指标&lt;/h4&gt;   - 引入了一种适合的评估指标，能够以一致的方式量化全景质量和深度估计的准确性。&lt;br&gt;&lt;h4&gt;6. 基线与新架构&lt;/h4&gt;   - 提出了两个基线，并提出了新颖的PDcast架构，通过结合基于变换器的编码器、预测模块和任务特定解码器来学习丰富的时空表示，以预测未来的全景深度输出。&lt;br&gt;&lt;h4&gt;7. 评估结果&lt;/h4&gt;   - 大量评估展示了PDcast在两个数据集和三个预测任务中的有效性，持续解决主要挑战。&lt;br&gt;&lt;h4&gt;8. 代码开放获取&lt;/h4&gt;   - 代码已公开发布，网址为[PDcast GitHub](https://pdcast.cs.uni-freiburg.de)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Forecasting the semantics and 3D structure of scenes is essential for robots
to navigate and plan actions safely. Recent methods have explored semantic and
panoptic scene forecasting; however, they do not consider the geometry of the
scene. In this work, we propose the panoptic-depth forecasting task for jointly
predicting the panoptic segmentation and depth maps of unobserved future
frames, from monocular camera images. To facilitate this work, we extend the
popular KITTI-360 and Cityscapes benchmarks by computing depth maps from LiDAR
point clouds and leveraging sequential labeled data. We also introduce a
suitable evaluation metric that quantifies both the panoptic quality and depth
estimation accuracy of forecasts in a coherent manner. Furthermore, we present
two baselines and propose the novel PDcast architecture that learns rich
spatio-temporal representations by incorporating a transformer-based encoder, a
forecasting module, and task-specific decoders to predict future panoptic-depth
outputs. Extensive evaluations demonstrate the effectiveness of PDcast across
two datasets and three forecasting tasks, consistently addressing the primary
challenges. We make the code publicly available at
https://pdcast.cs.uni-freiburg.de.</description>
      <guid isPermaLink="false">2409.12008v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>All-in-one foundational models learning across quantum chemical levels</title>
      <link>http://arxiv.org/abs/2409.12015v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 传统的机器学习（ML）势能通常针对单一的量子化学（QC）水平，而多保真度学习的ML模型未能提供可扩展的基础模型解决方案。&lt;br&gt;&lt;h4&gt;2. 新模型架构&lt;/h4&gt;   - 引入了“全合一”（AIO）ANI模型架构，基于多模态学习，能够学习任意数量的QC水平。&lt;br&gt;&lt;h4&gt;3. 学习方法&lt;/h4&gt;   - AIO学习方法提供了一种比迁移学习更通用且易于使用的替代方案。&lt;br&gt;&lt;h4&gt;4. 模型训练&lt;/h4&gt;   - 训练了AIO-ANI-UIP基础模型，其泛化能力与半经验GFN2-xTB和使用双ζ基组的密度泛函理论（DFT）相当，特别适用于有机分子。&lt;br&gt;&lt;h4&gt;5. 跨QC水平学习&lt;/h4&gt;   - AIO-ANI模型能够跨越不同的QC水平进行学习，包括从半经验方法到DFT以及耦合簇方法。&lt;br&gt;&lt;h4&gt;6. 新模型设计&lt;/h4&gt;   - 基于Δ学习设计了新的基础模型Δ-AIO-ANI，相较于AIO-ANI-UIP具有更高的准确性和稳健性。&lt;br&gt;&lt;h4&gt;7. 代码和模型可用性&lt;/h4&gt;   - 代码及基础模型可在[GitHub](https://github.com/dralgroup/aio-ani)上获取，并将整合到通用可更新的AI增强量子化学（UAIQM）库中。&lt;br&gt;&lt;h4&gt;8. 在线使用&lt;/h4&gt;   - 这些模型将集成到MLatom包中，用户可通过XACS云计算平台在线使用，具体更新可查看[MLatom GitHub](https://github.com/dralgroup/mlatom)。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/dralgroup/aio-ani&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine learning (ML) potentials typically target a single quantum chemical
(QC) level while the ML models developed for multi-fidelity learning have not
been shown to provide scalable solutions for foundational models. Here we
introduce the all-in-one (AIO) ANI model architecture based on multimodal
learning which can learn an arbitrary number of QC levels. Our all-in-one
learning approach offers a more general and easier-to-use alternative to
transfer learning. We use it to train the AIO-ANI-UIP foundational model with
the generalization capability comparable to semi-empirical GFN2-xTB and DFT
with a double-zeta basis set for organic molecules. We show that the AIO-ANI
model can learn across different QC levels ranging from semi-empirical to
density functional theory to coupled cluster. We also use AIO models to design
the foundational model {\Delta}-AIO-ANI based on {\Delta}-learning with
increased accuracy and robustness compared to AIO-ANI-UIP. The code and the
foundational models are available at https://github.com/dralgroup/aio-ani; they
will be integrated into the universal and updatable AI-enhanced QM (UAIQM)
library and made available in the MLatom package so that they can be used
online at the XACS cloud computing platform (see
https://github.com/dralgroup/mlatom for updates).</description>
      <guid isPermaLink="false">2409.12015v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Non-Rigid Point Cloud Matching through Large Vision Models</title>
      <link>http://arxiv.org/abs/2408.08568v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 4 figures&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究主题&lt;/h4&gt;   - 提出了一个新颖的基于学习的框架，用于非刚性点云匹配。&lt;br&gt;&lt;h4&gt;2. 训练方式&lt;/h4&gt;   - 该框架能够仅通过点云训练，无需任何对应注释，并且可以自然扩展到部分到完整的匹配。&lt;br&gt;&lt;h4&gt;3. 核心见解&lt;/h4&gt;   - 关键在于将来自大型视觉模型（LVMs）的语义特征与基于几何的形状特征学习结合。&lt;br&gt;&lt;h4&gt;4. 结构信息利用&lt;/h4&gt;   - 框架有效利用语义特征中的结构信息，以解决局部几何体之间自相似性带来的模糊性问题。&lt;br&gt;&lt;h4&gt;5. 泛化能力与鲁棒性&lt;/h4&gt;   - 该框架在处理LVM部分观察时展现出强大的泛化能力和鲁棒性，从而改善点云匹配任务的效果。&lt;br&gt;&lt;h4&gt;6. 模块设计&lt;/h4&gt;   - 提出了像素到点特征聚合模块、局部和全局注意力网络，以及几何相似性损失函数，以实现上述目标。&lt;br&gt;&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - 实验结果表明，该方法在近等距和异构形状集的非刚性点云匹配任务中达到了最先进的结果，同时在更真实的部分和噪声数据中也表现良好。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we propose a novel learning-based framework for non-rigid
point cloud matching, which can be trained purely on point clouds without any
correspondence annotation but also be extended naturally to partial-to-full
matching. Our key insight is to incorporate semantic features derived from
large vision models (LVMs) to geometry-based shape feature learning. Our
framework effectively leverages the structural information contained in the
semantic features to address ambiguities arise from self-similarities among
local geometries. Furthermore, our framework also enjoys the strong
generalizability and robustness regarding partial observations of LVMs, leading
to improvements in the regarding point cloud matching tasks. In order to
achieve the above, we propose a pixel-to-point feature aggregation module, a
local and global attention network as well as a geometrical similarity loss
function. Experimental results show that our method achieves state-of-the-art
results in matching non-rigid point clouds in both near-isometric and
heterogeneous shape collection as well as more realistic partial and noisy
data.</description>
      <guid isPermaLink="false">2408.08568v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Pre-Training and Personalized Fine-Tuning via Over-the-Air Federated Meta-Learning: Convergence-Generalization Trade-Offs</title>
      <link>http://arxiv.org/abs/2406.11569v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  39 pages, 7 figures, submitted for possible journal publication&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 现代人工智能（AI）应用（如大型语言模型）正在采用预训练和微调的训练范式。&lt;br&gt;&lt;h4&gt;2. 数据开放性&lt;/h4&gt;   - 由于开放数据存储库减少，以及对AI模型获取的民主化努力，预训练预计将从集中式部署逐渐迁移到联邦学习（FL）实现。&lt;br&gt;&lt;h4&gt;3. 元学习框架&lt;/h4&gt;   - 元学习提供了一个通用框架，可以形式化预训练和微调的过程。&lt;br&gt;&lt;h4&gt;4. 个性化联邦学习&lt;/h4&gt;   - 基于元学习的个性化FL（meta-pFL）超越了基本个性化，旨在针对新代理和任务的泛化。&lt;br&gt;&lt;h4&gt;5. 无线设置研究&lt;/h4&gt;   - 本文研究了在无线环境中，参与预训练阶段的代理通过共享无线信道连接到服务器的元学习的泛化性能。&lt;br&gt;&lt;h4&gt;6. 空中计算&lt;/h4&gt;   - 采用空中计算方法，研究了对新代理和任务的泛化与收敛之间的权衡。&lt;br&gt;&lt;h4&gt;7. 权衡分析&lt;/h4&gt;   - 这种权衡源于信道损伤可能增强泛化能力，但却会降低收敛性。&lt;br&gt;&lt;h4&gt;8. 理论验证&lt;/h4&gt;   - 大量数值结果验证了理论分析的正确性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-06-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; For modern artificial intelligence (AI) applications such as large language
models (LLMs), the training paradigm has recently shifted to pre-training
followed by fine-tuning. Furthermore, owing to dwindling open repositories of
data and thanks to efforts to democratize access to AI models, pre-training is
expected to increasingly migrate from the current centralized deployments to
federated learning (FL) implementations. Meta-learning provides a general
framework in which pre-training and fine-tuning can be formalized.
Meta-learning-based personalized FL (meta-pFL) moves beyond basic
personalization by targeting generalization to new agents and tasks. This paper
studies the generalization performance of meta-pFL for a wireless setting in
which the agents participating in the pre-training phase, i.e., meta-learning,
are connected via a shared wireless channel to the server. Adopting
over-the-air computing, we study the trade-off between generalization to new
agents and tasks, on the one hand, and convergence, on the other hand. The
trade-off arises from the fact that channel impairments may enhance
generalization, while degrading convergence. Extensive numerical results
validate the theory.</description>
      <guid isPermaLink="false">2406.11569v3</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Panacea+: Panoramic and Controllable Video Generation for Autonomous Driving</title>
      <link>http://arxiv.org/abs/2408.07605v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://panacea-ad.github.io/. arXiv admin note: text
  overlap with arXiv:2311.16813&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 自动驾驶领域对高质量标注视频训练数据的需求日益增加。&lt;br&gt;&lt;h4&gt;2. 框架介绍&lt;/h4&gt;   - 提出了Panacea+，这是一个强大且适用范围广泛的视频数据生成框架，专注于驾驶场景。&lt;br&gt;&lt;h4&gt;3. 技术基础&lt;/h4&gt;   - Panacea+基于之前的研究成果Panacea，采用了多视角外观噪声先验机制和超分辨率模块，以增强一致性和提高分辨率。&lt;br&gt;&lt;h4&gt;4. 实验结果&lt;/h4&gt;   - 通过广泛的实验，证明Panacea+生成的视频样本在多个任务中具有显著效果，包括3D物体跟踪、3D物体检测和车道检测。&lt;br&gt;&lt;h4&gt;5. 数据集应用&lt;/h4&gt;   - 实验使用了nuScenes和Argoverse 2数据集，显示出框架的广泛适用性。&lt;br&gt;&lt;h4&gt;6. 研究意义&lt;/h4&gt;   - 结果强有力地证明了Panacea+作为自动驾驶数据生成框架的价值。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The field of autonomous driving increasingly demands high-quality annotated
video training data. In this paper, we propose Panacea+, a powerful and
universally applicable framework for generating video data in driving scenes.
Built upon the foundation of our previous work, Panacea, Panacea+ adopts a
multi-view appearance noise prior mechanism and a super-resolution module for
enhanced consistency and increased resolution. Extensive experiments show that
the generated video samples from Panacea+ greatly benefit a wide range of tasks
on different datasets, including 3D object tracking, 3D object detection, and
lane detection tasks on the nuScenes and Argoverse 2 dataset. These results
strongly prove Panacea+ to be a valuable data generation framework for
autonomous driving.</description>
      <guid isPermaLink="false">2408.07605v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
    <item>
      <title>Using Large Language Models to Generate Clinical Trial Tables and Figures</title>
      <link>http://arxiv.org/abs/2409.12046v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 表格、图形和清单（TFLs）是总结临床试验数据的重要工具。&lt;br&gt;&lt;h4&gt;2. 工作挑战&lt;/h4&gt;   - 创建TFL以进行报告通常是一项耗时的任务，常见于临床试验的执行过程中。&lt;br&gt;&lt;h4&gt;3. 研究目的&lt;/h4&gt;   - 本研究探索使用大型语言模型（LLMs）通过提示工程和少量样本迁移学习来自动生成TFL。&lt;br&gt;&lt;h4&gt;4. 数据来源&lt;/h4&gt;   - 使用公共临床试验数据，采用ADaM格式进行研究。&lt;br&gt;&lt;h4&gt;5. 研究结果&lt;/h4&gt;   - 结果表明，LLMs能够高效生成TFL，只需提供提示指令，展示了其在该领域的潜力。&lt;br&gt;&lt;h4&gt;6. 工具开发&lt;/h4&gt;   - 开发了一个名为“临床试验TFL生成代理”的对话式应用，能够将用户查询与预定义提示匹配，从而生成特定的自定义程序以生成预定义的TFL。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tables, figures, and listings (TFLs) are essential tools for summarizing
clinical trial data. Creation of TFLs for reporting activities is often a
time-consuming task encountered routinely during the execution of clinical
trials. This study explored the use of large language models (LLMs) to automate
the generation of TFLs through prompt engineering and few-shot transfer
learning. Using public clinical trial data in ADaM format, our results
demonstrated that LLMs can efficiently generate TFLs with prompt instructions,
showcasing their potential in this domain. Furthermore, we developed a
conservational agent named Clinical Trial TFL Generation Agent: An app that
matches user queries to predefined prompts that produce customized programs to
generate specific predefined TFLs.</description>
      <guid isPermaLink="false">2409.12046v1</guid>
      <pubDate>Fri, 20 Sep 2024 09:07:28 +0800</pubDate>
    </item>
  </channel>
</rss>
