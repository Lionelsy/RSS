<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Arxiv论文推荐</title>
    <link>https://github.com/lionelsy/RSS</link>
    <description>Arxiv论文推荐</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>zh-CN</language>
    <lastBuildDate>Thu, 19 Sep 2024 11:05:32 +0000</lastBuildDate>
    <item>
      <title>Interactive Masked Image Modeling for Multimodal Object Detection in Remote Sensing</title>
      <link>http://arxiv.org/abs/2409.08885v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Minh-Duc Vu', 'Zuheng Ming', 'Fangchen Feng', 'Bissmella Bahaduri', 'Anissa Mokraoui']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-13&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 远程感知图像中的目标检测在多种地球观测应用中发挥着重要作用。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;挑战&lt;/h4&gt;：&lt;br&gt;   - 与自然场景图像的目标检测相比，远程感知中目标检测面临更大挑战，因为存在大量小型且常常几乎不可见的物体，分布在多样的地形中。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;解决方案&lt;/h4&gt;：&lt;br&gt;   - 提出使用多模态学习来整合不同数据模态的特征，从而提高检测准确性。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;数据限制&lt;/h4&gt;：&lt;br&gt;   - 然而，多模态学习的性能往往受到标注数据集规模有限的制约。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;预训练技术&lt;/h4&gt;：&lt;br&gt;   - 本文建议使用掩蔽图像建模（MIM）作为预训练技术，通过自监督学习利用未标注数据来增强检测性能。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;传统MIM的局限&lt;/h4&gt;：&lt;br&gt;   - 传统的MIM方法（如MAE）使用掩蔽标记而没有上下文信息，难以捕捉细粒度的细节，因为缺乏与图像其他部分的交互。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;新方法提出&lt;/h4&gt;：&lt;br&gt;   - 本文提出了一种新的交互式MIM方法，能够在不同标记之间建立交互，这对于远程感知中的目标检测尤其有益。&lt;br&gt;&lt;br&gt;8. &lt;h4&gt;实验验证&lt;/h4&gt;：&lt;br&gt;   - 大量消融研究和评估实验证明了我们方法的有效性。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Object detection in remote sensing imagery plays a vital role in various
Earth observation applications. However, unlike object detection in natural
scene images, this task is particularly challenging due to the abundance of
small, often barely visible objects across diverse terrains. To address these
challenges, multimodal learning can be used to integrate features from
different data modalities, thereby improving detection accuracy. Nonetheless,
the performance of multimodal learning is often constrained by the limited size
of labeled datasets. In this paper, we propose to use Masked Image Modeling
(MIM) as a pre-training technique, leveraging self-supervised learning on
unlabeled data to enhance detection performance. However, conventional MIM such
as MAE which uses masked tokens without any contextual information, struggles
to capture the fine-grained details due to a lack of interactions with other
parts of image. To address this, we propose a new interactive MIM method that
can establish interactions between different tokens, which is particularly
beneficial for object detection in remote sensing. The extensive ablation
studies and evluation demonstrate the effectiveness of our approach.</description>
      <guid isPermaLink="false">2409.08885v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>Advances and Challenges in Meta-Learning: A Technical Review</title>
      <link>http://arxiv.org/abs/2307.04722v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Anna Vettoruzzo', 'Mohamed-Rafik Bouguelia', 'Joaquin Vanschoren', 'Thorsteinn Rögnvaldsson', 'KC Santosh']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2023-07-10&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 元学习使学习系统能够从多个任务中获取知识，从而加速适应新任务和提高泛化能力。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;综述目的&lt;/h4&gt;：&lt;br&gt;   - 本文提供了元学习的全面技术概述，强调其在数据稀缺或获取成本高的实际应用中的重要性。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;研究范围&lt;/h4&gt;：&lt;br&gt;   - 论文涵盖了最先进的元学习方法，并探讨了元学习与以下领域之间的关系：&lt;br&gt;     - 多任务学习&lt;br&gt;     - 迁移学习&lt;br&gt;     - 领域适应与泛化&lt;br&gt;     - 自监督学习&lt;br&gt;     - 个性化联邦学习&lt;br&gt;     - 持续学习&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;主题协同&lt;/h4&gt;：&lt;br&gt;   - 通过强调这些主题与元学习领域之间的协同作用，论文展示了一个领域的进展如何促进整体发展，并避免不必要的重复努力。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;高级主题探讨&lt;/h4&gt;：&lt;br&gt;   - 论文深入探讨了几个高级元学习主题，包括：&lt;br&gt;     - 从复杂的多模态任务分布中学习&lt;br&gt;     - 无监督元学习&lt;br&gt;     - 有效适应数据分布变化的学习&lt;br&gt;     - 持续元学习&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;未来研究方向&lt;/h4&gt;：&lt;br&gt;   - 文章指出了元学习领域的开放问题和挑战，为未来的研究提供了方向。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;研究贡献&lt;/h4&gt;：&lt;br&gt;   - 通过综合最新的研究进展，本文提供了对元学习及其在各种机器学习应用中潜在影响的深入理解，旨在推动元学习的发展及其实际应用。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Meta-learning empowers learning systems with the ability to acquire knowledge
from multiple tasks, enabling faster adaptation and generalization to new
tasks. This review provides a comprehensive technical overview of
meta-learning, emphasizing its importance in real-world applications where data
may be scarce or expensive to obtain. The paper covers the state-of-the-art
meta-learning approaches and explores the relationship between meta-learning
and multi-task learning, transfer learning, domain adaptation and
generalization, self-supervised learning, personalized federated learning, and
continual learning. By highlighting the synergies between these topics and the
field of meta-learning, the paper demonstrates how advancements in one area can
benefit the field as a whole, while avoiding unnecessary duplication of
efforts. Additionally, the paper delves into advanced meta-learning topics such
as learning from complex multi-modal task distributions, unsupervised
meta-learning, learning to efficiently adapt to data distribution shifts, and
continual meta-learning. Lastly, the paper highlights open problems and
challenges for future research in the field. By synthesizing the latest
research developments, this paper provides a thorough understanding of
meta-learning and its potential impact on various machine learning
applications. We believe that this technical overview will contribute to the
advancement of meta-learning and its practical implications in addressing
real-world problems.</description>
      <guid isPermaLink="false">2307.04722v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>Augment, Drop &amp; Swap: Improving Diversity in LLM Captions for Efficient Music-Text Representation Learning</title>
      <link>http://arxiv.org/abs/2409.11498v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Ilaria Manco', 'Justin Salamon', 'Oriol Nieto']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-17&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; To appear in the Proceedings of the 25th International Society for
  Music Information Retrieval Conference (ISMIR 2024)&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 音频-文本对比模型在音乐表示学习中已成为一种强有力的方法，但对关键设计选择对音乐-文本表示质量影响的了解仍然有限。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;研究目标&lt;/h4&gt;：&lt;br&gt;   - 本文旨在揭示在有限数据和计算预算下，设计选择对音乐-文本表示学习的影响，并通过实证观察建立更扎实的理解。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;研究维度&lt;/h4&gt;：&lt;br&gt;   - 研究从三个方面分析影响：&lt;br&gt;     - 基础编码器的选择&lt;br&gt;     - 训练数据的整理程度&lt;br&gt;     - 文本增强的使用&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;主要发现&lt;/h4&gt;：&lt;br&gt;   - 数据整理是资源受限场景中音乐-文本对比训练中最重要的因素。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;新技术引入&lt;/h4&gt;：&lt;br&gt;   - 为了提高文本输入的多样性和描述性，提出了两种新技术：&lt;br&gt;     - &lt;h4&gt;增强视图丢弃（Augmented View Dropout）&lt;/h4&gt;&lt;br&gt;     - &lt;h4&gt;文本交换（TextSwap）&lt;/h4&gt;&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;实验验证&lt;/h4&gt;：&lt;br&gt;   - 通过实验证明，这些新技术在不同的预训练方案、模型架构和下游数据分布中有效提升了性能。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;成本效益&lt;/h4&gt;：&lt;br&gt;   - 新方法的引入没有增加计算成本或需要额外的训练数据。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Audio-text contrastive models have become a powerful approach in music
representation learning. Despite their empirical success, however, little is
known about the influence of key design choices on the quality of music-text
representations learnt through this framework. In this work, we expose these
design choices within the constraints of limited data and computation budgets,
and establish a more solid understanding of their impact grounded in empirical
observations along three axes: the choice of base encoders, the level of
curation in training data, and the use of text augmentation. We find that data
curation is the single most important factor for music-text contrastive
training in resource-constrained scenarios. Motivated by this insight, we
introduce two novel techniques, Augmented View Dropout and TextSwap, which
increase the diversity and descriptiveness of text inputs seen in training.
Through our experiments we demonstrate that these are effective at boosting
performance across different pre-training regimes, model architectures, and
downstream data distributions, without incurring higher computational costs or
requiring additional training data.</description>
      <guid isPermaLink="false">2409.11498v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>D3Former: Jointly Learning Repeatable Dense Detectors and Feature-enhanced Descriptors via Saliency-guided Transformer</title>
      <link>http://arxiv.org/abs/2312.12970v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Junjie Gao', 'Pengfei Wang', 'Qiujie Dong', 'Qiong Zeng', 'Shiqing Xin', 'Caiming Zhang']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2023-12-20&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; 15 pages, 6 figures&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 精确和具有代表性的匹配是解决点云配准问题的关键步骤。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;常见方法的局限性&lt;/h4&gt;：&lt;br&gt;   - 现有方法通常通过检测具有显著几何特征的关键点，并将这些关键点从一个点云帧映射到另一个帧。然而，这些方法受到采样关键点重复性的限制。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;新方法介绍&lt;/h4&gt;：&lt;br&gt;   - 本文提出了一种基于显著性引导的变换器，称为&lt;h4&gt;D3Former&lt;/h4&gt;，该方法联合学习可重复的&lt;h4&gt;D&lt;/h4&gt;ense &lt;h4&gt;D&lt;/h4&gt;etectors和特征增强的&lt;h4&gt;D&lt;/h4&gt;escriptors。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;模型组成&lt;/h4&gt;：&lt;br&gt;   - D3Former模型包括两个模块：&lt;br&gt;     - &lt;h4&gt;特征增强描述子学习（FEDL）&lt;/h4&gt;：利用区域注意机制来增强特征的独特性。&lt;br&gt;     - &lt;h4&gt;重复关键点检测学习（RKDL）&lt;/h4&gt;：专注于检测可重复的关键点，以增强匹配能力。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;实验结果&lt;/h4&gt;：&lt;br&gt;   - 在具有挑战性的室内和室外基准测试中，实验结果表明该方法在点云匹配方面始终优于当前最先进的方法。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;具体表现&lt;/h4&gt;：&lt;br&gt;   - 在3DLoMatch测试中，即使在低重叠率下，D3Former也持续优于近期发表的方法，如RoReg和RoITr。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;性能对比&lt;/h4&gt;：&lt;br&gt;   - 当提取的关键点数量减少到250时，RoReg、RoITr和我们的方法的配准召回率分别为64.3%、73.6%和76.5%。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Establishing accurate and representative matches is a crucial step in
addressing the point cloud registration problem. A commonly employed approach
involves detecting keypoints with salient geometric features and subsequently
mapping these keypoints from one frame of the point cloud to another. However,
methods within this category are hampered by the repeatability of the sampled
keypoints. In this paper, we introduce a saliency-guided trans\textbf{former},
referred to as \textit{D3Former}, which entails the joint learning of
repeatable \textbf{D}ense \textbf{D}etectors and feature-enhanced
\textbf{D}escriptors. The model comprises a Feature Enhancement Descriptor
Learning (FEDL) module and a Repetitive Keypoints Detector Learning (RKDL)
module. The FEDL module utilizes a region attention mechanism to enhance
feature distinctiveness, while the RKDL module focuses on detecting repeatable
keypoints to enhance matching capabilities. Extensive experimental results on
challenging indoor and outdoor benchmarks demonstrate that our proposed method
consistently outperforms state-of-the-art point cloud matching methods.
Notably, tests on 3DLoMatch, even with a low overlap ratio, show that our
method consistently outperforms recently published approaches such as RoReg and
RoITr. For instance, with the number of extracted keypoints reduced to 250, the
registration recall scores for RoReg, RoITr, and our method are 64.3\%, 73.6\%,
and 76.5\%, respectively.</description>
      <guid isPermaLink="false">2312.12970v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>Graph Neural Network-State Predictive Information Bottleneck (GNN-SPIB) approach for learning molecular thermodynamics and kinetics</title>
      <link>http://arxiv.org/abs/2409.11843v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Ziyue Zou', 'Dedi Wang', 'Pratyush Tiwary']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-18&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 分子动力学模拟提供了对原子运动的详细洞察，但面临时间尺度的限制。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;增强采样方法的挑战&lt;/h4&gt;：&lt;br&gt;   - 尽管增强采样方法已解决了一些挑战，但即使结合机器学习，它们通常依赖于预先选择的专家特征。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;新框架介绍&lt;/h4&gt;：&lt;br&gt;   - 本文提出了图神经网络-状态预测信息瓶颈（GNN-SPIB）框架，结合了图神经网络和状态预测信息瓶颈。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;自动学习表示&lt;/h4&gt;：&lt;br&gt;   - GNN-SPIB框架能够直接从原子坐标中自动学习低维表示。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;基准测试&lt;/h4&gt;：&lt;br&gt;   - 在三个基准系统上进行测试，表明该方法能够预测慢过程的基本结构、热力学和动力学信息。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;鲁棒性&lt;/h4&gt;：&lt;br&gt;   - 方法在不同系统中表现出稳定性和鲁棒性。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;复杂系统应用&lt;/h4&gt;：&lt;br&gt;   - 该方法在复杂系统中显示出潜力，能够有效进行增强采样，而无需预定义的反应坐标或输入特征。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Molecular dynamics simulations offer detailed insights into atomic motions
but face timescale limitations. Enhanced sampling methods have addressed these
challenges but even with machine learning, they often rely on pre-selected
expert-based features. In this work, we present the Graph Neural Network-State
Predictive Information Bottleneck (GNN-SPIB) framework, which combines graph
neural networks and the State Predictive Information Bottleneck to
automatically learn low-dimensional representations directly from atomic
coordinates. Tested on three benchmark systems, our approach predicts essential
structural, thermodynamic and kinetic information for slow processes,
demonstrating robustness across diverse systems. The method shows promise for
complex systems, enabling effective enhanced sampling without requiring
pre-defined reaction coordinates or input features.</description>
      <guid isPermaLink="false">2409.11843v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>RF-GML: Reference-Free Generative Machine Listener</title>
      <link>http://arxiv.org/abs/2409.10210v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Arijit Biswas', 'Guanxin Jiang']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-16&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; Pre-review version submitted to ICASSP 2025&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究目的&lt;/h4&gt;：&lt;br&gt;   - 本文介绍了一种新颖的无参考（RF）音频质量度量标准，称为RF-Generative Machine Listener (RF-GML)，用于评估编码的单声道、立体声和双耳音频，采样率为48 kHz。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;模型基础&lt;/h4&gt;：&lt;br&gt;   - RF-GML基于现有的全参考（FR）生成机器监听器（GML），通过最小的架构修改进行迁移学习。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;生成能力&lt;/h4&gt;：&lt;br&gt;   - "生成"一词指的是该模型能够生成任意数量的模拟听感评分。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;预测准确性&lt;/h4&gt;：&lt;br&gt;   - 与现有的RF模型不同，RF-GML能够准确预测不同内容类型和编码的主观质量评分。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;广泛评估&lt;/h4&gt;：&lt;br&gt;   - 大量评估表明，RF-GML在对未编码音频的评级和区分不同编码伪影方面表现优越。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;应用价值&lt;/h4&gt;：&lt;br&gt;   - RF-GML的性能和多功能性使其成为在各种应用中评估和监测编码音频质量的有价值工具，且无需参考信号。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces a novel reference-free (RF) audio quality metric called
the RF-Generative Machine Listener (RF-GML), designed to evaluate coded mono,
stereo, and binaural audio at a 48 kHz sample rate. RF-GML leverages transfer
learning from a state-of-the-art full-reference (FR) Generative Machine
Listener (GML) with minimal architectural modifications. The term "generative"
refers to the model's ability to generate an arbitrary number of simulated
listening scores. Unlike existing RF models, RF-GML accurately predicts
subjective quality scores across diverse content types and codecs. Extensive
evaluations demonstrate its superiority in rating unencoded audio and
distinguishing different levels of coding artifacts. RF-GML's performance and
versatility make it a valuable tool for coded audio quality assessment and
monitoring in various applications, all without the need for a reference
signal.</description>
      <guid isPermaLink="false">2409.10210v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>Multi-Correlation Siamese Transformer Network with Dense Connection for 3D Single Object Tracking</title>
      <link>http://arxiv.org/abs/2312.11051v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Shihao Feng', 'Pengpeng Liang', 'Jin Gao', 'Erkang Cheng']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2023-12-18&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; Preprint version for IEEE Robotics and Automation Letters (RAL)&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 基于点云的3D物体跟踪是自动驾驶中的重要任务，尽管Siamese网络在3D跟踪方面取得了显著进展，但有效学习模板和搜索分支之间的关联仍然具有挑战性，尤其是在稀疏的激光雷达点云数据下。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;研究创新&lt;/h4&gt;：&lt;br&gt;   - 本文提出了一种多相关性Siamese Transformer网络，具有多个阶段，在每个阶段的末尾基于稀疏柱体进行特征相关性计算，而不是仅在网络中的一个点进行。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;自注意力与交叉注意力&lt;/h4&gt;：&lt;br&gt;   - 在每个阶段，首先对每个分支应用自注意力，以捕获非局部上下文信息，然后使用交叉注意力将模板信息注入到搜索区域中。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;特征学习策略&lt;/h4&gt;：&lt;br&gt;   - 这种策略使得搜索区域的特征学习能够关注模板，同时保持模板的个体特征不变。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;密集连接设计&lt;/h4&gt;：&lt;br&gt;   - 为了便于网络保留不同阶段学习的信息，并简化优化过程，搜索区域的初始输入稀疏柱体与每个阶段的输出之间进行密集连接，连接至所有后续阶段和目标定位网络。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;鸟瞰图特征映射&lt;/h4&gt;：&lt;br&gt;   - 目标定位网络将柱体转换为鸟瞰图（BEV）特征图，并使用小型密集连接卷积网络预测目标状态。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;深度监督&lt;/h4&gt;：&lt;br&gt;   - 在每个阶段添加深度监督，以进一步提升性能。&lt;br&gt;&lt;br&gt;8. &lt;h4&gt;实验评估&lt;/h4&gt;：&lt;br&gt;   - 提出的算法在KITTI、nuScenes和Waymo等流行数据集上进行了评估，实验结果显示方法相较于当前最先进的技术具有良好性能。&lt;br&gt;&lt;br&gt;9. &lt;h4&gt;消融研究&lt;/h4&gt;：&lt;br&gt;   - 提供了消融研究，以展示各个组件的有效性。&lt;br&gt;&lt;br&gt;10. &lt;h4&gt;代码可用性&lt;/h4&gt;：&lt;br&gt;    - 相关代码可在 [GitHub](https://github.com/liangp/MCSTN-3DSOT) 上获取。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; 10.1109/LRA.2023.3325715&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; https://github.com/liangp/mcstn-3dsot&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud-based 3D object tracking is an important task in autonomous
driving. Though great advances regarding Siamese-based 3D tracking have been
made recently, it remains challenging to learn the correlation between the
template and search branches effectively with the sparse LIDAR point cloud
data. Instead of performing correlation of the two branches at just one point
in the network, in this paper, we present a multi-correlation Siamese
Transformer network that has multiple stages and carries out feature
correlation at the end of each stage based on sparse pillars. More
specifically, in each stage, self-attention is first applied to each branch
separately to capture the non-local context information. Then, cross-attention
is used to inject the template information into the search area. This strategy
allows the feature learning of the search area to be aware of the template
while keeping the individual characteristics of the template intact. To enable
the network to easily preserve the information learned at different stages and
ease the optimization, for the search area, we densely connect the initial
input sparse pillars and the output of each stage to all subsequent stages and
the target localization network, which converts pillars to bird's eye view
(BEV) feature maps and predicts the state of the target with a small densely
connected convolution network. Deep supervision is added to each stage to
further boost the performance as well. The proposed algorithm is evaluated on
the popular KITTI, nuScenes, and Waymo datasets, and the experimental results
show that our method achieves promising performance compared with the
state-of-the-art. Ablation study that shows the effectiveness of each component
is provided as well. Code is available at
https://github.com/liangp/MCSTN-3DSOT.</description>
      <guid isPermaLink="false">2312.11051v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>Self-supervised 3D Point Cloud Completion via Multi-view Adversarial Learning</title>
      <link>http://arxiv.org/abs/2407.09786v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Lintai Wu', 'Xianjing Cheng', 'Junhui Hou', 'Yong Xu', 'Huanqiang Zeng']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-07-13&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; 12 pages,8 figures&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 在实际场景中，扫描的点云常因遮挡问题而不完整，导致自监督点云补全的任务需要在没有完整地面真相的情况下重建缺失区域。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;现有方法问题&lt;/h4&gt;：&lt;br&gt;   - 当前的自监督方法通常依赖于多个部分观察的视图进行监督，或忽略了可以从给定部分点云中识别和利用的内在几何相似性。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;提出的方法&lt;/h4&gt;：&lt;br&gt;   - 本文提出了MAL-SPC框架，利用物体级别和类别特定的几何相似性来完成缺失结构的重建。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;无监督学习&lt;/h4&gt;：&lt;br&gt;   - MAL-SPC不需要任何3D完整监督，只需每个对象的单一部分点云。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;模式检索网络&lt;/h4&gt;：&lt;br&gt;   - 首先引入模式检索网络，检索部分输入和预测形状之间的相似位置和曲率模式，以此增强和细化重建结果。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;多视图深度图渲染&lt;/h4&gt;：&lt;br&gt;   - 将重建的完整形状渲染为多视图深度图，并设计对抗学习模块，从类别特定的单视图深度图像中学习目标形状的几何特性。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;密度感知半径估计算法&lt;/h4&gt;：&lt;br&gt;   - 为实现各向异性渲染，设计了一种密度感知半径估计算法，以提高渲染图像的质量。&lt;br&gt;&lt;br&gt;8. &lt;h4&gt;实验结果&lt;/h4&gt;：&lt;br&gt;   - MAL-SPC在与当前最先进方法的比较中，获得了最佳结果。&lt;br&gt;&lt;br&gt;9. &lt;h4&gt;代码发布&lt;/h4&gt;：&lt;br&gt;   - 相关源代码将公开发布在 [GitHub](https://github.com/ltwu6/malspc)。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; https://github.com/ltwu6/malspc&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In real-world scenarios, scanned point clouds are often incomplete due to
occlusion issues. The task of self-supervised point cloud completion involves
reconstructing missing regions of these incomplete objects without the
supervision of complete ground truth. Current self-supervised methods either
rely on multiple views of partial observations for supervision or overlook the
intrinsic geometric similarity that can be identified and utilized from the
given partial point clouds. In this paper, we propose MAL-SPC, a framework that
effectively leverages both object-level and category-specific geometric
similarities to complete missing structures. Our MAL-SPC does not require any
3D complete supervision and only necessitates a single partial point cloud for
each object. Specifically, we first introduce a Pattern Retrieval Network to
retrieve similar position and curvature patterns between the partial input and
the predicted shape, then leverage these similarities to densify and refine the
reconstructed results. Additionally, we render the reconstructed complete shape
into multi-view depth maps and design an adversarial learning module to learn
the geometry of the target shape from category-specific single-view depth
images. To achieve anisotropic rendering, we design a density-aware radius
estimation algorithm to improve the quality of the rendered images. Our MAL-SPC
yields the best results compared to current state-of-the-art methods.We will
make the source code publicly available at \url{https://github.com/ltwu6/malspc</description>
      <guid isPermaLink="false">2407.09786v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>Online Nonconvex Bilevel Optimization with Bregman Divergences</title>
      <link>http://arxiv.org/abs/2409.10470v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Jason Bohne', 'David Rosenberg', 'Gary Kazantsev', 'Pawel Polak']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-16&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 双层优化方法在机器学习中变得越来越重要，特别是在超参数优化和元学习等任务中。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;在线双层优化&lt;/h4&gt;：&lt;br&gt;   - 与离线设置相比，在线双层优化（OBO）提供了一个更动态的框架，能够处理时变函数和顺序到达的数据。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;研究问题&lt;/h4&gt;：&lt;br&gt;   - 本研究针对在线非凸-强凸双层优化问题。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;新方法介绍&lt;/h4&gt;：&lt;br&gt;   - 在确定性设置中，提出了一种新颖的在线Bregman双层优化器（OBBO），利用自适应Bregman散度。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;性能提升&lt;/h4&gt;：&lt;br&gt;   - 证明OBBO通过新颖的超梯度误差分解方法改善了已知的双层局部遗憾的次线性速率，适应了问题的底层几何特性。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;随机上下文中的方法&lt;/h4&gt;：&lt;br&gt;   - 在随机上下文中，首次引入随机在线双层优化器（SOBBO），采用窗口平均方法更新外层变量，利用最近超梯度的加权平均进行随机近似。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;效果与优势&lt;/h4&gt;：&lt;br&gt;   - 该方法不仅实现了双层局部遗憾的次线性速率，还作为有效的方差减少策略，消除了在每个时间步需要额外随机梯度样本的需求。&lt;br&gt;&lt;br&gt;8. &lt;h4&gt;实验结果&lt;/h4&gt;：&lt;br&gt;   - 在在线超参数优化和在线元学习方面的实验表明，与现有的在线和离线双层基准相比，所提出的基于Bregman的算法在性能、效率和适应性上具有显著优势。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Bilevel optimization methods are increasingly relevant within machine
learning, especially for tasks such as hyperparameter optimization and
meta-learning. Compared to the offline setting, online bilevel optimization
(OBO) offers a more dynamic framework by accommodating time-varying functions
and sequentially arriving data. This study addresses the online
nonconvex-strongly convex bilevel optimization problem. In deterministic
settings, we introduce a novel online Bregman bilevel optimizer (OBBO) that
utilizes adaptive Bregman divergences. We demonstrate that OBBO enhances the
known sublinear rates for bilevel local regret through a novel hypergradient
error decomposition that adapts to the underlying geometry of the problem. In
stochastic contexts, we introduce the first stochastic online bilevel optimizer
(SOBBO), which employs a window averaging method for updating outer-level
variables using a weighted average of recent stochastic approximations of
hypergradients. This approach not only achieves sublinear rates of bilevel
local regret but also serves as an effective variance reduction strategy,
obviating the need for additional stochastic gradient samples at each timestep.
Experiments on online hyperparameter optimization and online meta-learning
highlight the superior performance, efficiency, and adaptability of our
Bregman-based algorithms compared to established online and offline bilevel
benchmarks.</description>
      <guid isPermaLink="false">2409.10470v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>Speaker-IPL: Unsupervised Learning of Speaker Characteristics with i-Vector based Pseudo-Labels</title>
      <link>http://arxiv.org/abs/2409.10791v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Zakaria Aldeneh', 'Takuya Higuchi', 'Jee-weon Jung', 'Li-Wei Chen', 'Stephen Shum', 'Ahmed Hussen Abdelaziz', 'Shinji Watanabe', 'Tatiana Likhomanenko', 'Barry-John Theobald']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-16&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; Submitted to ICASSP 2025&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究主题&lt;/h4&gt;：&lt;br&gt;   - 迭代自我训练（IPL）是一种利用当前迭代改进模型为下一迭代提供伪标签的方法，已被证明能有效提升说话人表示的质量。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;背景&lt;/h4&gt;：&lt;br&gt;   - 近期在无监督说话人识别中的IPL应用起始于从复杂的自监督方法（如DINO）提取的表示。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;训练挑战&lt;/h4&gt;：&lt;br&gt;   - 训练强大的自监督模型并不简单，涉及超参数调优，且可能无法对域外数据进行良好的泛化。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;研究贡献&lt;/h4&gt;：&lt;br&gt;   - 本研究表明，简单且成熟的i-vector生成模型足以启动无监督学习的IPL过程，以学习说话人表示。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;组件影响分析&lt;/h4&gt;：&lt;br&gt;   - 系统研究了影响IPL过程的其他组件，包括初始模型、编码器、数据增强、聚类数量和聚类算法。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;实验发现&lt;/h4&gt;：&lt;br&gt;   - 即使使用简单且显著较弱的初始模型（i-vector），IPL仍能实现与最先进方法相当的说话人验证性能。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Iterative self-training, or iterative pseudo-labeling (IPL)--using an
improved model from the current iteration to provide pseudo-labels for the next
iteration--has proven to be a powerful approach to enhance the quality of
speaker representations. Recent applications of IPL in unsupervised speaker
recognition start with representations extracted from very elaborate
self-supervised methods (e.g., DINO). However, training such strong
self-supervised models is not straightforward (they require hyper-parameters
tuning and may not generalize to out-of-domain data) and, moreover, may not be
needed at all. To this end, we show the simple, well-studied, and established
i-vector generative model is enough to bootstrap the IPL process for
unsupervised learning of speaker representations. We also systematically study
the impact of other components on the IPL process, which includes the initial
model, the encoder, augmentations, the number of clusters, and the clustering
algorithm. Remarkably, we find that even with a simple and significantly weaker
initial model like i-vector, IPL can still achieve speaker verification
performance that rivals state-of-the-art methods.</description>
      <guid isPermaLink="false">2409.10791v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>SRIF: Semantic Shape Registration Empowered by Diffusion-based Image Morphing and Flow Estimation</title>
      <link>http://arxiv.org/abs/2409.11682v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Mingze Sun', 'Chen Guo', 'Puhua Jiang', 'Shiwei Mao', 'Yurun Chen', 'Ruqi Huang']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-18&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究目标&lt;/h4&gt;：&lt;br&gt;   - 提出SRIF，一种基于扩散图像变形和流估计的新颖语义形状配准框架。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;方法概述&lt;/h4&gt;：&lt;br&gt;   - 对于一对外部对齐的形状，首先从多个视角渲染它们，然后利用基于扩散模型的图像插值框架生成它们之间的中间图像序列。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;动态3D高斯点云重建&lt;/h4&gt;：&lt;br&gt;   - 将生成的中间图像输入到动态3D高斯点云喷射框架中，进行重建和后处理，以符合图像变形处理。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;注册模块&lt;/h4&gt;：&lt;br&gt;   - 提出一种新的注册模块，用于估计连续归一化流，使源形状一致地变形为目标形状，使用中间点云作为弱指导。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;关键见解&lt;/h4&gt;：&lt;br&gt;   - 利用大型视觉模型（LVMs）关联形状，从而获取比传统特征提取和对齐更丰富的语义信息。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;结果&lt;/h4&gt;：&lt;br&gt;   - SRIF在处理挑战性形状对时实现高质量的密集对应关系，并在两者之间提供平滑且语义上有意义的插值。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;实证支持&lt;/h4&gt;：&lt;br&gt;   - 实验结果证实了该方法的有效性和优越性，以及特定设计选择的合理性。&lt;br&gt;&lt;br&gt;8. &lt;h4&gt;代码发布&lt;/h4&gt;：&lt;br&gt;   - 相关代码已在[GitHub](https://github.com/rqhuang88/SRIF)上公开。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we propose SRIF, a novel Semantic shape Registration framework
based on diffusion-based Image morphing and Flow estimation. More concretely,
given a pair of extrinsically aligned shapes, we first render them from
multi-views, and then utilize an image interpolation framework based on
diffusion models to generate sequences of intermediate images between them. The
images are later fed into a dynamic 3D Gaussian splatting framework, with which
we reconstruct and post-process for intermediate point clouds respecting the
image morphing processing. In the end, tailored for the above, we propose a
novel registration module to estimate continuous normalizing flow, which
deforms source shape consistently towards the target, with intermediate point
clouds as weak guidance. Our key insight is to leverage large vision models
(LVMs) to associate shapes and therefore obtain much richer semantic
information on the relationship between shapes than the ad-hoc feature
extraction and alignment. As a consequence, SRIF achieves high-quality dense
correspondences on challenging shape pairs, but also delivers smooth,
semantically meaningful interpolation in between. Empirical evidence justifies
the effectiveness and superiority of our method as well as specific design
choices. The code is released at https://github.com/rqhuang88/SRIF.</description>
      <guid isPermaLink="false">2409.11682v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>Exploring Active Learning in Meta-Learning: Enhancing Context Set Labeling</title>
      <link>http://arxiv.org/abs/2311.02879v3</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Wonho Bae', 'Jing Wang', 'Danica J. Sutherland']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2023-11-06&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; Accepted to ECCV2024&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 大多数元学习方法假设在测试时使用的（非常小的）上下文集是被动提供的。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;主动选择的潜力&lt;/h4&gt;：&lt;br&gt;   - 在某些情况下，可以主动选择标记哪些点，这种选择的潜在收益显著，但与典型的主动学习设置有重大差异。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;主动元学习的框架&lt;/h4&gt;：&lt;br&gt;   - 阐明了主动元学习如何用于标记上下文集，具体取决于元学习过程中的哪些部分使用主动学习。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;提出的算法&lt;/h4&gt;：&lt;br&gt;   - 提出了一种基于拟合高斯混合的自然算法，用于选择标记点；该算法虽然简单，但具有理论动机。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;实验结果&lt;/h4&gt;：&lt;br&gt;   - 在与多种元学习算法结合使用时，该算法在多个基准数据集上超越了最先进的主动学习方法。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Most meta-learning methods assume that the (very small) context set used to
establish a new task at test time is passively provided. In some settings,
however, it is feasible to actively select which points to label; the potential
gain from a careful choice is substantial, but the setting requires major
differences from typical active learning setups. We clarify the ways in which
active meta-learning can be used to label a context set, depending on which
parts of the meta-learning process use active learning. Within this framework,
we propose a natural algorithm based on fitting Gaussian mixtures for selecting
which points to label; though simple, the algorithm also has theoretical
motivation. The proposed algorithm outperforms state-of-the-art active learning
methods when used with various meta-learning algorithms across several
benchmark datasets.</description>
      <guid isPermaLink="false">2311.02879v3</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>VertiEncoder: Self-Supervised Kinodynamic Representation Learning on Vertically Challenging Terrain</title>
      <link>http://arxiv.org/abs/2409.11570v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Mohammad Nazeri', 'Aniket Datar', 'Anuj Pokhrel', 'Chenhui Pan', 'Garrett Warnell', 'Xuesu Xiao']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-17&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; 8 pages. Code: https://github.com/mhnazeri/VertiEncoder&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究目标&lt;/h4&gt;：&lt;br&gt;   - 提出VertiEncoder，一种自监督表示学习方法，旨在提高机器人在垂直挑战地形上的移动能力。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;多任务处理&lt;/h4&gt;：&lt;br&gt;   - VertiEncoder采用相同的预训练过程，可处理四种不同的下游任务：&lt;br&gt;     - 前向运动动力学学习&lt;br&gt;     - 逆向运动动力学学习&lt;br&gt;     - 行为克隆&lt;br&gt;     - 块重建&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;技术框架&lt;/h4&gt;：&lt;br&gt;   - 使用TransformerEncoder，通过随机遮罩和下一个块重建学习周围环境的局部上下文。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;性能优势&lt;/h4&gt;：&lt;br&gt;   - VertiEncoder在四个不同任务上的表现优于专门的端到端模型，参数减少了77%。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;与现有方法比较&lt;/h4&gt;：&lt;br&gt;   - 在真实机器人部署中，VertiEncoder在运动动力学建模和规划方面与最先进的方法表现相当。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;研究意义&lt;/h4&gt;：&lt;br&gt;   - 结果表明，VertiEncoder有效缓解了过拟合问题，并促进了在不同环境上下文和下游车辆运动动力学任务中的更强鲁棒性和泛化能力。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present VertiEncoder, a self-supervised representation learning approach
for robot mobility on vertically challenging terrain. Using the same
pre-training process, VertiEncoder can handle four different downstream tasks,
including forward kinodynamics learning, inverse kinodynamics learning,
behavior cloning, and patch reconstruction with a single representation.
VertiEncoder uses a TransformerEncoder to learn the local context of its
surroundings by random masking and next patch reconstruction. We show that
VertiEncoder achieves better performance across all four different tasks
compared to specialized End-to-End models with 77% fewer parameters. We also
show VertiEncoder's comparable performance against state-of-the-art kinodynamic
modeling and planning approaches in real-world robot deployment. These results
underscore the efficacy of VertiEncoder in mitigating overfitting and fostering
more robust generalization across diverse environmental contexts and downstream
vehicle kinodynamic tasks.</description>
      <guid isPermaLink="false">2409.11570v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>Exploring ChatGPT-based Augmentation Strategies for Contrastive Aspect-based Sentiment Analysis</title>
      <link>http://arxiv.org/abs/2409.11218v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Lingling Xu', 'Haoran Xie', 'S. Joe Qin', 'Fu Lee Wang', 'Xiaohui Tao']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-17&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; 8 pages, 3 figures&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究主题&lt;/h4&gt;：&lt;br&gt;   - 方面基础情感分析（ABSA）旨在识别句子中针对特定方面术语的情感，帮助揭示对产品、服务或话题的细致看法和态度。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;挑战&lt;/h4&gt;：&lt;br&gt;   - 标注数据的稀缺性对训练高质量模型构成显著挑战。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;解决方案&lt;/h4&gt;：&lt;br&gt;   - 探索使用ChatGPT这一表现良好的大型语言模型（LLM）进行数据增强，以提升对方面术语的情感分类性能。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;数据增强策略&lt;/h4&gt;：&lt;br&gt;   - 研究了三种基于ChatGPT的数据增强策略：&lt;br&gt;     - &lt;h4&gt;上下文聚焦数据增强&lt;/h4&gt;：改变句子中上下文词的表达，而保持方面术语不变。&lt;br&gt;     - &lt;h4&gt;方面聚焦数据增强&lt;/h4&gt;：改变方面术语，保持上下文词不变。&lt;br&gt;     - &lt;h4&gt;上下文-方面数据增强&lt;/h4&gt;：结合上述两种增强策略，生成增强样本。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;对比学习&lt;/h4&gt;：&lt;br&gt;   - 在ABSA任务中引入对比学习，以进一步提升性能。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;实验结果&lt;/h4&gt;：&lt;br&gt;   - 大量实验表明，所有三种数据增强技术均提升了性能，其中上下文-方面数据增强策略表现最佳，超越了基线模型的性能。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Aspect-based sentiment analysis (ABSA) involves identifying sentiment towards
specific aspect terms in a sentence and allows us to uncover nuanced
perspectives and attitudes on particular aspects of a product, service, or
topic. However, the scarcity of labeled data poses a significant challenge to
training high-quality models. To address this issue, we explore the potential
of data augmentation using ChatGPT, a well-performing large language model
(LLM), to enhance the sentiment classification performance towards aspect
terms. Specifically, we explore three data augmentation strategies based on
ChatGPT: context-focused, aspect-focused, and context-aspect data augmentation
techniques. Context-focused data augmentation focuses on changing the word
expression of context words in the sentence while keeping aspect terms
unchanged. In contrast, aspect-focused data augmentation aims to change aspect
terms but keep context words unchanged. Context-Aspect data augmentation
integrates the above two data augmentations to generate augmented samples.
Furthermore, we incorporate contrastive learning into the ABSA tasks to improve
performance. Extensive experiments show that all three data augmentation
techniques lead to performance improvements, with the context-aspect data
augmentation strategy performing best and surpassing the performance of the
baseline models.</description>
      <guid isPermaLink="false">2409.11218v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>Edge-Based Graph Component Pooling</title>
      <link>http://arxiv.org/abs/2409.11856v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['T. Snelleman', 'B. M. Renting', 'H. H. Hoos', 'J. N. van Rijn']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-18&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; 15 pages, presented at 21st International Workshop on Mining and
  Learning with Graphs, AstraZenica Bio &amp; Healthcare award Paper, ECML PKDD
  2024 Vilnius&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 图结构数据广泛存在于多个研究领域，如化学和社会学，包含的关系信息可用于通过几何深度学习统计建模图属性。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;图神经网络&lt;/h4&gt;：&lt;br&gt;   - 图神经网络采用消息传递层等技术，通过图传播局部特征。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;计算挑战&lt;/h4&gt;：&lt;br&gt;   - 在处理大型稀疏图时，消息传递层可能导致计算开销较大。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;图池化操作&lt;/h4&gt;：&lt;br&gt;   - 图池化操作可以通过移除或合并节点来降低计算成本，但移除节点会导致数据损失，而合并节点往往计算复杂。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;提出的解决方案&lt;/h4&gt;：&lt;br&gt;   - 提出了一个新的池化操作，旨在合并节点以避免数据损失，同时保持概念简单和计算高效。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;实验结果&lt;/h4&gt;：&lt;br&gt;   - 实证表明，所提池化操作在四个流行基准数据集上显著优于边池（edge pool），并将时间复杂度和可训练参数数量平均降低70.6%。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;与其他方法比较&lt;/h4&gt;：&lt;br&gt;   - 相较于另一种强大的方法——图同构网络（Graph Isomorphic Network），在两个流行基准数据集上表现更佳，同时将可学习参数数量平均降低60.9%。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph-structured data naturally occurs in many research fields, such as
chemistry and sociology. The relational information contained therein can be
leveraged to statistically model graph properties through geometrical deep
learning. Graph neural networks employ techniques, such as message-passing
layers, to propagate local features through a graph. However, message-passing
layers can be computationally expensive when dealing with large and sparse
graphs. Graph pooling operators offer the possibility of removing or merging
nodes in such graphs, thus lowering computational costs. However, pooling
operators that remove nodes cause data loss, and pooling operators that merge
nodes are often computationally expensive. We propose a pooling operator that
merges nodes so as not to cause data loss but is also conceptually simple and
computationally inexpensive. We empirically demonstrate that the proposed
pooling operator performs statistically significantly better than edge pool on
four popular benchmark datasets while reducing time complexity and the number
of trainable parameters by 70.6% on average. Compared to another maximally
powerful method named Graph Isomporhic Network, we show that we outperform them
on two popular benchmark datasets while reducing the number of learnable
parameters on average by 60.9%.</description>
      <guid isPermaLink="false">2409.11856v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>GBOT: Graph-Based 3D Object Tracking for Augmented Reality-Assisted Assembly Guidance</title>
      <link>http://arxiv.org/abs/2402.07677v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Shiyu Li', 'Hannah Schieber', 'Niklas Corell', 'Bernhard Egger', 'Julian Kreimeier', 'Daniel Roth']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-02-12&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; 9 pages&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 组装零件的指导在增强现实（AR）领域具有前景，尤其在医疗和工业等时间敏感的环境中。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;需求&lt;/h4&gt;：&lt;br&gt;   - 增强现实的组装指导需要实时获取目标物体的6D位姿，确保无标记的连续跟踪。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;挑战&lt;/h4&gt;：&lt;br&gt;   - 用户手部或其他物体的遮挡，以及不同组装状态的复杂性，使得无标记的多物体跟踪变得困难。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;解决方案&lt;/h4&gt;：&lt;br&gt;   - 提出了基于图的物体跟踪（GBOT）方法，采用单视角RGB-D跟踪技术。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;跟踪过程&lt;/h4&gt;：&lt;br&gt;   - 通过6D位姿估计初始化实时无标记的多物体跟踪，并更新基于图的组装位姿。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;多状态组装图&lt;/h4&gt;：&lt;br&gt;   - 通过利用各个组装部件的相对位姿，更新多状态组装图，确保在不同组装状态下的有效跟踪。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;图的链接&lt;/h4&gt;：&lt;br&gt;   - 将个体对象连接到该图中，增强组装过程中的物体跟踪鲁棒性。&lt;br&gt;&lt;br&gt;8. &lt;h4&gt;评估方法&lt;/h4&gt;：&lt;br&gt;   - 引入了一种合成数据集，包含公开的可3D打印组装资产，作为未来工作的基准。&lt;br&gt;&lt;br&gt;9. &lt;h4&gt;实验结果&lt;/h4&gt;：&lt;br&gt;   - 在合成数据和真实测试数据中的定量实验和定性研究表明，GBOT在实现上下文感知的增强现实组装指导方面优于现有方法。&lt;br&gt;&lt;br&gt;10. &lt;h4&gt;数据和代码公开&lt;/h4&gt;：&lt;br&gt;    - 数据集和代码将公开发布，以供进一步研究使用。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; https://github.com/roth-hex-lab/gbot&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Guidance for assemblable parts is a promising field for augmented reality.
Augmented reality assembly guidance requires 6D object poses of target objects
in real time. Especially in time-critical medical or industrial settings,
continuous and markerless tracking of individual parts is essential to
visualize instructions superimposed on or next to the target object parts. In
this regard, occlusions by the user's hand or other objects and the complexity
of different assembly states complicate robust and real-time markerless
multi-object tracking. To address this problem, we present Graph-based Object
Tracking (GBOT), a novel graph-based single-view RGB-D tracking approach. The
real-time markerless multi-object tracking is initialized via 6D pose
estimation and updates the graph-based assembly poses. The tracking through
various assembly states is achieved by our novel multi-state assembly graph. We
update the multi-state assembly graph by utilizing the relative poses of the
individual assembly parts. Linking the individual objects in this graph enables
more robust object tracking during the assembly process. For evaluation, we
introduce a synthetic dataset of publicly available and 3D printable assembly
assets as a benchmark for future work. Quantitative experiments in synthetic
data and further qualitative study in real test data show that GBOT can
outperform existing work towards enabling context-aware augmented reality
assembly guidance. Dataset and code will be made publically available.</description>
      <guid isPermaLink="false">2402.07677v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>Efficient and Scalable Point Cloud Generation with Sparse Point-Voxel Diffusion Models</title>
      <link>http://arxiv.org/abs/2408.06145v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Ioannis Romanelis', 'Vlassios Fotis', 'Athanasios Kalogeras', 'Christos Alexakos', 'Konstantinos Moustakas', 'Adrian Munteanu']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-08-12&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究目标&lt;/h4&gt;：&lt;br&gt;   - 提出一种新型的点云U-Net扩散架构，用于3D生成建模，旨在生成高质量和多样化的3D形状，同时保持快速生成时间。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;网络架构&lt;/h4&gt;：&lt;br&gt;   - 采用双分支架构，结合高分辨率的点表示和稀疏体素的计算效率。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;性能对比&lt;/h4&gt;：&lt;br&gt;   - 最快的变体在无条件形状生成方面超越了所有非扩散生成方法，这是评估点云生成模型的主要基准。&lt;br&gt;   - 最大模型在扩散方法中达到了最新的 state-of-the-art 结果，运行时间约为之前最先进的PVD模型的70%。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;扩展评估&lt;/h4&gt;：&lt;br&gt;   - 除了无条件生成外，还进行了广泛的评估，包括在ShapeNet的所有类别上的条件生成，展示了模型在更大数据集上的可扩展性。&lt;br&gt;   - 实现了隐式生成，允许网络在更少的时间步长下产生高质量点云，进一步降低生成时间。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;额外任务评估&lt;/h4&gt;：&lt;br&gt;   - 在点云补全和超分辨率任务中的表现也进行了评估，模型在所有任务中表现优异。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;结论&lt;/h4&gt;：&lt;br&gt;   - 确立了该模型作为点云生成建模领域的先进扩散U-Net。&lt;br&gt;   - 代码可在[GitHub](https://github.com/JohnRomanelis/SPVD.git)上公开获取。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; https://github.com/JohnRomanelis/SPVD_Lightning&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a novel point cloud U-Net diffusion architecture for 3D generative
modeling capable of generating high-quality and diverse 3D shapes while
maintaining fast generation times. Our network employs a dual-branch
architecture, combining the high-resolution representations of points with the
computational efficiency of sparse voxels. Our fastest variant outperforms all
non-diffusion generative approaches on unconditional shape generation, the most
popular benchmark for evaluating point cloud generative models, while our
largest model achieves state-of-the-art results among diffusion methods, with a
runtime approximately 70% of the previously state-of-the-art PVD. Beyond
unconditional generation, we perform extensive evaluations, including
conditional generation on all categories of ShapeNet, demonstrating the
scalability of our model to larger datasets, and implicit generation which
allows our network to produce high quality point clouds on fewer timesteps,
further decreasing the generation time. Finally, we evaluate the architecture's
performance in point cloud completion and super-resolution. Our model excels in
all tasks, establishing it as a state-of-the-art diffusion U-Net for point
cloud generative modeling. The code is publicly available at
https://github.com/JohnRomanelis/SPVD.git.</description>
      <guid isPermaLink="false">2408.06145v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>Geometry Aware Meta-Learning Neural Network for Joint Phase and Precoder Optimization in RIS</title>
      <link>http://arxiv.org/abs/2409.11270v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Dahlia Devapriya', 'Sheetal Kalyani']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-17&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 在可重构智能表面（RIS）辅助系统中，基站的预编码矩阵和RIS元素的相位偏移的联合优化涉及复杂性较高。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;提出的方法&lt;/h4&gt;：&lt;br&gt;   - 本文提出了一种复值的、几何感知的元学习神经网络，旨在最大化多用户多输入单输出系统中的加权总速率。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;几何特性利用&lt;/h4&gt;：&lt;br&gt;   - 该方法利用复数圆几何进行相位偏移和球面几何进行预编码，从而在黎曼流形上进行优化，实现更快的收敛。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;网络架构&lt;/h4&gt;：&lt;br&gt;   - 使用复值神经网络来处理相位偏移，并采用欧拉启发式更新方法来更新预编码网络。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;性能优势&lt;/h4&gt;：&lt;br&gt;   - 本方法优于现有基于神经网络的算法，具体表现为：&lt;br&gt;     - 更高的加权总速率&lt;br&gt;     - 更低的能耗&lt;br&gt;     - 收敛速度显著更快&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;具体改进&lt;/h4&gt;：&lt;br&gt;   - 相较于现有工作，收敛速度提高近100个迭代周期，加权总速率提升0.7 bps，功率增益达到1.8 dBm。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In reconfigurable intelligent surface (RIS) aided systems, the joint
optimization of the precoder matrix at the base station and the phase shifts of
the RIS elements involves significant complexity. In this paper, we propose a
complex-valued, geometry aware meta-learning neural network that maximizes the
weighted sum rate in a multi-user multiple input single output system. By
leveraging the complex circle geometry for phase shifts and spherical geometry
for the precoder, the optimization occurs on Riemannian manifolds, leading to
faster convergence. We use a complex-valued neural network for phase shifts and
an Euler inspired update for the precoder network. Our approach outperforms
existing neural network-based algorithms, offering higher weighted sum rates,
lower power consumption, and significantly faster convergence. Specifically, it
converges faster by nearly 100 epochs, with a 0.7 bps improvement in weighted
sum rate and a 1.8 dBm power gain when compared with existing work.</description>
      <guid isPermaLink="false">2409.11270v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>CLIP Adaptation by Intra-modal Overlap Reduction</title>
      <link>http://arxiv.org/abs/2409.11338v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Alexey Kravets', 'Vinay Namboodiri']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-17&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; BMVC 2024, Oral&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 许多方法已被提出，以适应预训练的基础CLIP模型进行少量样本分类。由于CLIP在大规模语料库上训练，具有良好的泛化能力。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;研究重点&lt;/h4&gt;：&lt;br&gt;   - 本文分析了图像空间中的模态内部重叠，特别是嵌入表示方面的重叠情况。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;主要发现&lt;/h4&gt;：&lt;br&gt;   - 由于对比学习的影响，CLIP模型的嵌入在图像空间中，配对与未配对样本之间存在高余弦相似度分布重叠，这影响了依赖图像空间相似度进行预测的少量样本无训练分类方法的性能。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;问题解决方案&lt;/h4&gt;：&lt;br&gt;   - 为了解决模态内部重叠问题，提出在谷歌开放图像数据集上训练一个轻量级适配器，证明这能提高少量样本无训练分类的准确性。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;验证方法&lt;/h4&gt;：&lt;br&gt;   - 通过广泛的实证分析验证了所提出的方法。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;性能提升&lt;/h4&gt;：&lt;br&gt;   - 减少模态内部重叠带来了以下改进：&lt;br&gt;     - a) 在多个标准数据集上提升了性能。&lt;br&gt;     - b) 增强了对分布变化的鲁棒性。&lt;br&gt;     - c) 提高了特征方差，使得特征在下游任务中更具辨别力。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Numerous methods have been proposed to adapt a pre-trained foundational CLIP
model for few-shot classification. As CLIP is trained on a large corpus, it
generalises well through adaptation to few-shot classification. In this work,
we analyse the intra-modal overlap in image space in terms of embedding
representation. Our analysis shows that, due to contrastive learning,
embeddings from CLIP model exhibit high cosine similarity distribution overlap
in the image space between paired and unpaired examples affecting the
performance of few-shot training-free classification methods which rely on
similarity in the image space for their predictions. To tackle intra-modal
overlap we propose to train a lightweight adapter on a generic set of samples
from the Google Open Images dataset demonstrating that this improves accuracy
for few-shot training-free classification. We validate our contribution through
extensive empirical analysis and demonstrate that reducing the intra-modal
overlap leads to a) improved performance on a number of standard datasets, b)
increased robustness to distribution shift and c) higher feature variance
rendering the features more discriminative for downstream tasks.</description>
      <guid isPermaLink="false">2409.11338v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>Towards Modality-agnostic Label-efficient Segmentation with Entropy-Regularized Distribution Alignment</title>
      <link>http://arxiv.org/abs/2408.16520v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Liyao Tang', 'Zhe Chen', 'Shanshan Zhao', 'Chaoyue Wang', 'Dacheng Tao']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-08-29&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; Extended version of arXiv:2305.15832; Code at
  https://github.com/LiyaoTang/ERDA&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 标签高效分割旨在使用稀疏和有限的真实标签进行有效的输入数据分割，尤其在3D点云分割中尤为重要，因为密集标注点云的难度较大。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;应用范围&lt;/h4&gt;：&lt;br&gt;   - 此方法同样适用于成本效益高的2D图像分割。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;伪标签的使用&lt;/h4&gt;：&lt;br&gt;   - 最近，伪标签被广泛应用于在有限真实标签下促进训练，并在2D和3D分割中取得了显著进展。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;现有问题&lt;/h4&gt;：&lt;br&gt;   - 现有的伪标签方法可能受到未标记数据中的噪声和变动的影响，导致生成的伪标签与当前模型预测之间存在显著差异。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;模型学习影响&lt;/h4&gt;：&lt;br&gt;   - 这种差异会进一步混淆并影响模型的学习过程，这在2D和3D模态的标签高效学习中是一个共同问题。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;新策略提出&lt;/h4&gt;：&lt;br&gt;   - 为了解决上述问题，提出了一种新颖的学习策略来规范化生成的伪标签，从而有效缩小伪标签与模型预测之间的差距。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;具体方法&lt;/h4&gt;：&lt;br&gt;   - 引入了熵正则化损失和分布对齐损失，形成ERDA学习策略。&lt;br&gt;&lt;br&gt;8. &lt;h4&gt;损失函数优化&lt;/h4&gt;：&lt;br&gt;   - 通过使用KL距离来制定分布对齐损失，ERDA简化为一个看似简单的交叉熵损失，能够同时优化伪标签生成模块和分割模型。&lt;br&gt;&lt;br&gt;9. &lt;h4&gt;创新点&lt;/h4&gt;：&lt;br&gt;   - 在伪标签生成方面进行了创新，使得ERDA在2D和3D数据模态下都能持续有效。&lt;br&gt;&lt;br&gt;10. &lt;h4&gt;性能表现&lt;/h4&gt;：&lt;br&gt;    - 该方法因其简单性和更具模态无关性的伪标签生成，已显示出在充分利用所有未标记数据进行训练方面的卓越性能。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; https://github.com/LiyaoTang/ERDA&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Label-efficient segmentation aims to perform effective segmentation on input
data using only sparse and limited ground-truth labels for training. This topic
is widely studied in 3D point cloud segmentation due to the difficulty of
annotating point clouds densely, while it is also essential for cost-effective
segmentation on 2D images. Until recently, pseudo-labels have been widely
employed to facilitate training with limited ground-truth labels, and promising
progress has been witnessed in both the 2D and 3D segmentation. However,
existing pseudo-labeling approaches could suffer heavily from the noises and
variations in unlabelled data, which would result in significant discrepancies
between generated pseudo-labels and current model predictions during training.
We analyze that this can further confuse and affect the model learning process,
which shows to be a shared problem in label-efficient learning across both 2D
and 3D modalities. To address this issue, we propose a novel learning strategy
to regularize the pseudo-labels generated for training, thus effectively
narrowing the gaps between pseudo-labels and model predictions. More
specifically, our method introduces an Entropy Regularization loss and a
Distribution Alignment loss for label-efficient learning, resulting in an ERDA
learning strategy. Interestingly, by using KL distance to formulate the
distribution alignment loss, ERDA reduces to a deceptively simple
cross-entropy-based loss which optimizes both the pseudo-label generation
module and the segmentation model simultaneously. In addition, we innovate in
the pseudo-label generation to make our ERDA consistently effective across both
2D and 3D data modalities for segmentation. Enjoying simplicity and more
modality-agnostic pseudo-label generation, our method has shown outstanding
performance in fully utilizing all unlabeled data points for training across
...</description>
      <guid isPermaLink="false">2408.16520v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>ChefFusion: Multimodal Foundation Model Integrating Recipe and Food Image Generation</title>
      <link>http://arxiv.org/abs/2409.12010v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Peiyu Li', 'Xiaobao Huang', 'Yijun Tian', 'Nitesh V. Chawla']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-18&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 在食品计算领域已进行了大量研究，但大多数研究仅聚焦于单一任务，如：&lt;br&gt;     - t2t（根据食品标题和成分生成指令）&lt;br&gt;     - i2t（根据食品图像生成食谱）&lt;br&gt;     - t2i（根据食谱生成食品图像）&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;研究缺口&lt;/h4&gt;：&lt;br&gt;   - 现有方法未能同时整合所有模态，存在多模态整合的空白。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;模型创新&lt;/h4&gt;：&lt;br&gt;   - 本文提出了一种新颖的食品计算基础模型，真正实现了多模态整合，支持多种任务，包括：&lt;br&gt;     - t2t&lt;br&gt;     - t2i&lt;br&gt;     - i2t&lt;br&gt;     - it2t&lt;br&gt;     - t2ti&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;技术方法&lt;/h4&gt;：&lt;br&gt;   - 该模型利用大型语言模型（LLMs）以及预训练的图像编码器和解码器模型，能够执行多种与食品计算相关的任务，如：&lt;br&gt;     - 食品理解&lt;br&gt;     - 食品识别&lt;br&gt;     - 食谱生成&lt;br&gt;     - 食品图像生成&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;性能优势&lt;/h4&gt;：&lt;br&gt;   - 与之前的模型相比，基础模型展示了显著更广泛的能力，特别是在食品图像生成和食谱生成任务中表现突出。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;开源资源&lt;/h4&gt;：&lt;br&gt;   - 项目ChefFusion已在GitHub上开源，供公众使用。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; https://github.com/peiyu-georgia-li/cheffusion-multimodal-foundation-model-integrating-recipe-and-food-image-generation&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Significant work has been conducted in the domain of food computing, yet
these studies typically focus on single tasks such as t2t (instruction
generation from food titles and ingredients), i2t (recipe generation from food
images), or t2i (food image generation from recipes). None of these approaches
integrate all modalities simultaneously. To address this gap, we introduce a
novel food computing foundation model that achieves true multimodality,
encompassing tasks such as t2t, t2i, i2t, it2t, and t2ti. By leveraging large
language models (LLMs) and pre-trained image encoder and decoder models, our
model can perform a diverse array of food computing-related tasks, including
food understanding, food recognition, recipe generation, and food image
generation. Compared to previous models, our foundation model demonstrates a
significantly broader range of capabilities and exhibits superior performance,
particularly in food image generation and recipe generation tasks. We
open-sourced ChefFusion at GitHub.</description>
      <guid isPermaLink="false">2409.12010v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>Multi-Grid Graph Neural Networks with Self-Attention for Computational Mechanics</title>
      <link>http://arxiv.org/abs/2409.11899v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Paul Garnier', 'Jonathan Viquerat', 'Elie Hachem']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-18&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 有限元方法在多个学科中变得至关重要，尤其是在计算流体动力学（CFD）中，推动了对提高精度和效率的研究。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;技术转变&lt;/h4&gt;：&lt;br&gt;   - 尽管卷积神经网络（CNN）已成功地将网格映射为图像，但最近的关注点转向了利用图神经网络（GNN）进行直接网格处理。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;模型创新&lt;/h4&gt;：&lt;br&gt;   - 本文介绍了一种新模型，将自注意力机制与GNN中的消息传递相结合，实现了在著名的圆柱体流动基准测试中减少15%的均方根误差（RMSE）。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;动态网格修剪&lt;/h4&gt;：&lt;br&gt;   - 提出了基于自注意力机制的动态网格修剪技术，进一步推动了基于GNN的多网格方法，同样减少了15%的RMSE。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;自监督训练方法&lt;/h4&gt;：&lt;br&gt;   - 介绍了一种基于BERT的新自监督训练方法，导致RMSE减少25%。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;消融研究&lt;/h4&gt;：&lt;br&gt;   - 论文包含消融研究，表明所提模型在多个挑战性数据集上优于现有最先进模型，展示了与自然语言处理和图像处理领域相似的进展潜力。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;数据集贡献&lt;/h4&gt;：&lt;br&gt;   - 论文引入了一个新的数据集，其网格规模比现有数据集大至少一个数量级。&lt;br&gt;&lt;br&gt;8. &lt;h4&gt;资源获取&lt;/h4&gt;：&lt;br&gt;   - 代码和数据集将发布在[GitHub上](https://github.com/DonsetPG/multigrid-gnn)。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; https://github.com/donsetpg/multigrid-gnn&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Advancement in finite element methods have become essential in various
disciplines, and in particular for Computational Fluid Dynamics (CFD), driving
research efforts for improved precision and efficiency. While Convolutional
Neural Networks (CNNs) have found success in CFD by mapping meshes into images,
recent attention has turned to leveraging Graph Neural Networks (GNNs) for
direct mesh processing. This paper introduces a novel model merging
Self-Attention with Message Passing in GNNs, achieving a 15\% reduction in RMSE
on the well known flow past a cylinder benchmark. Furthermore, a dynamic mesh
pruning technique based on Self-Attention is proposed, that leads to a robust
GNN-based multigrid approach, also reducing RMSE by 15\%. Additionally, a new
self-supervised training method based on BERT is presented, resulting in a 25\%
RMSE reduction. The paper includes an ablation study and outperforms
state-of-the-art models on several challenging datasets, promising advancements
similar to those recently achieved in natural language and image processing.
Finally, the paper introduces a dataset with meshes larger than existing ones
by at least an order of magnitude. Code and Datasets will be released at
https://github.com/DonsetPG/multigrid-gnn.</description>
      <guid isPermaLink="false">2409.11899v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>SceneTracker: Long-term Scene Flow Estimation Network</title>
      <link>http://arxiv.org/abs/2403.19924v3</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Bo Wang', 'Jian Li', 'Yang Yu', 'Li Liu', 'Zhenping Sun', 'Dewen Hu']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-03-29&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 本研究关注场景流估计在空间域的聚焦能力与3D物体跟踪在时间域的一致性之间的互补性。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;研究目标&lt;/h4&gt;：&lt;br&gt;   - 针对一个综合性的新任务：长时间场景流估计（LSFE），旨在在线捕捉细粒度和长期的3D运动。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;方法介绍&lt;/h4&gt;：&lt;br&gt;   - 引入了SceneTracker，一个基于学习的LSFE网络，采用迭代方法来逼近最佳轨迹。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;特征构建&lt;/h4&gt;：&lt;br&gt;   - 动态索引和构建外观与深度相关特征，同时利用Transformer探索和利用轨迹内外的长程连接。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;实验结果&lt;/h4&gt;：&lt;br&gt;   - 通过详细实验，SceneTracker在处理3D空间遮挡和深度噪声干扰方面表现出色，满足LSFE任务的需求。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;数据集构建&lt;/h4&gt;：&lt;br&gt;   - 构建了第一个真实世界评估数据集LSFDriving，进一步证明SceneTracker的良好泛化能力。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;资源获取&lt;/h4&gt;：&lt;br&gt;   - SceneTracker的代码和数据可在[GitHub上获取](https://github.com/wwsource/SceneTracker)。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; https://github.com/wwsource/scenetracker&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Considering the complementarity of scene flow estimation in the spatial
domain's focusing capability and 3D object tracking in the temporal domain's
coherence, this study aims to address a comprehensive new task that can
simultaneously capture fine-grained and long-term 3D motion in an online
manner: long-term scene flow estimation (LSFE). We introduce SceneTracker, a
novel learning-based LSFE network that adopts an iterative approach to
approximate the optimal trajectory. Besides, it dynamically indexes and
constructs appearance and depth correlation features simultaneously and employs
the Transformer to explore and utilize long-range connections within and
between trajectories. With detailed experiments, SceneTracker shows superior
capabilities in handling 3D spatial occlusion and depth noise interference,
highly tailored to the LSFE task's needs. Finally, we build the first
real-world evaluation dataset, LSFDriving, further substantiating
SceneTracker's commendable generalization capacity. The code and data for
SceneTracker is available at https://github.com/wwsource/SceneTracker.</description>
      <guid isPermaLink="false">2403.19924v3</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>Unleashing the Potential of Mamba: Boosting a LiDAR 3D Sparse Detector by Using Cross-Model Knowledge Distillation</title>
      <link>http://arxiv.org/abs/2409.11018v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Rui Yu', 'Runkai Zhao', 'Jiagen Li', 'Qingsong Zhao', 'Songhao Zhu', 'HuaiCheng Yan', 'Meng Wang']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-17&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - LiDAR基础的3D物体检测器在自动驾驶和机器人导航系统中实现实时感知至关重要，需要在准确性和速度之间取得平衡。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;现有问题&lt;/h4&gt;：&lt;br&gt;   - 许多现有的LiDAR检测模型依赖复杂的特征变换和提取过程，导致实时性能差和资源消耗高，这限制了它们的实际有效性。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;提出的新框架&lt;/h4&gt;：&lt;br&gt;   - 提出了一个名为FASD的快速LiDAR 3D物体检测框架，通过自适应统一的跨模型体素特征实现异构模型蒸馏。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;目标&lt;/h4&gt;：&lt;br&gt;   - 旨在将变压器的高性能序列建模能力蒸馏到低FLOPs的Mamba模型中，通过知识转移显著提高准确性。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;关键技术&lt;/h4&gt;：&lt;br&gt;   - &lt;h4&gt;动态体素组&lt;/h4&gt;和&lt;h4&gt;自适应注意力策略&lt;/h4&gt;集成到稀疏主干网络中，创建具有规模自适应注意力的强大教师模型，以有效建模全局视觉上下文。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;知识转移&lt;/h4&gt;：&lt;br&gt;   - 通过适配器进行特征对齐，利用潜在空间特征监督和跨度头蒸馏将知识从变压器转移到Mamba，提升性能并形成高效的学生模型。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;实验结果&lt;/h4&gt;：&lt;br&gt;   - 在Waymo和nuScenes数据集上评估该框架，实现了资源消耗降低4倍，并比当前最先进的方法提高了1-2%的性能。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The LiDAR-based 3D object detector that strikes a balance between accuracy
and speed is crucial for achieving real-time perception in autonomous driving
and robotic navigation systems. To enhance the accuracy of point cloud
detection, integrating global context for visual understanding improves the
point clouds ability to grasp overall spatial information. However, many
existing LiDAR detection models depend on intricate feature transformation and
extraction processes, leading to poor real-time performance and high resource
consumption, which limits their practical effectiveness. In this work, we
propose a Faster LiDAR 3D object detection framework, called FASD, which
implements heterogeneous model distillation by adaptively uniform cross-model
voxel features. We aim to distill the transformer's capacity for
high-performance sequence modeling into Mamba models with low FLOPs, achieving
a significant improvement in accuracy through knowledge transfer. Specifically,
Dynamic Voxel Group and Adaptive Attention strategies are integrated into the
sparse backbone, creating a robust teacher model with scale-adaptive attention
for effective global visual context modeling. Following feature alignment with
the Adapter, we transfer knowledge from the Transformer to the Mamba through
latent space feature supervision and span-head distillation, resulting in
improved performance and an efficient student model. We evaluated the framework
on the Waymo and nuScenes datasets, achieving a 4x reduction in resource
consumption and a 1-2\% performance improvement over the current SoTA methods.</description>
      <guid isPermaLink="false">2409.11018v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>From Words to Poses: Enhancing Novel Object Pose Estimation with Vision Language Models</title>
      <link>http://arxiv.org/abs/2409.05413v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Tessa Pulli', 'Stefan Thalhammer', 'Simon Schwaiger', 'Markus Vincze']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-09&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 机器人在现实场景中交互日益增多，需要不断适应新情况。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;零-shot姿态估计&lt;/h4&gt;：&lt;br&gt;   - 零-shot姿态估计器可以在没有先前知识的情况下确定物体姿态，以检测和抓取新物体。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;视觉语言模型的应用&lt;/h4&gt;：&lt;br&gt;   - 最近，视觉语言模型（VLMs）在机器人应用中取得显著进展，通过建立语言输入与图像输入之间的理解。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;研究贡献&lt;/h4&gt;：&lt;br&gt;   - 提出了一种新的框架，利用语言嵌入进行可提示的零-shot 6D物体姿态估计。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;方法概述&lt;/h4&gt;：&lt;br&gt;   - 通过语言嵌入的NeRF重建的相关性图来推导物体的粗略位置，然后使用点云配准方法计算姿态估计。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;分析与适用性&lt;/h4&gt;：&lt;br&gt;   - 提供了对LERF在开放集物体姿态估计中适用性的分析，考察相关性图的激活阈值等超参数。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;零-shot能力的研究&lt;/h4&gt;：&lt;br&gt;   - 调查在实例级和类别级上的零-shot能力。&lt;br&gt;&lt;br&gt;8. &lt;h4&gt;未来计划&lt;/h4&gt;：&lt;br&gt;   - 计划在真实环境中进行机器人抓取实验。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robots are increasingly envisioned to interact in real-world scenarios, where
they must continuously adapt to new situations. To detect and grasp novel
objects, zero-shot pose estimators determine poses without prior knowledge.
Recently, vision language models (VLMs) have shown considerable advances in
robotics applications by establishing an understanding between language input
and image input. In our work, we take advantage of VLMs zero-shot capabilities
and translate this ability to 6D object pose estimation. We propose a novel
framework for promptable zero-shot 6D object pose estimation using language
embeddings. The idea is to derive a coarse location of an object based on the
relevancy map of a language-embedded NeRF reconstruction and to compute the
pose estimate with a point cloud registration method. Additionally, we provide
an analysis of LERF's suitability for open-set object pose estimation. We
examine hyperparameters, such as activation thresholds for relevancy maps and
investigate the zero-shot capabilities on an instance- and category-level.
Furthermore, we plan to conduct robotic grasping experiments in a real-world
setting.</description>
      <guid isPermaLink="false">2409.05413v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>GeoFormer: Learning Point Cloud Completion with Tri-Plane Integrated Transformer</title>
      <link>http://arxiv.org/abs/2408.06596v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Jinpeng Yu', 'Binbin Huang', 'Yuxuan Zhang', 'Huaxia Li', 'Xu Tang', 'Shenghua Gao']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-08-13&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; accepted by the 32nd ACM International Conference on Multimedia
  (MM'24)&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究目标&lt;/h4&gt;：&lt;br&gt;   - 点云补全旨在从部分点云中恢复准确的全局几何结构并保留细致的局部细节。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;现有方法的局限&lt;/h4&gt;：&lt;br&gt;   - 传统方法通常直接从三维点云坐标预测未见点，或使用自投影的多视图深度图简化任务。&lt;br&gt;   - 然而，这些灰度深度图无法实现多视图一致性，从而限制了性能。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;提出的解决方案&lt;/h4&gt;：&lt;br&gt;   - 引入GeoFormer模型，同时增强点的全局几何结构和局部细节。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;主要组件&lt;/h4&gt;：&lt;br&gt;   - &lt;h4&gt;CCM特征增强点生成器&lt;/h4&gt;：集成来自多视图一致的典型坐标图（CCMs）的图像特征，与纯点特征对齐，从而增强全局几何特征。&lt;br&gt;   - &lt;h4&gt;多尺度几何感知上采样模块&lt;/h4&gt;：逐步增强局部细节，通过跨注意力机制在从部分输入提取的多尺度特征和之前估计的点的特征之间进行交互。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;实验验证&lt;/h4&gt;：&lt;br&gt;   - 在PCN、ShapeNet-55/34和KITTI基准测试上的广泛实验表明，GeoFormer在性能上超越了近期方法，实现了最先进的表现。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;代码可用性&lt;/h4&gt;：&lt;br&gt;   - 相关代码可在GitHub上获取：[GeoFormer GitHub](https://github.com/Jinpeng-Yu/GeoFormer)。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; 10.1145/3664647.3680842&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; https://github.com/jinpeng-yu/geoformer&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud completion aims to recover accurate global geometry and preserve
fine-grained local details from partial point clouds. Conventional methods
typically predict unseen points directly from 3D point cloud coordinates or use
self-projected multi-view depth maps to ease this task. However, these
gray-scale depth maps cannot reach multi-view consistency, consequently
restricting the performance. In this paper, we introduce a GeoFormer that
simultaneously enhances the global geometric structure of the points and
improves the local details. Specifically, we design a CCM Feature Enhanced
Point Generator to integrate image features from multi-view consistent
canonical coordinate maps (CCMs) and align them with pure point features,
thereby enhancing the global geometry feature. Additionally, we employ the
Multi-scale Geometry-aware Upsampler module to progressively enhance local
details. This is achieved through cross attention between the multi-scale
features extracted from the partial input and the features derived from
previously estimated points. Extensive experiments on the PCN, ShapeNet-55/34,
and KITTI benchmarks demonstrate that our GeoFormer outperforms recent methods,
achieving the state-of-the-art performance. Our code is available at
\href{https://github.com/Jinpeng-Yu/GeoFormer}{https://github.com/Jinpeng-Yu/GeoFormer}.</description>
      <guid isPermaLink="false">2408.06596v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>Enhancing Few-Shot Classification without Forgetting through Multi-Level Contrastive Constraints</title>
      <link>http://arxiv.org/abs/2409.11286v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Bingzhi Chen', 'Haoming Zhou', 'Yishu Liu', 'Biqing Zeng', 'Jiahui Pan', 'Guangming Lu']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-17&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 大多数近期的少样本学习方法基于元学习和情景训练。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;存在的问题&lt;/h4&gt;：&lt;br&gt;   - (1) &lt;h4&gt;归纳偏差&lt;/h4&gt;：现有方法可能导致模型对特定任务或数据的过度适应。&lt;br&gt;   - (2) &lt;h4&gt;灾难性遗忘&lt;/h4&gt;：模型在学习新任务时可能会遗忘先前学到的知识。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;提出的框架&lt;/h4&gt;：&lt;br&gt;   - 提出了一个新颖的多层次对比约束（MLCC）框架，旨在将“情景内学习”和“情景间学习”整合为统一的交互学习范式。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;空间感知交互建模&lt;/h4&gt;：&lt;br&gt;   - 采用空间感知交互建模方案，探索每个类别在情景内相似性/不相似性分布之间的正确归纳范式。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;跨阶段分布适配策略&lt;/h4&gt;：&lt;br&gt;   - 设计了跨阶段分布适配策略，以对齐来自不同时间阶段的情景间分布，减少现有预测分布与过去预测分布之间的语义差距。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;实验验证&lt;/h4&gt;：&lt;br&gt;   - 在多个少样本数据集上的广泛实验表明，MLCC方法在性能上始终优于现有最先进的基线模型。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Most recent few-shot learning approaches are based on meta-learning with
episodic training. However, prior studies encounter two crucial problems: (1)
\textit{the presence of inductive bias}, and (2) \textit{the occurrence of
catastrophic forgetting}. In this paper, we propose a novel Multi-Level
Contrastive Constraints (MLCC) framework, that jointly integrates
within-episode learning and across-episode learning into a unified interactive
learning paradigm to solve these issues. Specifically, we employ a space-aware
interaction modeling scheme to explore the correct inductive paradigms for each
class between within-episode similarity/dis-similarity distributions.
Additionally, with the aim of better utilizing former prior knowledge, a
cross-stage distribution adaption strategy is designed to align the
across-episode distributions from different time stages, thus reducing the
semantic gap between existing and past prediction distribution. Extensive
experiments on multiple few-shot datasets demonstrate the consistent
superiority of MLCC approach over the existing state-of-the-art baselines.</description>
      <guid isPermaLink="false">2409.11286v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>Learning Spatially-Aware Language and Audio Embedding</title>
      <link>http://arxiv.org/abs/2409.11369v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Bhavika Devnani', 'Skyler Seto', 'Zakaria Aldeneh', 'Alessandro Toso', 'Elena Menyaylenko', 'Barry-John Theobald', 'Jonathan Sheaffer', 'Miguel Sarabia']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-17&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; 25 pages, 7 figures&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 人类能够根据不精确的自然语言描述想象声音场景，例如“狮子吼声来自我身后”。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;机器理解的挑战&lt;/h4&gt;：&lt;br&gt;   - 机器要具备相同的理解能力，必须了解“狮子”的语义属性、空间概念“身后”，以及声音的语义和空间属性如何结合。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;现有模型的局限&lt;/h4&gt;：&lt;br&gt;   - 现有的音频基础模型主要基于非空间音频和文本对进行训练，缺乏空间感知能力。&lt;br&gt;   - 声音事件定位和检测模型仅能识别固定类别的声音，并将声音源定位到绝对位置，而非自然语言描述的位置。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;研究贡献&lt;/h4&gt;：&lt;br&gt;   - 提出ELSA（Spatially Aware Audio and Text Embedding），一个通过多模态对比学习训练的空间感知音频和文本嵌入模型。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;ELSA功能&lt;/h4&gt;：&lt;br&gt;   - 支持非空间音频、空间音频和开放词汇的文本描述，涵盖声音的空间和语义成分。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;训练方法&lt;/h4&gt;：&lt;br&gt;   - (a) 对三个开放源音频数据集进行空间增强，总计4,738小时音频和文本。&lt;br&gt;   - (b) 设计编码器以捕捉非空间音频的语义，以及空间音频的语义和空间属性。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;性能评估&lt;/h4&gt;：&lt;br&gt;   - ELSA在语义检索和3D源定位方面与现有最先进技术具有竞争力。&lt;br&gt;   - 在音频到文本和文本到音频的检索率上，ELSA比基线提高了2.8%。&lt;br&gt;   - 在3D源定位的平均绝对误差上，ELSA比基线减少了11.6度。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humans can picture a sound scene given an imprecise natural language
description. For example, it is easy to imagine an acoustic environment given a
phrase like "the lion roar came from right behind me!". For a machine to have
the same degree of comprehension, the machine must know what a lion is
(semantic attribute), what the concept of "behind" is (spatial attribute) and
how these pieces of linguistic information align with the semantic and spatial
attributes of the sound (what a roar sounds like when its coming from behind).
State-of-the-art audio foundation models which learn to map between audio
scenes and natural textual descriptions, are trained on non-spatial audio and
text pairs, and hence lack spatial awareness. In contrast, sound event
localization and detection models are limited to recognizing sounds from a
fixed number of classes, and they localize the source to absolute position
(e.g., 0.2m) rather than a position described using natural language (e.g.,
"next to me"). To address these gaps, we present ELSA a spatially aware-audio
and text embedding model trained using multimodal contrastive learning. ELSA
supports non-spatial audio, spatial audio, and open vocabulary text captions
describing both the spatial and semantic components of sound. To train ELSA:
(a) we spatially augment the audio and captions of three open-source audio
datasets totaling 4,738 hours of audio, and (b) we design an encoder to capture
the semantics of non-spatial audio, and the semantics and spatial attributes of
spatial audio using contrastive learning. ELSA is competitive with
state-of-the-art for both semantic retrieval and 3D source localization. In
particular, ELSA achieves +2.8% mean audio-to-text and text-to-audio R@1 above
the baseline, and outperforms by -11.6{\deg} mean-absolute-error in 3D source
localization over the baseline.</description>
      <guid isPermaLink="false">2409.11369v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>Multimodal Fusion with LLMs for Engagement Prediction in Natural Conversation</title>
      <link>http://arxiv.org/abs/2409.09135v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Cheng Charles Ma', 'Kevin Hyekang Joo', 'Alexandria K. Vail', 'Sunreeta Bhattacharya', 'Álvaro Fernández García', 'Kailana Baker-Matsuoka', 'Sheryl Mathew', 'Lori L. Holt', 'Fernando De la Torre']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-13&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; 22 pages, first three authors equal contribution&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;背景与发展&lt;/h4&gt;：&lt;br&gt;   - 智能眼镜在传感器技术、设计和处理能力方面经历了显著进步，带来了高密度人类行为数据的新机遇。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;研究重点&lt;/h4&gt;：&lt;br&gt;   - 研究目标是通过分析言语和非言语线索来预测双边互动中的参与度，特别是识别不感兴趣或困惑的迹象。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;影响与应用&lt;/h4&gt;：&lt;br&gt;   - 该研究有望革新人类沟通的理解，提升专业环境中的协作效果，改善心理健康支持，以及增强对沟通障碍人士的可及性。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;数据集收集&lt;/h4&gt;：&lt;br&gt;   - 收集了34名参与者在轻松双边对话中的数据，每位参与者在对话结束时提供自我报告的参与度评分。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;创新方法&lt;/h4&gt;：&lt;br&gt;   - 引入了一种新颖的融合策略，利用大型语言模型（LLMs）将多种行为模态整合为“多模态转录”，以便进行行为推理任务。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;初步结果&lt;/h4&gt;：&lt;br&gt;   - 该方法在初步实施中表现出与已有融合技术相当的性能，显示出进一步研究和优化的强大潜力。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;研究的独特性&lt;/h4&gt;：&lt;br&gt;   - 这是首次通过语言模型“推理”现实世界人类行为的方法之一。&lt;br&gt;&lt;br&gt;8. &lt;h4&gt;数据收集的优势&lt;/h4&gt;：&lt;br&gt;   - 智能眼镜能够无干扰地收集人类行为的高密度多模态数据，为理解和改善人类沟通开辟了新方法，具有重要的社会利益潜力。&lt;br&gt;&lt;br&gt;9. &lt;h4&gt;数据共享&lt;/h4&gt;：&lt;br&gt;   - 研究中收集的特征和数据将公开，以促进进一步研究。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Over the past decade, wearable computing devices (``smart glasses'') have
undergone remarkable advancements in sensor technology, design, and processing
power, ushering in a new era of opportunity for high-density human behavior
data. Equipped with wearable cameras, these glasses offer a unique opportunity
to analyze non-verbal behavior in natural settings as individuals interact. Our
focus lies in predicting engagement in dyadic interactions by scrutinizing
verbal and non-verbal cues, aiming to detect signs of disinterest or confusion.
Leveraging such analyses may revolutionize our understanding of human
communication, foster more effective collaboration in professional
environments, provide better mental health support through empathetic virtual
interactions, and enhance accessibility for those with communication barriers.
  In this work, we collect a dataset featuring 34 participants engaged in
casual dyadic conversations, each providing self-reported engagement ratings at
the end of each conversation. We introduce a novel fusion strategy using Large
Language Models (LLMs) to integrate multiple behavior modalities into a
``multimodal transcript'' that can be processed by an LLM for behavioral
reasoning tasks. Remarkably, this method achieves performance comparable to
established fusion techniques even in its preliminary implementation,
indicating strong potential for further research and optimization. This fusion
method is one of the first to approach ``reasoning'' about real-world human
behavior through a language model. Smart glasses provide us the ability to
unobtrusively gather high-density multimodal data on human behavior, paving the
way for new approaches to understanding and improving human communication with
the potential for important societal benefits. The features and data collected
during the studies will be made publicly available to promote further research.</description>
      <guid isPermaLink="false">2409.09135v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>DGD: Dynamic 3D Gaussians Distillation</title>
      <link>http://arxiv.org/abs/2405.19321v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Isaac Labe', 'Noam Issachar', 'Itai Lang', 'Sagie Benaim']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-05-29&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究目标&lt;/h4&gt;：&lt;br&gt;   - 本文旨在学习动态3D语义辐射场，输入为单一的单目视频。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;语义辐射场特性&lt;/h4&gt;：&lt;br&gt;   - 学习到的语义辐射场能够捕捉每个点的语义信息、颜色和几何属性，适用于动态3D场景。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;功能实现&lt;/h4&gt;：&lt;br&gt;   - 该方法支持生成新的视角及其相应的语义信息，实现对多种3D语义实体的分割和跟踪。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;用户交互&lt;/h4&gt;：&lt;br&gt;   - 提供简单直观的接口，通过用户点击或文本提示来指定要跟踪的对象。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;方法介绍&lt;/h4&gt;：&lt;br&gt;   - 提出DGD，作为动态3D场景外观和语义的统一3D表示，基于新近提出的动态3D高斯表示。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;优化策略&lt;/h4&gt;：&lt;br&gt;   - 该表示随着时间的推移进行优化，同时考虑颜色和语义信息。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;联合优化&lt;/h4&gt;：&lt;br&gt;   - 关键在于外观和语义属性的联合优化，这共同影响场景的几何特性。&lt;br&gt;&lt;br&gt;8. &lt;h4&gt;评估结果&lt;/h4&gt;：&lt;br&gt;   - 评估表明，该方法能够实现高密度的语义3D对象跟踪，且渲染速度快，适用于多样化的场景。&lt;br&gt;&lt;br&gt;9. &lt;h4&gt;项目链接&lt;/h4&gt;：&lt;br&gt;   - 项目网页可访问 [DGD-Website](https://isaaclabe.github.io/DGD-Website/)。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We tackle the task of learning dynamic 3D semantic radiance fields given a
single monocular video as input. Our learned semantic radiance field captures
per-point semantics as well as color and geometric properties for a dynamic 3D
scene, enabling the generation of novel views and their corresponding
semantics. This enables the segmentation and tracking of a diverse set of 3D
semantic entities, specified using a simple and intuitive interface that
includes a user click or a text prompt. To this end, we present DGD, a unified
3D representation for both the appearance and semantics of a dynamic 3D scene,
building upon the recently proposed dynamic 3D Gaussians representation. Our
representation is optimized over time with both color and semantic information.
Key to our method is the joint optimization of the appearance and semantic
attributes, which jointly affect the geometric properties of the scene. We
evaluate our approach in its ability to enable dense semantic 3D object
tracking and demonstrate high-quality results that are fast to render, for a
diverse set of scenes. Our project webpage is available on
https://isaaclabe.github.io/DGD-Website/</description>
      <guid isPermaLink="false">2405.19321v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>Mahalanobis k-NN: A Statistical Lens for Robust Point-Cloud Registrations</title>
      <link>http://arxiv.org/abs/2409.06267v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Tejas Anvekar', 'Shivanand Venkanna Sheshappanavar']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-10&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究主题&lt;/h4&gt;：&lt;br&gt;   - 本文讨论了Mahalanobis k-NN，旨在解决在学习型点云配准中，面对任意密度的源点云或目标点云时的特征匹配挑战。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;方法特点&lt;/h4&gt;：&lt;br&gt;   - 采用Mahalanobis k-NN的固有特性，捕捉局部邻域的分布和表面几何特征。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;方法集成性&lt;/h4&gt;：&lt;br&gt;   - 本方法可无缝集成到任何基于局部图的方法中进行点云分析。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;研究方法&lt;/h4&gt;：&lt;br&gt;   - 重点关注两种不同的方法论：&lt;br&gt;     - 深度最近点（Deep Closest Point, DCP）&lt;br&gt;     - 深度通用流形嵌入（Deep Universal Manifold Embedding, DeepUME）&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;实验结果&lt;/h4&gt;：&lt;br&gt;   - 在ModelNet40和Faust数据集上的广泛基准测试显示了所提方法在点云配准任务中的有效性。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;特征能力发现&lt;/h4&gt;：&lt;br&gt;   - 首次证明，通过点云配准获得的特征本质上具备区分能力。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;性能提升&lt;/h4&gt;：&lt;br&gt;   - 在ModelNet40和ScanObjectNN上的点云少样本分类任务中，平均准确率提升约20%。&lt;br&gt;&lt;br&gt;8. &lt;h4&gt;代码获取&lt;/h4&gt;：&lt;br&gt;   - 相关代码已公开，链接为 [Mahalanobis-k-NN](https://github.com/TejasAnvekar/Mahalanobis-k-NN)。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; https://github.com/TejasAnvekar/Mahalanobis-k-NN&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we discuss Mahalanobis k-NN: a statistical lens designed to
address the challenges of feature matching in learning-based point cloud
registration when confronted with an arbitrary density of point clouds, either
in the source or target point cloud. We tackle this by adopting Mahalanobis
k-NN's inherent property to capture the distribution of the local neighborhood
and surficial geometry. Our method can be seamlessly integrated into any
local-graph-based point cloud analysis method. In this paper, we focus on two
distinct methodologies: Deep Closest Point (DCP) and Deep Universal Manifold
Embedding (DeepUME). Our extensive benchmarking on the ModelNet40 and Faust
datasets highlights the efficacy of the proposed method in point cloud
registration tasks. Moreover, we establish for the first time that the features
acquired through point cloud registration inherently can possess discriminative
capabilities. This is evident by a substantial improvement of about 20\% in the
average accuracy observed in the point cloud few-shot classification task
benchmarked on ModelNet40 and ScanObjectNN. The code is publicly available at
https://github.com/TejasAnvekar/Mahalanobis-k-NN</description>
      <guid isPermaLink="false">2409.06267v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>Diffusion-Occ: 3D Point Cloud Completion via Occupancy Diffusion</title>
      <link>http://arxiv.org/abs/2408.14846v2</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Guoqing Zhang', 'Jian Liu']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-08-27&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; After a closer examination of our work, we've determined that our
  experiments are not thorough and robust enough, possibly impacting the
  accuracy of our conclusions. Hence, we've decided to withdraw our article
  and, after refining our experiments, intend to resubmit the paper once
  significant improvements have been made&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 点云对于捕捉三维数据至关重要，但常因分辨率和遮挡等限制而导致不完整。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;传统方法的局限性&lt;/h4&gt;：&lt;br&gt;   - 传统方法通常依赖于判别框架中的点基方法进行点云补全，效果有限。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;新框架介绍&lt;/h4&gt;：&lt;br&gt;   - 本文提出了&lt;h4&gt;Diffusion-Occ&lt;/h4&gt;，一种新颖的扩散点云补全框架。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;两阶段方法&lt;/h4&gt;：&lt;br&gt;   - 采用两阶段的粗到细策略：&lt;br&gt;     - &lt;h4&gt;第一阶段&lt;/h4&gt;：粗密度体素预测网络（CDNet）处理部分点，预测粗体素密度，通过体素分类简化全局特征提取，而非之前的回归方法。&lt;br&gt;     - &lt;h4&gt;第二阶段&lt;/h4&gt;：引入占据生成网络（OccGen），基于变换器架构的条件占据扩散模型，利用我们的点-体素融合（PVF）块增强模型，结合粗体素密度和部分点以利用全局和局部特征。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;点云补全过程&lt;/h4&gt;：&lt;br&gt;   - 通过对占据场进行阈值化，将其转换为完整的点云。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;训练与推理效率&lt;/h4&gt;：&lt;br&gt;   - 方法采用多样化的训练组合和高效的扩散参数化，使得在训练和推理过程中实现有效的一步采样。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;实验结果&lt;/h4&gt;：&lt;br&gt;   - 实验结果表明，Diffusion-Occ在性能上超越了现有的判别和生成方法。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point clouds are crucial for capturing three-dimensional data but often
suffer from incompleteness due to limitations such as resolution and occlusion.
Traditional methods typically rely on point-based approaches within
discriminative frameworks for point cloud completion. In this paper, we
introduce \textbf{Diffusion-Occ}, a novel framework for Diffusion Point Cloud
Completion. Diffusion-Occ utilizes a two-stage coarse-to-fine approach. In the
first stage, the Coarse Density Voxel Prediction Network (CDNet) processes
partial points to predict coarse density voxels, streamlining global feature
extraction through voxel classification, as opposed to previous
regression-based methods. In the second stage, we introduce the Occupancy
Generation Network (OccGen), a conditional occupancy diffusion model based on a
transformer architecture and enhanced by our Point-Voxel Fuse (PVF) block. This
block integrates coarse density voxels with partial points to leverage both
global and local features for comprehensive completion. By thresholding the
occupancy field, we convert it into a complete point cloud. Additionally, our
method employs diverse training mixtures and efficient diffusion
parameterization to enable effective one-step sampling during both training and
inference. Experimental results demonstrate that Diffusion-Occ outperforms
existing discriminative and generative methods.</description>
      <guid isPermaLink="false">2408.14846v2</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>Can Transfer Learning be Used to Identify Tropical State-Dependent Bias Relevant to Midlatitude Subseasonal Predictability?</title>
      <link>http://arxiv.org/abs/2409.10755v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Kirsten J. Mayer', 'Katherine Dagon', 'Maria J. Molina']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-16&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; This work has been submitted for publication in Artificial
  Intelligence for the Earth Systems (AIES). Copyright in this work may be
  transferred without further notice&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 先前研究表明，气候系统的特定状态可以增强亚季节预测能力，即状态依赖的可预测性。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;模型偏差问题&lt;/h4&gt;：&lt;br&gt;   - 地球系统模型中的偏差可能影响这些状态的表现及其后续演变，从而影响预测效果。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;研究目的&lt;/h4&gt;：&lt;br&gt;   - 本文提出一种机器学习框架，用于识别地球系统模型中的状态依赖偏差。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;研究方法&lt;/h4&gt;：&lt;br&gt;   - 特别关注使用可解释的神经网络进行迁移学习，以识别与亚季节可预测性相关的热带状态偏差，使用的是能源超级地球系统模型版本2（E3SMv2）的历史模拟数据。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;数据需求发现&lt;/h4&gt;：&lt;br&gt;   - 通过完美模型框架的研究发现，迁移学习可能需要比现有再分析数据集提供的更多的数据来更新神经网络权重。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;研究启示&lt;/h4&gt;：&lt;br&gt;   - 研究结果提出了对未来聚焦于亚季节变异模式的迁移学习方法的警示，强调了数据充足性的重要性。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Previous research has demonstrated that specific states of the climate system
can lead to enhanced subseasonal predictability (i.e., state-dependent
predictability). However, biases in Earth system models can affect the
representation of these states and their subsequent evolution. Here, we present
a machine learning framework to identify state-dependent biases in Earth system
models. In particular, we investigate the utility of transfer learning with
explainable neural networks to identify tropical state-dependent biases in
historical simulations of the Energy Exascale Earth System Model version 2
(E3SMv2) relevant for midlatitude subseasonal predictability. Using a perfect
model framework, we find transfer learning may require substantially more data
than provided by present-day reanalysis datasets to update neural network
weights, imparting a cautionary tale for future transfer learning approaches
focused on subseasonal modes of variability.</description>
      <guid isPermaLink="false">2409.10755v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>Learnable Chamfer Distance for Point Cloud Reconstruction</title>
      <link>http://arxiv.org/abs/2312.16582v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Tianxin Huang', 'Qingyao Liu', 'Xiangrui Zhao', 'Jun Chen', 'Yong Liu']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2023-12-27&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; Accepted by Pattern Recognition Letters&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 点云是具有排列不变性的3D信号，现有的重建网络通常通过预定义规则测量点云之间的形状差异。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;静态匹配规则的局限性&lt;/h4&gt;：&lt;br&gt;   - 静态匹配规则可能与实际形状差异存在偏差，影响重建效果。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;动态学习结构的挑战&lt;/h4&gt;：&lt;br&gt;   - 尽管一些研究提出了动态更新的可学习结构来替代匹配规则，但它们通常需要更多迭代才能良好收敛。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;提出的解决方案&lt;/h4&gt;：&lt;br&gt;   - 本文提出了一种简单而有效的重建损失函数，称为可学习的Chamfer距离（LCD），通过动态关注不同权重分布的匹配距离来优化重建。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;对抗训练策略&lt;/h4&gt;：&lt;br&gt;   - 通过对抗训练策略，LCD能够识别重建结果中的缺陷，克服静态匹配规则的不足，同时在低迭代情况下也能保证性能。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;实验验证&lt;/h4&gt;：&lt;br&gt;   - 在多个重建网络上的实验表明，LCD能够实现更好的重建性能，并提取更具代表性的表示，同时具有更快的收敛速度和相当的训练效率。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;代码获取&lt;/h4&gt;：&lt;br&gt;   - 相关源代码可在GitHub上获取，链接为 [LCDNet](https://github.com/Tianxinhuang/LCDNet.git)。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; https://github.com/tianxinhuang/lcdnet&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As point clouds are 3D signals with permutation invariance, most existing
works train their reconstruction networks by measuring shape differences with
the average point-to-point distance between point clouds matched with
predefined rules. However, the static matching rules may deviate from actual
shape differences. Although some works propose dynamically-updated learnable
structures to replace matching rules, they need more iterations to converge
well. In this work, we propose a simple but effective reconstruction loss,
named Learnable Chamfer Distance (LCD) by dynamically paying attention to
matching distances with different weight distributions controlled with a group
of learnable networks. By training with adversarial strategy, LCD learns to
search defects in reconstructed results and overcomes the weaknesses of static
matching rules, while the performances at low iterations can also be guaranteed
by the basic matching algorithm. Experiments on multiple reconstruction
networks confirm that LCD can help achieve better reconstruction performances
and extract more representative representations with faster convergence and
comparable training efficiency. The source codes are provided in
https://github.com/Tianxinhuang/LCDNet.git.</description>
      <guid isPermaLink="false">2312.16582v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>Self-supervised Multimodal Speech Representations for the Assessment of Schizophrenia Symptoms</title>
      <link>http://arxiv.org/abs/2409.09733v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Gowtham Premananth', 'Carol Espy-Wilson']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-15&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; Submitted to ICASSP 2025&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 多模态精神分裂症评估系统近年来受到关注，旨在提高对精神分裂症的理解和诊断。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;系统介绍&lt;/h4&gt;：&lt;br&gt;   - 本文介绍了一种精神分裂症评估系统，旨在区分精神分裂症的主要症状类别，并预测整体严重性评分。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;模型开发&lt;/h4&gt;：&lt;br&gt;   - 开发了一种基于向量量化变分自编码器（VQ-VAE）的多模态表征学习（MRL）模型，用于生成与任务无关的语音表征，这些表征来源于声道变量（TVs）和面部动作单元（FAUs）。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;多任务学习&lt;/h4&gt;：&lt;br&gt;   - 这些表征被用于基于多任务学习（MTL）的下游预测模型，以获得类别标签和整体严重性评分。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;性能提升&lt;/h4&gt;：&lt;br&gt;   - 提出的框架在多类分类任务中超越了之前的研究，所有评估指标（加权F1分数、AUC-ROC分数和加权准确率）均表现优越。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;新任务的解决&lt;/h4&gt;：&lt;br&gt;   - 此外，该系统能够估计精神分裂症的严重性评分，这是早期方法未能解决的任务。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal schizophrenia assessment systems have gained traction over the
last few years. This work introduces a schizophrenia assessment system to
discern between prominent symptom classes of schizophrenia and predict an
overall schizophrenia severity score. We develop a Vector Quantized Variational
Auto-Encoder (VQ-VAE) based Multimodal Representation Learning (MRL) model to
produce task-agnostic speech representations from vocal Tract Variables (TVs)
and Facial Action Units (FAUs). These representations are then used in a
Multi-Task Learning (MTL) based downstream prediction model to obtain class
labels and an overall severity score. The proposed framework outperforms the
previous works on the multi-class classification task across all evaluation
metrics (Weighted F1 score, AUC-ROC score, and Weighted Accuracy).
Additionally, it estimates the schizophrenia severity score, a task not
addressed by earlier approaches.</description>
      <guid isPermaLink="false">2409.09733v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>Multi-frequency Electrical Impedance Tomography Reconstruction with Multi-Branch Attention Image Prior</title>
      <link>http://arxiv.org/abs/2409.10794v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Hao Fang', 'Zhe Liu', 'Yi Feng', 'Zhen Qiu', 'Pierre Bagnaninchi', 'Yunjie Yang']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-17&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; 10 pages, 10 figures, journal&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 多频率电阻抗成像（mfEIT）是一种有前景的生物医学成像技术，旨在估计不同频率下的组织导电性。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;现有方法的局限性&lt;/h4&gt;：&lt;br&gt;   - 当前的先进算法依赖于监督学习和多测量向量（MMV），需要大量训练数据，这使得训练过程耗时、成本高，并且不够实用。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;训练数据的依赖性问题&lt;/h4&gt;：&lt;br&gt;   - 监督MMV方法对训练数据的依赖可能导致不同频率下的导电性对比出现错误，这在生物医学应用中引发重大担忧。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;提出的新方法&lt;/h4&gt;：&lt;br&gt;   - 本文提出了一种基于多分支注意力图像先验（MAIP）的新型无监督学习方法，用于mfEIT重建。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;多分支注意力网络（MBA-Net）&lt;/h4&gt;：&lt;br&gt;   - 该方法利用精心设计的MBA-Net来表示多种频率依赖的导电性图像，并通过迭代更新参数同时重建mfEIT图像。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;隐式正则化能力&lt;/h4&gt;：&lt;br&gt;   - MBA-Net的隐式正则化能力使算法能够捕捉显著的频率间和频率内的相关性，从而实现稳健的mfEIT重建，无需训练数据。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;实验验证&lt;/h4&gt;：&lt;br&gt;   - 通过模拟实验和真实世界实验，所提方法的性能与现有先进算法相当，甚至更优，并展现出更强的泛化能力。&lt;br&gt;&lt;br&gt;8. &lt;h4&gt;应用潜力&lt;/h4&gt;：&lt;br&gt;   - 这些结果表明，基于MAIP的方法可以提高mfEIT在各种环境中的可靠性和适用性。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-frequency Electrical Impedance Tomography (mfEIT) is a promising
biomedical imaging technique that estimates tissue conductivities across
different frequencies. Current state-of-the-art (SOTA) algorithms, which rely
on supervised learning and Multiple Measurement Vectors (MMV), require
extensive training data, making them time-consuming, costly, and less practical
for widespread applications. Moreover, the dependency on training data in
supervised MMV methods can introduce erroneous conductivity contrasts across
frequencies, posing significant concerns in biomedical applications. To address
these challenges, we propose a novel unsupervised learning approach based on
Multi-Branch Attention Image Prior (MAIP) for mfEIT reconstruction. Our method
employs a carefully designed Multi-Branch Attention Network (MBA-Net) to
represent multiple frequency-dependent conductivity images and simultaneously
reconstructs mfEIT images by iteratively updating its parameters. By leveraging
the implicit regularization capability of the MBA-Net, our algorithm can
capture significant inter- and intra-frequency correlations, enabling robust
mfEIT reconstruction without the need for training data. Through simulation and
real-world experiments, our approach demonstrates performance comparable to, or
better than, SOTA algorithms while exhibiting superior generalization
capability. These results suggest that the MAIP-based method can be used to
improve the reliability and applicability of mfEIT in various settings.</description>
      <guid isPermaLink="false">2409.10794v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>U2UData: A Large-scale Cooperative Perception Dataset for Swarm UAVs Autonomous Flight</title>
      <link>http://arxiv.org/abs/2408.00606v3</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Tongtong Feng', 'Xin Wang', 'Feilin Han', 'Leping Zhang', 'Wenwu Zhu']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-08-01&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; Accepted by ACM MM24&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 现代自主飞行感知系统对遮挡敏感，且远程能力有限，这是提高低空经济任务表现的关键瓶颈。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;U2U合作感知系统的潜力&lt;/h4&gt;：&lt;br&gt;   - UAV到UAV（U2U）合作感知系统被认为有潜力革新自主飞行行业。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;数据集缺乏的问题&lt;/h4&gt;：&lt;br&gt;   - 目前缺乏大规模数据集，阻碍了这一领域的进展。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;U2UData的介绍&lt;/h4&gt;：&lt;br&gt;   - 本文提出了U2UData，这是第一个大规模的群体UAV自主飞行合作感知数据集。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;数据集收集方式&lt;/h4&gt;：&lt;br&gt;   - 数据集由三架UAV在U2USim环境中自主飞行收集，覆盖9 km²的飞行区域。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;数据集内容&lt;/h4&gt;：&lt;br&gt;   - 包含315K个LiDAR帧、945K个RGB和深度帧，以及241万个注释的3D边界框，涵盖3个类别。&lt;br&gt;   - 还记录了亮度、温度、湿度、烟雾和气流值，涵盖所有飞行路线。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;U2USim环境&lt;/h4&gt;：&lt;br&gt;   - U2USim是第一个真实世界映射的群体UAV仿真环境，以云南省为原型，包含4种地形、7种天气条件和8种传感器类型。&lt;br&gt;&lt;br&gt;8. &lt;h4&gt;感知任务的定义&lt;/h4&gt;：&lt;br&gt;   - U2UData引入了两个感知任务：合作3D物体检测和合作3D物体跟踪。&lt;br&gt;&lt;br&gt;9. &lt;h4&gt;基准测试&lt;/h4&gt;：&lt;br&gt;   - 本文提供了对这些任务的近期合作感知算法的全面基准测试。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; 10.1145/3664647.3681151&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; https://github.com/fengtt42/u2udata&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern perception systems for autonomous flight are sensitive to occlusion
and have limited long-range capability, which is a key bottleneck in improving
low-altitude economic task performance. Recent research has shown that the
UAV-to-UAV (U2U) cooperative perception system has great potential to
revolutionize the autonomous flight industry. However, the lack of a
large-scale dataset is hindering progress in this area. This paper presents
U2UData, the first large-scale cooperative perception dataset for swarm UAVs
autonomous flight. The dataset was collected by three UAVs flying autonomously
in the U2USim, covering a 9 km$^2$ flight area. It comprises 315K LiDAR frames,
945K RGB and depth frames, and 2.41M annotated 3D bounding boxes for 3 classes.
It also includes brightness, temperature, humidity, smoke, and airflow values
covering all flight routes. U2USim is the first real-world mapping swarm UAVs
simulation environment. It takes Yunnan Province as the prototype and includes
4 terrains, 7 weather conditions, and 8 sensor types. U2UData introduces two
perception tasks: cooperative 3D object detection and cooperative 3D object
tracking. This paper provides comprehensive benchmarks of recent cooperative
perception algorithms on these tasks.</description>
      <guid isPermaLink="false">2408.00606v3</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>Unsupervised Point Cloud Registration with Self-Distillation</title>
      <link>http://arxiv.org/abs/2409.07558v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Christian Löwens', 'Thorben Funke', 'André Wagner', 'Alexandru Paul Condurache']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-11&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; Oral at BMVC 2024&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 刚性点云注册是机器人技术和自动驾驶领域中的一个基本问题。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;现有方法的局限性&lt;/h4&gt;：&lt;br&gt;   - 目前深度学习方法可以训练模型来匹配一对点云，但收集真实变换的成本高，导致训练不具可扩展性。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;自蒸馏方法的提出&lt;/h4&gt;：&lt;br&gt;   - 提出了一个自蒸馏的方法，以无监督的方式学习点云注册。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;教师网络和学生网络&lt;/h4&gt;：&lt;br&gt;   - 每个样本传递给教师网络，同时增强视图传递给学生网络。教师网络包含可训练的特征提取器和无学习的鲁棒求解器（如RANSAC）。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;一致性强制&lt;/h4&gt;：&lt;br&gt;   - 求解器强制保证对应关系的一致性，并优化无监督的内点比率，消除了对真实标签的需求。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;简化训练过程&lt;/h4&gt;：&lt;br&gt;   - 通过去除对初始手工特征或连续点云帧的需求，简化了训练过程。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;实验结果&lt;/h4&gt;：&lt;br&gt;   - 方法在RGB-D基准数据集3DMatch上超越了相关方法，并且在汽车雷达数据上具有良好的泛化能力，而经典特征在该领域的表现较差。&lt;br&gt;&lt;br&gt;8. &lt;h4&gt;代码可用性&lt;/h4&gt;：&lt;br&gt;   - 相关代码可在GitHub上获取，链接为 [https://github.com/boschresearch/direg](https://github.com/boschresearch/direg)。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; https://github.com/boschresearch/direg&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Rigid point cloud registration is a fundamental problem and highly relevant
in robotics and autonomous driving. Nowadays deep learning methods can be
trained to match a pair of point clouds, given the transformation between them.
However, this training is often not scalable due to the high cost of collecting
ground truth poses. Therefore, we present a self-distillation approach to learn
point cloud registration in an unsupervised fashion. Here, each sample is
passed to a teacher network and an augmented view is passed to a student
network. The teacher includes a trainable feature extractor and a learning-free
robust solver such as RANSAC. The solver forces consistency among
correspondences and optimizes for the unsupervised inlier ratio, eliminating
the need for ground truth labels. Our approach simplifies the training
procedure by removing the need for initial hand-crafted features or consecutive
point cloud frames as seen in related methods. We show that our method not only
surpasses them on the RGB-D benchmark 3DMatch but also generalizes well to
automotive radar, where classical features adopted by others fail. The code is
available at https://github.com/boschresearch/direg .</description>
      <guid isPermaLink="false">2409.07558v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>Physically-Based Photometric Bundle Adjustment in Non-Lambertian Environments</title>
      <link>http://arxiv.org/abs/2409.11854v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Lei Cheng', 'Junpeng Hu', 'Haodong Yan', 'Mariia Gladkova', 'Tianyu Huang', 'Yun-Hui Liu', 'Daniel Cremers', 'Haoang Li']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-18&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; Accepted to 2024 IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS 2024)&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 光度束调整（PBA）广泛用于通过假设一个朗伯世界来估计相机姿态和3D几何形状。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;问题陈述&lt;/h4&gt;：&lt;br&gt;   - 由于真实环境中常见的非漫反射，光度一致性的假设常常被违反。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;影响&lt;/h4&gt;：&lt;br&gt;   - 光度不一致性显著影响现有PBA方法的可靠性。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;新方法的提出&lt;/h4&gt;：&lt;br&gt;   - 提出了一个新颖的基于物理的PBA方法，以解决光度不一致性问题。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;物理基础权重&lt;/h4&gt;：&lt;br&gt;   - 引入与材料、照明和光路径相关的物理基础权重，以区分具有不同光度不一致性水平的像素对。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;模型设计&lt;/h4&gt;：&lt;br&gt;   - 设计了基于序列图像的材料估计模型和基于点云的照明估计模型。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;数据集建立&lt;/h4&gt;：&lt;br&gt;   - 建立了第一个与SLAM相关的非朗伯场景数据集，并提供了完整的照明和材料的真实值。&lt;br&gt;&lt;br&gt;8. &lt;h4&gt;实验验证&lt;/h4&gt;：&lt;br&gt;   - 大量实验表明，所提出的PBA方法在准确性上优于现有方法。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Photometric bundle adjustment (PBA) is widely used in estimating the camera
pose and 3D geometry by assuming a Lambertian world. However, the assumption of
photometric consistency is often violated since the non-diffuse reflection is
common in real-world environments. The photometric inconsistency significantly
affects the reliability of existing PBA methods. To solve this problem, we
propose a novel physically-based PBA method. Specifically, we introduce the
physically-based weights regarding material, illumination, and light path.
These weights distinguish the pixel pairs with different levels of photometric
inconsistency. We also design corresponding models for material estimation
based on sequential images and illumination estimation based on point clouds.
In addition, we establish the first SLAM-related dataset of non-Lambertian
scenes with complete ground truth of illumination and material. Extensive
experiments demonstrated that our PBA method outperforms existing approaches in
accuracy.</description>
      <guid isPermaLink="false">2409.11854v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>Coupled Laplacian Eigenmaps for Locally-Aware 3D Rigid Point Cloud Matching</title>
      <link>http://arxiv.org/abs/2402.17372v2</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Matteo Bastico', 'Etienne Decencière', 'Laurent Corté', 'Yannick Tillier', 'David Ryckelynck']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-02-27&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; This paper has been accepted at Computer Vision and Patter
  Recognition (CVPR) 2024&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 点云匹配在计算机视觉、医学和机器人领域至关重要，主要用于寻找一对点云或体素之间的对应关系。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;局部差异的重要性&lt;/h4&gt;：&lt;br&gt;   - 在某些实际场景中，强调局部差异对于准确识别正确匹配至关重要，从而提高匹配过程的整体鲁棒性和可靠性。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;现有方法的局限性&lt;/h4&gt;：&lt;br&gt;   - 常用的形状描述符存在多个局限，通常无法提供关于配对几何体的有意义的局部信息。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;新方法的提出&lt;/h4&gt;：&lt;br&gt;   - 本研究提出了一种基于图拉普拉斯特征映射的新技术，通过考虑细致的局部结构来匹配点云。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;Coupled Laplacian的引入&lt;/h4&gt;：&lt;br&gt;   - 为了解决拉普拉斯特征映射的顺序和符号模糊性，提出了一种新的算子——Coupled Laplacian，能够轻松生成多个已注册几何体的对齐特征空间。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;相似性度量&lt;/h4&gt;：&lt;br&gt;   - 显示对齐的高维空间之间的相似性提供了一个局部有意义的分数，用于形状匹配。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;性能评估&lt;/h4&gt;：&lt;br&gt;   - 首先在MVTec 3D-AD数据集上进行点对点的性能评估，专注于对象异常定位任务。&lt;br&gt;&lt;br&gt;8. &lt;h4&gt;新医疗任务的定义&lt;/h4&gt;：&lt;br&gt;   - 定义了一项新的医疗任务——自动骨侧估计（BSE），通过来源于耦合特征空间的全局相似性分数来解决。&lt;br&gt;&lt;br&gt;9. &lt;h4&gt;基准测试的提出&lt;/h4&gt;：&lt;br&gt;   - 提出了一个基准测试，收集来自各种公共数据集的骨表面结构。&lt;br&gt;&lt;br&gt;10. &lt;h4&gt;结果表现&lt;/h4&gt;：&lt;br&gt;    - 基于Coupled Laplacian的匹配技术在这两个任务上表现优于其他方法，达到令人印象深刻的准确率。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; https://github.com/matteo-bastico/couplap&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud matching, a crucial technique in computer vision, medical and
robotics fields, is primarily concerned with finding correspondences between
pairs of point clouds or voxels. In some practical scenarios, emphasizing local
differences is crucial for accurately identifying a correct match, thereby
enhancing the overall robustness and reliability of the matching process.
Commonly used shape descriptors have several limitations and often fail to
provide meaningful local insights about the paired geometries. In this work, we
propose a new technique, based on graph Laplacian eigenmaps, to match point
clouds by taking into account fine local structures. To deal with the order and
sign ambiguity of Laplacian eigenmaps, we introduce a new operator, called
Coupled Laplacian (https://github.com/matteo-bastico/CoupLap), that allows to
easily generate aligned eigenspaces for multiple registered geometries. We show
that the similarity between those aligned high-dimensional spaces provides a
locally meaningful score to match shapes. We firstly evaluate the performance
of the proposed technique in a point-wise manner, focusing on the task of
object anomaly localization on the MVTec 3D-AD dataset. Additionally, we define
a new medical task, called automatic Bone Side Estimation (BSE), which we
address through a global similarity score derived from coupled eigenspaces. In
order to test it, we propose a benchmark collecting bone surface structures
from various public datasets. Our matching technique, based on Coupled
Laplacian, outperforms other methods by reaching an impressive accuracy on both
tasks.</description>
      <guid isPermaLink="false">2402.17372v2</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>Loss Distillation via Gradient Matching for Point Cloud Completion with Weighted Chamfer Distance</title>
      <link>http://arxiv.org/abs/2409.06171v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Fangzhou Lin', 'Haotian Liu', 'Haoying Zhou', 'Songlin Hou', 'Kazunori D Yamada', 'Gregory S. Fischer', 'Yanhua Li', 'Haichong K. Zhang', 'Ziming Zhang']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-10&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; 10 pages, 7 figures, 7 tables, this paper was accepted to IEEE/RSJ
  International Conference on Intelligent Robots and Systems (IROS) 2024&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 3D点云提升了机器人感知环境几何信息的能力，使得抓取姿态检测和场景理解等下游任务成为可能。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;问题陈述&lt;/h4&gt;：&lt;br&gt;   - 这些任务的性能严重依赖于输入数据的质量，不完整的数据会导致较差的结果和失败案例。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;现有方法&lt;/h4&gt;：&lt;br&gt;   - 近年来，针对深度学习基础的点云补全设计的训练损失函数（如Chamfer距离及其变种HyperCD）表明良好的梯度加权方案可以显著提升性能。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;挑战&lt;/h4&gt;：&lt;br&gt;   - CD类损失函数通常需要数据相关的参数调优，这对于数据量大的任务来说是耗时的。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;研究目标&lt;/h4&gt;：&lt;br&gt;   - 本研究旨在找到一系列无需参数调优的加权训练损失（称为weighted CD）。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;方法论&lt;/h4&gt;：&lt;br&gt;   - 提出了一种搜索方案“通过梯度匹配的损失蒸馏”，以模拟HyperCD和weighted CD之间的反向传播学习行为，从而找到合适的候选损失函数。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;优化策略&lt;/h4&gt;：&lt;br&gt;   - 一旦找到合适的损失函数，提出了一种新颖的双层优化公式，以基于weighted CD损失训练主干网络。&lt;br&gt;&lt;br&gt;8. &lt;h4&gt;实验结果&lt;/h4&gt;：&lt;br&gt;   - 观察到： &lt;br&gt;     - 使用适当的加权函数，weighted CD的性能可以与HyperCD相媲美。&lt;br&gt;     - Landau加权CD（称为Landau CD）在点云补全任务中表现优于HyperCD，并在多个基准数据集上取得新状态的结果。&lt;br&gt;&lt;br&gt;9. &lt;h4&gt;代码可用性&lt;/h4&gt;：&lt;br&gt;   - 提供了演示代码的链接，方便其他研究人员使用。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; https://github.com/zhang-vislab/iros2024-lossdistillationweightedcd&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D point clouds enhanced the robot's ability to perceive the geometrical
information of the environments, making it possible for many downstream tasks
such as grasp pose detection and scene understanding. The performance of these
tasks, though, heavily relies on the quality of data input, as incomplete can
lead to poor results and failure cases. Recent training loss functions designed
for deep learning-based point cloud completion, such as Chamfer distance (CD)
and its variants (\eg HyperCD ), imply a good gradient weighting scheme can
significantly boost performance. However, these CD-based loss functions usually
require data-related parameter tuning, which can be time-consuming for
data-extensive tasks. To address this issue, we aim to find a family of
weighted training losses ({\em weighted CD}) that requires no parameter tuning.
To this end, we propose a search scheme, {\em Loss Distillation via Gradient
Matching}, to find good candidate loss functions by mimicking the learning
behavior in backpropagation between HyperCD and weighted CD. Once this is done,
we propose a novel bilevel optimization formula to train the backbone network
based on the weighted CD loss. We observe that: (1) with proper weighted
functions, the weighted CD can always achieve similar performance to HyperCD,
and (2) the Landau weighted CD, namely {\em Landau CD}, can outperform HyperCD
for point cloud completion and lead to new state-of-the-art results on several
benchmark datasets. {\it Our demo code is available at
\url{https://github.com/Zhang-VISLab/IROS2024-LossDistillationWeightedCD}.}</description>
      <guid isPermaLink="false">2409.06171v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>Leveraging Reviewer Experience in Code Review Comment Generation</title>
      <link>http://arxiv.org/abs/2409.10959v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Hong Yi Lin', 'Patanamon Thongtanunam', 'Christoph Treude', 'Michael W. Godfrey', 'Chunhua Liu', 'Wachiraphan Charoenwet']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-17&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 现代代码审查是一种普遍的软件质量保证过程，旨在识别新编写代码中的潜在问题。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;挑战&lt;/h4&gt;：&lt;br&gt;   - 尽管代码审查有效，但对人类审查员来说，过程消耗大量精力。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;研究目的&lt;/h4&gt;：&lt;br&gt;   - 为减轻审查员的工作负担，研究人员训练深度学习模型模拟人类审查员生成自然语言代码审查评论，称为代码审查评论生成。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;现有方法&lt;/h4&gt;：&lt;br&gt;   - 先前的研究利用机器学习技术和神经模型（如迁移学习和变换器架构）在该任务上取得了改进。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;问题&lt;/h4&gt;：&lt;br&gt;   - 生成的评论质量仍然不理想，原因在于用于模型训练的开源代码审查数据质量较低，尤其是来自具有不同软件开发经验的审查员的反馈。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;提出的新方法&lt;/h4&gt;：&lt;br&gt;   - 为了适应这种经验差异，提出了一套经验感知训练方法，利用审查员的过往创作和审查经验作为评论质量的信号。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;经验感知损失函数（ELF）&lt;/h4&gt;：&lt;br&gt;   - 具体而言，提出了经验感知损失函数（ELF），将审查员在项目中的创作和审查权重作为模型损失函数的权重，从而增强经验丰富的审查员对模型行为的影响。&lt;br&gt;&lt;br&gt;8. &lt;h4&gt;实验结果&lt;/h4&gt;：&lt;br&gt;   - 与现有最先进模型相比，ELF能够生成更高质量的评论，在准确性、信息量和评论类型生成方面均有所提升。&lt;br&gt;&lt;br&gt;9. &lt;h4&gt;主要贡献&lt;/h4&gt;：&lt;br&gt;   - 本研究的关键贡献在于展示如何将传统软件工程概念（如审查员经验）集成到基于AI的自动化代码审查模型设计中。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern code review is a ubiquitous software quality assurance process aimed
at identifying potential issues within newly written code. Despite its
effectiveness, the process demands large amounts of effort from the human
reviewers involved. To help alleviate this workload, researchers have trained
deep learning models to imitate human reviewers in providing natural language
code reviews. Formally, this task is known as code review comment generation.
Prior work has demonstrated improvements in this task by leveraging machine
learning techniques and neural models, such as transfer learning and the
transformer architecture. However, the quality of the model generated reviews
remain sub-optimal due to the quality of the open-source code review data used
in model training. This is in part due to the data obtained from open-source
projects where code reviews are conducted in a public forum, and reviewers
possess varying levels of software development experience, potentially
affecting the quality of their feedback. To accommodate for this variation, we
propose a suite of experience-aware training methods that utilise the
reviewers' past authoring and reviewing experiences as signals for review
quality. Specifically, we propose experience-aware loss functions (ELF), which
use the reviewers' authoring and reviewing ownership of a project as weights in
the model's loss function. Through this method, experienced reviewers' code
reviews yield larger influence over the model's behaviour. Compared to the SOTA
model, ELF was able to generate higher quality reviews in terms of accuracy,
informativeness, and comment types generated. The key contribution of this work
is the demonstration of how traditional software engineering concepts such as
reviewer experience can be integrated into the design of AI-based automated
code review models.</description>
      <guid isPermaLink="false">2409.10959v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>DETECLAP: Enhancing Audio-Visual Representation Learning with Object Information</title>
      <link>http://arxiv.org/abs/2409.11729v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Shota Nakada', 'Taichi Nishimura', 'Hokuto Munakata', 'Masayoshi Kondo', 'Tatsuya Komatsu']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-18&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; under review&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 当前的音频-视觉表示学习能够捕捉粗略的物体类别（如“动物”和“乐器”），但缺乏识别细粒度细节的能力（如“狗”和“长笛”）。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;提出的方法&lt;/h4&gt;：&lt;br&gt;   - 引入DETECLAP方法，旨在增强音频-视觉表示学习的物体信息感知能力。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;核心思想&lt;/h4&gt;：&lt;br&gt;   - 将音频-视觉标签预测损失引入现有的对比音频-视觉掩蔽自编码器（Contrastive Audio-Visual Masked AutoEncoder），以提升模型的物体意识。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;标签准备&lt;/h4&gt;：&lt;br&gt;   - 为避免高成本的手动注释，利用最先进的语言-音频模型和物体检测器从音频和视觉输入中准备物体标签。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;评估方法&lt;/h4&gt;：&lt;br&gt;   - 使用VGGSound和AudioSet20K数据集评估音频-视觉检索和分类的方法。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;实验结果&lt;/h4&gt;：&lt;br&gt;   - 在音频到视觉检索中，召回率提升1.5%；在视觉到音频检索中，召回率提升1.2%；音频-视觉分类准确率提升0.6%。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current audio-visual representation learning can capture rough object
categories (e.g., ``animals'' and ``instruments''), but it lacks the ability to
recognize fine-grained details, such as specific categories like ``dogs'' and
``flutes'' within animals and instruments. To address this issue, we introduce
DETECLAP, a method to enhance audio-visual representation learning with object
information. Our key idea is to introduce an audio-visual label prediction loss
to the existing Contrastive Audio-Visual Masked AutoEncoder to enhance its
object awareness. To avoid costly manual annotations, we prepare object labels
from both audio and visual inputs using state-of-the-art language-audio models
and object detectors. We evaluate the method of audio-visual retrieval and
classification using the VGGSound and AudioSet20K datasets. Our method achieves
improvements in recall@10 of +1.5% and +1.2% for audio-to-visual and
visual-to-audio retrieval, respectively, and an improvement in accuracy of
+0.6% for audio-visual classification.</description>
      <guid isPermaLink="false">2409.11729v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>Hybrid-Task Meta-Learning: A Graph Neural Network Approach for Scalable and Transferable Bandwidth Allocation</title>
      <link>http://arxiv.org/abs/2401.10253v2</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Xin Hao', 'Changyang She', 'Phee Lep Yeoh', 'Yuhong Liu', 'Branka Vucetic', 'Yonghui Li']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2023-12-23&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究目标&lt;/h4&gt;：&lt;br&gt;   - 开发一个基于深度学习的带宽分配策略，具备以下特性：&lt;br&gt;     - 1) 可扩展性：能够适应用户数量的变化。&lt;br&gt;     - 2) 可迁移性：适用于不同通信场景，如非平稳无线信道、不同的服务质量（QoS）要求以及动态可用资源。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;可扩展性实现&lt;/h4&gt;：&lt;br&gt;   - 带宽分配策略通过图神经网络（GNN）表示，训练参数的数量不随用户数量变化，确保了可扩展性。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;通用性增强&lt;/h4&gt;：&lt;br&gt;   - 为了提升GNN的泛化能力，提出了一种混合任务元学习（HML）算法：&lt;br&gt;     - 在元训练阶段，利用不同通信场景训练GNN的初始参数。&lt;br&gt;     - 在元测试阶段，使用少量样本对GNN进行微调，以适应未见过的通信场景。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;实验结果&lt;/h4&gt;：&lt;br&gt;   - 仿真结果表明，HML方法相较于现有基准可提升初始性能8.79%，并提高采样效率73%。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;微调效果&lt;/h4&gt;：&lt;br&gt;   - 在微调后，基于GNN的近优政策在推理复杂度显著降低的情况下，能够实现接近于最佳策略的相似奖励，这一最佳策略是通过迭代优化获得的。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we develop a deep learning-based bandwidth allocation policy
that is: 1) scalable with the number of users and 2) transferable to different
communication scenarios, such as non-stationary wireless channels, different
quality-of-service (QoS) requirements, and dynamically available resources. To
support scalability, the bandwidth allocation policy is represented by a graph
neural network (GNN), with which the number of training parameters does not
change with the number of users. To enable the generalization of the GNN, we
develop a hybrid-task meta-learning (HML) algorithm that trains the initial
parameters of the GNN with different communication scenarios during
meta-training. Next, during meta-testing, a few samples are used to fine-tune
the GNN with unseen communication scenarios. Simulation results demonstrate
that our HML approach can improve the initial performance by $8.79\%$, and
sampling efficiency by $73\%$, compared with existing benchmarks. After
fine-tuning, our near-optimal GNN-based policy can achieve close to the same
reward with much lower inference complexity compared to the optimal policy
obtained using iterative optimization.</description>
      <guid isPermaLink="false">2401.10253v2</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>Multimodal Attention-Enhanced Feature Fusion-based Weekly Supervised Anomaly Violence Detection</title>
      <link>http://arxiv.org/abs/2409.11223v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Yuta Kaneko', 'Abu Saleh Musa Miah', 'Najmul Hassan', 'Hyoun-Sup Lee', 'Si-Woong Jang', 'Jungpil Shin']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-17&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 弱监督视频异常检测（WS-VAD）是计算机视觉中的关键领域，旨在开发智能监控系统。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;特征流&lt;/h4&gt;：&lt;br&gt;   - 系统使用三种特征流：RGB视频、光流和音频信号，每个流提取互补的空间和时间特征，以提高检测准确性和鲁棒性。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;RGB视频流处理&lt;/h4&gt;：&lt;br&gt;   - 第一个流采用基于注意力的多阶段特征增强方法：&lt;br&gt;     - &lt;h4&gt;第一阶段&lt;/h4&gt;：使用基于视觉变换器（ViT）的CLIP模块，结合I3D和时间上下文聚合（TCA）的丰富时空特征。&lt;br&gt;     - &lt;h4&gt;第二阶段&lt;/h4&gt;：通过不确定性调节的双重记忆单元（UR-DMU）模型有效捕捉时间依赖性，同时学习正常和异常数据的表示。&lt;br&gt;     - &lt;h4&gt;第三阶段&lt;/h4&gt;：选择最相关的时空特征。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;光流特征提取&lt;/h4&gt;：&lt;br&gt;   - 第二个流通过深度学习和注意力模块的整合，从光流数据中提取增强的时空特征。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;音频特征处理&lt;/h4&gt;：&lt;br&gt;   - 音频流使用与VGGish模型集成的注意力模块捕捉听觉线索，旨在基于声音模式检测异常。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;多模态融合&lt;/h4&gt;：&lt;br&gt;   - 通过融合不同模态的特征，模型整合运动和音频信号，这些信号通常指示无法通过视觉分析单独检测的异常事件。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;性能提升&lt;/h4&gt;：&lt;br&gt;   - 多模态融合的特征集显著提高了异常检测的准确性和鲁棒性，在三个数据集上表现出色。&lt;br&gt;&lt;br&gt;8. &lt;h4&gt;实验结果&lt;/h4&gt;：&lt;br&gt;   - 大规模实验表明，所提系统在三个基准数据集上的性能优于现有最先进的系统，证明了该方法的有效性。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Weakly supervised video anomaly detection (WS-VAD) is a crucial area in
computer vision for developing intelligent surveillance systems. This system
uses three feature streams: RGB video, optical flow, and audio signals, where
each stream extracts complementary spatial and temporal features using an
enhanced attention module to improve detection accuracy and robustness. In the
first stream, we employed an attention-based, multi-stage feature enhancement
approach to improve spatial and temporal features from the RGB video where the
first stage consists of a ViT-based CLIP module, with top-k features
concatenated in parallel with I3D and Temporal Contextual Aggregation (TCA)
based rich spatiotemporal features. The second stage effectively captures
temporal dependencies using the Uncertainty-Regulated Dual Memory Units
(UR-DMU) model, which learns representations of normal and abnormal data
simultaneously, and the third stage is employed to select the most relevant
spatiotemporal features. The second stream extracted enhanced attention-based
spatiotemporal features from the flow data modality-based feature by taking
advantage of the integration of the deep learning and attention module. The
audio stream captures auditory cues using an attention module integrated with
the VGGish model, aiming to detect anomalies based on sound patterns. These
streams enrich the model by incorporating motion and audio signals often
indicative of abnormal events undetectable through visual analysis alone. The
concatenation of the multimodal fusion leverages the strengths of each
modality, resulting in a comprehensive feature set that significantly improves
anomaly detection accuracy and robustness across three datasets. The extensive
experiment and high performance with the three benchmark datasets proved the
effectiveness of the proposed system over the existing state-of-the-art system.</description>
      <guid isPermaLink="false">2409.11223v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>Unsupervised state learning from pairs of states</title>
      <link>http://arxiv.org/abs/2409.11120v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Pranjal Agarwal', 'Nada Ali', 'Camilla Polvara', 'Martin Isbjörn Trappe', 'Berthold-Georg Englert', 'Mark Hillery']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-17&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 收到一系列量子比特（qubits），每个量子比特保证处于两种纯态之一，但具体状态未知。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;任务目标&lt;/h4&gt;：&lt;br&gt;   - 任务是确定这些状态或构建一个正算子值测量（POVM）来区分这两种状态，类似于无监督学习的量子版本。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;问题挑战&lt;/h4&gt;：&lt;br&gt;   - 在没有更多信息的情况下，仅能确定序列的密度矩阵，而密度矩阵通常可以以多种方式分解成纯态。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;额外信息的需求&lt;/h4&gt;：&lt;br&gt;   - 为了解决此问题，需要额外的信息，可能是经典信息或量子信息。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;解决方案&lt;/h4&gt;：&lt;br&gt;   - 如果提供每个量子比特的额外副本，即接收到配对的量子比特（两者处于相同状态），则可以完成任务。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;模拟实验&lt;/h4&gt;：&lt;br&gt;   - 进行了数值模拟，测量了一系列量子比特对，结果显示可以高精度地找到未知状态及其出现的概率。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Suppose you receive a sequence of qubits where each qubit is guaranteed to be
in one of two pure states, but you do not know what those states are. Your task
is to either determine the states or to construct a POVM (Positive Operator
Valued Measure) that will discriminate them. This can be viewed as a quantum
analog of unsupervised learning. A problem is that without more information,
all that can be determined is the density matrix of the sequence, and, in
general, density matrices can be decomposed into pure states in many different
ways. To solve the problem additional information, either classical or quantum,
is required. We show that if an additional copy of each qubit is supplied, that
is, one receives pairs of qubits, both in the same state, rather than single
qubits, the task can be accomplished. We then simulate numerically the
measurement of a sequence of qubit pairs and show that the unknown states and
their respective probabilities of occurrence can be found with high accuracy.</description>
      <guid isPermaLink="false">2409.11120v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>Efficiently Expanding Receptive Fields: Local Split Attention and Parallel Aggregation for Enhanced Large-scale Point Cloud Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2409.01662v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Haodong Wang', 'Chongyu Wang', 'Yinghui Quan', 'Di Wang']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-03&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 扩大深度学习模型的感受野对于大规模3D点云分割是一种有效技术，能捕获丰富的上下文信息，增强网络学习有意义特征的能力。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;挑战&lt;/h4&gt;：&lt;br&gt;   - 扩大感受野常导致计算复杂性增加和过拟合风险，影响学习效率和效果。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;提出的解决方案&lt;/h4&gt;：&lt;br&gt;   - 提出了局部分割注意力池化（LSAP）机制，通过一系列局部分割操作有效扩大感受野，便于获取更广泛的上下文知识。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;优化计算负载&lt;/h4&gt;：&lt;br&gt;   - 同时优化与注意力池化层相关的计算负载，以确保更流畅的处理流程。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;引入PAE模块&lt;/h4&gt;：&lt;br&gt;   - 基于LSAP，提出了并行聚合增强（PAE）模块，使数据的2D和3D邻域信息能够并行处理，进一步增强网络中的上下文表示。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;新框架LSNet&lt;/h4&gt;：&lt;br&gt;   - 提出了一个新框架，称为LSNet，用于大规模点云语义分割。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;实验结果&lt;/h4&gt;：&lt;br&gt;   - 广泛的评估表明，PAE模块的无缝集成显著改善了现有框架的平均交并比（mIoU）指标，提升幅度最高可达11%。&lt;br&gt;&lt;br&gt;8. &lt;h4&gt;性能比较&lt;/h4&gt;：&lt;br&gt;   - LSNet在三个基准数据集（S3DIS、Toronto3D和SensatUrban）上表现优于最先进的语义分割网络。&lt;br&gt;&lt;br&gt;9. &lt;h4&gt;计算效率&lt;/h4&gt;：&lt;br&gt;   - 相较于使用相似感受野的其他方法，LSNet实现了约38.8%的显著加速，突显了其计算效率和在真实大规模场景中的实用性。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Expanding the receptive field in a deep learning model for large-scale 3D
point cloud segmentation is an effective technique for capturing rich
contextual information, which consequently enhances the network's ability to
learn meaningful features. However, this often leads to increased computational
complexity and risk of overfitting, challenging the efficiency and
effectiveness of the learning paradigm. To address these limitations, we
propose the Local Split Attention Pooling (LSAP) mechanism to effectively
expand the receptive field through a series of local split operations, thus
facilitating the acquisition of broader contextual knowledge. Concurrently, it
optimizes the computational workload associated with attention-pooling layers
to ensure a more streamlined processing workflow. Based on LSAP, a Parallel
Aggregation Enhancement (PAE) module is introduced to enable parallel
processing of data using both 2D and 3D neighboring information to further
enhance contextual representations within the network. In light of the
aforementioned designs, we put forth a novel framework, designated as LSNet,
for large-scale point cloud semantic segmentation. Extensive evaluations
demonstrated the efficacy of seamlessly integrating the proposed PAE module
into existing frameworks, yielding significant improvements in mean
intersection over union (mIoU) metrics, with a notable increase of up to 11%.
Furthermore, LSNet demonstrated superior performance compared to
state-of-the-art semantic segmentation networks on three benchmark datasets,
including S3DIS, Toronto3D, and SensatUrban. It is noteworthy that our method
achieved a substantial speedup of approximately 38.8% compared to those
employing similar-sized receptive fields, which serves to highlight both its
computational efficiency and practical utility in real-world large-scale
scenes.</description>
      <guid isPermaLink="false">2409.01662v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>Metric-Semantic Factor Graph Generation based on Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2409.11972v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Jose Andres Millan-Romera', 'Hriday Bavle', 'Muhammad Shaheer', 'Holger Voos', 'Jose Luis Sanchez-Lopez']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-18&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; Submitted to ICRA 2025&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 理解几何结构与语义概念之间的关系对于构建复杂环境的准确模型至关重要。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;室内空间的特点&lt;/h4&gt;：&lt;br&gt;   - 在室内环境中，某些空间约束（如平面之间的相对位置）在布局变化中保持一致。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;研究目标&lt;/h4&gt;：&lt;br&gt;   - 探索如何在图形SLAM框架中捕捉这些不变关系，通过优化因子图将高层概念（如房间和墙壁）与几何元素（如平面）连接。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;现有方法的局限&lt;/h4&gt;：&lt;br&gt;   - 之前的研究主要采用针对每个概念生成的临时解决方案，并使用手动定义的因子。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;新方法的提出&lt;/h4&gt;：&lt;br&gt;   - 本文提出了一种新颖的度量-语义因子图生成方法，包括定义语义场景图、整合几何信息和学习相互连接的因子，全部基于图神经网络（GNNs）。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;边缘分类网络&lt;/h4&gt;：&lt;br&gt;   - 边缘分类网络（G-GNN）将平面之间的边缘分类为“同一房间”、“同一墙壁”或“无”类型。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;聚类生成&lt;/h4&gt;：&lt;br&gt;   - 将生成的关系进行聚类，为每个聚类生成一个房间或墙壁。&lt;br&gt;&lt;br&gt;8. &lt;h4&gt;几何起源推断&lt;/h4&gt;：&lt;br&gt;   - 第二类网络（F-GNN）推断新节点的几何起源。&lt;br&gt;&lt;br&gt;9. &lt;h4&gt;因子的定义&lt;/h4&gt;：&lt;br&gt;   - 因子的定义使用与生成节点的度量属性相同的F-GNN。&lt;br&gt;&lt;br&gt;10. &lt;h4&gt;与S-Graphs+算法的共享&lt;/h4&gt;：&lt;br&gt;    - 将新的因子图与S-Graphs+算法共享，扩展其图的表现力和场景表示，最终目标是提高SLAM性能。&lt;br&gt;&lt;br&gt;11. &lt;h4&gt;环境复杂性&lt;/h4&gt;：&lt;br&gt;    - 通过在L形房间上训练网络，将环境复杂性增加到N平面房间。&lt;br&gt;&lt;br&gt;12. &lt;h4&gt;评估方法&lt;/h4&gt;：&lt;br&gt;    - 在合成和模拟场景中进行评估，因为缺乏所需复杂布局的真实数据集。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding the relationships between geometric structures and semantic
concepts is crucial for building accurate models of complex environments. In
indoors, certain spatial constraints, such as the relative positioning of
planes, remain consistent despite variations in layout. This paper explores how
these invariant relationships can be captured in a graph SLAM framework by
representing high-level concepts like rooms and walls, linking them to
geometric elements like planes through an optimizable factor graph. Several
efforts have tackled this issue with add-hoc solutions for each concept
generation and with manually-defined factors.
  This paper proposes a novel method for metric-semantic factor graph
generation which includes defining a semantic scene graph, integrating
geometric information, and learning the interconnecting factors, all based on
Graph Neural Networks (GNNs). An edge classification network (G-GNN) sorts the
edges between planes into same room, same wall or none types. The resulting
relations are clustered, generating a room or wall for each cluster. A second
family of networks (F-GNN) infers the geometrical origin of the new nodes. The
definition of the factors employs the same F-GNN used for the metric attribute
of the generated nodes. Furthermore, share the new factor graph with the
S-Graphs+ algorithm, extending its graph expressiveness and scene
representation with the ultimate goal of improving the SLAM performance. The
complexity of the environments is increased to N-plane rooms by training the
networks on L-shaped rooms. The framework is evaluated in synthetic and
simulated scenarios as no real datasets of the required complex layouts are
available.</description>
      <guid isPermaLink="false">2409.11972v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>SpheriGait: Enriching Spatial Representation via Spherical Projection for LiDAR-based Gait Recognition</title>
      <link>http://arxiv.org/abs/2409.11869v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Yanxi Wang', 'Zhigang Chang', 'Chen Wu', 'Zihao Cheng', 'Hongmin Gao']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-18&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 步态识别是一种快速发展的远程识别个体的技术。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;现有研究局限&lt;/h4&gt;：&lt;br&gt;   - 之前的研究主要使用2D传感器收集步态数据，虽然取得了显著进展，但忽视了3D动态特征对识别的影响。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;LiDAR的优势&lt;/h4&gt;：&lt;br&gt;   - 使用LiDAR 3D点云进行步态识别不仅能直接捕捉3D空间特征，还能减少光照条件的影响，同时保护隐私。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;研究问题&lt;/h4&gt;：&lt;br&gt;   - 关键在于如何有效提取点云中的区分性3D动态表示。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;提出的方法&lt;/h4&gt;：&lt;br&gt;   - 本文提出了一种名为SpheriGait的方法，用于从点云中提取和增强动态特征，以实现基于LiDAR的步态识别。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;方法细节&lt;/h4&gt;：&lt;br&gt;   - SpheriGait用球面投影替代传统的点云平面投影方法，以增强动态特征的感知。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;网络模块&lt;/h4&gt;：&lt;br&gt;   - 提出了一个名为DAM-L的网络模块，用于从投影后的点云数据中提取步态线索。&lt;br&gt;&lt;br&gt;8. &lt;h4&gt;实验结果&lt;/h4&gt;：&lt;br&gt;   - 进行了广泛实验，结果表明SpheriGait在SUSTech1K数据集上达到了最先进的性能。&lt;br&gt;   - 验证了球面投影方法可作为通用数据预处理技术，增强其他基于LiDAR的步态识别方法的性能，展现出卓越的灵活性和实用性。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Gait recognition is a rapidly progressing technique for the remote
identification of individuals. Prior research predominantly employing 2D
sensors to gather gait data has achieved notable advancements; nonetheless,
they have unavoidably neglected the influence of 3D dynamic characteristics on
recognition. Gait recognition utilizing LiDAR 3D point clouds not only directly
captures 3D spatial features but also diminishes the impact of lighting
conditions while ensuring privacy protection.The essence of the problem lies in
how to effectively extract discriminative 3D dynamic representation from point
clouds.In this paper, we proposes a method named SpheriGait for extracting and
enhancing dynamic features from point clouds for Lidar-based gait recognition.
Specifically, it substitutes the conventional point cloud plane projection
method with spherical projection to augment the perception of dynamic
feature.Additionally, a network block named DAM-L is proposed to extract gait
cues from the projected point cloud data. We conducted extensive experiments
and the results demonstrated the SpheriGait achieved state-of-the-art
performance on the SUSTech1K dataset, and verified that the spherical
projection method can serve as a universal data preprocessing technique to
enhance the performance of other LiDAR-based gait recognition methods,
exhibiting exceptional flexibility and practicality.</description>
      <guid isPermaLink="false">2409.11869v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>IMRL: Integrating Visual, Physical, Temporal, and Geometric Representations for Enhanced Food Acquisition</title>
      <link>http://arxiv.org/abs/2409.12092v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Rui Liu', 'Zahiruddin Mahammad', 'Amisha Bhaskar', 'Pratap Tokekar']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-18&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 机器人辅助进食对改善有进食障碍个体的生活质量具有重要潜力。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;研究挑战&lt;/h4&gt;：&lt;br&gt;   - 在不同条件下获取多样化的食品并泛化到未见过的食品面临独特挑战。&lt;br&gt;   - 现有方法依赖于表面几何信息（如边界框和姿态），这些信息基于视觉线索（如颜色、形状和纹理），但适应性和鲁棒性不足。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;问题原因&lt;/h4&gt;：&lt;br&gt;   - 食品在物理属性相似但视觉外观不同的情况下，现有方法往往表现较差。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;方法介绍&lt;/h4&gt;：&lt;br&gt;   - 本文采用模仿学习（IL）来学习食品获取策略。&lt;br&gt;   - 现有方法使用IL或强化学习（RL）基于现成的图像编码器（如ResNet-50），但这些表示不够鲁棒，难以在多样化的获取场景中泛化。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;新方法的提出&lt;/h4&gt;：&lt;br&gt;   - 提出了IMRL（集成多维表示学习），该方法集成视觉、物理、时间和几何表示，以增强IL在食品获取中的鲁棒性和泛化能力。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;方法特点&lt;/h4&gt;：&lt;br&gt;   - IMRL能够捕捉食品类型和物理属性（如固体、半固体、颗粒状、液体和混合物），建模获取动作的时间动态，并引入几何信息来确定最佳舀取点和评估碗的饱和度。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;适应性&lt;/h4&gt;：&lt;br&gt;   - IMRL使IL能够根据上下文自适应调整舀取策略，从而提高机器人处理多样食品获取场景的能力。&lt;br&gt;&lt;br&gt;8. &lt;h4&gt;实验结果&lt;/h4&gt;：&lt;br&gt;   - 在真实机器人上的实验表明，所提方法在各种食品和碗配置下表现出鲁棒性和适应性，包括对未见设置的零-shot泛化。&lt;br&gt;   - 与最佳基线相比，成功率提高了最多35%。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robotic assistive feeding holds significant promise for improving the quality
of life for individuals with eating disabilities. However, acquiring diverse
food items under varying conditions and generalizing to unseen food presents
unique challenges. Existing methods that rely on surface-level geometric
information (e.g., bounding box and pose) derived from visual cues (e.g.,
color, shape, and texture) often lacks adaptability and robustness, especially
when foods share similar physical properties but differ in visual appearance.
We employ imitation learning (IL) to learn a policy for food acquisition.
Existing methods employ IL or Reinforcement Learning (RL) to learn a policy
based on off-the-shelf image encoders such as ResNet-50. However, such
representations are not robust and struggle to generalize across diverse
acquisition scenarios. To address these limitations, we propose a novel
approach, IMRL (Integrated Multi-Dimensional Representation Learning), which
integrates visual, physical, temporal, and geometric representations to enhance
the robustness and generalizability of IL for food acquisition. Our approach
captures food types and physical properties (e.g., solid, semi-solid, granular,
liquid, and mixture), models temporal dynamics of acquisition actions, and
introduces geometric information to determine optimal scooping points and
assess bowl fullness. IMRL enables IL to adaptively adjust scooping strategies
based on context, improving the robot's capability to handle diverse food
acquisition scenarios. Experiments on a real robot demonstrate our approach's
robustness and adaptability across various foods and bowl configurations,
including zero-shot generalization to unseen settings. Our approach achieves
improvement up to $35\%$ in success rate compared with the best-performing
baseline.</description>
      <guid isPermaLink="false">2409.12092v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>GLIM: 3D Range-Inertial Localization and Mapping with GPU-Accelerated Scan Matching Factors</title>
      <link>http://arxiv.org/abs/2407.10344v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Kenji Koide', 'Masashi Yokozuka', 'Shuji Oishi', 'Atsuhiko Banno']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-07-14&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; Robotics and Autonomous Systems&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究主题&lt;/h4&gt;：&lt;br&gt;   - 本文介绍了GLIM，一个结合GPU加速扫描匹配因子的3D范围惯性定位与地图构建框架。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;里程计估计模块&lt;/h4&gt;：&lt;br&gt;   - GLIM的里程计估计模块结合了固定滞后平滑和基于关键帧的点云匹配，可以处理几秒钟内完全退化的范围数据，同时有效减少轨迹估计漂移。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;多摄像头视觉特征约束&lt;/h4&gt;：&lt;br&gt;   - 模块以紧密耦合的方式整合多摄像头视觉特征约束，进一步提升稳定性和准确性。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;全局轨迹优化模块&lt;/h4&gt;：&lt;br&gt;   - 全局轨迹优化模块直接最小化整个地图上子图之间的配准误差，使得可以准确约束子图之间相对姿态，即使重叠部分较小。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;实时性能&lt;/h4&gt;：&lt;br&gt;   - 尽管里程计估计和全局轨迹优化算法的计算需求远高于现有方法，但由于对配准误差评估算法和整个系统的精心设计，使得它们能够实时运行，充分利用GPU并行处理能力。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; 10.1016/j.robot.2024.104750&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; https://github.com/koide3/glim&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This article presents GLIM, a 3D range-inertial localization and mapping
framework with GPU-accelerated scan matching factors. The odometry estimation
module of GLIM employs a combination of fixed-lag smoothing and keyframe-based
point cloud matching that makes it possible to deal with a few seconds of
completely degenerated range data while efficiently reducing trajectory
estimation drift. It also incorporates multi-camera visual feature constraints
in a tightly coupled way to further improve the stability and accuracy. The
global trajectory optimization module directly minimizes the registration
errors between submaps over the entire map. This approach enables us to
accurately constrain the relative pose between submaps with a small overlap.
Although both the odometry estimation and global trajectory optimization
algorithms require much more computation than existing methods, we show that
they can be run in real-time due to the careful design of the registration
error evaluation algorithm and the entire system to fully leverage GPU parallel
processing.</description>
      <guid isPermaLink="false">2407.10344v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>CF-PRNet: Coarse-to-Fine Prototype Refining Network for Point Cloud Completion and Reconstruction</title>
      <link>http://arxiv.org/abs/2409.08443v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Zhi Chen', 'Tianqi Wei', 'Zecheng Zhao', 'Jia Syuen Lim', 'Yadan Luo', 'Hu Zhang', 'Xin Yu', 'Scott Chapman', 'Zi Huang']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-13&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; Technical Report of the 1st place solution to CVPPA@ECCV2024: Shape
  Completion and Reconstruction of Sweet Peppers Challenge&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 在现代农业中，精确监测植物和水果对于高通量表型分析和自动化收获至关重要。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;研究问题&lt;/h4&gt;：&lt;br&gt;   - 本文解决了从部分视角重建水果准确3D形状的挑战，这在农业环境中很常见。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;方法介绍&lt;/h4&gt;：&lt;br&gt;   - 提出了CF-PRNet，一种粗细网格原型精炼网络，训练阶段利用高分辨率3D数据，但在实时推理时仅需单张RGB-D图像。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;方法流程&lt;/h4&gt;：&lt;br&gt;   - 从水果的部分视角提取不完整的点云数据，通过一系列卷积块进行处理。&lt;br&gt;   - 提取的特征用于生成缩放向量，以精炼两个逐步构建的3D网格原型：一个粗略的和一个细致的。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;精细化过程&lt;/h4&gt;：&lt;br&gt;   - 这种渐进式精炼有助于详细完成最终点云，实现精确的重建。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;性能评估&lt;/h4&gt;：&lt;br&gt;   - CF-PRNet在性能指标上表现优异，Chamfer距离为3.78，F1得分为66.76%，精度为56.56%，召回率为85.31%。&lt;br&gt;   - 在“甜椒形状补全与重建挑战”中获得第一名。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; https://github.com/uqzhichen/CF-PRNet&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In modern agriculture, precise monitoring of plants and fruits is crucial for
tasks such as high-throughput phenotyping and automated harvesting. This paper
addresses the challenge of reconstructing accurate 3D shapes of fruits from
partial views, which is common in agricultural settings. We introduce CF-PRNet,
a coarse-to-fine prototype refining network, leverages high-resolution 3D data
during the training phase but requires only a single RGB-D image for real-time
inference. Our approach begins by extracting the incomplete point cloud data
that constructed from a partial view of a fruit with a series of convolutional
blocks. The extracted features inform the generation of scaling vectors that
refine two sequentially constructed 3D mesh prototypes - one coarse and one
fine-grained. This progressive refinement facilitates the detailed completion
of the final point clouds, achieving detailed and accurate reconstructions.
CF-PRNet demonstrates excellent performance metrics with a Chamfer Distance of
3.78, an F1 Score of 66.76%, a Precision of 56.56%, and a Recall of 85.31%, and
win the first place in the Shape Completion and Reconstruction of Sweet Peppers
Challenge.</description>
      <guid isPermaLink="false">2409.08443v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>UltimateDO: An Efficient Framework to Marry Occupancy Prediction with 3D Object Detection via Channel2height</title>
      <link>http://arxiv.org/abs/2409.11160v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Zichen Yu', 'Changyong Shu']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-17&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 占用检测和3D物体检测是现代自动驾驶系统中的两个标准任务。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;现有方法的局限性&lt;/h4&gt;：&lt;br&gt;   - 目前的做法是为每个任务部署独立模型，或设计具有独立头部的多任务范式，但存在部署困难（如3D卷积、变换器等）和任务协调不足的问题。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;框架设计的目标&lt;/h4&gt;：&lt;br&gt;   - 提出应设计一个有利的框架，以实现对多种边缘芯片的便捷部署，同时保持高精度和较少的时间消耗。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;新范式的提出&lt;/h4&gt;：&lt;br&gt;   - 重新审视3D物体检测与占用预测之间的交互范式，采用2D卷积对模型进行重新构建，并优先考虑每个任务对其他任务的贡献。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;提出的方法&lt;/h4&gt;：&lt;br&gt;   - 提出了一种快速3D物体检测和占用预测的方法（UltimateDO），将轻量级占用预测头（FlashOcc）与3D物体检测网络结合，实现了仅增加1.1毫秒的额外时间消耗，同时促进了相互支持。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;实验验证&lt;/h4&gt;：&lt;br&gt;   - 在具有挑战性的nuScenes系列基准上实现UltimateDO，验证其效果。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Occupancy and 3D object detection are characterized as two standard tasks in
modern autonomous driving system. In order to deploy them on a series of edge
chips with better precision and time-consuming trade-off, contemporary
approaches either deploy standalone models for individual tasks, or design a
multi-task paradigm with separate heads. However, they might suffer from
deployment difficulties (i.e., 3D convolution, transformer and so on) or
deficiencies in task coordination. Instead, we argue that a favorable framework
should be devised in pursuit of ease deployment on diverse chips and high
precision with little time-consuming. Oriented at this, we revisit the paradigm
for interaction between 3D object detection and occupancy prediction,
reformulate the model with 2D convolution and prioritize the tasks such that
each contributes to other. Thus, we propose a method to achieve fast 3D object
detection and occupancy prediction (UltimateDO), wherein the light occupancy
prediction head in FlashOcc is married to 3D object detection network, with
negligible additional timeconsuming of only 1.1ms while facilitating each
other. We instantiate UltimateDO on the challenging nuScenes-series benchmarks.</description>
      <guid isPermaLink="false">2409.11160v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>Domain Generalization through Meta-Learning: A Survey</title>
      <link>http://arxiv.org/abs/2404.02785v3</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Arsham Gholamzadeh Khoee', 'Yinan Yu', 'Robert Feldt']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-04-03&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 深度神经网络（DNNs）在人工智能领域取得了革命性进展，但在处理分布外（OOD）数据时常表现不佳。&lt;br&gt;   - OOD数据是由于现实应用中不可避免的领域变化导致的常见情况。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;问题描述&lt;/h4&gt;：&lt;br&gt;   - DNNs通常假设训练和测试数据具有相同的分布，但这一假设在实践中常常被违反。&lt;br&gt;   - 尽管DNNs在大数据和计算能力上表现出色，但在分布变化和有限标记数据的情况下，容易导致过拟合和在不同任务和领域中的泛化能力较差。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;元学习的潜力&lt;/h4&gt;：&lt;br&gt;   - 元学习是一种有前景的方法，通过使用算法获取不同任务之间的可转移知识，实现快速适应，避免每个任务从头学习的需求。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;论文重点&lt;/h4&gt;：&lt;br&gt;   - 本文重点探讨元学习在领域泛化中的贡献，首先阐明元学习的概念，并基于特征提取策略和分类器学习方法引入一种新分类法，为方法提供更细致的视角。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;决策图的设计&lt;/h4&gt;：&lt;br&gt;   - 提出了一个决策图，帮助读者根据数据可用性和领域变化导航分类法，从而选择和开发适合其特定问题需求的模型。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;文献综述&lt;/h4&gt;：&lt;br&gt;   - 通过对现有方法及其理论的全面回顾，阐明该领域的基础知识。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;研究方向&lt;/h4&gt;：&lt;br&gt;   - 本文提供了实用的见解，并对有前景的研究方向进行了深入讨论。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep neural networks (DNNs) have revolutionized artificial intelligence but
often lack performance when faced with out-of-distribution (OOD) data, a common
scenario due to the inevitable domain shifts in real-world applications. This
limitation stems from the common assumption that training and testing data
share the same distribution--an assumption frequently violated in practice.
Despite their effectiveness with large amounts of data and computational power,
DNNs struggle with distributional shifts and limited labeled data, leading to
overfitting and poor generalization across various tasks and domains.
Meta-learning presents a promising approach by employing algorithms that
acquire transferable knowledge across various tasks for fast adaptation,
eliminating the need to learn each task from scratch. This survey paper delves
into the realm of meta-learning with a focus on its contribution to domain
generalization. We first clarify the concept of meta-learning for domain
generalization and introduce a novel taxonomy based on the feature extraction
strategy and the classifier learning methodology, offering a granular view of
methodologies. Additionally, we present a decision graph to assist readers in
navigating the taxonomy based on data availability and domain shifts, enabling
them to select and develop a proper model tailored to their specific problem
requirements. Through an exhaustive review of existing methods and underlying
theories, we map out the fundamentals of the field. Our survey provides
practical insights and an informed discussion on promising research directions.</description>
      <guid isPermaLink="false">2404.02785v3</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>Learning a Terrain- and Robot-Aware Dynamics Model for Autonomous Mobile Robot Navigation</title>
      <link>http://arxiv.org/abs/2409.11452v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Jan Achterhold', 'Suresh Guttikonda', 'Jens U. Kreber', 'Haolong Li', 'Joerg Stueckler']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-17&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; Submitted to Robotics and Autonomous Systems. arXiv admin note:
  substantial text overlap with arXiv:2307.09206&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 移动机器人需要能够规划成本有效的路径以实现自主导航。&lt;br&gt;   - 机器人和地形的属性可能会有所变化，例如地形的摩擦力可能在不同位置不同。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;挑战&lt;/h4&gt;：&lt;br&gt;   - 机器人属性（如负载或磨损）也会变化，这可能导致执行器增益或关节摩擦的变化。&lt;br&gt;   - 自主导航方法需要能够适应这些变化。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;提出的方法&lt;/h4&gt;：&lt;br&gt;   - 本文提出了一种新颖的学习方法，构建一个概率的、考虑地形和机器人属性的前向动力学模型（TRADYN）。&lt;br&gt;   - TRADYN能够适应这些变化，并展示其在导航中的应用。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;学习方法的创新&lt;/h4&gt;：&lt;br&gt;   - 该学习方法基于近年在移动机器人导航中利用神经过程的元学习的进展。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;实验评估&lt;/h4&gt;：&lt;br&gt;   - 在模拟环境中评估该方法，针对具有单轮动力学的机器人进行2D导航，地形具有空间变化的摩擦系数。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;性能对比&lt;/h4&gt;：&lt;br&gt;   - 实验结果表明，TRADYN在长时间预测中相比于未适应机器人或地形变化的模型消融版本具有更低的预测误差。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;规划控制的应用&lt;/h4&gt;：&lt;br&gt;   - 在模型预测控制框架下对导航规划进行评估，考虑不同噪声来源。&lt;br&gt;   - 结果显示，该方法在规划控制有效路径时，通过考虑机器人和地形属性，提升了性能。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mobile robots should be capable of planning cost-efficient paths for
autonomous navigation. Typically, the terrain and robot properties are subject
to variations. For instance, properties of the terrain such as friction may
vary across different locations. Also, properties of the robot may change such
as payloads or wear and tear, e.g., causing changing actuator gains or joint
friction. Autonomous navigation approaches should thus be able to adapt to such
variations. In this article, we propose a novel approach for learning a
probabilistic, terrain- and robot-aware forward dynamics model (TRADYN) which
can adapt to such variations and demonstrate its use for navigation. Our
learning approach extends recent advances in meta-learning forward dynamics
models based on Neural Processes for mobile robot navigation. We evaluate our
method in simulation for 2D navigation of a robot with uni-cycle dynamics with
varying properties on terrain with spatially varying friction coefficients. In
our experiments, we demonstrate that TRADYN has lower prediction error over
long time horizons than model ablations which do not adapt to robot or terrain
variations. We also evaluate our model for navigation planning in a
model-predictive control framework and under various sources of noise. We
demonstrate that our approach yields improved performance in planning
control-efficient paths by taking robot and terrain properties into account.</description>
      <guid isPermaLink="false">2409.11452v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>Multimodal Generalized Category Discovery</title>
      <link>http://arxiv.org/abs/2409.11624v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Yuchang Su', 'Renping Zhou', 'Siyu Huang', 'Xingjian Li', 'Tianyang Wang', 'Ziyue Wang', 'Min Xu']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-18&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究目标&lt;/h4&gt;：&lt;br&gt;   - &lt;h4&gt;通用类别发现（GCD）&lt;/h4&gt;：旨在将输入分类为已知类别和新类别，这对开放世界的科学发现至关重要。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;现有方法的局限性&lt;/h4&gt;：&lt;br&gt;   - 当前的GCD方法仅限于单模态数据，未能考虑大多数真实世界数据的多模态特性。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;研究创新&lt;/h4&gt;：&lt;br&gt;   - 本文将GCD扩展到多模态设置，不同模态的输入提供更丰富和互补的信息。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;关键挑战&lt;/h4&gt;：&lt;br&gt;   - 理论分析和实证验证表明，多模态GCD的主要挑战在于有效对齐不同模态之间的异质信息。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;提出的新框架&lt;/h4&gt;：&lt;br&gt;   - 提出了MM-GCD，一个新颖的框架，通过对比学习和蒸馏技术对齐不同模态的特征和输出空间。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;性能表现&lt;/h4&gt;：&lt;br&gt;   - MM-GCD在UPMC-Food101和N24News数据集上达到了新的最先进性能，分别超越了之前的方法11.5%和4.7%。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generalized Category Discovery (GCD) aims to classify inputs into both known
and novel categories, a task crucial for open-world scientific discoveries.
However, current GCD methods are limited to unimodal data, overlooking the
inherently multimodal nature of most real-world data. In this work, we extend
GCD to a multimodal setting, where inputs from different modalities provide
richer and complementary information. Through theoretical analysis and
empirical validation, we identify that the key challenge in multimodal GCD lies
in effectively aligning heterogeneous information across modalities. To address
this, we propose MM-GCD, a novel framework that aligns both the feature and
output spaces of different modalities using contrastive learning and
distillation techniques. MM-GCD achieves new state-of-the-art performance on
the UPMC-Food101 and N24News datasets, surpassing previous methods by 11.5\%
and 4.7\%, respectively.</description>
      <guid isPermaLink="false">2409.11624v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>SiamMo: Siamese Motion-Centric 3D Object Tracking</title>
      <link>http://arxiv.org/abs/2408.01688v2</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Yuxiang Yang', 'Yingqi Deng', 'Jing Zhang', 'Hongjie Gu', 'Zhekang Dong']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-08-03&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 当前的3D单目标跟踪方法主要依赖于Siamese匹配基础的范式，但在处理无纹理和不完整的LiDAR点云时存在困难。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;新方法的提出&lt;/h4&gt;：&lt;br&gt;   - 本文介绍了SiamMo，一种新颖且简单的Siamese运动中心跟踪方法。 &lt;br&gt;&lt;br&gt;3. &lt;h4&gt;创新点&lt;/h4&gt;：&lt;br&gt;   - 与传统的单流架构不同，SiamMo采用Siamese特征提取，使特征提取与时间融合解耦，从而显著提升跟踪性能。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;模块设计&lt;/h4&gt;：&lt;br&gt;   - 设计了一个时空特征聚合模块，以多尺度整合Siamese特征，有效捕捉运动信息。&lt;br&gt;   - 引入了一个基于框的信息编码模块，将物体大小先验编码到运动估计中。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;简化流程&lt;/h4&gt;：&lt;br&gt;   - SiamMo是一个纯粹的运动中心跟踪器，省去了额外的步骤，如分割和框优化。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;性能表现&lt;/h4&gt;：&lt;br&gt;   - SiamMo在多个基准测试中超越了最先进的方法，并在复杂场景中表现出色。&lt;br&gt;   - 在KITTI跟踪基准上，SiamMo以90.1%的精度创下新纪录，同时保持108 FPS的高推理速度。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;代码可获取性&lt;/h4&gt;：&lt;br&gt;   - 代码将在 https://github.com/HDU-VRLab/SiamMo 发布。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; https://github.com/hdu-vrlab/siammo&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current 3D single object tracking methods primarily rely on the Siamese
matching-based paradigm, which struggles with textureless and incomplete LiDAR
point clouds. Conversely, the motion-centric paradigm avoids appearance
matching, thus overcoming these issues. However, its complex multi-stage
pipeline and the limited temporal modeling capability of a single-stream
architecture constrain its potential. In this paper, we introduce SiamMo, a
novel and simple Siamese motion-centric tracking approach. Unlike the
traditional single-stream architecture, we employ Siamese feature extraction
for motion-centric tracking. This decouples feature extraction from temporal
fusion, significantly enhancing tracking performance. Additionally, we design a
Spatio-Temporal Feature Aggregation module to integrate Siamese features at
multiple scales, capturing motion information effectively. We also introduce a
Box-aware Feature Encoding module to encode object size priors into motion
estimation. SiamMo is a purely motion-centric tracker that eliminates the need
for additional processes like segmentation and box refinement. Without whistles
and bells, SiamMo not only surpasses state-of-the-art methods across multiple
benchmarks but also demonstrates exceptional robustness in challenging
scenarios. SiamMo sets a new record on the KITTI tracking benchmark with 90.1\%
precision while maintaining a high inference speed of 108 FPS. The code will be
released at https://github.com/HDU-VRLab/SiamMo.</description>
      <guid isPermaLink="false">2408.01688v2</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>Bridging Domain Gap for Flight-Ready Spaceborne Vision</title>
      <link>http://arxiv.org/abs/2409.11661v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Tae Ha Park', "Simone D'Amico"]&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-18&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; Submitted to Journal of Spacecraft and Rockets; Appeared as Chapter 4
  of Tae Ha Park's PhD thesis&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究目标&lt;/h4&gt;：&lt;br&gt;   - 本文介绍了航天器姿态网络第3版（SPNv3），一种用于已知非合作目标航天器的单目姿态估计的神经网络（NN）。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;设计特点&lt;/h4&gt;：&lt;br&gt;   - 与现有文献不同，SPNv3旨在高效计算的同时，对在地面离线训练和验证中未观察到的太空图像具有鲁棒性。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;重要性&lt;/h4&gt;：&lt;br&gt;   - 这些特性对于在空间级边缘设备上部署神经网络至关重要。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;关键设计选择&lt;/h4&gt;：&lt;br&gt;   - 通过细致的神经网络设计选择实现这些特性，进行的广泛权衡分析揭示了数据增强、迁移学习和视觉变换器架构等特性，这些特性有助于同时最大化鲁棒性和最小化计算开销。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;实验结果&lt;/h4&gt;：&lt;br&gt;   - 实验表明，最终的SPNv3在来自机器人测试平台的硬件在环图像上能够实现最先进的姿态精度，且训练完全基于计算机生成的合成图像，这有效地弥合了合成图像与真实图像之间的领域差距。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;性能表现&lt;/h4&gt;：&lt;br&gt;   - SPNv3在代表性的图形处理单元系统上运行频率远高于现代卫星导航滤波器的更新频率，并具有飞行遗产。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;应用范围&lt;/h4&gt;：&lt;br&gt;   - 总体而言，SPNv3是一个高效、适合飞行的神经网络模型，适用于多种近距离会合和与目标在轨物体的接近操作。&lt;br&gt;&lt;br&gt;8. &lt;h4&gt;代码可获取性&lt;/h4&gt;：&lt;br&gt;   - SPNv3的代码实现将公开提供。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work presents Spacecraft Pose Network v3 (SPNv3), a Neural Network (NN)
for monocular pose estimation of a known, non-cooperative target spacecraft. As
opposed to existing literature, SPNv3 is designed and trained to be
computationally efficient while providing robustness to spaceborne images that
have not been observed during offline training and validation on the ground.
These characteristics are essential to deploying NNs on space-grade edge
devices. They are achieved through careful NN design choices, and an extensive
trade-off analysis reveals features such as data augmentation, transfer
learning and vision transformer architecture as a few of those that contribute
to simultaneously maximizing robustness and minimizing computational overhead.
Experiments demonstrate that the final SPNv3 can achieve state-of-the-art pose
accuracy on hardware-in-the-loop images from a robotic testbed while having
trained exclusively on computer-generated synthetic images, effectively
bridging the domain gap between synthetic and real imagery. At the same time,
SPNv3 runs well above the update frequency of modern satellite navigation
filters when tested on a representative graphical processing unit system with
flight heritage. Overall, SPNv3 is an efficient, flight-ready NN model readily
applicable to a wide range of close-range rendezvous and proximity operations
with target resident space objects. The code implementation of SPNv3 will be
made publicly available.</description>
      <guid isPermaLink="false">2409.11661v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>Registration between Point Cloud Streams and Sequential Bounding Boxes via Gradient Descent</title>
      <link>http://arxiv.org/abs/2409.09312v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Xuesong Li', 'Xinge Zhu', 'Yuexin Ma', 'Subhan Khan', 'Jose Guivant']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-14&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究目的&lt;/h4&gt;：&lt;br&gt;   - 本文提出了一种算法，用于将序列边界框与点云流进行配准。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;方法背景&lt;/h4&gt;：&lt;br&gt;   - 不同于常见的点云配准技术，该方法利用边界框的特性（如大小、形状和时间信息）来辅助对齐，从而提供显著的支持和性能提升。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;新方法提出&lt;/h4&gt;：&lt;br&gt;   - 提出了一个新的方法来解决该问题，具体通过整体目标函数来建模配准过程，该函数包含最终目标和所有约束。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;优化方法&lt;/h4&gt;：&lt;br&gt;   - 使用梯度下降法对目标函数进行优化。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;实验结果&lt;/h4&gt;：&lt;br&gt;   - 实验结果显示，所提方法在IoU（交并比）方面有40%的改善，并且在点云流与序列边界框之间的配准上表现出更强的鲁棒性。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we propose an algorithm for registering sequential bounding
boxes with point cloud streams. Unlike popular point cloud registration
techniques, the alignment of the point cloud and the bounding box can rely on
the properties of the bounding box, such as size, shape, and temporal
information, which provides substantial support and performance gains.
Motivated by this, we propose a new approach to tackle this problem.
Specifically, we model the registration process through an overall objective
function that includes the final goal and all constraints. We then optimize the
function using gradient descent. Our experiments show that the proposed method
performs remarkably well with a 40\% improvement in IoU and demonstrates more
robust registration between point cloud streams and sequential bounding boxes</description>
      <guid isPermaLink="false">2409.09312v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>RUIE: Retrieval-based Unified Information Extraction using Large Language Model</title>
      <link>http://arxiv.org/abs/2409.11673v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Xincheng Liao', 'Junwen Duan', 'Yixi Huang', 'Jianxin Wang']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-18&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; 14 pages, 3 figures&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 统一信息提取（UIE）旨在通过单一模型或框架完成所有信息提取任务。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;现有方法的局限性&lt;/h4&gt;：&lt;br&gt;   - 之前的研究主要集中在使用构建数据集对大语言模型（LLMs）进行指令调优，这些方法需要大量计算资源，且在未见任务上泛化能力较差。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;新方法提出&lt;/h4&gt;：&lt;br&gt;   - 本文提出了RUIE（基于检索的统一信息提取）框架，通过利用上下文学习来实现快速泛化，同时降低计算成本。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;关键挑战&lt;/h4&gt;：&lt;br&gt;   - RUIE的关键挑战在于选择最有利于LLMs有效处理多样化信息提取任务的示例。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;解决方案&lt;/h4&gt;：&lt;br&gt;   - 通过整合LLMs对候选示例的偏好进行排名，并设计一个增强关键词的奖励模型，以捕捉查询与示例之间的细粒度关系。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;训练方法&lt;/h4&gt;：&lt;br&gt;   - 通过对比学习和知识蒸馏训练用于UIE的双编码器检索器。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;创新点&lt;/h4&gt;：&lt;br&gt;   - 据我们所知，RUIE是首个可训练的检索框架，用于统一信息提取。&lt;br&gt;&lt;br&gt;8. &lt;h4&gt;实验结果&lt;/h4&gt;：&lt;br&gt;   - 在8个保留数据集上的实验结果表明，RUIE在泛化到未见任务方面表现有效，平均F1分数分别比指令调优方法提高了19.22，比其他检索器提高了3.13。&lt;br&gt;&lt;br&gt;9. &lt;h4&gt;进一步分析&lt;/h4&gt;：&lt;br&gt;   - 进一步的分析确认了RUIE对不同规模的LLMs的适应性以及其关键组件的重要性。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unified information extraction (UIE) aims to complete all information
extraction tasks using a single model or framework. While previous work has
primarily focused on instruction-tuning large language models (LLMs) with
constructed datasets, these methods require significant computational resources
and struggle to generalize to unseen tasks. To address these limitations, we
propose RUIE (Retrieval-based Unified Information Extraction), a framework that
leverages in-context learning to enable rapid generalization while reducing
computational costs. The key challenge in RUIE is selecting the most beneficial
demonstrations for LLMs to effectively handle diverse IE tasks. To achieve
this, we integrate LLM preferences for ranking candidate demonstrations and
design a keyword-enhanced reward model to capture fine-grained relationships
between queries and demonstrations. We then train a bi-encoder retriever for
UIE through contrastive learning and knowledge distillation. To the best of our
knowledge, RUIE is the first trainable retrieval framework for UIE.
Experimental results on 8 held-out datasets demonstrate RUIE's effectiveness in
generalizing to unseen tasks, with average F1-score improvements of 19.22 and
3.13 compared to instruction-tuning methods and other retrievers, respectively.
Further analysis confirms RUIE's adaptability to LLMs of varying sizes and the
importance of its key components.</description>
      <guid isPermaLink="false">2409.11673v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>Fusion in Context: A Multimodal Approach to Affective State Recognition</title>
      <link>http://arxiv.org/abs/2409.11906v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Youssef Mohamed', 'Severin Lemaignan', 'Arzu Guneysu', 'Patric Jensfelt', 'Christian Smith']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-18&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 准确识别人类情感是情感计算和人机交互（HRI）中的关键挑战。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;情感的重要性&lt;/h4&gt;：&lt;br&gt;   - 情感状态在塑造行为、决策和社会交往中起着重要作用。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;上下文影响&lt;/h4&gt;：&lt;br&gt;   - 情感表达受上下文因素影响，如果不考虑这些因素，可能会导致误解。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;多模态融合的前景&lt;/h4&gt;：&lt;br&gt;   - 结合面部表情、语音和生理信号等多种模态的多模态融合，已显示出在提高情感识别方面的潜力。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;提出的新方法&lt;/h4&gt;：&lt;br&gt;   - 本文提出了一种基于变压器的多模态融合方法，利用面部热数据、面部动作单元和文本上下文信息进行上下文感知的情感识别。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;模态特定编码器&lt;/h4&gt;：&lt;br&gt;   - 探索针对特定模态的编码器，以学习定制的表示。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;融合与处理&lt;/h4&gt;：&lt;br&gt;   - 采用加法融合将不同模态的表示结合，并通过共享的变压器编码器处理，以捕捉时间依赖性和交互作用。&lt;br&gt;&lt;br&gt;8. &lt;h4&gt;实验设计&lt;/h4&gt;：&lt;br&gt;   - 提出的方法在一个设计用于诱发各种情感状态的具体桌面“吃豆人”游戏中进行评估。&lt;br&gt;&lt;br&gt;9. &lt;h4&gt;实验结果&lt;/h4&gt;：&lt;br&gt;   - 结果证明了结合上下文信息和多模态融合在情感状态识别中的有效性。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate recognition of human emotions is a crucial challenge in affective
computing and human-robot interaction (HRI). Emotional states play a vital role
in shaping behaviors, decisions, and social interactions. However, emotional
expressions can be influenced by contextual factors, leading to
misinterpretations if context is not considered. Multimodal fusion, combining
modalities like facial expressions, speech, and physiological signals, has
shown promise in improving affect recognition. This paper proposes a
transformer-based multimodal fusion approach that leverages facial thermal
data, facial action units, and textual context information for context-aware
emotion recognition. We explore modality-specific encoders to learn tailored
representations, which are then fused using additive fusion and processed by a
shared transformer encoder to capture temporal dependencies and interactions.
The proposed method is evaluated on a dataset collected from participants
engaged in a tangible tabletop Pacman game designed to induce various affective
states. Our results demonstrate the effectiveness of incorporating contextual
information and multimodal fusion for affective state recognition.</description>
      <guid isPermaLink="false">2409.11906v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>Topological Deep Learning with State-Space Models: A Mamba Approach for Simplicial Complexes</title>
      <link>http://arxiv.org/abs/2409.12033v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Marco Montagna', 'Simone Scardapane', 'Lev Telyatnikov']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-18&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 基于消息传递（MP）机制的图神经网络是处理图结构数据的主要方法。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;现有方法的局限性&lt;/h4&gt;：&lt;br&gt;   - 这些网络仅能建模成对交互，难以明确捕捉具有$n$-体关系的系统复杂性。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;新兴领域&lt;/h4&gt;：&lt;br&gt;   - 拓扑深度学习作为一个有前景的领域，专注于使用各种拓扑领域（如单纯形和细胞复合体）研究和建模高阶交互。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;新挑战&lt;/h4&gt;：&lt;br&gt;   - 尽管新领域提供了强大的表示能力，但也带来了新的挑战，如如何通过高阶消息传递有效建模高阶结构间的交互。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;序列建模的有效性&lt;/h4&gt;：&lt;br&gt;   - 结构状态空间序列模型在序列建模中表现出色，最近通过将节点的邻域编码为序列，适应于图数据，避免了消息传递机制。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;新方法提出&lt;/h4&gt;：&lt;br&gt;   - 本文提出了一种新颖的架构，旨在与单纯形复合体共同工作，利用Mamba状态空间模型作为基础。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;节点序列生成&lt;/h4&gt;：&lt;br&gt;   - 我们的方法基于邻近单元生成节点的序列，允许所有高阶结构之间的直接通信，无论其等级如何。&lt;br&gt;&lt;br&gt;8. &lt;h4&gt;模型验证&lt;/h4&gt;：&lt;br&gt;   - 我们对模型进行了广泛验证，结果表明其在单纯形复合体上的性能与最先进的模型相当。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks based on the message-passing (MP) mechanism are a
dominant approach for handling graph-structured data. However, they are
inherently limited to modeling only pairwise interactions, making it difficult
to explicitly capture the complexity of systems with $n$-body relations. To
address this, topological deep learning has emerged as a promising field for
studying and modeling higher-order interactions using various topological
domains, such as simplicial and cellular complexes. While these new domains
provide powerful representations, they introduce new challenges, such as
effectively modeling the interactions among higher-order structures through
higher-order MP. Meanwhile, structured state-space sequence models have proven
to be effective for sequence modeling and have recently been adapted for graph
data by encoding the neighborhood of a node as a sequence, thereby avoiding the
MP mechanism. In this work, we propose a novel architecture designed to operate
with simplicial complexes, utilizing the Mamba state-space model as its
backbone. Our approach generates sequences for the nodes based on the
neighboring cells, enabling direct communication between all higher-order
structures, regardless of their rank. We extensively validate our model,
demonstrating that it achieves competitive performance compared to
state-of-the-art models developed for simplicial complexes.</description>
      <guid isPermaLink="false">2409.12033v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>When 3D Partial Points Meets SAM: Tooth Point Cloud Segmentation with Sparse Labels</title>
      <link>http://arxiv.org/abs/2409.01691v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Yifan Liu', 'Wuyang Li', 'Cheng Wang', 'Hui Chen', 'Yixuan Yuan']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-03&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; To appear at MICCAI24&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 牙齿点云分割是许多正畸应用中的基础任务。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;现有方法问题&lt;/h4&gt;：&lt;br&gt;   - 当前研究主要集中在完全监督学习上，这需要昂贵且繁琐的手动逐点标注。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;弱监督学习的局限&lt;/h4&gt;：&lt;br&gt;   - 虽然最近提出了一些弱监督替代方法，利用弱标签进行三维分割并取得了一定成果，但在标签极其稀疏时，它们的表现往往不佳。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;新方法提出&lt;/h4&gt;：&lt;br&gt;   - 基于Segment Anything Model (SAM) 的强大提示分割能力，提出了名为SAMTooth的框架，以补充极其稀疏的监督。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;提示生成策略&lt;/h4&gt;：&lt;br&gt;   - 引入了一种新颖的“基于置信度的提示生成”策略，通过粗略类别预测与置信度过滤相结合，自动生成适当的点提示。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;特征学习的优化&lt;/h4&gt;：&lt;br&gt;   - 为充分利用SAM输出中的结构和形状线索，提出了一种“掩膜引导表示学习”，将生成的牙齿掩膜重投影到三维空间，并约束不同牙齿的点具有独特的表示。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;实验结果&lt;/h4&gt;：&lt;br&gt;   - 在公共数据集上进行实验，发现仅使用0.1%的标注（每颗牙齿一个点），该方法的性能显著优于最近的弱监督方法，并且与最新的完全监督方法相当。&lt;br&gt;&lt;br&gt;8. &lt;h4&gt;应用潜力&lt;/h4&gt;：&lt;br&gt;   - 展示了在稀疏标签下应用SAM于三维感知任务的显著潜力。&lt;br&gt;&lt;br&gt;9. &lt;h4&gt;代码可获取性&lt;/h4&gt;：&lt;br&gt;   - 代码可在GitHub上获得，链接为 https://github.com/CUHK-AIM-Group/SAMTooth。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tooth point cloud segmentation is a fundamental task in many orthodontic
applications. Current research mainly focuses on fully supervised learning
which demands expensive and tedious manual point-wise annotation. Although
recent weakly-supervised alternatives are proposed to use weak labels for 3D
segmentation and achieve promising results, they tend to fail when the labels
are extremely sparse. Inspired by the powerful promptable segmentation
capability of the Segment Anything Model (SAM), we propose a framework named
SAMTooth that leverages such capacity to complement the extremely sparse
supervision. To automatically generate appropriate point prompts for SAM, we
propose a novel Confidence-aware Prompt Generation strategy, where coarse
category predictions are aggregated with confidence-aware filtering.
Furthermore, to fully exploit the structural and shape clues in SAM's outputs
for assisting the 3D feature learning, we advance a Mask-guided Representation
Learning that re-projects the generated tooth masks of SAM into 3D space and
constrains these points of different teeth to possess distinguished
representations. To demonstrate the effectiveness of the framework, we conduct
experiments on the public dataset and surprisingly find with only 0.1\%
annotations (one point per tooth), our method can surpass recent weakly
supervised methods by a large margin, and the performance is even comparable to
the recent fully-supervised methods, showcasing the significant potential of
applying SAM to 3D perception tasks with sparse labels. Code is available at
https://github.com/CUHK-AIM-Group/SAMTooth.</description>
      <guid isPermaLink="false">2409.01691v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>Differentiable Collision-Supervised Tooth Arrangement Network with a Decoupling Perspective</title>
      <link>http://arxiv.org/abs/2409.11937v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Zhihui He', 'Chengyuan Wang', 'Shidong Yang', 'Li Chen', 'Yanheng Zhou', 'Shuo Wang']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-18&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; 16 pages, 13 figures&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 牙齿排列是数字正畸规划过程中的重要步骤。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;现有方法问题&lt;/h4&gt;：&lt;br&gt;   - 现有基于学习的方法直接回归牙齿运动，使用隐藏牙齿特征，这样会导致目标姿态感知和运动回归的耦合，可能导致三维变换的感知不佳。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;忽视问题&lt;/h4&gt;：&lt;br&gt;   - 现有方法忽略了预测牙齿排列中的重叠或间隙，这在实际应用中通常是不可接受的。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;新方法提出&lt;/h4&gt;：&lt;br&gt;   - 提出了DTAN（可微分碰撞监督牙齿排列网络），通过解耦预测任务和特征建模来解决这些问题。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;任务解耦&lt;/h4&gt;：&lt;br&gt;   - DTAN首先预测最终牙齿姿态的隐藏特征，然后利用这些特征辅助回归起始和目标牙齿之间的运动，从而解耦牙齿排列任务。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;特征学习&lt;/h4&gt;：&lt;br&gt;   - DTAN将隐藏特征进一步解耦为几何特征和位置特征，并通过特征一致性约束进行监督，以提高特征学习效果。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;新损失函数&lt;/h4&gt;：&lt;br&gt;   - 提出了新颖的可微分碰撞损失函数，用于点云数据，约束牙齿之间的相关姿态，这一方法可扩展至其他三维点云任务。&lt;br&gt;&lt;br&gt;8. &lt;h4&gt;可控性增强&lt;/h4&gt;：&lt;br&gt;   - 提出了基于弓宽的牙齿排列网络C-DTAN，以增强结果的可控性。&lt;br&gt;&lt;br&gt;9. &lt;h4&gt;性能提升&lt;/h4&gt;：&lt;br&gt;   - 构建了三个不同的牙齿排列数据集，并与现有方法相比，在准确性和速度上实现了显著提升。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tooth arrangement is an essential step in the digital orthodontic planning
process. Existing learning-based methods use hidden teeth features to directly
regress teeth motions, which couples target pose perception and motion
regression. It could lead to poor perceptions of three-dimensional
transformation. They also ignore the possible overlaps or gaps between teeth of
predicted dentition, which is generally unacceptable. Therefore, we propose
DTAN, a differentiable collision-supervised tooth arrangement network,
decoupling predicting tasks and feature modeling. DTAN decouples the tooth
arrangement task by first predicting the hidden features of the final teeth
poses and then using them to assist in regressing the motions between the
beginning and target teeth. To learn the hidden features better, DTAN also
decouples the teeth-hidden features into geometric and positional features,
which are further supervised by feature consistency constraints. Furthermore,
we propose a novel differentiable collision loss function for point cloud data
to constrain the related gestures between teeth, which can be easily extended
to other 3D point cloud tasks. We propose an arch-width guided tooth
arrangement network, named C-DTAN, to make the results controllable. We
construct three different tooth arrangement datasets and achieve drastically
improved performance on accuracy and speed compared with existing methods.</description>
      <guid isPermaLink="false">2409.11937v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>Self-Contrastive Forward-Forward Algorithm</title>
      <link>http://arxiv.org/abs/2409.11593v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Xing Chen', 'Dongshu Liu', 'Jeremie Laydevant', 'Julie Grollier']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-17&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - Forward-Forward (FF) 算法是一种新颖的前向模式学习方法，能够逐层和局部地更新权重，支持监督和无监督学习。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;应用领域&lt;/h4&gt;：&lt;br&gt;   - FF算法适用于脑启发学习、低功耗硬件神经网络和大规模模型的分布式学习等应用。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;现存挑战&lt;/h4&gt;：&lt;br&gt;   - 虽然FF在手写数字识别任务中表现良好，但在自然图像和时间序列任务中的性能仍然存在挑战。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;主要限制&lt;/h4&gt;：&lt;br&gt;   - 一个关键限制是需要生成高质量的负样本以用于对比学习，尤其是在无监督任务中，目前缺乏多样化的解决方案。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;新方法提出&lt;/h4&gt;：&lt;br&gt;   - 提出了自对比前向-前向（Self-Contrastive Forward-Forward, SCFF）方法，受自监督对比学习的启发。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;样本生成能力&lt;/h4&gt;：&lt;br&gt;   - SCFF能够生成适用于不同数据集的正负样本，超越现有的局部前向算法，在无监督分类准确性上取得更好效果。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;实验结果&lt;/h4&gt;：&lt;br&gt;   - 在MNIST（MLP: 98.7%）、CIFAR-10（CNN: 80.75%）和STL-10（CNN: 77.3%）上，SCFF的表现超过了现有算法。&lt;br&gt;&lt;br&gt;8. &lt;h4&gt;新应用领域&lt;/h4&gt;：&lt;br&gt;   - SCFF是首个支持FF训练递归神经网络的方法，为更复杂的任务以及连续时间的视频和文本处理打开了新的可能性。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Forward-Forward (FF) algorithm is a recent, purely forward-mode learning
method, that updates weights locally and layer-wise and supports supervised as
well as unsupervised learning. These features make it ideal for applications
such as brain-inspired learning, low-power hardware neural networks, and
distributed learning in large models. However, while FF has shown promise on
written digit recognition tasks, its performance on natural images and
time-series remains a challenge. A key limitation is the need to generate
high-quality negative examples for contrastive learning, especially in
unsupervised tasks, where versatile solutions are currently lacking. To address
this, we introduce the Self-Contrastive Forward-Forward (SCFF) method, inspired
by self-supervised contrastive learning. SCFF generates positive and negative
examples applicable across different datasets, surpassing existing local
forward algorithms for unsupervised classification accuracy on MNIST (MLP:
98.7%), CIFAR-10 (CNN: 80.75%), and STL-10 (CNN: 77.3%). Additionally, SCFF is
the first to enable FF training of recurrent neural networks, opening the door
to more complex tasks and continuous-time video and text processing.</description>
      <guid isPermaLink="false">2409.11593v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>Location based Probabilistic Load Forecasting of EV Charging Sites: Deep Transfer Learning with Multi-Quantile Temporal Convolutional Network</title>
      <link>http://arxiv.org/abs/2409.11862v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Mohammad Wazed Ali', 'Asif bin Mustafa', 'Md. Aukerul Moin Shuvo', 'Bernhard Sick']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-18&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; 11 pages, 10 figures&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 电动汽车（EVs）的电气化是减少化石燃料使用和环境污染的潜在方法。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;EV类型和用户群体&lt;/h4&gt;：&lt;br&gt;   - 各种类型的电动汽车适用于不同的运输模式（包括空中、水上和陆地），并且不同的用户群体（通勤者、商业用户、家庭用户、驾驶员）使用不同的充电基础设施（公共、私人、家庭和工作场所）。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;使用模式和能量需求&lt;/h4&gt;：&lt;br&gt;   - EV的使用模式和能量需求具有很大的随机性，因此需要对充电需求进行特征化和预测，以防止电力短缺。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;现有模型限制&lt;/h4&gt;：&lt;br&gt;   - 之前开发的数据驱动负载模型仅限于特定用例和地点，缺乏足够的适应性，无法在多样化位置的充电站之间有效进行日预测知识转移。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;新方法提出&lt;/h4&gt;：&lt;br&gt;   - 本文提出了一种基于位置的EV充电站负载预测方法，采用深度多分位时间卷积网络（MQ-TCN），以克服早期模型的限制。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;实验数据来源&lt;/h4&gt;：&lt;br&gt;   - 实验数据来自四个充电站：Caltech、JPL、Office-1和NREL，这些站点有不同的EV用户类型，包括学生、全职和兼职员工、随机访客等。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;模型性能&lt;/h4&gt;：&lt;br&gt;   - 在JPL充电站，所提出的深度MQ-TCN模型在日负载预测中的预测区间覆盖概率（PICP）得分为93.62%，比XGBoost模型提高了28.93%。&lt;br&gt;&lt;br&gt;8. &lt;h4&gt;知识转移能力&lt;/h4&gt;：&lt;br&gt;   - 通过采用归纳转移学习（TL）方法，MQ-TCN模型在NREL站点的负载预测任务中仅使用两周数据就达到了96.88%的PICP得分。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electrification of vehicles is a potential way of reducing fossil fuel usage
and thus lessening environmental pollution. Electric Vehicles (EVs) of various
types for different transport modes (including air, water, and land) are
evolving. Moreover, different EV user groups (commuters, commercial or domestic
users, drivers) may use different charging infrastructures (public, private,
home, and workplace) at various times. Therefore, usage patterns and energy
demand are very stochastic. Characterizing and forecasting the charging demand
of these diverse EV usage profiles is essential in preventing power outages.
Previously developed data-driven load models are limited to specific use cases
and locations. None of these models are simultaneously adaptive enough to
transfer knowledge of day-ahead forecasting among EV charging sites of diverse
locations, trained with limited data, and cost-effective. This article presents
a location-based load forecasting of EV charging sites using a deep
Multi-Quantile Temporal Convolutional Network (MQ-TCN) to overcome the
limitations of earlier models. We conducted our experiments on data from four
charging sites, namely Caltech, JPL, Office-1, and NREL, which have diverse EV
user types like students, full-time and part-time employees, random visitors,
etc. With a Prediction Interval Coverage Probability (PICP) score of 93.62\%,
our proposed deep MQ-TCN model exhibited a remarkable 28.93\% improvement over
the XGBoost model for a day-ahead load forecasting at the JPL charging site. By
transferring knowledge with the inductive Transfer Learning (TL) approach, the
MQ-TCN model achieved a 96.88\% PICP score for the load forecasting task at the
NREL site using only two weeks of data.</description>
      <guid isPermaLink="false">2409.11862v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>RealDiff: Real-world 3D Shape Completion using Self-Supervised Diffusion Models</title>
      <link>http://arxiv.org/abs/2409.10180v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Başak Melis Öcal', 'Maxim Tatarchenko', 'Sezer Karaoglu', 'Theo Gevers']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-16&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 点云补全旨在从部分观察中恢复物体的完整3D形状。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;现有方法限制&lt;/h4&gt;：&lt;br&gt;   - 尽管依赖于合成形状先验的方法在该领域取得了良好效果，但其在真实数据中的适用性和泛化能力仍然有限。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;新方法提出&lt;/h4&gt;：&lt;br&gt;   - 提出了一个自监督框架，称为RealDiff，将点云补全问题公式化为直接基于真实测量的条件生成问题。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;处理噪声观察&lt;/h4&gt;：&lt;br&gt;   - 为了更好地处理噪声观察，RealDiff利用额外的几何线索，而不依赖于合成数据进行训练。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;扩散过程模拟&lt;/h4&gt;：&lt;br&gt;   - RealDiff在缺失的物体部分模拟扩散过程，同时根据部分输入条件生成，以应对任务的多模态特性。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;训练正则化&lt;/h4&gt;：&lt;br&gt;   - 通过将我们方法预测的物体轮廓和深度图与外部估计的结果进行匹配，进一步正则化训练过程。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;实验结果&lt;/h4&gt;：&lt;br&gt;   - 实验结果表明，我们的方法在真实世界点云补全任务中始终优于最先进的方法。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud completion aims to recover the complete 3D shape of an object
from partial observations. While approaches relying on synthetic shape priors
achieved promising results in this domain, their applicability and
generalizability to real-world data are still limited. To tackle this problem,
we propose a self-supervised framework, namely RealDiff, that formulates point
cloud completion as a conditional generation problem directly on real-world
measurements. To better deal with noisy observations without resorting to
training on synthetic data, we leverage additional geometric cues.
Specifically, RealDiff simulates a diffusion process at the missing object
parts while conditioning the generation on the partial input to address the
multimodal nature of the task. We further regularize the training by matching
object silhouettes and depth maps, predicted by our method, with the externally
estimated ones. Experimental results show that our method consistently
outperforms state-of-the-art methods in real-world point cloud completion.</description>
      <guid isPermaLink="false">2409.10180v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>3D Geometric Shape Assembly via Efficient Point Cloud Matching</title>
      <link>http://arxiv.org/abs/2407.10542v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Nahyuk Lee', 'Juhong Min', 'Junha Lee', 'Seungwook Kim', 'Kanghee Lee', 'Jaesik Park', 'Minsu Cho']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-07-15&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; Accepted to ICML 2024&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 组装几何形状成为更大目标结构的能力在多个实际应用中至关重要。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;研究目标&lt;/h4&gt;：&lt;br&gt;   - 本文旨在通过在部分形状的点云之间建立局部对应关系来解决这一问题，分为粗略和精细两个层面。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;新方法介绍&lt;/h4&gt;：&lt;br&gt;   - 引入了Proxy Match Transform (PMT)，这是一种近似的高阶特征变换层，能够在低内存和计算成本下实现部件 mating surfaces 的可靠匹配。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;框架构建&lt;/h4&gt;：&lt;br&gt;   - 基于PMT，提出了一种新框架，称为Proxy Match TransformeR (PMTR)，专门用于几何组装任务。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;实验评估&lt;/h4&gt;：&lt;br&gt;   - 在大规模3D几何形状组装基准数据集“Breaking Bad”上评估PMTR，结果显示其在性能和效率上优于现有的最先进方法。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;项目资源&lt;/h4&gt;：&lt;br&gt;   - 项目页面可访问：[PMTR](https://nahyuklee.github.io/pmtr)。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning to assemble geometric shapes into a larger target structure is a
pivotal task in various practical applications. In this work, we tackle this
problem by establishing local correspondences between point clouds of part
shapes in both coarse- and fine-levels. To this end, we introduce Proxy Match
Transform (PMT), an approximate high-order feature transform layer that enables
reliable matching between mating surfaces of parts while incurring low costs in
memory and computation. Building upon PMT, we introduce a new framework, dubbed
Proxy Match TransformeR (PMTR), for the geometric assembly task. We evaluate
the proposed PMTR on the large-scale 3D geometric shape assembly benchmark
dataset of Breaking Bad and demonstrate its superior performance and efficiency
compared to state-of-the-art methods. Project page:
https://nahyuklee.github.io/pmtr.</description>
      <guid isPermaLink="false">2407.10542v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>MoRAG -- Multi-Fusion Retrieval Augmented Generation for Human Motion</title>
      <link>http://arxiv.org/abs/2409.12140v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Kalakonda Sai Shashank', 'Shubh Maheshwari', 'Ravi Kiran Sarvadevabhatla']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-18&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究介绍&lt;/h4&gt;：&lt;br&gt;   - 本文提出MoRAG，一种基于多部分融合的检索增强生成策略，用于文本基础的人类动作生成。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;方法优势&lt;/h4&gt;：&lt;br&gt;   - 该方法通过改进的动作检索过程，增强了动作扩散模型的性能。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;大语言模型应用&lt;/h4&gt;：&lt;br&gt;   - 通过有效提示大语言模型（LLMs），解决了动作检索中的拼写错误和重述问题。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;多部分检索策略&lt;/h4&gt;：&lt;br&gt;   - 利用多部分检索策略，提高了在语言空间中的动作检索的普适性。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;样本生成&lt;/h4&gt;：&lt;br&gt;   - 通过空间组合检索到的动作，创造多样的动作样本。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;低级运动信息&lt;/h4&gt;：&lt;br&gt;   - 通过利用低级、部件特定的运动信息，可以为未见的文本描述构建动作样本。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;实验结果&lt;/h4&gt;：&lt;br&gt;   - 实验表明，该框架可以作为即插即用模块，提升动作扩散模型的性能。&lt;br&gt;&lt;br&gt;8. &lt;h4&gt;资源共享&lt;/h4&gt;：&lt;br&gt;   - 代码、预训练模型和示例视频将会在网站上提供：[MoRAG](https://motion-rag.github.io/)。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce MoRAG, a novel multi-part fusion based retrieval-augmented
generation strategy for text-based human motion generation. The method enhances
motion diffusion models by leveraging additional knowledge obtained through an
improved motion retrieval process. By effectively prompting large language
models (LLMs), we address spelling errors and rephrasing issues in motion
retrieval. Our approach utilizes a multi-part retrieval strategy to improve the
generalizability of motion retrieval across the language space. We create
diverse samples through the spatial composition of the retrieved motions.
Furthermore, by utilizing low-level, part-specific motion information, we can
construct motion samples for unseen text descriptions. Our experiments
demonstrate that our framework can serve as a plug-and-play module, improving
the performance of motion diffusion models. Code, pretrained models and sample
videos will be made available at: https://motion-rag.github.io/</description>
      <guid isPermaLink="false">2409.12140v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>Enhancing the Reliability of LiDAR Point Cloud Sampling: A Colorization and Super-Resolution Approach Based on LiDAR-Generated Images</title>
      <link>http://arxiv.org/abs/2409.11532v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Sier Ha', 'Honghao Du', 'Xianjia Yu', 'Jian Song', 'Tomi Westerlund']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-17&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; 9 pages&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - LiDAR（激光雷达）技术在机器人和自主系统中是关键传感器，近年来取得显著进展。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;技术改进&lt;/h4&gt;：&lt;br&gt;   - LiDAR的改进包括点云分辨率提升和提供360度低分辨率图像的能力。&lt;br&gt;   - 这些图像编码了深度、反射率和近红外光等多种数据。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;问题陈述&lt;/h4&gt;：&lt;br&gt;   - 点密度过高及传统点云采样可能导致问题，尤其在LiDAR里程计等应用中，误导性点和几何信息退化会引起漂移错误。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;研究方向&lt;/h4&gt;：&lt;br&gt;   - 当前研究重点在利用LiDAR生成的图像提升情境感知。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;文献回顾&lt;/h4&gt;：&lt;br&gt;   - 本文回顾了当前深度学习技术，包括上色和超分辨率，这些技术通常用于传统计算机视觉任务。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;方法应用&lt;/h4&gt;：&lt;br&gt;   - 将这些技术应用于LiDAR生成的图像，并进行定性分析。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;新方法提出&lt;/h4&gt;：&lt;br&gt;   - 基于分析，开发了一种新方法，选择性地整合最适合的上色和超分辨率方法与LiDAR图像，以从LiDAR点云中采样可靠点。&lt;br&gt;&lt;br&gt;8. &lt;h4&gt;目标&lt;/h4&gt;：&lt;br&gt;   - 该方法旨在提高点云配准的准确性，避免因缺乏几何信息导致的匹配错误，从而增强LiDAR系统在实际应用中的效用和精度。&lt;br&gt;&lt;br&gt;9. &lt;h4&gt;实验评估&lt;/h4&gt;：&lt;br&gt;   - 评估结果表明，所提方法相比于之前的工作表现优越，减少了平移和旋转误差，并使用了更少的点。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, Light Detection and Ranging (LiDAR) technology, a critical
sensor in robotics and autonomous systems, has seen significant advancements.
These improvements include enhanced resolution of point clouds and the
capability to provide 360{\deg} low-resolution images. These images encode
various data such as depth, reflectivity, and near-infrared light within the
pixels. However, an excessive density of points and conventional point cloud
sampling can be counterproductive, particularly in applications such as LiDAR
odometry, where misleading points and degraded geometry information may induce
drift errors. Currently, extensive research efforts are being directed towards
leveraging LiDAR-generated images to improve situational awareness. This paper
presents a comprehensive review of current deep learning (DL) techniques,
including colorization and super-resolution, which are traditionally utilized
in conventional computer vision tasks. These techniques are applied to
LiDAR-generated images and are analyzed qualitatively. Based on this analysis,
we have developed a novel approach that selectively integrates the most suited
colorization and super-resolution methods with LiDAR imagery to sample reliable
points from the LiDAR point cloud. This approach aims to not only improve the
accuracy of point cloud registration but also avoid mismatching caused by
lacking geometry information, thereby augmenting the utility and precision of
LiDAR systems in practical applications. In our evaluation, the proposed
approach demonstrates superior performance compared to our previous work,
achieving lower translation and rotation errors with a reduced number of
points.</description>
      <guid isPermaLink="false">2409.11532v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>DynaMo: In-Domain Dynamics Pretraining for Visuo-Motor Control</title>
      <link>http://arxiv.org/abs/2409.12192v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Zichen Jeff Cui', 'Hengkai Pan', 'Aadhithya Iyer', 'Siddhant Haldar', 'Lerrel Pinto']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-18&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 模仿学习被证明是训练复杂的视动政策（visuomotor policies）的有效工具。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;现有方法的局限性&lt;/h4&gt;：&lt;br&gt;   - 当前方法通常需要数百到数千个专家示例来处理高维视觉观测。&lt;br&gt;   - 数据效率低的主要原因是视觉表示主要是在域外数据上预训练或通过行为克隆目标直接训练。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;新方法提出&lt;/h4&gt;：&lt;br&gt;   - 本文提出DynaMo，一种新的域内自监督学习方法，用于学习视觉表示。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;方法概述&lt;/h4&gt;：&lt;br&gt;   - 在一组专家示例的基础上，联合学习潜在逆动态模型和前向动态模型，预测潜在空间中的下一个帧。&lt;br&gt;   - 该方法不依赖于增强、对比采样或真实动作的访问。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;数据需求&lt;/h4&gt;：&lt;br&gt;   - DynaMo不需要任何域外数据，如互联网数据集或跨实体数据集。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;实验验证&lt;/h4&gt;：&lt;br&gt;   - 在六个模拟和真实环境中，使用DynaMo学习的表示在下游模仿学习性能上显著优于先前的自监督学习目标和预训练表示。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;适用性&lt;/h4&gt;：&lt;br&gt;   - DynaMo的性能提升适用于多种策略类别，如行为变换器（Behavior Transformer）、扩散策略（Diffusion Policy）、多层感知器（MLP）和最近邻（nearest neighbors）。&lt;br&gt;&lt;br&gt;8. &lt;h4&gt;消融研究&lt;/h4&gt;：&lt;br&gt;   - 最后，论文对DynaMo的关键组件进行了消融研究，并测量其对下游策略性能的影响。&lt;br&gt;&lt;br&gt;9. &lt;h4&gt;附加资源&lt;/h4&gt;：&lt;br&gt;   - 机器人视频可在指定网站上观看：[DynaMo](https://dynamo-ssl.github.io)。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Imitation learning has proven to be a powerful tool for training complex
visuomotor policies. However, current methods often require hundreds to
thousands of expert demonstrations to handle high-dimensional visual
observations. A key reason for this poor data efficiency is that visual
representations are predominantly either pretrained on out-of-domain data or
trained directly through a behavior cloning objective. In this work, we present
DynaMo, a new in-domain, self-supervised method for learning visual
representations. Given a set of expert demonstrations, we jointly learn a
latent inverse dynamics model and a forward dynamics model over a sequence of
image embeddings, predicting the next frame in latent space, without
augmentations, contrastive sampling, or access to ground truth actions.
Importantly, DynaMo does not require any out-of-domain data such as Internet
datasets or cross-embodied datasets. On a suite of six simulated and real
environments, we show that representations learned with DynaMo significantly
improve downstream imitation learning performance over prior self-supervised
learning objectives, and pretrained representations. Gains from using DynaMo
hold across policy classes such as Behavior Transformer, Diffusion Policy, MLP,
and nearest neighbors. Finally, we ablate over key components of DynaMo and
measure its impact on downstream policy performance. Robot videos are best
viewed at https://dynamo-ssl.github.io</description>
      <guid isPermaLink="false">2409.12192v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>RockTrack: A 3D Robust Multi-Camera-Ken Multi-Object Tracking Framework</title>
      <link>http://arxiv.org/abs/2409.11749v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Xiaoyu Li', 'Peidong Li', 'Lijun Zhao', 'Dedong Liu', 'Jinghan Gao', 'Xian Wu', 'Yitao Wu', 'Dixiao Cui']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-18&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; RockTrack establishes a new state-of-the-art with 59.1% AMOTA on the
  nuScenes vision-only test leaderboard with ResNet50-level backbone&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 3D多目标跟踪（MOT）在3D物体检测的快速进展，尤其是在成本效益高的多摄像头设置下，取得了显著的性能提升。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;现有方法的局限性&lt;/h4&gt;：&lt;br&gt;   - 目前的端到端训练方法导致多摄像头跟踪器变得特定于检测器，限制了其通用性。&lt;br&gt;   - 现有通用跟踪器忽视了多摄像头检测器的独特特征，如运动观测的不可靠性和视觉信息的可行性。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;新方法提出&lt;/h4&gt;：&lt;br&gt;   - 本文提出RockTrack，一种针对多摄像头检测器的3D MOT方法。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;方法框架&lt;/h4&gt;：&lt;br&gt;   - RockTrack遵循检测跟踪（Tracking-By-Detection）框架，兼容多种现成的检测器。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;预处理模块&lt;/h4&gt;：&lt;br&gt;   - 引入信心引导的预处理模块，从单个检测器中提取可靠的运动和图像观测，利用不同的表示空间。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;观测融合&lt;/h4&gt;：&lt;br&gt;   - 这些观测在关联模块中融合，利用几何和外观线索最小化不匹配。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;匹配传播&lt;/h4&gt;：&lt;br&gt;   - 生成的匹配通过分阶段估计过程传播，为启发式噪声建模奠定基础。&lt;br&gt;&lt;br&gt;8. &lt;h4&gt;相似度度量&lt;/h4&gt;：&lt;br&gt;   - 提出了一种新颖的外观相似度度量，用于在多摄像头环境中明确表征对象的亲和力。&lt;br&gt;&lt;br&gt;9. &lt;h4&gt;实验结果&lt;/h4&gt;：&lt;br&gt;   - RockTrack在nuScenes视觉跟踪排行榜上实现了59.1%的平均多目标跟踪准确率（AMOTA），表现出卓越的计算效率。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Multi-Object Tracking (MOT) obtains significant performance improvements
with the rapid advancements in 3D object detection, particularly in
cost-effective multi-camera setups. However, the prevalent end-to-end training
approach for multi-camera trackers results in detector-specific models,
limiting their versatility. Moreover, current generic trackers overlook the
unique features of multi-camera detectors, i.e., the unreliability of motion
observations and the feasibility of visual information. To address these
challenges, we propose RockTrack, a 3D MOT method for multi-camera detectors.
Following the Tracking-By-Detection framework, RockTrack is compatible with
various off-the-shelf detectors. RockTrack incorporates a confidence-guided
preprocessing module to extract reliable motion and image observations from
distinct representation spaces from a single detector. These observations are
then fused in an association module that leverages geometric and appearance
cues to minimize mismatches. The resulting matches are propagated through a
staged estimation process, forming the basis for heuristic noise modeling.
Additionally, we introduce a novel appearance similarity metric for explicitly
characterizing object affinities in multi-camera settings. RockTrack achieves
state-of-the-art performance on the nuScenes vision-only tracking leaderboard
with 59.1% AMOTA while demonstrating impressive computational efficiency.</description>
      <guid isPermaLink="false">2409.11749v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>Privacy Challenges in Meta-Learning: An Investigation on Model-Agnostic Meta-Learning</title>
      <link>http://arxiv.org/abs/2406.00249v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Mina Rafiei', 'Mohammadmahdi Maheri', 'Hamid R. Rabiee']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-06-01&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究主题&lt;/h4&gt;：&lt;br&gt;   - 论文讨论元学习（meta-learning），涉及多个学习者在数据受限环境中协作。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;当前方法概述&lt;/h4&gt;：&lt;br&gt;   - 当前的元学习方法中，任务学习者从敏感数据（称为支持集）中局部学习模型。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;信息共享机制&lt;/h4&gt;：&lt;br&gt;   - 任务学习者使用另一部分数据（称为查询集）计算模型相关信息（如梯度或损失值），并与元学习者共享这些信息。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;元学习者的作用&lt;/h4&gt;：&lt;br&gt;   - 元学习者利用共享的信息更新其元知识。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;隐私问题&lt;/h4&gt;：&lt;br&gt;   - 尽管没有显式的数据共享，隐私问题依然存在。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;研究重点&lt;/h4&gt;：&lt;br&gt;   - 本文重点检视一个重要的元学习算法——模型无关元学习（MAML）中的潜在数据泄漏。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;梯度共享分析&lt;/h4&gt;：&lt;br&gt;   - 在MAML中，梯度在元学习者与任务学习者之间共享，研究的主要目标是分析梯度及其包含的任务数据集信息。&lt;br&gt;&lt;br&gt;8. &lt;h4&gt;会员推断攻击&lt;/h4&gt;：&lt;br&gt;   - 提出针对包含支持集和查询集的任务数据集的会员推断攻击。&lt;br&gt;&lt;br&gt;9. &lt;h4&gt;噪声注入方法&lt;/h4&gt;：&lt;br&gt;   - 探索多种噪声注入方法，以保护任务数据的隐私并抵御潜在攻击。&lt;br&gt;&lt;br&gt;10. &lt;h4&gt;实验结果&lt;/h4&gt;：&lt;br&gt;    - 实验结果表明，对MAML的攻击是有效的，而适当的噪声注入方法在抵御这些攻击方面也有效。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Meta-learning involves multiple learners, each dedicated to specific tasks,
collaborating in a data-constrained setting. In current meta-learning methods,
task learners locally learn models from sensitive data, termed support sets.
These task learners subsequently share model-related information, such as
gradients or loss values, which is computed using another part of the data
termed query set, with a meta-learner. The meta-learner employs this
information to update its meta-knowledge. Despite the absence of explicit data
sharing, privacy concerns persist. This paper examines potential data leakage
in a prominent metalearning algorithm, specifically Model-Agnostic
Meta-Learning (MAML). In MAML, gradients are shared between the metalearner and
task-learners. The primary objective is to scrutinize the gradient and the
information it encompasses about the task dataset. Subsequently, we endeavor to
propose membership inference attacks targeting the task dataset containing
support and query sets. Finally, we explore various noise injection methods
designed to safeguard the privacy of task data and thwart potential attacks.
Experimental results demonstrate the effectiveness of these attacks on MAML and
the efficacy of proper noise injection methods in countering them.</description>
      <guid isPermaLink="false">2406.00249v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>RoboMorph: In-Context Meta-Learning for Robot Dynamics Modeling</title>
      <link>http://arxiv.org/abs/2409.11815v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Manuel Bianchi Bazzi', 'Asad Ali Shahid', 'Christopher Agia', 'John Alora', 'Marco Forgione', 'Dario Piga', 'Francesco Braghin', 'Marco Pavone', 'Loris Roveda']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-18&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 深度学习领域经历了重大变革，Transformer架构的广泛应用，特别是在自然语言处理（NLP）领域。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;应用现状&lt;/h4&gt;：&lt;br&gt;   - 物理应用（如求解偏微分方程和图像视觉）有了新的探索，但在机器人领域，尤其是面临高非线性挑战时，Transformer的应用仍然稀少。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;现有局限&lt;/h4&gt;：&lt;br&gt;   - 现有研究多集中于利用Transformers为机器人提供高层次任务知识，较少关注系统识别。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;新方法提出&lt;/h4&gt;：&lt;br&gt;   - 本文提出了一种新方法，使用Transformer架构学习高维物理系统（如Franka机器人臂）的元动态模型，无需先验的物理参数知识。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;研究目标&lt;/h4&gt;：&lt;br&gt;   - 目标是根据每个关节的扭矩信号预测关心的量（如末端执行器位姿和关节位置）。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;应用价值&lt;/h4&gt;：&lt;br&gt;   - 这种预测可作为深度模型预测控制框架中的一个组成部分。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;模型功能&lt;/h4&gt;：&lt;br&gt;   - 元模型建立了扭矩与位置之间的关联，并预测完整轨迹的输出。&lt;br&gt;&lt;br&gt;8. &lt;h4&gt;实证证据&lt;/h4&gt;：&lt;br&gt;   - 该研究提供了在上下文学习范式有效性的实证证据，暗示未来在没有显式物理参数知识的情况下学习机器人系统动态的改进。&lt;br&gt;&lt;br&gt;9. &lt;h4&gt;附加资源&lt;/h4&gt;：&lt;br&gt;   - 代码、视频和补充材料可在项目网站找到：[RoboMorph](https://sites.google.com/view/robomorph/)。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The landscape of Deep Learning has experienced a major shift with the
pervasive adoption of Transformer-based architectures, particularly in Natural
Language Processing (NLP). Novel avenues for physical applications, such as
solving Partial Differential Equations and Image Vision, have been explored.
However, in challenging domains like robotics, where high non-linearity poses
significant challenges, Transformer-based applications are scarce. While
Transformers have been used to provide robots with knowledge about high-level
tasks, few efforts have been made to perform system identification. This paper
proposes a novel methodology to learn a meta-dynamical model of a
high-dimensional physical system, such as the Franka robotic arm, using a
Transformer-based architecture without prior knowledge of the system's physical
parameters. The objective is to predict quantities of interest (end-effector
pose and joint positions) given the torque signals for each joint. This
prediction can be useful as a component for Deep Model Predictive Control
frameworks in robotics. The meta-model establishes the correlation between
torques and positions and predicts the output for the complete trajectory. This
work provides empirical evidence of the efficacy of the in-context learning
paradigm, suggesting future improvements in learning the dynamics of robotic
systems without explicit knowledge of physical parameters. Code, videos, and
supplementary materials can be found at project website. See
https://sites.google.com/view/robomorph/</description>
      <guid isPermaLink="false">2409.11815v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>VoxelTrack: Exploring Voxel Representation for 3D Point Cloud Object Tracking</title>
      <link>http://arxiv.org/abs/2408.02263v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Yuxuan Lu', 'Jiahao Nie', 'Zhiwei He', 'Hongjie Gu', 'Xudong Lv']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-08-05&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 当前基于LiDAR点云的3D单目标跟踪（SOT）方法通常依赖于点基表示网络。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;现有方法的局限性&lt;/h4&gt;：&lt;br&gt;   - 这些网络存在一些基本问题：&lt;br&gt;     1. &lt;h4&gt;池化操作问题&lt;/h4&gt;：池化操作用于处理无序点云，限制了对有助于跟踪的3D空间信息的捕捉。&lt;br&gt;     2. &lt;h4&gt;集合抽象操作问题&lt;/h4&gt;：采用的集合抽象操作难以处理密度不一致的点云，阻碍了3D空间信息的建模。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;新方法提出&lt;/h4&gt;：&lt;br&gt;   - 提出了一个新颖的跟踪框架，称为VoxelTrack。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;方法概述&lt;/h4&gt;：&lt;br&gt;   - VoxelTrack通过将无序点云体素化为3D体素，并通过稀疏卷积块提取特征，有效建模精确且稳健的3D空间信息，从而指导被跟踪物体的准确位置预测。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;双流编码器&lt;/h4&gt;：&lt;br&gt;   - VoxelTrack结合了双流编码器和交叉迭代特征融合模块，以进一步探索细粒度的3D空间信息进行跟踪。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;模型简化&lt;/h4&gt;：&lt;br&gt;   - 由于准确的3D空间信息建模，VoxelTrack简化了跟踪管道，仅使用单一回归损失。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;实验结果&lt;/h4&gt;：&lt;br&gt;   - 在KITTI、NuScenes和Waymo Open Dataset三个广泛采用的数据集上进行了广泛实验。&lt;br&gt;   - 实验结果表明，VoxelTrack在这三个数据集上分别达到了88.3%、71.4%和63.6%的平均精度，表现出最先进的性能。&lt;br&gt;&lt;br&gt;8. &lt;h4&gt;实时性能&lt;/h4&gt;：&lt;br&gt;   - 在单个TITAN RTX GPU上以36帧每秒的速度超越了现有跟踪器。&lt;br&gt;&lt;br&gt;9. &lt;h4&gt;开放获取&lt;/h4&gt;：&lt;br&gt;   - 源代码和模型将会发布。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current LiDAR point cloud-based 3D single object tracking (SOT) methods
typically rely on point-based representation network. Despite demonstrated
success, such networks suffer from some fundamental problems: 1) It contains
pooling operation to cope with inherently disordered point clouds, hindering
the capture of 3D spatial information that is useful for tracking, a regression
task. 2) The adopted set abstraction operation hardly handles
density-inconsistent point clouds, also preventing 3D spatial information from
being modeled. To solve these problems, we introduce a novel tracking
framework, termed VoxelTrack. By voxelizing inherently disordered point clouds
into 3D voxels and extracting their features via sparse convolution blocks,
VoxelTrack effectively models precise and robust 3D spatial information,
thereby guiding accurate position prediction for tracked objects. Moreover,
VoxelTrack incorporates a dual-stream encoder with cross-iterative feature
fusion module to further explore fine-grained 3D spatial information for
tracking. Benefiting from accurate 3D spatial information being modeled, our
VoxelTrack simplifies tracking pipeline with a single regression loss.
Extensive experiments are conducted on three widely-adopted datasets including
KITTI, NuScenes and Waymo Open Dataset. The experimental results confirm that
VoxelTrack achieves state-of-the-art performance (88.3%, 71.4% and 63.6% mean
precision on the three datasets, respectively), and outperforms the existing
trackers with a real-time speed of 36 Fps on a single TITAN RTX GPU. The source
code and model will be released.</description>
      <guid isPermaLink="false">2408.02263v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>JEAN: Joint Expression and Audio-guided NeRF-based Talking Face Generation</title>
      <link>http://arxiv.org/abs/2409.12156v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Sai Tanmay Reddy Chakkera', 'Aggelina Chatziagapi', 'Dimitris Samaras']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-18&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; Accepted by BMVC 2024. Project Page:
  https://starc52.github.io/publications/2024-07-19-JEAN&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究目的&lt;/h4&gt;：&lt;br&gt;   - 提出了一种新方法，用于联合生成表情和音频引导的说话人脸。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;现有方法的局限性&lt;/h4&gt;：&lt;br&gt;   - 近期的方法要么难以保持说话者的身份，要么无法生成真实的面部表情。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;新方法概述&lt;/h4&gt;：&lt;br&gt;   - 本文提出了一种基于NeRF的网络，以应对上述挑战。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;训练数据与表示学习&lt;/h4&gt;：&lt;br&gt;   - 网络在没有任何真实标签的单目视频上训练，因此必须学习音频和表情的解耦表示。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;音频特征学习&lt;/h4&gt;：&lt;br&gt;   - 首先以自监督方式学习音频特征，使用来自多个主体的语音片段。&lt;br&gt;   - 通过对比学习技术，确保学习到的音频特征与唇部运动对齐，并与面部其余部分的肌肉运动解耦。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;表情特征学习&lt;/h4&gt;：&lt;br&gt;   - 设计了一种基于变换器的架构，学习表情特征，捕捉长距离面部表情，并将其与特定于语言的口部动作解耦。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;评估结果&lt;/h4&gt;：&lt;br&gt;   - 通过定量和定性评估，证明了该方法能够合成高保真度的说话人脸视频，实现了最先进的面部表情转移和与未见音频的唇部同步。&lt;br&gt;&lt;br&gt;8. &lt;h4&gt;成果意义&lt;/h4&gt;：&lt;br&gt;   - 方法在生成自然的说话人脸视频方面表现出色，具有重要的应用潜力。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce a novel method for joint expression and audio-guided talking
face generation. Recent approaches either struggle to preserve the speaker
identity or fail to produce faithful facial expressions. To address these
challenges, we propose a NeRF-based network. Since we train our network on
monocular videos without any ground truth, it is essential to learn
disentangled representations for audio and expression. We first learn audio
features in a self-supervised manner, given utterances from multiple subjects.
By incorporating a contrastive learning technique, we ensure that the learned
audio features are aligned to the lip motion and disentangled from the muscle
motion of the rest of the face. We then devise a transformer-based architecture
that learns expression features, capturing long-range facial expressions and
disentangling them from the speech-specific mouth movements. Through
quantitative and qualitative evaluation, we demonstrate that our method can
synthesize high-fidelity talking face videos, achieving state-of-the-art facial
expression transfer along with lip synchronization to unseen audio.</description>
      <guid isPermaLink="false">2409.12156v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>Panoptic-Depth Forecasting</title>
      <link>http://arxiv.org/abs/2409.12008v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Juana Valeria Hurtado', 'Riya Mohan', 'Abhinav Valada']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-18&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 预测场景的语义和三维结构对机器人安全导航和行动规划至关重要。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;现有方法的局限性&lt;/h4&gt;：&lt;br&gt;   - 最近的方法探讨了语义和全景场景预测，但忽略了场景的几何信息。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;新任务提出&lt;/h4&gt;：&lt;br&gt;   - 本文提出了“全景深度预测”任务，旨在从单目相机图像中联合预测未观察到的未来帧的全景分割和深度图。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;数据集扩展&lt;/h4&gt;：&lt;br&gt;   - 扩展了流行的KITTI-360和Cityscapes基准，利用LiDAR点云计算深度图，并利用顺序标记数据。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;评估指标&lt;/h4&gt;：&lt;br&gt;   - 引入了一种适合的评估指标，能够以一致的方式量化全景质量和深度估计的准确性。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;基线与新架构&lt;/h4&gt;：&lt;br&gt;   - 提出了两个基线，并提出了新颖的PDcast架构，通过结合基于变换器的编码器、预测模块和任务特定解码器来学习丰富的时空表示，以预测未来的全景深度输出。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;评估结果&lt;/h4&gt;：&lt;br&gt;   - 大量评估展示了PDcast在两个数据集和三个预测任务中的有效性，持续解决主要挑战。&lt;br&gt;&lt;br&gt;8. &lt;h4&gt;代码开放获取&lt;/h4&gt;：&lt;br&gt;   - 代码已公开发布，网址为[PDcast GitHub](https://pdcast.cs.uni-freiburg.de)。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Forecasting the semantics and 3D structure of scenes is essential for robots
to navigate and plan actions safely. Recent methods have explored semantic and
panoptic scene forecasting; however, they do not consider the geometry of the
scene. In this work, we propose the panoptic-depth forecasting task for jointly
predicting the panoptic segmentation and depth maps of unobserved future
frames, from monocular camera images. To facilitate this work, we extend the
popular KITTI-360 and Cityscapes benchmarks by computing depth maps from LiDAR
point clouds and leveraging sequential labeled data. We also introduce a
suitable evaluation metric that quantifies both the panoptic quality and depth
estimation accuracy of forecasts in a coherent manner. Furthermore, we present
two baselines and propose the novel PDcast architecture that learns rich
spatio-temporal representations by incorporating a transformer-based encoder, a
forecasting module, and task-specific decoders to predict future panoptic-depth
outputs. Extensive evaluations demonstrate the effectiveness of PDcast across
two datasets and three forecasting tasks, consistently addressing the primary
challenges. We make the code publicly available at
https://pdcast.cs.uni-freiburg.de.</description>
      <guid isPermaLink="false">2409.12008v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>All-in-one foundational models learning across quantum chemical levels</title>
      <link>http://arxiv.org/abs/2409.12015v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Yuxinxin Chen', 'Pavlo O. Dral']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-18&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 传统的机器学习（ML）势能通常针对单一的量子化学（QC）水平，而多保真度学习的ML模型未能提供可扩展的基础模型解决方案。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;新模型架构&lt;/h4&gt;：&lt;br&gt;   - 引入了“全合一”（AIO）ANI模型架构，基于多模态学习，能够学习任意数量的QC水平。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;学习方法&lt;/h4&gt;：&lt;br&gt;   - AIO学习方法提供了一种比迁移学习更通用且易于使用的替代方案。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;模型训练&lt;/h4&gt;：&lt;br&gt;   - 训练了AIO-ANI-UIP基础模型，其泛化能力与半经验GFN2-xTB和使用双ζ基组的密度泛函理论（DFT）相当，特别适用于有机分子。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;跨QC水平学习&lt;/h4&gt;：&lt;br&gt;   - AIO-ANI模型能够跨越不同的QC水平进行学习，包括从半经验方法到DFT以及耦合簇方法。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;新模型设计&lt;/h4&gt;：&lt;br&gt;   - 基于Δ学习设计了新的基础模型Δ-AIO-ANI，相较于AIO-ANI-UIP具有更高的准确性和稳健性。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;代码和模型可用性&lt;/h4&gt;：&lt;br&gt;   - 代码及基础模型可在[GitHub](https://github.com/dralgroup/aio-ani)上获取，并将整合到通用可更新的AI增强量子化学（UAIQM）库中。&lt;br&gt;&lt;br&gt;8. &lt;h4&gt;在线使用&lt;/h4&gt;：&lt;br&gt;   - 这些模型将集成到MLatom包中，用户可通过XACS云计算平台在线使用，具体更新可查看[MLatom GitHub](https://github.com/dralgroup/mlatom)。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; https://github.com/dralgroup/aio-ani&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine learning (ML) potentials typically target a single quantum chemical
(QC) level while the ML models developed for multi-fidelity learning have not
been shown to provide scalable solutions for foundational models. Here we
introduce the all-in-one (AIO) ANI model architecture based on multimodal
learning which can learn an arbitrary number of QC levels. Our all-in-one
learning approach offers a more general and easier-to-use alternative to
transfer learning. We use it to train the AIO-ANI-UIP foundational model with
the generalization capability comparable to semi-empirical GFN2-xTB and DFT
with a double-zeta basis set for organic molecules. We show that the AIO-ANI
model can learn across different QC levels ranging from semi-empirical to
density functional theory to coupled cluster. We also use AIO models to design
the foundational model {\Delta}-AIO-ANI based on {\Delta}-learning with
increased accuracy and robustness compared to AIO-ANI-UIP. The code and the
foundational models are available at https://github.com/dralgroup/aio-ani; they
will be integrated into the universal and updatable AI-enhanced QM (UAIQM)
library and made available in the MLatom package so that they can be used
online at the XACS cloud computing platform (see
https://github.com/dralgroup/mlatom for updates).</description>
      <guid isPermaLink="false">2409.12015v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>Unsupervised Non-Rigid Point Cloud Matching through Large Vision Models</title>
      <link>http://arxiv.org/abs/2408.08568v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Zhangquan Chen', 'Puhua Jiang', 'Ruqi Huang']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-08-16&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; 12 pages, 4 figures&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究主题&lt;/h4&gt;：&lt;br&gt;   - 提出了一个新颖的基于学习的框架，用于非刚性点云匹配。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;训练方式&lt;/h4&gt;：&lt;br&gt;   - 该框架能够仅通过点云训练，无需任何对应注释，并且可以自然扩展到部分到完整的匹配。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;核心见解&lt;/h4&gt;：&lt;br&gt;   - 关键在于将来自大型视觉模型（LVMs）的语义特征与基于几何的形状特征学习结合。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;结构信息利用&lt;/h4&gt;：&lt;br&gt;   - 框架有效利用语义特征中的结构信息，以解决局部几何体之间自相似性带来的模糊性问题。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;泛化能力与鲁棒性&lt;/h4&gt;：&lt;br&gt;   - 该框架在处理LVM部分观察时展现出强大的泛化能力和鲁棒性，从而改善点云匹配任务的效果。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;模块设计&lt;/h4&gt;：&lt;br&gt;   - 提出了像素到点特征聚合模块、局部和全局注意力网络，以及几何相似性损失函数，以实现上述目标。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;实验结果&lt;/h4&gt;：&lt;br&gt;   - 实验结果表明，该方法在近等距和异构形状集的非刚性点云匹配任务中达到了最先进的结果，同时在更真实的部分和噪声数据中也表现良好。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we propose a novel learning-based framework for non-rigid
point cloud matching, which can be trained purely on point clouds without any
correspondence annotation but also be extended naturally to partial-to-full
matching. Our key insight is to incorporate semantic features derived from
large vision models (LVMs) to geometry-based shape feature learning. Our
framework effectively leverages the structural information contained in the
semantic features to address ambiguities arise from self-similarities among
local geometries. Furthermore, our framework also enjoys the strong
generalizability and robustness regarding partial observations of LVMs, leading
to improvements in the regarding point cloud matching tasks. In order to
achieve the above, we propose a pixel-to-point feature aggregation module, a
local and global attention network as well as a geometrical similarity loss
function. Experimental results show that our method achieves state-of-the-art
results in matching non-rigid point clouds in both near-isometric and
heterogeneous shape collection as well as more realistic partial and noisy
data.</description>
      <guid isPermaLink="false">2408.08568v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>Pre-Training and Personalized Fine-Tuning via Over-the-Air Federated Meta-Learning: Convergence-Generalization Trade-Offs</title>
      <link>http://arxiv.org/abs/2406.11569v3</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Haifeng Wen', 'Hong Xing', 'Osvaldo Simeone']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-06-17&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; 39 pages, 7 figures, submitted for possible journal publication&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 现代人工智能（AI）应用（如大型语言模型）正在采用预训练和微调的训练范式。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;数据开放性&lt;/h4&gt;：&lt;br&gt;   - 由于开放数据存储库减少，以及对AI模型获取的民主化努力，预训练预计将从集中式部署逐渐迁移到联邦学习（FL）实现。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;元学习框架&lt;/h4&gt;：&lt;br&gt;   - 元学习提供了一个通用框架，可以形式化预训练和微调的过程。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;个性化联邦学习&lt;/h4&gt;：&lt;br&gt;   - 基于元学习的个性化FL（meta-pFL）超越了基本个性化，旨在针对新代理和任务的泛化。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;无线设置研究&lt;/h4&gt;：&lt;br&gt;   - 本文研究了在无线环境中，参与预训练阶段的代理通过共享无线信道连接到服务器的元学习的泛化性能。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;空中计算&lt;/h4&gt;：&lt;br&gt;   - 采用空中计算方法，研究了对新代理和任务的泛化与收敛之间的权衡。&lt;br&gt;&lt;br&gt;7. &lt;h4&gt;权衡分析&lt;/h4&gt;：&lt;br&gt;   - 这种权衡源于信道损伤可能增强泛化能力，但却会降低收敛性。&lt;br&gt;&lt;br&gt;8. &lt;h4&gt;理论验证&lt;/h4&gt;：&lt;br&gt;   - 大量数值结果验证了理论分析的正确性。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; For modern artificial intelligence (AI) applications such as large language
models (LLMs), the training paradigm has recently shifted to pre-training
followed by fine-tuning. Furthermore, owing to dwindling open repositories of
data and thanks to efforts to democratize access to AI models, pre-training is
expected to increasingly migrate from the current centralized deployments to
federated learning (FL) implementations. Meta-learning provides a general
framework in which pre-training and fine-tuning can be formalized.
Meta-learning-based personalized FL (meta-pFL) moves beyond basic
personalization by targeting generalization to new agents and tasks. This paper
studies the generalization performance of meta-pFL for a wireless setting in
which the agents participating in the pre-training phase, i.e., meta-learning,
are connected via a shared wireless channel to the server. Adopting
over-the-air computing, we study the trade-off between generalization to new
agents and tasks, on the one hand, and convergence, on the other hand. The
trade-off arises from the fact that channel impairments may enhance
generalization, while degrading convergence. Extensive numerical results
validate the theory.</description>
      <guid isPermaLink="false">2406.11569v3</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>Panacea+: Panoramic and Controllable Video Generation for Autonomous Driving</title>
      <link>http://arxiv.org/abs/2408.07605v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Yuqing Wen', 'Yucheng Zhao', 'Yingfei Liu', 'Binyuan Huang', 'Fan Jia', 'Yanhui Wang', 'Chi Zhang', 'Tiancai Wang', 'Xiaoyan Sun', 'Xiangyu Zhang']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-08-14&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; Project page: https://panacea-ad.github.io/. arXiv admin note: text
  overlap with arXiv:2311.16813&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 自动驾驶领域对高质量标注视频训练数据的需求日益增加。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;框架介绍&lt;/h4&gt;：&lt;br&gt;   - 提出了Panacea+，这是一个强大且适用范围广泛的视频数据生成框架，专注于驾驶场景。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;技术基础&lt;/h4&gt;：&lt;br&gt;   - Panacea+基于之前的研究成果Panacea，采用了多视角外观噪声先验机制和超分辨率模块，以增强一致性和提高分辨率。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;实验结果&lt;/h4&gt;：&lt;br&gt;   - 通过广泛的实验，证明Panacea+生成的视频样本在多个任务中具有显著效果，包括3D物体跟踪、3D物体检测和车道检测。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;数据集应用&lt;/h4&gt;：&lt;br&gt;   - 实验使用了nuScenes和Argoverse 2数据集，显示出框架的广泛适用性。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;研究意义&lt;/h4&gt;：&lt;br&gt;   - 结果强有力地证明了Panacea+作为自动驾驶数据生成框架的价值。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The field of autonomous driving increasingly demands high-quality annotated
video training data. In this paper, we propose Panacea+, a powerful and
universally applicable framework for generating video data in driving scenes.
Built upon the foundation of our previous work, Panacea, Panacea+ adopts a
multi-view appearance noise prior mechanism and a super-resolution module for
enhanced consistency and increased resolution. Extensive experiments show that
the generated video samples from Panacea+ greatly benefit a wide range of tasks
on different datasets, including 3D object tracking, 3D object detection, and
lane detection tasks on the nuScenes and Argoverse 2 dataset. These results
strongly prove Panacea+ to be a valuable data generation framework for
autonomous driving.</description>
      <guid isPermaLink="false">2408.07605v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:40 +0000</pubDate>
    </item>
    <item>
      <title>Using Large Language Models to Generate Clinical Trial Tables and Figures</title>
      <link>http://arxiv.org/abs/2409.12046v1</link>
      <description>&lt;h3&gt;作者:&lt;/h3&gt; ['Yumeng Yang', 'Peter Krusche', 'Kristyn Pantoja', 'Cheng Shi', 'Ethan Ludmir', 'Kirk Roberts', 'Gen Zhu']&lt;br&gt;&lt;br&gt;&lt;h3&gt;发布时间:&lt;/h3&gt; 2024-09-18&lt;br&gt;&lt;br&gt;&lt;h3&gt;评论:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; 以下是对论文摘要的分点解读：&lt;br&gt;&lt;br&gt;1. &lt;h4&gt;研究背景&lt;/h4&gt;：&lt;br&gt;   - 表格、图形和清单（TFLs）是总结临床试验数据的重要工具。&lt;br&gt;&lt;br&gt;2. &lt;h4&gt;工作挑战&lt;/h4&gt;：&lt;br&gt;   - 创建TFL以进行报告通常是一项耗时的任务，常见于临床试验的执行过程中。&lt;br&gt;&lt;br&gt;3. &lt;h4&gt;研究目的&lt;/h4&gt;：&lt;br&gt;   - 本研究探索使用大型语言模型（LLMs）通过提示工程和少量样本迁移学习来自动生成TFL。&lt;br&gt;&lt;br&gt;4. &lt;h4&gt;数据来源&lt;/h4&gt;：&lt;br&gt;   - 使用公共临床试验数据，采用ADaM格式进行研究。&lt;br&gt;&lt;br&gt;5. &lt;h4&gt;研究结果&lt;/h4&gt;：&lt;br&gt;   - 结果表明，LLMs能够高效生成TFL，只需提供提示指令，展示了其在该领域的潜力。&lt;br&gt;&lt;br&gt;6. &lt;h4&gt;工具开发&lt;/h4&gt;：&lt;br&gt;   - 开发了一个名为“临床试验TFL生成代理”的对话式应用，能够将用户查询与预定义提示匹配，从而生成特定的自定义程序以生成预定义的TFL。&lt;br&gt;&lt;br&gt;&lt;h3&gt;DOI:&lt;/h3&gt; None&lt;br&gt;&lt;br&gt;&lt;h3&gt;repo:&lt;/h3&gt; null&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tables, figures, and listings (TFLs) are essential tools for summarizing
clinical trial data. Creation of TFLs for reporting activities is often a
time-consuming task encountered routinely during the execution of clinical
trials. This study explored the use of large language models (LLMs) to automate
the generation of TFLs through prompt engineering and few-shot transfer
learning. Using public clinical trial data in ADaM format, our results
demonstrated that LLMs can efficiently generate TFLs with prompt instructions,
showcasing their potential in this domain. Furthermore, we developed a
conservational agent named Clinical Trial TFL Generation Agent: An app that
matches user queries to predefined prompts that produce customized programs to
generate specific predefined TFLs.</description>
      <guid isPermaLink="false">2409.12046v1</guid>
      <pubDate>Thu, 19 Sep 2024 07:06:36 +0000</pubDate>
    </item>
  </channel>
</rss>
